<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Laurent U Perrinet">

  
  
  
    
  
  <meta name="description" content="In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations.">

  
  <link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/publication/perrinet-12-pred/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140381649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-140381649-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://laurentperrinet.github.io/publication/perrinet-12-pred/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@laurentperrinet">
  <meta property="twitter:creator" content="@laurentperrinet">
  
  <meta property="og:site_name" content="Novel visual computations">
  <meta property="og:url" content="https://laurentperrinet.github.io/publication/perrinet-12-pred/">
  <meta property="og:title" content="Motion-based prediction is sufficient to solve the aperture problem | Novel visual computations">
  <meta property="og:description" content="In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations."><meta property="og:image" content="https://laurentperrinet.github.io/publication/perrinet-12-pred/featured.png">
  <meta property="twitter:image" content="https://laurentperrinet.github.io/publication/perrinet-12-pred/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-17T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-06-15T11:40:45&#43;02:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laurentperrinet.github.io/publication/perrinet-12-pred/"
  },
  "headline": "Motion-based prediction is sufficient to solve the aperture problem",
  
  "image": [
    "https://laurentperrinet.github.io/publication/perrinet-12-pred/featured.png"
  ],
  
  "datePublished": "2019-09-17T00:00:00Z",
  "dateModified": "2020-06-15T11:40:45+02:00",
  
  "author": {
    "@type": "Person",
    "name": "Laurent U Perrinet"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Novel visual computations",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laurentperrinet.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations."
}
</script>

  

  


  


  





  <title>Motion-based prediction is sufficient to solve the aperture problem | Novel visual computations</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Events</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#grants"><span>Grants</span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <div class="pub">

  




















  
  


<div class="article-container pt-3">
  <h1>Motion-based prediction is sufficient to solve the aperture problem</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span >Laurent U Perrinet</span>, <span >Guillaume S Masson</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2012-01-01
  </span>
  

  

  

  
  
  

  
  

</div>

  








  




  



<div class="btn-links mb-3">
  
  








  



<a class="btn btn-outline-primary my-1 mr-1" href="/publication/perrinet-12-pred/perrinet-12-pred.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/perrinet-12-pred/cite.bib">
  Cite
</button>















</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 661px; max-height: 301px;">
  <div style="position: relative">
    <img src="/publication/perrinet-12-pred/featured.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            
            
            <a href="/publication/#2">
              Journal article
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><em>Neural Computation</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"><p><img src="perrinet-12-pred.png" alt="header">





  
  











<figure id="figure-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-receptive-field-of-a-neuron-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-introducing-predictive-coding-resolves-the-aperture-problem">


  <a data-fancybox="" href="/publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_2000x2000_fit_lanczos.gif" data-caption="The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: it’s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.">


  <img data-src="/publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_2000x2000_fit_lanczos.gif" class="lazyload" alt="" width="80%" height="300">
</a>


  
  
  <figcaption>
    The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: it’s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.
  </figcaption>


</figure>






  
  











<figure id="figure-figure-1-a-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-dotted-circle-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-due-to-the-limited-size-of-receptive-fields-in-sensory-cortical-areas-such-as-shown-by-the-dotted-white-circle-such-problem-is-faced-by-local-populations-of-neurons-that-visually-estimate-the-motion-of-objects-a-inset-on-a-polar-representation-of-possible-velocity-vectors-the-cross-in-the-center-corresponds-to-the-null-velocity-the-outer-circle-corresponding-to-twice-the-amplitude-of-physical-speed-we-plot-the-empirical-histogram-of-detected-velocity-vectors-this-representation-gives-a-quantification-of-the-aperture-problem-in-the-velocity-domain-at-the-onset-of-motion-detection-information-is-concentrated-along-an-elongated-constraint-line-whitehigh-probability-blackzero-probability-b-we-use-the-prior-knowledge-that-in-natural-scenes-motion-as-defined-by-its-position-and-velocity-is-following-smooth-trajectories-quantitatively-it-means-that-velocity-is-approximately-conserved-and-that-position-is-transported-according-to-the-known-velocity-we-show-here-such-a-transition-on-position-and-velocity-respectively-x_t-and-v_t-from-time-t-to-t--dt-with-the-perturbation-modeling-the-smoothness-of-prediction-in-position-and-velocity-respectively-n_x-and-n_v-c-applying-such-a-prior-on-a-dynamical-system-detecting-motion-we-show-that-motion-converges-to-the-physical-motion-after-approximately-one-spatial-period-the-line-moved-by-twice-its-height-c-inset-the-read-out-of-the-system-converged-to-the-physical-motion-motion-based-prediction-is-sufficient-to-resolve-the-aperture-problem-d-as-observed-at-the-perceptual-level-castet-et-al-1993-pei-et-al-2010-size-and-duration-of-the-tracking-angle-bias-decreased-with-respect-to-the-height-of-the-line-height-was-measured-relative-to-a-spatial-period-respectively-60-40-and-20-here-we-show-the-average-tracking-angle-red-out-from-the-probabilistic-representation-as-a-function-of-time-averaged-over-20-trials-error-bars-show-one-standard-deviation">


  <a data-fancybox="" href="/publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_2000x2000_fit_q90_lanczos.jpg" data-caption="Figure 1: &lt;em&gt;(A)&lt;/em&gt; The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: it’s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. &lt;em&gt;(A-inset)&lt;/em&gt; On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). &lt;em&gt;(B)&lt;/em&gt; We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t &#43; dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). &lt;em&gt;(C)&lt;/em&gt; Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). &lt;em&gt;(C-Inset)&lt;/em&gt; The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. &lt;em&gt;(D)&lt;/em&gt; As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).">


  <img data-src="/publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_2000x2000_fit_q90_lanczos.jpg" class="lazyload" alt="" width="80%" height="717">
</a>


  
  
  <figcaption>
    Figure 1: <em>(A)</em> The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: it’s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. <em>(A-inset)</em> On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). <em>(B)</em> We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t + dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). <em>(C)</em> Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). <em>(C-Inset)</em> The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. <em>(D)</em> As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).
  </figcaption>


</figure>






  
  











<figure id="figure-figure-2-architecture-of-the-model-the-model-is-constituted-by-a-classical-measurement-stage-and-of-a-predictive-coding-layer-the-measurement-stage-consists-of-a-inferring-from-two-consecutive-frames-of-the-input-flow-b-a-likelihood-distribution-of-motion-this-layer-interacts-with-the-predictive-layer-which-consists-of-c-a-prediction-stage-that-infers-from-the-current-estimate-and-the-transition-prior-the-upcoming-state-estimate-and-d-an-estimation-stage-that-merges-the-current-prediction-of-motion-with-the-likelihood-measured-at-the-same-instant-in-the-previous-layer-b">


  <a data-fancybox="" href="/publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_2000x2000_fit_q90_lanczos.jpg" data-caption="Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).">


  <img data-src="/publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_2000x2000_fit_q90_lanczos.jpg" class="lazyload" alt="" width="80%" height="695">
</a>


  
  
  <figcaption>
    Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).
  </figcaption>


</figure>






  
  











<figure id="figure-figure-3-to-explore-the-state-space-of-the-dynamical-system-we-simulated-motion-based-prediction-for-a-simple-small-dot-size-25-of-a-spatial-period-moving-horizontally-from-the-left-to-the-right-of-the-screen-we-tested-different-levels-of-sensory-noise-with-respect-to-different-levels-of-internal-noise-that-is-to-different-values-of-the-strength-of-prediction-right-results-show-the-emergence-of-different-states-for-different-prediction-precisions-a-regime-when-prediction-is-weak-and-which-shows-high-tracking-error-and-variability-no-tracking---nt-a-phase-for-intermediate-values-of-prediction-strength-as-in-figure-1-exhibiting-a-low-tracking-error-and-low-variability-in-the-tracking-phase-true-tracking---tt-and-finally-a-phase-corresponding-to-higher-precisions-with-relatively-efficient-mean-detection-but-high-variability-false-tracking---ft-we-give-3-representative-examples-of-the-emerging-states-at-one-contrast-level-c--01-with-starting-red-and-ending-blue-points-and-respectively-nt-tt-and-ft-by-showing-inferred-trajectories-for-each-trial-left-we-define-tracking-error-as-the-ratio-between-detected-speed-and-target-speed-and-we-plot-it-with-respect-to-the-stimulus-contrast-as-given-by-the-inverse-of-sensory-noise-error-bars-give-the-variability-in-tracking-error-as-averaged-over-20-trials-as-prediction-strength-increases-there-is-a-transition-from-smooth-contrast-response-function-nt-to-more-binary-responses-tt-and-ft">


  <a data-fancybox="" href="/publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_2000x2000_fit_q90_lanczos.jpg" data-caption="Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. &lt;em&gt;(Right)&lt;/em&gt; Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. &lt;em&gt;(Left)&lt;/em&gt; We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).">


  <img data-src="/publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_2000x2000_fit_q90_lanczos.jpg" class="lazyload" alt="" width="80%" height="483">
</a>


  
  
  <figcaption>
    Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. <em>(Right)</em> Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. <em>(Left)</em> We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).
  </figcaption>


</figure>






  
  











<figure id="figure-figure-4-top-prediction-implements-a-competition-between-different-trajectories-here-we-focus-on-one-step-of-the-algorithm-by-testing-different-trajectories-at-three-key-positions-of-the-segment-stimulus-the-two-edges-and-the-center-dashed-circles-compared-to-the-pure-sensory-velocity-likelihood-left-insets-in-grayscale-prediction-modulates-response-as-shown-by-the-velocity-vectors-direction-coded-as-hue-as-in-figure-1-and-by-the-ratio-of-velocity-probabilities-log-ratio-in-bits-right-insets-there-is-no-change-for-the-middle-of-the-segment-yellow-tone-but-trajectories-that-are-predicted-out-of-the-line-are-explained-away-navy-tone-while-others-may-be-amplified-orange-tone-notice-the-asymmetry-between-both-edges-the-upper-edge-carrying-a-suppressive-predictive-information-while-the-bottom-edge-diffuses-coherent-motion-bottom-finally-the-aperture-problem-is-solved-due-to-the-repeated-application-of-this-spatio-temporal-contextual-information-modulation-to-highlight-the-anisotropic-diffusion-of-information-over-the-rest-of-the-line-we-plot-as-a-function-of-time-horizontal-axis-the-histogram-of-the-detected-motion-marginalized-over-horizontal-positions-vertical-axis-while-detected-direction-of-velocity-is-given-by-the-distribution-of-hues-blueish-colors-correspond-to-the-direction-perpendicular-to-the-diagonal-while-a-green-color-represents-a-disambiguated-motion-to-the-right-as-in-figure-1-the-plot-shows-that-motion-is-disambiguated-by-progressively-explaining-away-incoherent-motion-note-the-asymmetry-in-the-propagation-of-coherent-information">


  <a data-fancybox="" href="/publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_2000x2000_fit_q90_lanczos.jpg" data-caption="Figure 4: &lt;em&gt;(Top)&lt;/em&gt; Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are “explained away” (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. &lt;em&gt;(Bottom)&lt;/em&gt; Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.">


  <img data-src="/publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_2000x2000_fit_q90_lanczos.jpg" class="lazyload" alt="" width="80%" height="968">
</a>


  
  
  <figcaption>
    Figure 4: <em>(Top)</em> Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are “explained away” (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. <em>(Bottom)</em> Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.
  </figcaption>


</figure>
</p>
</div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/bayesian-model/">Bayesian model</a>
  
  <a class="badge badge-light" href="/tag/motion-prediction/">motion prediction</a>
  
  <a class="badge badge-light" href="/tag/predictive-coding/">predictive coding</a>
  
  <a class="badge badge-light" href="/tag/aperture-problem/">aperture problem</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://laurentperrinet.github.io/publication/perrinet-12-pred/&amp;text=Motion-based%20prediction%20is%20sufficient%20to%20solve%20the%20aperture%20problem" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://laurentperrinet.github.io/publication/perrinet-12-pred/&amp;t=Motion-based%20prediction%20is%20sufficient%20to%20solve%20the%20aperture%20problem" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Motion-based%20prediction%20is%20sufficient%20to%20solve%20the%20aperture%20problem&amp;body=https://laurentperrinet.github.io/publication/perrinet-12-pred/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://laurentperrinet.github.io/publication/perrinet-12-pred/&amp;title=Motion-based%20prediction%20is%20sufficient%20to%20solve%20the%20aperture%20problem" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Motion-based%20prediction%20is%20sufficient%20to%20solve%20the%20aperture%20problem%20https://laurentperrinet.github.io/publication/perrinet-12-pred/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://laurentperrinet.github.io/publication/perrinet-12-pred/&amp;title=Motion-based%20prediction%20is%20sufficient%20to%20solve%20the%20aperture%20problem" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/laurent-u-perrinet/avatar_hu81bac28c1b36f818c6c9e904daaf0b0f_15796_270x270_fill_q90_lanczos_center.jpeg" alt="Laurent U Perrinet">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://laurentperrinet.github.io/">Laurent U Perrinet</a></h5>
        <h6 class="card-subtitle">Researcher in Computational Neuroscience</h6>
        <p class="card-text">My research interests include Machine Learning and computational neuroscience applied to Vision.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=TVyUV38AAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.instagram.com/laurentperrinet/" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  
    
    





  


  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/masson-12-areadne/">Motion-based prediction is sufficient to solve the aperture problem</a></li>
      
      <li><a href="/publication/kaplan-13/">Anisotropic connectivity implements motion-based prediction in a spiking neural network</a></li>
      
      <li><a href="/talk/2012-01-12-vision-at-ucl/">Motion-based prediction is sufficient to solve the aperture problem</a></li>
      
      <li><a href="/publication/montagnini-06-neurocomp/">Bayesian modeling of dynamic motion integration</a></li>
      
      <li><a href="/publication/perrinet-10-areadne/">Dynamical emergence of a neural solution for motion integration</a></li>
      
    </ul>
  </div>
  





  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&rsquo;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License</a>
Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared.
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
