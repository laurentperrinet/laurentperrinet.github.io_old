<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Laurent U Perrinet">

  
  
  
    
  
  <meta name="description" content="Making a judgment about the semantic category of a visual scene, such as whether it contains an animal, is typically assumed to involve high-level associative brain areas. Previous explanations require progressively analyzing the scene hierarchically at increasing levels of abstraction, from edge extraction to mid-level object recognition and then object categorization. Here we show that the statistics of edge co-occurrences alone are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. We first extracted the edges from images using a scale-space analysis coupled with a sparse coding algorithm. We then computed the ``association field&#39;&#39; for different categories (natural, man-made, or containing an animal) by computing the statistics of edge co-occurrences. These differed strongly, with animal images having more curved configurations. We show that this geometry alone is sufficient for categorization, and that the pattern of errors made by humans is consistent with this procedure. Because these statistics could be measured as early as the primary visual cortex, the results challenge widely held assumptions about the flow of computations in the visual system. The results also suggest new algorithms for image classification and signal processing that exploit correlations between low-level structure and the underlying semantic category.">

  
  <link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/publication/perrinet-bednar-15/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140381649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-140381649-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://laurentperrinet.github.io/publication/perrinet-bednar-15/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@laurentperrinet">
  <meta property="twitter:creator" content="@laurentperrinet">
  
  <meta property="og:site_name" content="Novel visual computations">
  <meta property="og:url" content="https://laurentperrinet.github.io/publication/perrinet-bednar-15/">
  <meta property="og:title" content="Edge co-occurrences can account for rapid categorization of natural versus animal images | Novel visual computations">
  <meta property="og:description" content="Making a judgment about the semantic category of a visual scene, such as whether it contains an animal, is typically assumed to involve high-level associative brain areas. Previous explanations require progressively analyzing the scene hierarchically at increasing levels of abstraction, from edge extraction to mid-level object recognition and then object categorization. Here we show that the statistics of edge co-occurrences alone are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. We first extracted the edges from images using a scale-space analysis coupled with a sparse coding algorithm. We then computed the ``association field&#39;&#39; for different categories (natural, man-made, or containing an animal) by computing the statistics of edge co-occurrences. These differed strongly, with animal images having more curved configurations. We show that this geometry alone is sufficient for categorization, and that the pattern of errors made by humans is consistent with this procedure. Because these statistics could be measured as early as the primary visual cortex, the results challenge widely held assumptions about the flow of computations in the visual system. The results also suggest new algorithms for image classification and signal processing that exploit correlations between low-level structure and the underlying semantic category."><meta property="og:image" content="https://laurentperrinet.github.io/publication/perrinet-bednar-15/featured.jpg">
  <meta property="twitter:image" content="https://laurentperrinet.github.io/publication/perrinet-bednar-15/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-17T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-06-15T11:44:58&#43;02:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laurentperrinet.github.io/publication/perrinet-bednar-15/"
  },
  "headline": "Edge co-occurrences can account for rapid categorization of natural versus animal images",
  
  "image": [
    "https://laurentperrinet.github.io/publication/perrinet-bednar-15/featured.jpg"
  ],
  
  "datePublished": "2019-09-17T00:00:00Z",
  "dateModified": "2020-06-15T11:44:58+02:00",
  
  "author": {
    "@type": "Person",
    "name": "Laurent U Perrinet"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Novel visual computations",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laurentperrinet.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Making a judgment about the semantic category of a visual scene, such as whether it contains an animal, is typically assumed to involve high-level associative brain areas. Previous explanations require progressively analyzing the scene hierarchically at increasing levels of abstraction, from edge extraction to mid-level object recognition and then object categorization. Here we show that the statistics of edge co-occurrences alone are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. We first extracted the edges from images using a scale-space analysis coupled with a sparse coding algorithm. We then computed the ``association field'' for different categories (natural, man-made, or containing an animal) by computing the statistics of edge co-occurrences. These differed strongly, with animal images having more curved configurations. We show that this geometry alone is sufficient for categorization, and that the pattern of errors made by humans is consistent with this procedure. Because these statistics could be measured as early as the primary visual cortex, the results challenge widely held assumptions about the flow of computations in the visual system. The results also suggest new algorithms for image classification and signal processing that exploit correlations between low-level structure and the underlying semantic category."
}
</script>

  

  


  


  





  <title>Edge co-occurrences can account for rapid categorization of natural versus animal images | Novel visual computations</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Events</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#grants"><span>Grants</span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <div class="pub">

  




















  
  


<div class="article-container pt-3">
  <h1>Edge co-occurrences can account for rapid categorization of natural versus animal images</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span >Laurent U Perrinet</span>, <span >James A Bednar</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2015-01-01
  </span>
  

  

  

  
  
  

  
  

</div>

  











  



<div class="btn-links mb-3">
  
  





  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://hal-amu.archives-ouvertes.fr/hal-01202447" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary my-1 mr-1" href="http://www.nature.com/articles/srep11400" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/perrinet-bednar-15/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/laurentperrinet/PerrinetBednar15" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.1038/srep11400" target="_blank" rel="noopener">
  DOI
</a>



</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 600px; max-height: 265px;">
  <div style="position: relative">
    <img src="/publication/perrinet-bednar-15/featured.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Making a judgment about the semantic category of a visual scene, such as whether it contains an animal, is typically assumed to involve high-level associative brain areas. Previous explanations require progressively analyzing the scene hierarchically at increasing levels of abstraction, from edge extraction to mid-level object recognition and then object categorization. Here we show that the statistics of edge co-occurrences alone are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. We first extracted the edges from images using a scale-space analysis coupled with a sparse coding algorithm. We then computed the ``association field&rsquo;&rsquo; for different categories (natural, man-made, or containing an animal) by computing the statistics of edge co-occurrences. These differed strongly, with animal images having more curved configurations. We show that this geometry alone is sufficient for categorization, and that the pattern of errors made by humans is consistent with this procedure. Because these statistics could be measured as early as the primary visual cortex, the results challenge widely held assumptions about the flow of computations in the visual system. The results also suggest new algorithms for image classification and signal processing that exploit correlations between low-level structure and the underlying semantic category.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            
            
            <a href="/publication/#2">
              Journal article
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><em>Scientific Reports</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"><ul>
<li>
<a href="http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html" target="_blank" rel="noopener">Press release</a></li>
<li>
<a href="http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html" target="_blank" rel="noopener">communiqué de presse</a></li>
<li>
<a href="http://www.nature.com/article-assets/npg/srep/2015/150622/srep11400/extref/srep11400-s1.pdf" target="_blank" rel="noopener">supplementary information</a></li>
<li>
<a href="PerrinetBednar15supplementary.pdf">supplementary material</a></li>
</ul>
<h1 id="a-study-of-how-people-can-quickly-spot-animals-by-sight-is-helping-uncover-the-workings-of-the-human-brain">A study of how people can quickly spot animals by sight is helping uncover the workings of the human brain.</h1>
<p>Scientists examined why volunteers who were shown hundreds of pictures - some with animals and some without - were able to detect animals in as little as one-tenth of a second.
They found that one of the first parts of the brain to process visual information - the primary visual cortex - can control this fast response.
More complex parts of the brain are not required at this stage, contrary to what was previously thought.
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">New info published on how the human brain processes visual information from <a href="https://twitter.com/EdinburghUni?ref_src=twsrc%5Etfw">@EdinburghUni</a> and <a href="https://twitter.com/uniamu?ref_src=twsrc%5Etfw">@uniamu</a> stuidy <a href="http://t.co/KUicugL8P7">http://t.co/KUicugL8P7</a></p>&mdash; EdinUniNeuro (@EdinUniNeuro) <a href="https://twitter.com/EdinUniNeuro/status/613011086829162497?ref_src=twsrc%5Etfw">June 22, 2015</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>






  
  











<figure id="figure-edge-co-occurrences-a-an-example-image-with-the-list-of-extracted-edges-overlaid-each-edge-is-represented-by-a-red-line-segment-which-represents-its-position-center-of-segment-orientation-and-scale-length-of-segment-we-controlled-the-quality-of-the-reconstruction-from-the-edge-information-such-that-the-residual-energy-was-less-than-5-b-the-relationship-between-a-reference-edge-a-and-another-edge-b-can-be-quantified-in-terms-of-the-difference-between-their-orientations-theta-ratio-of-scale-sigma-distance-d-between-their-centers-and-difference-of-azimuth-angular-location-phi-additionally-we-define-psiphi---theta2-which-is-symmetric-with-respect-to-the-choice-of-the-reference-edge-in-particular-psi0-for-co-circular-edges--see-text-as-incitetgeisler01-edges-outside-a-central-circular-mask-are-discarded-in-the-computation-of-the-statistics-to-avoid-artifacts-image-credit-andrew-shiva-creative-commons-attribution-share-alike-30-unported-licensehttpscommonswikimediaorgwikifileelephant_28loxodonta_africana29_05jpg-this-is-used-to-compute-the-chevron-map-in-figure2">


  <a data-fancybox="" href="/publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_2000x2000_fit_q90_lanczos.jpg" data-caption="Edge co-occurrences &lt;strong&gt;(A)&lt;/strong&gt; An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. &lt;strong&gt;(B)&lt;/strong&gt; The relationship between a reference edge &lt;em&gt;A&lt;/em&gt; and another edge &lt;em&gt;B&lt;/em&gt; can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Elephant_/%28Loxodonta_Africana/%29_05.jpg&#34;&gt;Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license&lt;/a&gt;). This is used to compute the chevron map in Figure~2.">


  <img data-src="/publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_2000x2000_fit_q90_lanczos.jpg" class="lazyload" alt="" width="310" height="393">
</a>


  
  
  <figcaption data-pre="Figure " data-post=":" class="numbered">
    Edge co-occurrences <strong>(A)</strong> An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. <strong>(B)</strong> The relationship between a reference edge <em>A</em> and another edge <em>B</em> can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: <a href="https://commons.wikimedia.org/wiki/File:Elephant_/%28Loxodonta_Africana/%29_05.jpg">Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license</a>). This is used to compute the chevron map in Figure~2.
  </figcaption>


</figure>

<blockquote class="twitter-tweet"><p lang="ja" dir="ltr">動物か否かの見分け方。<a href="http://t.co/TTY8MwZGoO">http://t.co/TTY8MwZGoO</a>　引用されてるけど、Thorpe (1996)の150msで区別されてるって話(なつかしい)と関係ありそう。</p>&mdash; Makito Oku (@okumakito) <a href="https://twitter.com/okumakito/status/613128456637841408?ref_src=twsrc%5Etfw">June 22, 2015</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>






  
  











<figure id="figure-the-probability-distribution-function-ppsi-theta-represents-the-distribution-of-the-different-geometrical-arrangements-of-edges-angles-which-we-call-a-chevron-map-we-show-here-the-histogram-for-non-animal-natural-images-illustrating-the-preference-for-co-linear-edge-configurations-for-each-chevron-configuration-deeper-and-deeper-red-circles-indicate-configurations-that-are-more-and-more-likely-with-respect-to-a-uniform-prior-with-an-average-maximum-of-about-3-times-more-likely-and-deeper-and-deeper-blue-circles-indicate-configurations-less-likely-than-a-flat-prior-with-a-minimum-of-about-08-times-as-likely-conveniently-this-chevron-map-shows-in-one-graph-that-non-animal-natural-images-have-on-average-a-preference-for-co-linear-and-parallel-edges-the-horizontal-middle-axis-and-orthogonal-angles-the-top-and-bottom-rowsalong-with-a-slight-preference-for-co-circular-configurations-for-psi0-and-psipm-frac-pi-2-just-above-and-below-the-central-row-we-compare-chevron-maps-in-different-image-categories-in-figure3">


  <a data-fancybox="" href="/publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_2000x2000_fit_lanczos_2.png" data-caption="The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&amp;rsquo; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.">


  <img data-src="/publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="550" height="495">
</a>


  
  
  <figcaption data-pre="Figure " data-post=":" class="numbered">
    The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&rsquo; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.
  </figcaption>


</figure>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Edge co-occurrences can account for rapid categorization of natural versus animal images<a href="http://t.co/NY9HapBx2S">http://t.co/NY9HapBx2S</a> <a href="http://t.co/rKQ8I5i6Ty">pic.twitter.com/rKQ8I5i6Ty</a></p>&mdash; Francis Villatoro (@emulenews) <a href="https://twitter.com/emulenews/status/612988348400070656?ref_src=twsrc%5Etfw">June 22, 2015</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>






  
  











<figure id="figure-as-for-figure-2-we-show-the-probability-of-edge-configurations-as-chevron-maps-for-two-databases-man-made-animal-here-we-show-the-ratio-of-histogram-counts-relative-to-that-of-the-non-animal-natural-image-dataset-deeper-and-deeper-red-circles-indicate-configurations-that-are-more-and-more-likely-and-blue-respectively-less-likely-with-respect-to-the-histogram-computed-for-non-animal-images-in-the-left-plot-the-animal-images-exhibit-relatively-more-circular-continuations-and-converging-angles-red-chevrons-in-the-central-vertical-axis-relative-to-non-animal-natural-images-at-the-expense-of-co-linear-parallel-and-orthogonal-configurations-blue-circles-along-the-middle-horizontal-axis-the-man-made-images-have-strikingly-more-co-linear-features-central-circle-which-reflects-the-prevalence-of-long-straight-lines-in-the-cage-images-in-that-dataset-we-use-this-representation-to-categorize-images-from-these-different-categories-in-figure4">


  <a data-fancybox="" href="/publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_2000x2000_fit_lanczos_2.png" data-caption="As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.">


  <img data-src="/publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="1100" height="679">
</a>


  
  
  <figcaption data-pre="Figure " data-post=":" class="numbered">
    As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.
  </figcaption>


</figure>






  
  











<figure id="figure-classification-results-to-quantify-the-difference-in-low-level-feature-statistics-across-categories-see-figure3-we-used-a-standard-support-vector-machine-svm-classifier-to-measure-how-each-representation-affected-the-classifiers-reliability-for-identifying-the-image-category-for-each-individual-image-we-constructed-a-vector-of-features-as-either-fo-the-histogram-of-first-order-statistics-as-the-histogram-of-edges-orientations-cm-the-chevron-map-subset-of-the-second-order-statistics-ie-the-two-dimensional-histogram-of-relative-orientation-and-azimuth-see-figure-2--or-so-the-full-four-dimensional-histogram-of-second-order-statistics-ie-all-parameters-of-the-edge-co-occurrences-we-gathered-these-vectors-for-each-different-class-of-images-and-report-here-the-results-of-the-svm-classifier-using-an-f1-score-50-represents-chance-level-while-it-was-expected-that-differences-would-be-clear-between-non-animal-natural-images-versus-laboratory-man-made-images-results-are-still-quite-high-for-classifying-animal-images-versus-non-animal-natural-images-and-are-in-the-range-reported-bycitetserre07-f1-score-of-80-for-human-observers-and-82-for-their-model-even-using-the-cm-features-alone-we-further-extend-this-results-to-the-psychophysical-results-of-serre-et-al-2007-in-figure-5">


  <a data-fancybox="" href="/publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_2000x2000_fit_lanczos_2.png" data-caption="Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&amp;rsquo;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&amp;rsquo; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80% for human observers and 82% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.">


  <img data-src="/publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="476" height="294">
</a>


  
  
  <figcaption data-pre="Figure " data-post=":" class="numbered">
    Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&rsquo;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&rsquo; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80% for human observers and 82% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.
  </figcaption>


</figure>






  
  











<figure id="figure-to-see-whether-the-patterns-of-errors-made-by-humans-are-consistent-with-our-model-we-studied-the-second-order-statistics-of-the-50-non-animal-images-that-human-subjects-in-serre-et-al-2007-most-commonly-falsely-reported-as-having-an-animal-we-call-this-set-of-images-the-false-alarm-image-dataset-left-this-chevron-map-plot-shows-the-ratio-between-the-second-order-statistics-of-the-false-alarm-images-and-the-full-non-animal-natural-image-dataset-computed-as-in-figure-3-left-just-as-for-the-images-that-actually-do-contain-animals-figure3-left-the-images-falsely-reported-as-having-animals-have-more-co-circular-and-converging-red-chevrons-and-fewer-collinear-and-orthogonal-configurations-blue-chevrons-right-to-quantify-this-similarity-we-computed-the-kullback-leibler-distance-between-the-histogram-of-each-of-these-images-from-the-false-alarm-image-dataset-and-the-average-histogram-of-each-class-the-difference-between-these-two-distances-gives-a-quantitative-measure-of-how-close-each-image-is-to-the-average-histograms-for-each-class-consistent-with-the-idea-that-humans-are-using-edge-co-occurences-to-do-rapid-image-categorization-the-50-non-animal-images-that-were-worst-classified-are-biased-toward-the-animal-histogram-d--104-while-the-550-best-classified-non-animal-images-are-closer-to-the-non-animal-histogram-">


  <a data-fancybox="" href="/publication/perrinet-bednar-15/figure_FA_humans_hu9158f735a2d21afb5c10a78c8717ec4e_575661_2000x2000_fit_lanczos_2.png" data-caption="To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&amp;rsquo; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram.">


  <img data-src="/publication/perrinet-bednar-15/figure_FA_humans_hu9158f735a2d21afb5c10a78c8717ec4e_575661_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="2887" height="1784">
</a>


  
  
  <figcaption data-pre="Figure " data-post=":" class="numbered">
    To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&rsquo; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram.
  </figcaption>


</figure>
</p>
</div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/sparse-coding/">sparse coding</a>
  
  <a class="badge badge-light" href="/tag/biologically-inspired-computer-vision/">Biologically Inspired Computer Vision</a>
  
  <a class="badge badge-light" href="/tag/association-field/">association field</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://laurentperrinet.github.io/publication/perrinet-bednar-15/&amp;text=Edge%20co-occurrences%20can%20account%20for%20rapid%20categorization%20of%20natural%20versus%20animal%20images" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://laurentperrinet.github.io/publication/perrinet-bednar-15/&amp;t=Edge%20co-occurrences%20can%20account%20for%20rapid%20categorization%20of%20natural%20versus%20animal%20images" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Edge%20co-occurrences%20can%20account%20for%20rapid%20categorization%20of%20natural%20versus%20animal%20images&amp;body=https://laurentperrinet.github.io/publication/perrinet-bednar-15/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://laurentperrinet.github.io/publication/perrinet-bednar-15/&amp;title=Edge%20co-occurrences%20can%20account%20for%20rapid%20categorization%20of%20natural%20versus%20animal%20images" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Edge%20co-occurrences%20can%20account%20for%20rapid%20categorization%20of%20natural%20versus%20animal%20images%20https://laurentperrinet.github.io/publication/perrinet-bednar-15/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://laurentperrinet.github.io/publication/perrinet-bednar-15/&amp;title=Edge%20co-occurrences%20can%20account%20for%20rapid%20categorization%20of%20natural%20versus%20animal%20images" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/laurent-u-perrinet/avatar_hu81bac28c1b36f818c6c9e904daaf0b0f_15796_270x270_fill_q90_lanczos_center.jpeg" alt="Laurent U Perrinet">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://laurentperrinet.github.io/">Laurent U Perrinet</a></h5>
        <h6 class="card-subtitle">Researcher in Computational Neuroscience</h6>
        <p class="card-text">My research interests include Machine Learning and computational neuroscience applied to Vision.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=TVyUV38AAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.instagram.com/laurentperrinet/" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  
    
    





  
    
    
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/james-a-bednar/avatar_huc34f96cf930cec3e58ab3359b634bce6_7490_270x270_fill_q90_lanczos_center.jpg" alt="James A Bednar">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/author/james-a-bednar/">James A Bednar</a></h5>
        
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/jbednar" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/7217561_James_A_Bednar" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/james-bednar-7602911b" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=h0e2kMQAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/perrinet-bednar-14-vss/">Edge co-occurrences are sufficient to categorize natural versus animal images</a></li>
      
      <li><a href="/publication/perrinet-15-eusipco/">Sparse Coding Of Natural Images Using A Prior On Edge Co-Occurences</a></li>
      
      <li><a href="/publication/perrinet-08-spie/">Adaptive Sparse Spike Coding : applications of Neuroscience to the compression of natural images</a></li>
      
      <li><a href="/publication/perrinet-03-ieee/">Coding static natural images using spiking event times: do neurons cooperate?</a></li>
      
      <li><a href="/publication/nava-13/">Advances in Texture Analysis for Emphysema Classification</a></li>
      
    </ul>
  </div>
  





  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&rsquo;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License</a>
Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared.
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
