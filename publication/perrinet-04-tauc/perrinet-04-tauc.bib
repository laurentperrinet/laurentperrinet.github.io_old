@article{Perrinet04tauc,
 abstract = {A goal of low-level neural processes is to build an efficient code extracting the relevant information from the sensory input. It is believed that this is implemented in cortical areas by elementary inferential computations dynamically extracting the most likely parameters corresponding to the sensory signal. We explore here a neuro-mimetic feed-forward model of the primary visual area (V1) solving this problem in the case where the signal may be described by a robust linear gen- erative model. This model uses an over-complete dictionary of primitives which provides a distributed probabilistic representation of input features. Relying on an efficiency criterion, we derive an algorithm as an approximate solution which uses incremental greedy inference processes. This algorithm is similar to 'Matching Pursuit' and mimics the parallel architecture of neural computations. We propose here a simple implementation using a network of spiking integrate-and-fire neu- rons which communicate using lateral interactions. Numerical simulations show that this Sparse Spike Coding strategy provides an efficient model for representing visual data from a set of natural images. Even though it is simplistic, this trans- formation of spatial data into a spatio-temporal pattern of binary events provides an accurate description of some complex neural patterns observed in the spiking activity of biological neural networks.},
 author = {Perrinet, Laurent U},
 bdsk-url-1 = {http://dx.doi.org/10.1016/j.jphysparis.2005.09.012},
 date = {2004-07},
 date-modified = {2019-02-26 12:31:12 +0100},
 doi = {10.1016/j.jphysparis.2005.09.012},
 journal = {Journal of Physiology-Paris},
 keywords = {Bayesian model,coding decoding,matching pursuit,sparse coding,sparse hebbian learning,spike},
 number = {4-6},
 pages = {530--9},
 preprint = {https://arxiv.org/abs/q-bio/0611003},
 title = {Feature detection using spikes : the greedy approach.},
 url = {http://dx.doi.org/10.1016/j.jphysparis.2005.09.012},
 volume = {98},
 year = {2004}
}

