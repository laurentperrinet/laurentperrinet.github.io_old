@inproceedings{Jeremie22AREADNE,
 abstract = {Visual search, that is, the simultaneous localization and detection of a visual target of interest, is a vital task. Applied to the case of natural scenes, searching for example to an animal (either a prey, a predator or a partner) constitutes a challenging problem due to large variability over numerous visual dimensions such as shape, pose, size, texture or position. Yet, biological visual systems are able to perform such detection efficiently
in  briefly flashed scenes and in a very short amount of time.Deep convolutional neuronal networks (CNNs) were shown to be well fitted to the image classification task, providing with human (or even super-human) performance. Previous models also managed to solve the visual search task, by roughly dividing the image into sub-areas. This is at the cost, however, of computer-intensive parallel processing on relatively low-resolution image samples. Taking inspiration from natural vision systems, we develop here a model that builds over the anatomical visual processing pathways observed in mammals, namely the What and the Where pathways. It operates in two steps, one by selecting regions of interest, before knowing their actual visual content, through an ultra-fast/low resolution analysis of the full visual field, and the second providing a detailed categorization over the detailed foveal selected region attained with a saccade.},
 author = {Jérémie, Jean-Nicolas and Daucé, Emmanuel and Perrinet, Laurent U},
 bdsk-url-1 = {https://areadne.org/},
 booktitle = {Proceedings of AREADNE},
 date = {2022-06-29},
 grants = {aprovis3D},
 keywords = {efficient coding,event-based vision,homeostasis,neuromorphic hardware,online classification},
 location = {Santorini (Greece)},
 time_end = {2022-06-29T15:30:00},
 time_start = {2022-06-29T09:00:00},
 title = {Ultra-rapid visual search in natural images using active deep learning},
 url = {https://areadne.org/},
 year = {2022}
}

