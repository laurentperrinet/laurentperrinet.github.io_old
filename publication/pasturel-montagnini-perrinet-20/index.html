<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Laurent U Perrinet">

  
  
  
    
  
  <meta name="description" content="Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high*resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye*to*target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate \&#34;how much they are confident that the target will move to the right or left in the next trial\&#34;  and to adjust the cursor&#39;s position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random*direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch*up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile).">

  
  <link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140381649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-140381649-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@laurentperrinet">
  <meta property="twitter:creator" content="@laurentperrinet">
  
  <meta property="og:site_name" content="Novel visual computations">
  <meta property="og:url" content="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/">
  <meta property="og:title" content="Humans adapt their anticipatory eye movements to the volatility of visual motion properties | Novel visual computations">
  <meta property="og:description" content="Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high*resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye*to*target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate \&#34;how much they are confident that the target will move to the right or left in the next trial\&#34;  and to adjust the cursor&#39;s position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random*direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch*up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile)."><meta property="og:image" content="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/featured.png">
  <meta property="twitter:image" content="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-17T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-09-04T08:50:58&#43;02:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/"
  },
  "headline": "Humans adapt their anticipatory eye movements to the volatility of visual motion properties",
  
  "image": [
    "https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/featured.png"
  ],
  
  "datePublished": "2019-09-17T00:00:00Z",
  "dateModified": "2020-09-04T08:50:58+02:00",
  
  "author": {
    "@type": "Person",
    "name": "Chloé Pasturel"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Novel visual computations",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laurentperrinet.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high*resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye*to*target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate \\\"how much they are confident that the target will move to the right or left in the next trial\\\"  and to adjust the cursor's position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random*direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch*up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile)."
}
</script>

  

  


  


  





  <title>Humans adapt their anticipatory eye movements to the volatility of visual motion properties | Novel visual computations</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class=" ">

  
  
  
    <script>window.staDarkLightChooser = true;</script>
  
  
    <script>const isSiteThemeDark = false;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Events</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#grants"><span>Grants</span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>



  <div class="pub">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Humans adapt their anticipatory eye movements to the volatility of visual motion properties</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span >Chloé Pasturel</span>, <span >Anna Montagnini</span>, <span >Laurent U Perrinet</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    2020-01-26
  </span>
  

  

  

  
  
  

  
  

</div>

  











  



<div class="btn-links mb-3">
  
  





  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://www.biorxiv.org/content/10.1101/784116v3" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.1371/journal.pcbi.1007438" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/pasturel-montagnini-perrinet-20/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.1371/journal.pcbi.1007438" target="_blank" rel="noopener">
  DOI
</a>



</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 746px;">
  <div style="position: relative">
    <img src="/publication/pasturel-montagnini-perrinet-20/featured_huf01a0a99995bf40c2e40e412d13550ad_125526_720x0_resize_lanczos_2.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high<em>resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye</em>to<em>target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp</em>motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate &quot;how much they are confident that the target will move to the right or left in the next trial&quot;  and to adjust the cursor&rsquo;s position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random*direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch*up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile).</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            
            
            <a href="/publication/#2">
              Journal article
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><em>PLoS Computational Biology</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"><h1 id="humans-adapt-their-anticipatory-eye-movements-to-the-volatility-of-visual-motion-properties">&ldquo;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&rdquo;</h1>














  


<video controls >
  <source src="https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4" type="video/mp4">
</video>
<h2 id="at-what-point-should-we-become-alarmed-when-faced-with-changes-in-the-environment-the-sensory-system-provides-an-effective-response">At what point should we become alarmed? When faced with changes in the environment, the sensory system provides an effective response.</h2>
<p>The current health situation has shown us how abruptly our environment can change from one state to another, tragically illustrating the volatility we can face. To understand this notion of volatility, let&rsquo;s take the case of a doctor who, among the patients he receives, usually diagnoses one out of ten cases of flu. Suddenly, he gets 5 out of 10 patients who test positive. Is this an unfortunate coincidence or are we now sure that there is a switch to a flu episode? Recent events have shown us how difficult it is to make a rational decision in times of uncertainty, and in particular to decide <em>when</em> to act. However, mathematical solutions exist that adapt our behavior by optimally combining the information explored recently with that exploited in the past. In an article published in PLoS Computational Biology, Pasturel, Montagnini and Perrinet show that our brain responds to changes in the sensory environment in the same way as this mathematical model.</p>






  



  
  











<figure id="figure-by-manipulating-the-probability-bias-of-the-presentation-of-a-visual-target-on-a-screen-this-experiment-manipulates-the-volatility-of-the-environment-in-a-controlled-way-by-introducing-switches-in-the-probability-bias-these-switches-randomly-change-the-bias-among-different-degrees-of-probability-both-left-and-right-at-each-trial-the-bias-then-generates-a-realization-either-left-l-or-right-r--the-target-moves-in-blocks-of-50-trials-1-to-50-and-these-realizations-are-the-only-ones-to-be-observed-the-evolution-of-the-bias-and-its-shifts-remaining-hidden-from-the-observer-compared-to-the-floating-average-that-is-conventionally-used-a-mathematical-model-can-be-deduced-as-a-predictive-average-that-allows-to-better-follow-the-dynamics-of-the-probability-bias-thanks-to-psychophysical-experiments-we-have-shown-that-observers-preferentially-follow-the-predictive-mean-rather-than-the-floating-mean-both-in-explicit-judgements-predictive-betting-and-more-surprisingly-in-the-anticipatory-movements-of-the-eyes-that-are-carried-out-without-the-observers-being-aware-of-them">


  <a data-fancybox="" href="/publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_2000x2000_fit_lanczos_2.png" data-caption="By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them.">


  <img data-src="/publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="80%" height="1790">
</a>


  
  
  <figcaption>
    By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them.
  </figcaption>


</figure>

<p>These theoretical and experimental results show that in this realistic situation in which the context changes at random moments throughout the experiment, our sensory system adapts to volatility in an adaptive manner over the course of the trials. In particular, the experiments show in two behavioural experiments that humans adapt to volatility at the early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive level, through explicit evaluations. These results thus suggest that humans (and future artificial systems) can use much richer adaptation strategies than previously assumed. They provide a better understanding of how humans adapt to changing environments in order to make judgements or plan responses based on information that varies over time.</p>
<ul>
<li>read the 
<a href="https://doi.org/10.1371/journal.pcbi.1007438" target="_blank" rel="noopener">publication</a> (or in 
<a href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007438&amp;type=printable" target="_blank" rel="noopener">PDF</a>)</li>
<li>get a 
<a href="https://www.biorxiv.org/content/10.1101/784116v3" target="_blank" rel="noopener">preprint</a></li>
<li>
<a href="https://www.biorxiv.org/content/10.1101/784116v3" target="_blank" rel="noopener">Abstract</a></li>
<li>
<a href="https://www.biorxiv.org/content/10.1101/784116v3" target="_blank" rel="noopener">HTML</a></li>
<li>
<a href="https://www.biorxiv.org/content/10.1101/784116v3.full.pdf" target="_blank" rel="noopener">PDF</a></li>
<li>supplementary info : <a href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf">https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf</a></li>
<li>code for paper: <a href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020">https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020</a></li>
<li>code for framework: <a href="https://github.com/chloepasturel/AnticipatorySPEM">https://github.com/chloepasturel/AnticipatorySPEM</a></li>
<li>code for figures 
<a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/1_protocole.ipynb" target="_blank" rel="noopener">Figure 1</a>, 
<a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2_raw-results.ipynb" target="_blank" rel="noopener">Figure 2</a>, 
<a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/3_Results_1-theory_BBCP.ipynb" target="_blank" rel="noopener">Figure 3</a>, 
<a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/4_Results_2_fitting_BBCP.ipynb" target="_blank" rel="noopener">Figure 4</a>, 
<a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/5_Meta_analysis.ipynb" target="_blank" rel="noopener">Figure 5</a></li>
<li>
<a href="https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4" target="_blank" rel="noopener">video abstract</a> (and the 
<a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2020-03_video-abstract/2020-03-24_video-abstract.ipynb" target="_blank" rel="noopener">code</a> for generating the video abstract)</li>
</ul>
</div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/motion-anticipation/">motion anticipation</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&amp;text=Humans%20adapt%20their%20anticipatory%20eye%20movements%20to%20the%20volatility%20of%20visual%20motion%20properties" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&amp;t=Humans%20adapt%20their%20anticipatory%20eye%20movements%20to%20the%20volatility%20of%20visual%20motion%20properties" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Humans%20adapt%20their%20anticipatory%20eye%20movements%20to%20the%20volatility%20of%20visual%20motion%20properties&amp;body=https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&amp;title=Humans%20adapt%20their%20anticipatory%20eye%20movements%20to%20the%20volatility%20of%20visual%20motion%20properties" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Humans%20adapt%20their%20anticipatory%20eye%20movements%20to%20the%20volatility%20of%20visual%20motion%20properties%20https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&amp;title=Humans%20adapt%20their%20anticipatory%20eye%20movements%20to%20the%20volatility%20of%20visual%20motion%20properties" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
    
    
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/chloe-pasturel/avatar_hucfafec1f19aab4f9e024b0deff0af481_1015588_270x270_fill_lanczos_center_2.png" alt="Chloé Pasturel">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/author/chloe-pasturel/">Chloé Pasturel</a></h5>
        
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/chloepasturel" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  
    
    





  
    
    
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/anna-montagnini/avatar_hub896de861e914f410b9e146cfca223b5_17258_270x270_fill_q90_lanczos_center.jpg" alt="Anna Montagnini">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/author/anna-montagnini/">Anna Montagnini</a></h5>
        
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/annamontagnini" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Anna_Montagnini" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/anna-montagnini-a292606" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  
    
    





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/laurent-u-perrinet/avatar_hu34ad5b8db880542c60288aed1996fc62_245755_270x270_fill_q90_lanczos_center.jpg" alt="Laurent U Perrinet">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://laurentperrinet.github.io/">Laurent U Perrinet</a></h5>
        <h6 class="card-subtitle">Researcher in Computational Neuroscience</h6>
        <p class="card-text">My research interests include Machine Learning and computational neuroscience applied to Vision.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=TVyUV38AAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="http://www.researcherid.com/rid/C-4900-2009" target="_blank" rel="noopener">
        <i class="ai ai-researcherid"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.zotero.org/groups/2485979/laurent_perrinet/library" target="_blank" rel="noopener">
        <i class="ai ai-zotero"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://publons.com/a/1206845/" target="_blank" rel="noopener">
        <i class="ai ai-publons"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="http://orcid.org/0000-0002-9536-010X" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://arxiv.org/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Perrinet%2C&#43;L&amp;terms-0-field=author&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first" target="_blank" rel="noopener">
        <i class="ai ai-arxiv"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://publons.com/a/1206845/" target="_blank" rel="noopener">
        <i class="ai ai-publons"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.instagram.com/laurentperrinet/" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/laurent-perrinet-1857b9/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  


  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/pasturel-18-anemo/">ANEMO: Quantitative tools for the ANalysis of Eye MOvements</a></li>
      
      <li><a href="/publication/pasturel-18-grenoble/">Estimating and anticipating a dynamic probabilistic bias in visual motion direction</a></li>
      
      <li><a href="/publication/pasturel-18/">Estimating and anticipating a dynamic probabilistic bias in visual motion direction</a></li>
      
      <li><a href="/publication/perrinet-18-gdr/">A low-cost, accessible eye tracking framework</a></li>
      
      <li><a href="/publication/perrinet-17-gdr/">Expériences autour de la perception de la forme en art et science</a></li>
      
    </ul>
  </div>
  





  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js" integrity="sha512-7t8APmYpzEsZP7CYoA7RfMPV9Bb+PJHa9x2WiUnDXZx3XHveuyWUtvNOexhkierl5flZ3tr92dP1mMS+SGlD+A==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/latex.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3b2b658c61ebd725bd5fc606c89fe44c.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&rsquo;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License</a>
Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared.
  </p>

  
  





  
  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">
      <img src="https://search.creativecommons.org/static/img/cc_icon.svg" alt="CC icon">
      <img src="https://search.creativecommons.org/static/img/cc-by_icon.svg" alt="CC by icon">
      
        <img src="https://search.creativecommons.org/static/img/cc-nc_icon.svg" alt="CC NC icon">
      
      
        <img src="https://search.creativecommons.org/static/img/cc-nd_icon.svg" alt="CC ND icon">
      
    </a>
  </p>




  <p class="powered-by">
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy Website Builder</a>
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
