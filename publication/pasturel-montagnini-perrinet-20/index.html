<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: January 9, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.6" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.32e2e32cf1a4c1ea152e519f8b1fda79.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Laurent U Perrinet" />





  

<meta name="description" content="Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high*resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye*to*target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate \&#34;how much they are confident that the target will move to the right or left in the next trial\&#34; and to adjust the cursor&#39;s position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random*direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch*up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile)." />



<link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/" />
<link rel="canonical" href="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  






<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/featured.png" />



  

<meta property="og:type" content="article" />
<meta property="og:site_name" content="Novel visual computations" />
<meta property="og:url" content="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/" />
<meta property="og:title" content="Humans adapt their anticipatory eye movements to the volatility of visual motion properties | Novel visual computations" />
<meta property="og:description" content="Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high*resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye*to*target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate \&#34;how much they are confident that the target will move to the right or left in the next trial\&#34; and to adjust the cursor&#39;s position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random*direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch*up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile)." /><meta property="og:image" content="https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/featured.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2020-01-26T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2020-01-26T00:00:00&#43;00:00">
  






    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/"
  },
  "headline": "Humans adapt their anticipatory eye movements to the volatility of visual motion properties",
  
  "image": [
    "https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/featured.png"
  ],
  
  "datePublished": "2020-01-26T00:00:00Z",
  "dateModified": "2020-01-26T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Chloé Pasturel"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Novel visual computations",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high*resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye*to*target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate \\\"how much they are confident that the target will move to the right or left in the next trial\\\" and to adjust the cursor's position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random*direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch*up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile)."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Humans adapt their anticipatory eye movements to the volatility of visual motion properties | Novel visual computations</title>

  
  
  
    <link rel="me" href="https://neuromatch.social/@laurentperrinet" />

  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="b4685fdaab1feb369c0aeda4e477a0a3" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.3a6bdbdff5d8a89d6e651adb3deec035.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Featured</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Events</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#people"><span>People</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#grants"><span>Grants</span></a>
          </li>

          
          

          

          
          
          
            
              
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    


  
    
    
  


<div class="pub">

  






















  
  



<div class="article-container pt-3">
  <h1>Humans adapt their anticipatory eye movements to the volatility of visual motion properties</h1>

  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/chloe-pasturel/">Chloé Pasturel</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January, 2020
  </span>
  

  

  

  
  
  
  

  
  

</div>

  




<div class="btn-links mb-3">
  
  





  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://www.biorxiv.org/content/10.1101/784116" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header" href="https://hal.archives-ouvertes.fr/hal-02394142" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header js-cite-modal"
        data-filename="/publication/pasturel-montagnini-perrinet-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header" href="https://doi.org/10.1371/journal.pcbi.1007438" target="_blank" rel="noopener">
  DOI
</a>



</div>


</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 746px;">
  <div style="position: relative">
    <img src="/publication/pasturel-montagnini-perrinet-20/featured_huf01a0a99995bf40c2e40e412d13550ad_125526_3e363b50d27f44aa8a45537c84a1a375.webp" width="720" height="746" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high<em>resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye</em>to<em>target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp</em>motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate &quot;how much they are confident that the target will move to the right or left in the next trial&quot; and to adjust the cursor&rsquo;s position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random<em>direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch</em>up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile).</p>
    

    
    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            <a href="/publication/#2">
              2
            </a>
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><em>PLoS Computational Biology</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"><h1 id="humans-adapt-their-anticipatory-eye-movements-to-the-volatility-of-visual-motion-properties">&ldquo;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&rdquo;</h1>
<p>










  





<video controls  >
  <source src="https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4" type="video/mp4">
</video>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">check out our new paper <a href="https://twitter.com/PLOSCompBiol?ref_src=twsrc%5Etfw">@PLOSCompBiol</a> : &quot;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&quot; with Chloé Pasturel and @MontagniniAnna  - talks about how to perform optimal decisions when the environment abruptly switches its statistics... <a href="https://t.co/GKg87lGqdS">pic.twitter.com/GKg87lGqdS</a></p>&mdash; @laurentperrinet@neuromatch.social (@laurentperrinet) <a href="https://twitter.com/laurentperrinet/status/1253715266124611586?ref_src=twsrc%5Etfw">April 24, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>
<h2 id="at-what-point-should-we-become-alarmed-when-faced-with-changes-in-the-environment-the-sensory-system-provides-an-effective-response">At what point should we become alarmed? When faced with changes in the environment, the sensory system provides an effective response.</h2>
<p>The current health situation has shown us how abruptly our environment can change from one state to another, tragically illustrating the volatility we can face. To understand this notion of volatility, let&rsquo;s take the case of a doctor who, among the patients he receives, usually diagnoses one out of ten cases of flu. Suddenly, he gets 5 out of 10 patients who test positive. Is this an unfortunate coincidence or are we now sure that there is a switch to a flu episode? Recent events have shown us how difficult it is to make a rational decision in times of uncertainty, and in particular to decide <em>when</em> to act. However, mathematical solutions exist that adapt our behavior by optimally combining the information explored recently with that exploited in the past. In an article published in PLoS Computational Biology, Pasturel, Montagnini and Perrinet show that our brain responds to changes in the sensory environment in the same way as this mathematical model.


















<figure  id="figure-by-manipulating-the-probability-bias-of-the-presentation-of-a-visual-target-on-a-screen-this-experiment-manipulates-the-volatility-of-the-environment-in-a-controlled-way-by-introducing-switches-in-the-probability-bias-these-switches-randomly-change-the-bias-among-different-degrees-of-probability-both-left-and-right-at-each-trial-the-bias-then-generates-a-realization-either-left-l-or-right-r--the-target-moves-in-blocks-of-50-trials-1-to-50-and-these-realizations-are-the-only-ones-to-be-observed-the-evolution-of-the-bias-and-its-shifts-remaining-hidden-from-the-observer-compared-to-the-floating-average-that-is-conventionally-used-a-mathematical-model-can-be-deduced-as-a-predictive-average-that-allows-to-better-follow-the-dynamics-of-the-probability-bias-thanks-to-psychophysical-experiments-we-have-shown-that-observers-preferentially-follow-the-predictive-mean-rather-than-the-floating-mean-both-in-explicit-judgements-predictive-betting-and-more-surprisingly-in-the-anticipatory-movements-of-the-eyes-that-are-carried-out-without-the-observers-being-aware-of-them">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img alt=" By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them. " srcset="
               /publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_9638384ce0ace9ebb979fc4d1bed3c7f.webp 400w,
               /publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_f0dda557df66e7e39ff25b2c99922a86.webp 760w,
               /publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
               src="/publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_9638384ce0ace9ebb979fc4d1bed3c7f.webp"
               width="80%"
               height="461"
               loading="lazy" data-zoomable /></div>
  </div><figcaption>
      By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them.
    </figcaption></figure>

These theoretical and experimental results show that in this realistic situation in which the context changes at random moments throughout the experiment, our sensory system adapts to volatility in an adaptive manner over the course of the trials. In particular, the experiments show in two behavioural experiments that humans adapt to volatility at the early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive level, through explicit evaluations. These results thus suggest that humans (and future artificial systems) can use much richer adaptation strategies than previously assumed. They provide a better understanding of how humans adapt to changing environments in order to make judgements or plan responses based on information that varies over time.</p>
<ul>
<li>read the <a href="https://www.biorxiv.org/content/10.1101/784116v3" target="_blank" rel="noopener">preprint</a> (the official online <a href="https://doi.org/10.1371/journal.pcbi.1007438" target="_blank" rel="noopener">publication</a> or in <a href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007438&amp;type=printable" target="_blank" rel="noopener">PDF</a> is <em>wrongly</em> typeset: the editors inverted the images of figures 2 &amp; 3, while keeping the captions. Unfortunately, the policy of the journal is to issue a correction, but not to correct it. There is therefore no official correct version on the PLoS* website.)</li>
<li>get a )</li>
<li><a href="https://www.biorxiv.org/content/10.1101/784116v3" target="_blank" rel="noopener">Abstract</a></li>
<li><a href="https://www.biorxiv.org/content/10.1101/784116v3" target="_blank" rel="noopener">HTML</a></li>
<li><a href="https://www.biorxiv.org/content/10.1101/784116v3.full.pdf" target="_blank" rel="noopener">PDF</a></li>
<li>supplementary info : <a href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf" target="_blank" rel="noopener">https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf</a></li>
<li><a href="https://www.insb.cnrs.fr/fr/cnrsinfo/la-reponse-du-cerveau-aux-changements-de-lenvironnement-sensoriel" target="_blank" rel="noopener">Communiqué de presse INSB-CNRS (en français)</a></li>
<li>code for paper: <a href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020" target="_blank" rel="noopener">https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020</a></li>
<li>code for framework: <a href="https://github.com/chloepasturel/AnticipatorySPEM" target="_blank" rel="noopener">https://github.com/chloepasturel/AnticipatorySPEM</a></li>
<li>code for the Bayesian model: <a href="https://github.com/laurentperrinet/bayesianchangepoint" target="_blank" rel="noopener">https://github.com/laurentperrinet/bayesianchangepoint</a></li>
<li>code for figures <a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/1_protocole.ipynb" target="_blank" rel="noopener">Figure 1</a>, <a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2_raw-results.ipynb" target="_blank" rel="noopener">Figure 2</a>, <a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/3_Results_1-theory_BBCP.ipynb" target="_blank" rel="noopener">Figure 3</a>, <a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/4_Results_2_fitting_BBCP.ipynb" target="_blank" rel="noopener">Figure 4</a>, <a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/5_Meta_analysis.ipynb" target="_blank" rel="noopener">Figure 5</a></li>
<li><a href="https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4" target="_blank" rel="noopener">video abstract</a> (and the <a href="https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2020-03_video-abstract/2020-03-24_video-abstract.ipynb" target="_blank" rel="noopener">code</a> for generating the video abstract)</li>
<li>Notre papier avec Chloé Pasturel et @MontagniniAnna  figure dans les <a href="https://indd.adobe.com/view/ea980f21-e298-43e8-abd7-fff6909d6755" target="_blank" rel="noopener">faits marquants 2020 de la Société des Neurosciences</a>! Voir aussi  <a href="https://lejournal.cnrs.fr/nos-blogs/aux-frontieres-du-cerveau/les-faits-marquants-2020-de-la-societe-de-neurosciences" target="_blank" rel="noopener">https://lejournal.cnrs.fr/nos-blogs/aux-frontieres-du-cerveau/les-faits-marquants-2020-de-la-societe-de-neurosciences</a> :
<blockquote class="twitter-tweet"><p lang="fr" dir="ltr">Notre papier avec Chloé Pasturel et @MontagniniAnna  figure dans les faits marquants 2020 de la Société des Neurosciences! <a href="https://t.co/w825oTz1o5">https://t.co/w825oTz1o5</a><a href="https://twitter.com/hashtag/neuroscience?src=hash&amp;ref_src=twsrc%5Etfw">#neuroscience</a> <a href="https://twitter.com/hashtag/actu?src=hash&amp;ref_src=twsrc%5Etfw">#actu</a> <a href="https://twitter.com/hashtag/sciences?src=hash&amp;ref_src=twsrc%5Etfw">#sciences</a>  <a href="https://twitter.com/SocNeuro_Tweets?ref_src=twsrc%5Etfw">@SocNeuro_Tweets</a><a href="https://twitter.com/CNRS?ref_src=twsrc%5Etfw">@CNRS</a> <a href="https://twitter.com/univamu?ref_src=twsrc%5Etfw">@univamu</a> <a href="https://t.co/KeKh5MNWu2">https://t.co/KeKh5MNWu2</a> <a href="https://t.co/eojTqsp6gD">https://t.co/eojTqsp6gD</a></p>&mdash; @laurentperrinet@neuromatch.social (@laurentperrinet) <a href="https://twitter.com/laurentperrinet/status/1371420462056620036?ref_src=twsrc%5Etfw">March 15, 2021</a></blockquote>
 <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</li>
</ul>
</div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/motion-anticipation/">motion anticipation</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Flaurentperrinet.github.io%2Fpublication%2Fpasturel-montagnini-perrinet-20%2F&amp;text=Humans&#43;adapt&#43;their&#43;anticipatory&#43;eye&#43;movements&#43;to&#43;the&#43;volatility&#43;of&#43;visual&#43;motion&#43;properties" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Flaurentperrinet.github.io%2Fpublication%2Fpasturel-montagnini-perrinet-20%2F&amp;t=Humans&#43;adapt&#43;their&#43;anticipatory&#43;eye&#43;movements&#43;to&#43;the&#43;volatility&#43;of&#43;visual&#43;motion&#43;properties" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Humans%20adapt%20their%20anticipatory%20eye%20movements%20to%20the%20volatility%20of%20visual%20motion%20properties&amp;body=https%3A%2F%2Flaurentperrinet.github.io%2Fpublication%2Fpasturel-montagnini-perrinet-20%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Flaurentperrinet.github.io%2Fpublication%2Fpasturel-montagnini-perrinet-20%2F&amp;title=Humans&#43;adapt&#43;their&#43;anticipatory&#43;eye&#43;movements&#43;to&#43;the&#43;volatility&#43;of&#43;visual&#43;motion&#43;properties" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Humans&#43;adapt&#43;their&#43;anticipatory&#43;eye&#43;movements&#43;to&#43;the&#43;volatility&#43;of&#43;visual&#43;motion&#43;properties%20https%3A%2F%2Flaurentperrinet.github.io%2Fpublication%2Fpasturel-montagnini-perrinet-20%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Flaurentperrinet.github.io%2Fpublication%2Fpasturel-montagnini-perrinet-20%2F&amp;title=Humans&#43;adapt&#43;their&#43;anticipatory&#43;eye&#43;movements&#43;to&#43;the&#43;volatility&#43;of&#43;visual&#43;motion&#43;properties" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/chloe-pasturel/"><img class="avatar mr-3 avatar-circle" src="/author/chloe-pasturel/avatar_hucfafec1f19aab4f9e024b0deff0af481_1015588_270x270_fill_lanczos_center_3.png" alt="Chloé Pasturel"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/chloe-pasturel/">Chloé Pasturel</a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/chloepasturel" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="/author/anna-montagnini/"><img class="avatar mr-3 avatar-circle" src="/author/anna-montagnini/avatar_hub896de861e914f410b9e146cfca223b5_17258_270x270_fill_q75_lanczos_center.jpg" alt="Anna Montagnini"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="/author/anna-montagnini/">Anna Montagnini</a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/annamontagnini" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Anna_Montagnini" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/anna-montagnini-a292606" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://laurentperrinet.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/laurent-u-perrinet/avatar_hufdd75f582622d56af8f81b6f2821d19b_501026_270x270_fill_lanczos_center_3.png" alt="Laurent U Perrinet"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://laurentperrinet.github.io/">Laurent U Perrinet</a></h5>
      <h6 class="card-subtitle">Researcher in Computational Neuroscience</h6>
      <p class="card-text">My research interests include Machine Learning and computational neuroscience applied to Vision.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://neuromatch.social/@laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-mastodon"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0002-9536-010X" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2024 <a rel="me" href="https://neuromatch.social/@laurentperrinet">Laurent U Perrinet</a>. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.62586ca65ca61821fe707eb9fa6268b7.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js" type="module"></script>




  
  <script async defer src="https://buttons.github.io/buttons.js"></script>

















</body>
</html>
