<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2 | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/publication-type/2/</link>
      <atom:link href="https://laurentperrinet.github.io/publication-type/2/index.xml" rel="self" type="application/rss+xml" />
    <description>2</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Fri, 05 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_512x512_fill_lanczos_center_2.png</url>
      <title>2</title>
      <link>https://laurentperrinet.github.io/publication-type/2/</link>
    </image>
    
    <item>
      <title>A dual foveal-peripheral visual processing model implements efficient saccade selection</title>
      <link>https://laurentperrinet.github.io/publication/dauce-20/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dauce-20/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New &lt;a href=&#34;https://twitter.com/ARVOJOV?ref_src=twsrc%5Etfw&#34;&gt;@ARVOJOV&lt;/a&gt; : &amp;quot;A dual foveal-peripheral visual processing model implements efficient saccade selection&amp;quot; &lt;a href=&#34;https://t.co/JqnpBM5bcd&#34;&gt;https://t.co/JqnpBM5bcd&lt;/a&gt; comes with code @ &lt;a href=&#34;https://t.co/5MoIh00Bb8&#34;&gt;https://t.co/5MoIh00Bb8&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/OpenAccess?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#OpenAccess&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/visionscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#visionscience&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1305488088412688385?ref_src=twsrc%5Etfw&#34;&gt;September 14, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My latest work with &lt;a href=&#34;https://twitter.com/Angelo_RDN?ref_src=twsrc%5Etfw&#34;&gt;@Angelo_RDN&lt;/a&gt;, Frederic Chavane, Franck Ruffier and &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; has been released in PLOS CB (&lt;a href=&#34;https://t.co/0uvFeiSuOR&#34;&gt;https://t.co/0uvFeiSuOR&lt;/a&gt;). Our model combines Sparse Coding and Predictive Coding and introduce a novel way to visualize neural representation : the interaction map &lt;a href=&#34;https://t.co/AORwdFAMw3&#34;&gt;pic.twitter.com/AORwdFAMw3&lt;/a&gt;&lt;/p&gt;&amp;mdash; Victor Boutin (@VictorBoutin) &lt;a href=&#34;https://twitter.com/VictorBoutin/status/1355810283835564033?ref_src=twsrc%5Etfw&#34;&gt;January 31, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  id=&#34;figure-fig-1-architecture-of-a-2-layered-sdpc-model&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;download=&amp;amp;id=10.1371/journal.pcbi.1008629.g001&#34; alt=&#34;Fig 1. Architecture of a 2-layered SDPC model.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 1. Architecture of a 2-layered SDPC model.
    &lt;/figcaption&gt;&lt;/figure&gt;
One often compares biological vision to a camera-like system where an image would be processed according to a sequence of successive transformations. In particular, this ‚Äúfeedforward‚Äù view is prevalent in models of visual processing such as deep learning. However, neuroscientists have long stressed that more complex information flow is necessary to reach natural vision efficiency. In particular, recurrent and feedback connections in the visual cortex allow to integrate contextual information in our representation of visual stimuli. These modulations have been observed both at the low-level of neural activity and at the higher level of perception.














&lt;figure  id=&#34;figure-fig-2-results-of-training-sdpc-on-the-natural-images-left-column-and-on-the-face-database-right-column-with-a-feedback-strength-kfb--1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;download=&amp;amp;id=10.1371/journal.pcbi.1008629.g002&#34; alt=&#34;Fig 2. Results of training SDPC on the natural images (left column) and on the face database (right column) with a feedback strength kFB = 1.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 2. Results of training SDPC on the natural images (left column) and on the face database (right column) with a feedback strength kFB = 1.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-fig-14-illustration-of-the-hierarchical-generative-model-learned-by-the-sdpc-model-on-the-face-database&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;download=&amp;amp;id=10.1371/journal.pcbi.1008629.g014&#34; alt=&#34;Fig 14. Illustration of the hierarchical generative model learned by the SDPC model on the face database.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 14. Illustration of the hierarchical generative model learned by the SDPC model on the face database.
    &lt;/figcaption&gt;&lt;/figure&gt;
In this study, we present an architecture that describes biological vision at both levels of analysis. It suggests that the brain uses feedforward and feedback connections to compare the sensory stimulus with its own internal representation. In contrast to classical deep learning approaches, we show that our model learns interpretable features.














&lt;figure  id=&#34;figure-fig-5-example-of-a-9--9-interaction-map-of-a-v1-area-centered-on-neurons-strongly-responding-to-a-central-preferred-orientation-of-30&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;download=&amp;amp;id=10.1371/journal.pcbi.1008629.g005&#34; alt=&#34;Fig 5. Example of a 9 √ó 9 interaction map of a V1 area centered on neurons strongly responding to a central preferred orientation of 30¬∞.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 5. Example of a 9 √ó 9 interaction map of a V1 area centered on neurons strongly responding to a central preferred orientation of 30¬∞.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-fig-7-example-of-a-9--9-interaction-map-of-a-v1-area-centered-on-neurons-strongly-responding-to-a-central-preferred-orientation-of-45-and-colored-with-the-relative-response-wrt-no-feedback&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;download=&amp;amp;id=10.1371/journal.pcbi.1008629.g007&#34; alt=&#34;Fig 7. Example of a 9 √ó 9 interaction map of a V1 area centered on neurons strongly responding to a central preferred orientation of 45¬∞, and colored with the relative response w.r.t. no feedback.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 7. Example of a 9 √ó 9 interaction map of a V1 area centered on neurons strongly responding to a central preferred orientation of 45¬∞, and colored with the relative response w.r.t. no feedback.
    &lt;/figcaption&gt;&lt;/figure&gt;
Moreover, we demonstrate that feedback signals modulate neural activity to promote good continuity of contours. Finally, the same model can disambiguate images corrupted by noise. To the best of our knowledge, this is the first time that the same model describes the effect of recurrent and feedback modulations at both neural and representational levels.














&lt;figure  id=&#34;figure-fig-10-effect-of-the-feedback-strength-on-noisy-images-from-natural-images-database&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;download=&amp;amp;id=10.1371/journal.pcbi.1008629.g010&#34; alt=&#34;Fig 10. Effect of the feedback strength on noisy images from natural images database.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 10. Effect of the feedback strength on noisy images from natural images database.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;more about the role of top-down connections: 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/franck-ruffier/&#34;&gt;Franck Ruffier&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/&#34;&gt;Effect of top-down connections in Hierarchical Sparse Coding&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.00892&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/boutin-franciosini-ruffier-perrinet-20-feedback.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/boutin-franciosini-ruffier-perrinet-20-feedback/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1162/neco_a_01325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;li&gt;presented during this &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/&#34;&gt;talk&lt;/a&gt;: 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2019).
  &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/&#34;&gt;From the retina to action: Predictive processing in the visual system&lt;/a&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/2019-03-25_HDR_RobinBaures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/talk/2019-03-25-hdr-robin-baures/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/2019-03-25_HDR_RobinBaures/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;








  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/2019-03-25_HDR_RobinBaures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Slides
&lt;/a&gt;






&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Anticipatory Responses along Motion Trajectories in Awake Monkey Area V1</title>
      <link>https://laurentperrinet.github.io/publication/benvenuti-20/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/benvenuti-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effect of top-down connections in Hierarchical Sparse Coding</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/</link>
      <pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;What is the effect of top-down connections in Hierarchical Sparse Coding? This work by &lt;a href=&#34;https://twitter.com/VictorBoutin?ref_src=twsrc%5Etfw&#34;&gt;@VictorBoutin&lt;/a&gt; + Angelo Franciosini @RaguDellaNonna Franck Ruffier and myself proposes to leverage this problem using predictive coding. Read more in Neural Computation:  &lt;a href=&#34;https://t.co/g0Sig7uDMq&#34;&gt;https://t.co/g0Sig7uDMq&lt;/a&gt; &lt;a href=&#34;https://t.co/fiJxlJ7tNG&#34;&gt;pic.twitter.com/fiJxlJ7tNG&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1323549136088694790?ref_src=twsrc%5Etfw&#34;&gt;November 3, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;get the code @ &lt;a href=&#34;https://github.com/VictorBoutin/SPC_2L&#34;&gt;https://github.com/VictorBoutin/SPC_2L&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Humans adapt their anticipatory eye movements to the volatility of visual motion properties</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/</guid>
      <description>&lt;h1 id=&#34;humans-adapt-their-anticipatory-eye-movements-to-the-volatility-of-visual-motion-properties&#34;&gt;&amp;ldquo;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&amp;rdquo;&lt;/h1&gt;














  


&lt;video controls &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;check out our new paper &lt;a href=&#34;https://twitter.com/PLOSCompBiol?ref_src=twsrc%5Etfw&#34;&gt;@PLOSCompBiol&lt;/a&gt; : &amp;quot;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&amp;quot; with Chlo√© Pasturel and &lt;a href=&#34;https://twitter.com/MontagniniAnna?ref_src=twsrc%5Etfw&#34;&gt;@MontagniniAnna&lt;/a&gt;  - talks about how to perform optimal decisions when the environment abruptly switches its statistics... &lt;a href=&#34;https://t.co/GKg87lGqdS&#34;&gt;pic.twitter.com/GKg87lGqdS&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1253715266124611586?ref_src=twsrc%5Etfw&#34;&gt;April 24, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;at-what-point-should-we-become-alarmed-when-faced-with-changes-in-the-environment-the-sensory-system-provides-an-effective-response&#34;&gt;At what point should we become alarmed? When faced with changes in the environment, the sensory system provides an effective response.&lt;/h2&gt;
&lt;p&gt;The current health situation has shown us how abruptly our environment can change from one state to another, tragically illustrating the volatility we can face. To understand this notion of volatility, let&amp;rsquo;s take the case of a doctor who, among the patients he receives, usually diagnoses one out of ten cases of flu. Suddenly, he gets 5 out of 10 patients who test positive. Is this an unfortunate coincidence or are we now sure that there is a switch to a flu episode? Recent events have shown us how difficult it is to make a rational decision in times of uncertainty, and in particular to decide &lt;em&gt;when&lt;/em&gt; to act. However, mathematical solutions exist that adapt our behavior by optimally combining the information explored recently with that exploited in the past. In an article published in PLoS Computational Biology, Pasturel, Montagnini and Perrinet show that our brain responds to changes in the sensory environment in the same way as this mathematical model.














&lt;figure  id=&#34;figure-by-manipulating-the-probability-bias-of-the-presentation-of-a-visual-target-on-a-screen-this-experiment-manipulates-the-volatility-of-the-environment-in-a-controlled-way-by-introducing-switches-in-the-probability-bias-these-switches-randomly-change-the-bias-among-different-degrees-of-probability-both-left-and-right-at-each-trial-the-bias-then-generates-a-realization-either-left-l-or-right-r--the-target-moves-in-blocks-of-50-trials-1-to-50-and-these-realizations-are-the-only-ones-to-be-observed-the-evolution-of-the-bias-and-its-shifts-remaining-hidden-from-the-observer-compared-to-the-floating-average-that-is-conventionally-used-a-mathematical-model-can-be-deduced-as-a-predictive-average-that-allows-to-better-follow-the-dynamics-of-the-probability-bias-thanks-to-psychophysical-experiments-we-have-shown-that-observers-preferentially-follow-the-predictive-mean-rather-than-the-floating-mean-both-in-explicit-judgements-predictive-betting-and-more-surprisingly-in-the-anticipatory-movements-of-the-eyes-that-are-carried-out-without-the-observers-being-aware-of-them&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34; By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them. &#34; srcset=&#34;
               /publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_8a2be5f59e720fa3833f8943c6e54cb0.png 400w,
               /publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_581befb04ff1ef2500ab30577707118a.png 760w,
               /publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_8a2be5f59e720fa3833f8943c6e54cb0.png&#34;
               width=&#34;80%&#34;
               height=&#34;461&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them.
    &lt;/figcaption&gt;&lt;/figure&gt;
These theoretical and experimental results show that in this realistic situation in which the context changes at random moments throughout the experiment, our sensory system adapts to volatility in an adaptive manner over the course of the trials. In particular, the experiments show in two behavioural experiments that humans adapt to volatility at the early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive level, through explicit evaluations. These results thus suggest that humans (and future artificial systems) can use much richer adaptation strategies than previously assumed. They provide a better understanding of how humans adapt to changing environments in order to make judgements or plan responses based on information that varies over time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;read the &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preprint&lt;/a&gt; (the official online &lt;a href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publication&lt;/a&gt; or in &lt;a href=&#34;https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007438&amp;amp;type=printable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt; is &lt;em&gt;wrongly&lt;/em&gt; typeset by interverting figures 2 &amp;amp; 3 - the poilicy of the journal is to issue a correction, but not to correct it.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;get a )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abstract&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;supplementary info : &lt;a href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf&#34;&gt;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.insb.cnrs.fr/fr/cnrsinfo/la-reponse-du-cerveau-aux-changements-de-lenvironnement-sensoriel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Communiqu√© de presse INSB-CNRS (en fran√ßais)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code for paper: &lt;a href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34;&gt;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code for framework: &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM&#34;&gt;https://github.com/chloepasturel/AnticipatorySPEM&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;code for figures &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/1_protocole.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 1&lt;/a&gt;, &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2_raw-results.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 2&lt;/a&gt;, &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/3_Results_1-theory_BBCP.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 3&lt;/a&gt;, &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/4_Results_2_fitting_BBCP.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 4&lt;/a&gt;, &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/5_Meta_analysis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 5&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video abstract&lt;/a&gt; (and the &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2020-03_video-abstract/2020-03-24_video-abstract.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; for generating the video abstract)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Notre papier avec Chlo√© Pasturel et @MontagniniAnna  figure dans les &lt;a href=&#34;https://indd.adobe.com/view/ea980f21-e298-43e8-abd7-fff6909d6755&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;faits marquants 2020 de la Soci√©t√© des Neurosciences&lt;/a&gt;! Voir aussi  &lt;a href=&#34;https://lejournal.cnrs.fr/nos-blogs/aux-frontieres-du-cerveau/les-faits-marquants-2020-de-la-societe-de-neurosciences&#34;&gt;https://lejournal.cnrs.fr/nos-blogs/aux-frontieres-du-cerveau/les-faits-marquants-2020-de-la-societe-de-neurosciences&lt;/a&gt; :&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Notre papier avec Chlo√© Pasturel et &lt;a href=&#34;https://twitter.com/MontagniniAnna?ref_src=twsrc%5Etfw&#34;&gt;@MontagniniAnna&lt;/a&gt;  figure dans les faits marquants 2020 de la Soci√©t√© des Neurosciences! &lt;a href=&#34;https://t.co/w825oTz1o5&#34;&gt;https://t.co/w825oTz1o5&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/neuroscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuroscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/actu?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#actu&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/sciences?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#sciences&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/SocNeuro_Tweets?ref_src=twsrc%5Etfw&#34;&gt;@SocNeuro_Tweets&lt;/a&gt;&lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; &lt;a href=&#34;https://t.co/KeKh5MNWu2&#34;&gt;https://t.co/KeKh5MNWu2&lt;/a&gt; &lt;a href=&#34;https://t.co/eojTqsp6gD&#34;&gt;https://t.co/eojTqsp6gD&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1371420462056620036?ref_src=twsrc%5Etfw&#34;&gt;March 15, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey V1</title>
      <link>https://laurentperrinet.github.io/publication/chemla-19/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/chemla-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli</title>
      <link>https://laurentperrinet.github.io/publication/ravello-19/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ravello-19/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www4.cnrs-dir.fr/insb/recherche/parutions/articles2019/l-perrinet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;d√®s-la-r√©tine-le-syst√®me-visuel-pr√©f√®re-des-images-naturelles&#34;&gt;D√®s la r√©tine, le syst√®me visuel pr√©f√®re des images naturelles&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Dans la r√©tine, au premier √©tage du traitement de l&amp;rsquo;image visuelle, on peut obtenir des repr√©sentations extr√™mement fines. Une collaboration entre des chercheurs fran√ßais et chiliens a permis de mettre en √©vidence que, dans la r√©tine de rongeurs, une repr√©sentation de la vitesse de l&amp;rsquo;image visuelle est pr√©cis√©ment cod√©e. Dans cette collaboration pluridisciplinaire, l&amp;rsquo;utilisation d&amp;rsquo;un mod√®le du fonctionnement de la r√©tine a permis de g√©n√©rer un nouveau type de stimuli visuels qui a r√©v√©l√© des r√©sultats exp√©rimentaux surprenants.&lt;/em&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Travail collaboratif et multi-disciplinaire entre &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar et &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; librement disponible sur &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; - merci √†  l&amp;#39;&lt;a href=&#34;https://twitter.com/AgenceRecherche?ref_src=twsrc%5Etfw&#34;&gt;@AgenceRecherche&lt;/a&gt; pour l&amp;#39;aide financi√®re et √† &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; + &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; pour l&amp;#39;&lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://t.co/YixRfpCrT3&#34;&gt;https://t.co/YixRfpCrT3&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1092139540788244480?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

La r√©tine est la premi√®re √©tape du traitement visuel, aux capacit√©s √©tonnantes. √Ä la diff√©rence d&amp;rsquo;un simple capteur comme ceux qu‚Äôon trouve dans les appareils photographiques num√©riques, ce mince tissu neuronal est un syst√®me complexe et encore largement m√©connu. Une meilleure connaissance de cette structure est essentielle pour la construction de capteurs du futur efficaces et √©conomes -par exemple ceux qui √©quiperont les futures voitures autonomes- mais aussi pour mieux comprendre des pathologies comme la D√©ficience Maculaire Li√©e √† l&amp;rsquo;Age (DMLA). Une des facettes m√©connues de la r√©tine est sa capacit√© √† d√©tecter des mouvements et cet article permet de mieux comprendre une partie des m√©canismes en jeu.
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New study on speed selectivity in the &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; showing that a majority of neurons prefer natural-like stimuli. Collaborative and multi-disciplinary work with &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar and &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; available with &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; at &lt;a href=&#34;https://t.co/Vb7GoRxjoT&#34;&gt;https://t.co/Vb7GoRxjoT&lt;/a&gt;&lt;/p&gt;&amp;mdash; Adrian Palacios (@APalacio_s) &lt;a href=&#34;https://twitter.com/APalacio_s/status/1092200890377879552?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Retinal cell preference for natural-like stimuli. Very elegant work by &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; et al. &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuroscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuroscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/decoding?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#decoding&lt;/a&gt; &lt;a href=&#34;https://t.co/3xNWaZd5x6&#34;&gt;https://t.co/3xNWaZd5x6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andres Canales-Johnson (@canalesjohnson) &lt;a href=&#34;https://twitter.com/canalesjohnson/status/1092211339311923201?ref_src=twsrc%5Etfw&#34;&gt;February 4, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Conciliant mod√©lisation et neurophysiologie, cette √©tude a permis de faire des pr√©dictions sur le traitement de l&amp;rsquo;information r√©tinienne et en particulier de g√©n√©rer des textures synth√©tiques qui sont optimales pour ces mod√®les (voir film). Les enregistrements effectu√©s sur la r√©tine de rongeurs diurnes Octodon degus ont ensuite permis de mesurer la s√©lectivit√© √† la vitesse mais aussi de valider une nouvelle fois ces mod√®les en reconstruisant l&amp;rsquo;image d&amp;rsquo;entr√©e √† partir de l&amp;rsquo;activit√© neurale.
Le r√©sultat le plus inattendu est la diff√©rence de s√©lectivit√© de certaines classes de neurones r√©tiniens par rapport √† la complexit√© du stimulus pr√©sent√©. En effet, la repr√©sentation de la vitesse est relativement peu pr√©cise si on utilise des r√©seaux de lignes (&amp;ldquo;Grating&amp;rdquo;), comme cela est d&amp;rsquo;habitude r√©alis√© dans la plupart des exp√©riences neurophysiologiques. Au contraire, elle devient plus pr√©cise si on utilise comme signaux visuels des textures artificielles ressemblant √† des nuages en mouvement (&amp;ldquo;MC Narrow&amp;rdquo;). En particulier, plus cette texture est complexe, plus la repr√©sentation est pr√©cise (&amp;ldquo;MC Broad&amp;rdquo;).
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/R%C3%A9sultatScientifique?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#R√©sultatScientifique&lt;/a&gt; üîç| D√®s la &lt;a href=&#34;https://twitter.com/hashtag/r%C3%A9tine?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#r√©tine&lt;/a&gt;, le syst√®me &lt;a href=&#34;https://twitter.com/hashtag/visuel?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#visuel&lt;/a&gt; pr√©f√®re des images naturelles&lt;br&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://t.co/BBY2IpGum6&#34;&gt;https://t.co/BBY2IpGum6&lt;/a&gt;&lt;br&gt;üìï &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; | &lt;a href=&#34;https://t.co/5mULuWTp3N&#34;&gt;https://t.co/5mULuWTp3N&lt;/a&gt;&lt;br&gt;ü§ù &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/LaurentPerrinet?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#LaurentPerrinet&lt;/a&gt; &lt;a href=&#34;https://t.co/34R1URHUic&#34;&gt;pic.twitter.com/34R1URHUic&lt;/a&gt;&lt;/p&gt;&amp;mdash; Biologie au CNRS (@INSB_CNRS) &lt;a href=&#34;https://twitter.com/INSB_CNRS/status/1091392027848294401?ref_src=twsrc%5Etfw&#34;&gt;February 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli. Beautiful article in Scientific Reports on an original animal model, the diurnal rodent Octodon degus. &lt;a href=&#34;https://t.co/BdzyzEVYnX&#34;&gt;https://t.co/BdzyzEVYnX&lt;/a&gt; (open access) &lt;a href=&#34;https://t.co/1UaoMYTFd2&#34;&gt;pic.twitter.com/1UaoMYTFd2&lt;/a&gt;&lt;/p&gt;&amp;mdash; St√©phane Deny (@StephaneDeny) &lt;a href=&#34;https://twitter.com/StephaneDeny/status/1090452532223045632?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Ces textures complexes sont plus proches des images naturellement observ√©es et ces r√©sultats montrent donc que d√®s la r√©tine, le syst√®me visuel est particuli√®rement adapt√© √† des stimulations naturelles. Ce r√©sultat devrait pouvoir s&amp;rsquo;√©tendre √† des textures encore plus complexes et encore plus proches d&amp;rsquo;images naturelles, mais aussi pouvoir se g√©n√©raliser √† d&amp;rsquo;autres aires visuelles plus complexes, comme le cortex visuel primaire, et √† d&amp;rsquo;autres esp√®ces.














&lt;figure  id=&#34;figure-pour-une-cellule-repr√©sentative-on-montre-ici-la-r√©ponse-au-cours-du-temps-sous-forme-dimpulsions-pour-diff√©rentes-pr√©sentations-trial-ainsi-que-la-moyenne-de-cette-r√©ponse-firing-rate-les-diff√©rentes-colonnes-repr√©sentent-diff√©rentes-vitesses-des-stimulations-sur-la-r√©tine-les-diff√©rentes-lignes-sont-diff√©rentes-stimulations-en-bleu-une-stimulation-classique-sous-forme-de-r√©seaux-de-lignes--grating--en-vert-et-orange-la-r√©ponse-√†-une-texture-progressivement-plus-complexe-de--mc-narrow--√†--mc-broad--si-les-r√©ponses-aux-diff√©rents-stimulations-sont-en-moyenne-similaires-elles-sont-variables-dessai-en-essai-et-une-analyse-statistique-a-permis-de-montrer-que-dans-la-majorit√©-des-cellules-les-r√©ponses-sont-dautant-plus-pr√©cises-que-la-stimulation-est-complexe--cesar-ravello&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Pour une cellule repr√©sentative, on montre ici la r√©ponse au cours du temps sous forme d&amp;#39;impulsions pour diff√©rentes pr√©sentations (Trial) ainsi que la moyenne de cette r√©ponse (Firing rate). Les diff√©rentes colonnes repr√©sentent diff√©rentes vitesses des stimulations sur la r√©tine. Les diff√©rentes lignes sont diff√©rentes stimulations. En bleu, une stimulation classique sous forme de r√©seaux de lignes (¬´ Grating ¬ª). En vert et Orange, la r√©ponse √† une texture progressivement plus complexe (de ¬´ Mc Narrow ¬ª √† ¬´ MC Broad ¬ª). Si les r√©ponses aux diff√©rents stimulations sont en moyenne similaires, elles sont variables d‚Äôessai en essai et une analyse statistique a permis de montrer que dans la majorit√© des cellules, les r√©ponses sont d&amp;#39;autant plus pr√©cises que la stimulation est complexe. ¬© Cesar Ravello &#34; srcset=&#34;
               /publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_c2126001c83be1c30c0c76245abd4a89.jpg 400w,
               /publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_fc6fff431ee0ced0ca2054631cda9ff5.jpg 760w,
               /publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_c2126001c83be1c30c0c76245abd4a89.jpg&#34;
               width=&#34;540&#34;
               height=&#34;416&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pour une cellule repr√©sentative, on montre ici la r√©ponse au cours du temps sous forme d&amp;rsquo;impulsions pour diff√©rentes pr√©sentations (Trial) ainsi que la moyenne de cette r√©ponse (Firing rate). Les diff√©rentes colonnes repr√©sentent diff√©rentes vitesses des stimulations sur la r√©tine. Les diff√©rentes lignes sont diff√©rentes stimulations. En bleu, une stimulation classique sous forme de r√©seaux de lignes (¬´ Grating ¬ª). En vert et Orange, la r√©ponse √† une texture progressivement plus complexe (de ¬´ Mc Narrow ¬ª √† ¬´ MC Broad ¬ª). Si les r√©ponses aux diff√©rents stimulations sont en moyenne similaires, elles sont variables d‚Äôessai en essai et une analyse statistique a permis de montrer que dans la majorit√© des cellules, les r√©ponses sont d&amp;rsquo;autant plus pr√©cises que la stimulation est complexe. ¬© Cesar Ravello
    &lt;/figcaption&gt;&lt;/figure&gt;














  


&lt;video controls &gt;
  &lt;source src=&#34;video_perrinet.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
Cette vid√©o montre les trois classes de stimulations utilis√©es dans cette √©tude. En plus des r√©seaux sinuso√Ødaux (‚ÄúGrating‚Äù) qui sont classiquement utilis√©s en neurosciences, cette √©tude a utilis√© des textures al√©atoires (Motion Clouds (MC)) qui sont inspir√©es de mod√®les du traitement visuel. Ils permettent en particulier de manipuler des param√®tres visuels critiques comme la vari√©t√© de fr√©quences spatiales qui sont superpos√©es: soit unique (‚ÄúGrating‚Äù), fine (‚ÄúMC Narrow‚Äù), soit plus large (‚ÄúMC Broad‚Äù). Ces vid√©os ont √©t√© directement projet√©es sur des r√©tines pos√©es sur des grilles d‚Äô√©lectrodes qui permettent de mesurer l‚Äôactivit√© neurale (voir figure). ¬© Laurent Perrinet / Cesar Ravello&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An adaptive homeostatic algorithm for the unsupervised learning of visual features</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-19-hulk/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-19-hulk/</guid>
      <description>&lt;h1 id=&#34;an-adaptive-algorithm-for-unsupervised-learning&#34;&gt;&amp;ldquo;An adaptive algorithm for unsupervised learning&amp;rdquo;&lt;/h1&gt;














  


&lt;video controls &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2019-09-11_Perrinet19.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;ul&gt;
&lt;li&gt;supplementary info : &lt;a href=&#34;https://spikeai.github.io/HULK/&#34;&gt;https://spikeai.github.io/HULK/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mdpi.com/2411-5150/3/3/47&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mdpi.com/2411-5150/3/3/47/htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mdpi.com/2411-5150/3/3/47/pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for paper: &lt;a href=&#34;https://github.com/SpikeAI/HULK&#34;&gt;https://github.com/SpikeAI/HULK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for framework: &lt;a href=&#34;https://github.com/bicv/SparseHebbianLearning/&#34;&gt;https://github.com/bicv/SparseHebbianLearning/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for figures &lt;a href=&#34;https://github.com/SpikeAI/HULK/blob/master/Annex.ipynb&#34;&gt;https://github.com/SpikeAI/HULK/blob/master/Annex.ipynb&lt;/a&gt; (which is rendered @ &lt;a href=&#34;https://spikeai.github.io/HULK/&#34;&gt;https://spikeai.github.io/HULK/&lt;/a&gt; )&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2019-09-11_Perrinet19.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video abstract&lt;/a&gt; (and the &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2019-09-11_video-abstract-vision.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; for generating it)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Illusions et hallucinations visuelles : une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-19-illusions/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-19-illusions/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Ce texte est disponible dans cet article de &lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Voir la @ &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34;&gt;pr√©sentation au NeuroStories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Temps et cerveau : comment notre perception nous fait voyager dans le temps</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-19-temps/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-19-temps/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Un article dans &lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; dont l&amp;rsquo;objectif est d&amp;rsquo;√™tre accessible et r√©utilisable (dans des cours d&amp;rsquo;introduction aux neurosciences, sciences cognitives, vision, r√©seaux de neurones, intelligence artificielle).&lt;/li&gt;
&lt;li&gt;Le flash-lag effect original:














  


&lt;video autoplay loop &gt;
  &lt;source src=&#34;flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/li&gt;
&lt;li&gt;la m√™me chose avec un arr√™t:














  


&lt;video autoplay loop &gt;
  &lt;source src=&#34;flash_lag_stop.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/li&gt;
&lt;li&gt;pour illustrer la fleche du temps (&amp;quot; Or dans tout syst√®me, d‚Äôapr√®s le second principe de la thermodynamique, le d√©sordre mesur√© par l‚Äôentropie se doit d‚Äôaugmenter. Voil√† pourquoi il existe une asym√©trie dans l‚Äô√©coulement du temps, c‚Äôest-√†-dire une fl√®che du temps. R√©sultat, si l‚Äôon filme une partie de billard, on trouvera incongru cette s√©quence si on la projette dans le sens inverse du temps. &amp;ldquo;), on peut aussi utiliser cette video d&amp;rsquo;un bocal qui se brise qu&amp;rsquo;il est ais√© de lire dans le sens inverse du temps:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/v30b5IAgwQw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2019-10-07-neurostories-videos-of-my-talk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurostories: d&amp;rsquo;autres videos du flash-lag effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Laurent Perrinet a re√ßu des financements de l&amp;rsquo;Agence Nationale de la Recherche (ANR HOR-V1 ANR-17-CE37-0006) et du CNRS (SpikeAI). Cet article n‚Äôaurait pas vu le jour sans la journ√©e des &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurostories&lt;/a&gt; de la NeuroSchool d‚ÄôAix-Marseille Universit√©, ceux qui l‚Äôont fait vivre et parmi eux: Fran√ßois F√©ron, Alexia Belleville, &lt;a href=&#34;https://fr.wikipedia.org/wiki/Jean-Marc_Michelangeli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jean-Marc Michelangeli&lt;/a&gt;, Camille Grasso, Daniele Sch√∂n, Anne-Marie Fran√ßois-Bellan, Jennifer Coull, Corine Sombrun et Francis Taulelle.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures</title>
      <link>https://laurentperrinet.github.io/publication/vacher-16/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reinforcement effects in anticipatory smooth eye movements</title>
      <link>https://laurentperrinet.github.io/publication/damasse-18/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The flash-lag effect as a motion-based predictive shift</title>
      <link>https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/</link>
      <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnrs.fr/insb/recherche/parutions/articles2017/l-perrinet.html%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;visual-illusions-their-origin-lies-in-prediction&#34;&gt;Visual illusions: their origin lies in prediction&lt;/h1&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-flash-lag-effect-when-a-visual-stimulus-moves-along-a-continuous-trajectory-it-may-be-seen-ahead-of-its-veridical-position-with-respect-to-an-unpredictable-event-such-as-a-punctuate-flash-this-illusion-tells-us-something-important-about-the-visual-system-contrary-to-classical-computers-neural-activity-travels-at-a-relatively-slow-speed-it-is-largely-accepted-that-the-resulting-delays-cause-this-perceived-spatial-lag-of-the-flash-still-after-several-decades-of-debates-there-is-no-consensus-regarding-the-underlying-mechanisms&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;*Flash-Lag Effect.* When a visual stimulus moves along a continuous trajectory, it may be seen ahead of its veridical position with respect to an unpredictable event such as a punctuate flash. This illusion tells us something important about the visual system: contrary to classical computers, neural activity travels at a relatively slow speed. It is largely accepted that the resulting delays cause this perceived spatial lag of the flash. Still, after several decades of debates, there is no consensus regarding the underlying mechanisms.&#34; srcset=&#34;
               /publication/khoei-masson-perrinet-17/flash_lag_hu277740dce32a2754cb1ad98f4208cb67_31157_42eff4b408a292dfdfd1e5e6e2e6abe9.gif 400w,
               /publication/khoei-masson-perrinet-17/flash_lag_hu277740dce32a2754cb1ad98f4208cb67_31157_8150842463d8df9760684bb4ed9a5f75.gif 760w,
               /publication/khoei-masson-perrinet-17/flash_lag_hu277740dce32a2754cb1ad98f4208cb67_31157_1200x1200_fit_lanczos.gif 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/flash_lag_hu277740dce32a2754cb1ad98f4208cb67_31157_42eff4b408a292dfdfd1e5e6e2e6abe9.gif&#34;
               width=&#34;760&#34;
               height=&#34;456&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;Flash-Lag Effect.&lt;/em&gt; When a visual stimulus moves along a continuous trajectory, it may be seen ahead of its veridical position with respect to an unpredictable event such as a punctuate flash. This illusion tells us something important about the visual system: contrary to classical computers, neural activity travels at a relatively slow speed. It is largely accepted that the resulting delays cause this perceived spatial lag of the flash. Still, after several decades of debates, there is no consensus regarding the underlying mechanisms.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;strong&gt;Researchers from the Timone Institute of Neurosciences bring a new theoretical hypothesis on a visual illusion discovered at the beginning of the 20th century. This illusion remained misunderstood while it poses fundamental questions about how our brains represent events in space and time. This study published on January 26, 2017 in the journal PLOS Computational Biology, shows that the solution lies in the predictive mechanisms intrinsic to the neural processing of information.&lt;/strong&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New Research: The Flash-Lag Effect as a Motion-Based Predictive Shift &lt;a href=&#34;https://t.co/K3KWPO8l4a&#34;&gt;https://t.co/K3KWPO8l4a&lt;/a&gt; Khoei et al. &lt;a href=&#34;https://twitter.com/hashtag/vision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#vision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/motion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#motion&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuralnetworks?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuralnetworks&lt;/a&gt; &lt;a href=&#34;https://t.co/RElm4Qqo58&#34;&gt;pic.twitter.com/RElm4Qqo58&lt;/a&gt;&lt;/p&gt;&amp;mdash; PLOS Comp Biol (@PLOSCompBiol) &lt;a href=&#34;https://twitter.com/PLOSCompBiol/status/829354100273745920?ref_src=twsrc%5Etfw&#34;&gt;February 8, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Visual illusions are still popular: in a quasi-magical way, they can make objects appear where they are not expected&amp;hellip; They are also excellent opportunities to question the constraints of our perceptual system. Many illusions are based on motion, such as the flash-lag effect. Observe a luminous dot that moves along a rectilinear trajectory. If a second light dot is flashed very briefly just above the first, the moving point will always be perceived in front of the flash while they are vertically aligned.














&lt;figure  id=&#34;figure-fig-2-diagonal-markov-chain-in-the-current-study-the-estimated-state-vector-z--x-y-u-v-is-composed-of-the-2d-position-x-and-y-and-velocity-u-and-v-of-a-moving-stimulus-a-first-we-extend-a-classical-markov-chain-using-nijhawans-diagonal-model-in-order-to-take-into-account-the-known-neural-delay-œÑ-at-time-t-information-is-integrated-until-time-t--œÑ-using-a-markov-chain-and-a-model-of-state-transitions-pztztŒ¥t-such-that-one-can-infer-the-state-until-the-last-accessible-information-pztœÑi0tœÑ-this-information-can-then-be-pushed-forward-in-time-by-predicting-its-trajectory-from-t--œÑ-to-t-in-particular-pzti0tœÑ-can-be-predicted-by-the-same-internal-model-by-using-the-state-transition-at-the-time-scale-of-the-delay-that-is-pztztœÑ-this-is-virtually-equivalent-to-a-motion-extrapolation-model-but-without-sensory-measurements-during-the-time-window-between-t--œÑ-and-t-note-that-both-predictions-in-this-model-are-based-on-the-same-model-of-state-transitions-b-one-can-write-a-second-equivalent-pull-mode-for-the-diagonal-model-now-the-current-state-is-directly-estimated-based-on-a-markov-chain-on-the-sequence-of-delayed-estimations-while-being-equivalent-to-the-push-mode-described-above-such-a-direct-computation-allows-to-more-easily-combine-information-from-areas-with-different-delays-such-a-model-implements-nijhawans-diagonal-model-but-now-motion-information-is-probabilistic-and-therefore-inferred-motion-may-be-modulated-by-the-respective-precisions-of-the-sensory-and-internal-representations-c-such-a-diagonal-delay-compensation-can-be-demonstrated-in-a-two-layered-neural-network-including-a-source-input-and-a-target-predictive-layer-44-the-source-layer-receives-the-delayed-sensory-information-and-encodes-both-position-and-velocity-topographically-within-the-different-retinotopic-maps-of-each-layer-for-the-sake-of-simplicity-we-illustrate-only-one-2d-map-of-the-motions-x-v-the-integration-of-coherent-information-can-either-be-done-in-the-source-layer-push-mode-or-in-the-target-layer-pull-mode-crucially-to-implement-a-delay-compensation-in-this-motion-based-prediction-model-one-may-simply-connect-each-source-neuron-to-a-predictive-neuron-corresponding-to-the-corrected-position-of-stimulus-x--v--œÑ-v-in-the-target-layer-the-precision-of-this-anisotropic-connectivity-map-can-be-tuned-by-the-width-of-convergence-from-the-source-to-the-target-populations-using-such-a-simple-mapping-we-have-previously-shown-that-the-neuronal-population-activity-can-infer-the-current-position-along-the-trajectory-despite-the-existence-of-neural-delays&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=info:doi/10.1371/journal.pcbi.1005068.g002&#34; alt=&#34; Fig 2. *Diagonal Markov chain.* In the current study, the estimated state vector z = {x, y, u, v} is composed of the 2D position (x and y) and velocity (u and v) of a (moving) stimulus. (A) First, we extend a classical Markov chain using Nijhawan‚Äôs diagonal model in order to take into account the known neural delay œÑ: At time t, information is integrated until time t ‚àí œÑ, using a Markov chain and a model of state transitions p(zt|zt‚àíŒ¥t) such that one can infer the state until the last accessible information p(zt‚àíœÑ|I0:t‚àíœÑ). This information can then be ‚Äúpushed‚Äù forward in time by predicting its trajectory from t ‚àí œÑ to t. In particular p(zt|I0:t‚àíœÑ) can be predicted by the same internal model by using the state transition at the time scale of the delay, that is, p(zt|zt‚àíœÑ). This is virtually equivalent to a motion extrapolation model but without sensory measurements during the time window between t ‚àí œÑ and t. Note that both predictions in this model are based on the same model of state transitions. (B) One can write a second, equivalent ‚Äúpull‚Äù mode for the diagonal model. Now, the current state is directly estimated based on a Markov chain on the sequence of delayed estimations. While being equivalent to the push-mode described above, such a direct computation allows to more easily combine information from areas with different delays. Such a model implements Nijhawan‚Äôs ‚Äúdiagonal model‚Äù, but now motion information is probabilistic and therefore, inferred motion may be modulated by the respective precisions of the sensory and internal representations. (C) Such a diagonal delay compensation can be demonstrated in a two-layered neural network including a source (input) and a target (predictive) layer [44]. The source layer receives the delayed sensory information and encodes both position and velocity topographically within the different retinotopic maps of each layer. For the sake of simplicity, we illustrate only one 2D map of the motions (x, v). The integration of coherent information can either be done in the source layer (push mode) or in the target layer (pull mode). Crucially, to implement a delay compensation in this motion-based prediction model, one may simply connect each source neuron to a predictive neuron corresponding to the corrected position of stimulus (x &amp;#43; v ‚ãÖ œÑ, v) in the target layer. The precision of this anisotropic connectivity map can be tuned by the width of convergence from the source to the target populations. Using such a simple mapping, we have previously shown that the neuronal population activity can infer the current position along the trajectory despite the existence of neural delays. &#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 2. &lt;em&gt;Diagonal Markov chain.&lt;/em&gt; In the current study, the estimated state vector z = {x, y, u, v} is composed of the 2D position (x and y) and velocity (u and v) of a (moving) stimulus. (A) First, we extend a classical Markov chain using Nijhawan‚Äôs diagonal model in order to take into account the known neural delay œÑ: At time t, information is integrated until time t ‚àí œÑ, using a Markov chain and a model of state transitions p(zt|zt‚àíŒ¥t) such that one can infer the state until the last accessible information p(zt‚àíœÑ|I0:t‚àíœÑ). This information can then be ‚Äúpushed‚Äù forward in time by predicting its trajectory from t ‚àí œÑ to t. In particular p(zt|I0:t‚àíœÑ) can be predicted by the same internal model by using the state transition at the time scale of the delay, that is, p(zt|zt‚àíœÑ). This is virtually equivalent to a motion extrapolation model but without sensory measurements during the time window between t ‚àí œÑ and t. Note that both predictions in this model are based on the same model of state transitions. (B) One can write a second, equivalent ‚Äúpull‚Äù mode for the diagonal model. Now, the current state is directly estimated based on a Markov chain on the sequence of delayed estimations. While being equivalent to the push-mode described above, such a direct computation allows to more easily combine information from areas with different delays. Such a model implements Nijhawan‚Äôs ‚Äúdiagonal model‚Äù, but now motion information is probabilistic and therefore, inferred motion may be modulated by the respective precisions of the sensory and internal representations. (C) Such a diagonal delay compensation can be demonstrated in a two-layered neural network including a source (input) and a target (predictive) layer [44]. The source layer receives the delayed sensory information and encodes both position and velocity topographically within the different retinotopic maps of each layer. For the sake of simplicity, we illustrate only one 2D map of the motions (x, v). The integration of coherent information can either be done in the source layer (push mode) or in the target layer (pull mode). Crucially, to implement a delay compensation in this motion-based prediction model, one may simply connect each source neuron to a predictive neuron corresponding to the corrected position of stimulus (x + v ‚ãÖ œÑ, v) in the target layer. The precision of this anisotropic connectivity map can be tuned by the width of convergence from the source to the target populations. Using such a simple mapping, we have previously shown that the neuronal population activity can infer the current position along the trajectory despite the existence of neural delays.
    &lt;/figcaption&gt;&lt;/figure&gt;
Processing visual information takes time and even if these delays are remarkably short, they are not negligible and the nervous system must compensate them. For an object that moves predictably, the neural network can infer its most probable position taking into account this processing time. For the flash, however, this prediction can not be established because its appearance is unpredictable. Thus, while the two targets are aligned on the retina at the time of the flash, the position of the moving object is anticipated by the brain to compensate for the processing time: it is this differentiated treatment that causes the flash-lag effect.
The researchers show that this hypothesis also makes it possible to explain the cases where this illusion does not work: for example if the flash appears at the end of the moving dot&amp;rsquo;s trajectory or if the target reverses its path in an unexpected way. In this work, the major innovation is to use the accuracy of information in the dynamics of the model. Thus, the corrected position of the moving target is calculated by combining the sensory flux with the internal representation of the trajectory, both of which exist in the form of probability distributions. To manipulate the trajectory is to change the precision and therefore the relative weight of these two information when they are optimally combined in order to know where an object is at the present time. The researchers propose to call parodiction (from the ancient Greek paron, the present) this new theory that joins Bayesian inference with taking into account neuronal delays.














&lt;figure  id=&#34;figure-fig-5-histogram-of-the-estimated-positions-as-a-function-of-time-for-the-dmbp-model-histograms-of-the-inferred-horizontal-positions-blueish-bottom-panel-and-horizontal-velocity-reddish-top-panel-as-a-function-of-time-frame-from-the-dmbp-model-darker-levels-correspond-to-higher-probabilities-while-a-light-color-corresponds-to-an-unlikely-estimation-we-highlight-three-successive-epochs-along-the-trajectory-corresponding-to-the-flash-initiated-standard-mid-point-and-flash-terminated-cycles-the-timing-of-the-flashes-are-respectively-indicated-by-the-dashed-vertical-lines-in-dark-the-physical-time-and-in-green-the-delayed-input-knowing-œÑ--100-ms-histograms-are-plotted-at-two-different-levels-of-our-model-in-the-push-mode-the-left-hand-column-illustrates-the-source-layer-that-corresponds-to-the-integration-of-delayed-sensory-information-including-the-prior-on-motion-the-right-hand-illustrates-the-target-layer-corresponding-to-the-same-information-but-after-the-occurrence-of-some-motion-extrapolation-compensating-for-the-known-neural-delay-œÑ&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=10.1371/journal.pcbi.1005068.g005&#34; alt=&#34;Fig 5. *Histogram of the estimated positions as a function of time for the dMBP model.* Histograms of the inferred horizontal positions (blueish bottom panel) and horizontal velocity (reddish top panel), as a function of time frame, from the dMBP model. Darker levels correspond to higher probabilities, while a light color corresponds to an unlikely estimation. We highlight three successive epochs along the trajectory, corresponding to the flash initiated, standard (mid-point) and flash terminated cycles. The timing of the flashes are respectively indicated by the dashed vertical lines. In dark, the physical time and in green the delayed input knowing œÑ = 100 ms. Histograms are plotted at two different levels of our model in the push mode. The left-hand column illustrates the source layer that corresponds to the integration of delayed sensory information, including the prior on motion. The right-hand illustrates the target layer corresponding to the same information but after the occurrence of some motion extrapolation compensating for the known neural delay œÑ.  &#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 5. &lt;em&gt;Histogram of the estimated positions as a function of time for the dMBP model.&lt;/em&gt; Histograms of the inferred horizontal positions (blueish bottom panel) and horizontal velocity (reddish top panel), as a function of time frame, from the dMBP model. Darker levels correspond to higher probabilities, while a light color corresponds to an unlikely estimation. We highlight three successive epochs along the trajectory, corresponding to the flash initiated, standard (mid-point) and flash terminated cycles. The timing of the flashes are respectively indicated by the dashed vertical lines. In dark, the physical time and in green the delayed input knowing œÑ = 100 ms. Histograms are plotted at two different levels of our model in the push mode. The left-hand column illustrates the source layer that corresponds to the integration of delayed sensory information, including the prior on motion. The right-hand illustrates the target layer corresponding to the same information but after the occurrence of some motion extrapolation compensating for the known neural delay œÑ.
    &lt;/figcaption&gt;&lt;/figure&gt;
Despite the simplicity of this solution, parodiction has elements that may seem counter-intuitive. Indeed, in this model, the physical world is considered &amp;ldquo;hidden&amp;rdquo;, that is to say, it can only be guessed by our sensations and our experience. The role of visual perception is then to deliver to our central nervous system the most likely information despite the different sources of noise, ambiguity and time delays. According to the authors of this publication, the visual treatment would consist in a &amp;ldquo;simulation&amp;rdquo; of the visual world projected at the present time, even before the visual information can actually modulate, confirm or cancel this simulation. This hypothesis, which seems to belong to &amp;ldquo;science fiction&amp;rdquo;, is being tested with more detailed and biologically plausible hierarchical neural network models that should allow us to better understand the mysteries underlying our perception. Visual illusions have still the power to amaze us!
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New from Khoei et al. The Flash-Lag Effect as a &lt;a href=&#34;https://twitter.com/hashtag/Motion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Motion&lt;/a&gt;-Based Predictive Shift &lt;a href=&#34;https://t.co/K3KWPO8l4a&#34;&gt;https://t.co/K3KWPO8l4a&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuralnetworks?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuralnetworks&lt;/a&gt; &lt;a href=&#34;https://t.co/iWsd9nK5qp&#34;&gt;pic.twitter.com/iWsd9nK5qp&lt;/a&gt;&lt;/p&gt;&amp;mdash; PLOS Comp Biol (@PLOSCompBiol) &lt;a href=&#34;https://twitter.com/PLOSCompBiol/status/829474896023474176?ref_src=twsrc%5Etfw&#34;&gt;February 8, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing the odds of inherent vs. observed overdispersion in neural spike counts</title>
      <link>https://laurentperrinet.github.io/publication/taouali-16/</link>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/taouali-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Push-Pull Receptive Field Organization and Synaptic Depression: Mechanisms for Reliably Encoding Naturalistic Stimuli in V1</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biologically Inspired Dynamic Textures for Probing Motion Perception</title>
      <link>https://laurentperrinet.github.io/publication/vacher-15-nips/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-15-nips/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/jonathan-vacher/&#34;&gt;Jonathan Vacher&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/andrew-isaac-meso/&#34;&gt;Andrew Isaac Meso&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/gabriel-peyre/&#34;&gt;Gabriel Peyr√©&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2018).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/vacher-16/&#34;&gt;Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1611.01390&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/vacher-16/vacher-16.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/vacher-16/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1162/neco_a_01142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge co-occurrences can account for rapid categorization of natural versus animal images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-bednar-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-bednar-15/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;communiqu√© de presse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.nature.com/article-assets/npg/srep/2015/150622/srep11400/extref/srep11400-s1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;supplementary information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;PerrinetBednar15supplementary.pdf&#34;&gt;supplementary material&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-study-of-how-people-can-quickly-spot-animals-by-sight-is-helping-uncover-the-workings-of-the-human-brain&#34;&gt;A study of how people can quickly spot animals by sight is helping uncover the workings of the human brain.&lt;/h1&gt;
&lt;p&gt;Scientists examined why volunteers who were shown hundreds of pictures - some with animals and some without - were able to detect animals in as little as one-tenth of a second.
They found that one of the first parts of the brain to process visual information - the primary visual cortex - can control this fast response.
More complex parts of the brain are not required at this stage, contrary to what was previously thought.
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New info published on how the human brain processes visual information from &lt;a href=&#34;https://twitter.com/EdinburghUni?ref_src=twsrc%5Etfw&#34;&gt;@EdinburghUni&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/uniamu?ref_src=twsrc%5Etfw&#34;&gt;@uniamu&lt;/a&gt; stuidy &lt;a href=&#34;http://t.co/KUicugL8P7&#34;&gt;http://t.co/KUicugL8P7&lt;/a&gt;&lt;/p&gt;&amp;mdash; EdinUniNeuro (@EdinUniNeuro) &lt;a href=&#34;https://twitter.com/EdinUniNeuro/status/613011086829162497?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  id=&#34;figure-edge-co-occurrences-a-an-example-image-with-the-list-of-extracted-edges-overlaid-each-edge-is-represented-by-a-red-line-segment-which-represents-its-position-center-of-segment-orientation-and-scale-length-of-segment-we-controlled-the-quality-of-the-reconstruction-from-the-edge-information-such-that-the-residual-energy-was-less-than-5-b-the-relationship-between-a-reference-edge-a-and-another-edge-b-can-be-quantified-in-terms-of-the-difference-between-their-orientations-theta-ratio-of-scale-sigma-distance-d-between-their-centers-and-difference-of-azimuth-angular-location-phi-additionally-we-define-psiphi---theta2-which-is-symmetric-with-respect-to-the-choice-of-the-reference-edge-in-particular-psi0-for-co-circular-edges--see-text-as-incitetgeisler01-edges-outside-a-central-circular-mask-are-discarded-in-the-computation-of-the-statistics-to-avoid-artifacts-image-credit-andrew-shiva-creative-commons-attribution-share-alike-30-unported-licensehttpscommonswikimediaorgwikifileelephant_28loxodonta_africana29_05jpg-this-is-used-to-compute-the-chevron-map-in-figure2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Edge co-occurrences **(A)** An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. **(B)** The relationship between a reference edge *A* and another edge *B* can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: [Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license](https://commons.wikimedia.org/wiki/File:Elephant_/%28Loxodonta_Africana/%29_05.jpg)). This is used to compute the chevron map in Figure~2.&#34; srcset=&#34;
               /publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_ca9eec4a20242c1ecbe0ba46c37eae80.jpg 400w,
               /publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_3b7b1b89e6a78e0e22c814251e959436.jpg 760w,
               /publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_ca9eec4a20242c1ecbe0ba46c37eae80.jpg&#34;
               width=&#34;310&#34;
               height=&#34;393&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Edge co-occurrences &lt;strong&gt;(A)&lt;/strong&gt; An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. &lt;strong&gt;(B)&lt;/strong&gt; The relationship between a reference edge &lt;em&gt;A&lt;/em&gt; and another edge &lt;em&gt;B&lt;/em&gt; can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Elephant_/%28Loxodonta_Africana/%29_05.jpg&#34;&gt;Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license&lt;/a&gt;). This is used to compute the chevron map in Figure~2.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;ÂãïÁâ©„ÅãÂê¶„Åã„ÅÆË¶ãÂàÜ„ÅëÊñπ„ÄÇ&lt;a href=&#34;http://t.co/TTY8MwZGoO&#34;&gt;http://t.co/TTY8MwZGoO&lt;/a&gt;„ÄÄÂºïÁî®„Åï„Çå„Å¶„Çã„Åë„Å©„ÄÅThorpe (1996)„ÅÆ150ms„ÅßÂå∫Âà•„Åï„Çå„Å¶„Çã„Å£„Å¶Ë©±(„Å™„Å§„Åã„Åó„ÅÑ)„Å®Èñ¢‰øÇ„ÅÇ„Çä„Åù„ÅÜ„ÄÇ&lt;/p&gt;&amp;mdash; Makito Oku (@okumakito) &lt;a href=&#34;https://twitter.com/okumakito/status/613128456637841408?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  id=&#34;figure-the-probability-distribution-function-ppsi-theta-represents-the-distribution-of-the-different-geometrical-arrangements-of-edges-angles-which-we-call-a-chevron-map-we-show-here-the-histogram-for-non-animal-natural-images-illustrating-the-preference-for-co-linear-edge-configurations-for-each-chevron-configuration-deeper-and-deeper-red-circles-indicate-configurations-that-are-more-and-more-likely-with-respect-to-a-uniform-prior-with-an-average-maximum-of-about-3-times-more-likely-and-deeper-and-deeper-blue-circles-indicate-configurations-less-likely-than-a-flat-prior-with-a-minimum-of-about-08-times-as-likely-conveniently-this-chevron-map-shows-in-one-graph-that-non-animal-natural-images-have-on-average-a-preference-for-co-linear-and-parallel-edges-the-horizontal-middle-axis-and-orthogonal-angles-the-top-and-bottom-rowsalong-with-a-slight-preference-for-co-circular-configurations-for-psi0-and-psipm-frac-pi-2-just-above-and-below-the-central-row-we-compare-chevron-maps-in-different-image-categories-in-figure3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&amp;#39; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.&#34; srcset=&#34;
               /publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_13347010109b3ad4b96ca62f88a89617.png 400w,
               /publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_f3969b0d6d6fe3cb9ed0030509860fbb.png 760w,
               /publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_13347010109b3ad4b96ca62f88a89617.png&#34;
               width=&#34;550&#34;
               height=&#34;495&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&#39; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;a href=&#34;http://t.co/NY9HapBx2S&#34;&gt;http://t.co/NY9HapBx2S&lt;/a&gt; &lt;a href=&#34;http://t.co/rKQ8I5i6Ty&#34;&gt;pic.twitter.com/rKQ8I5i6Ty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Francis Villatoro (@emulenews) &lt;a href=&#34;https://twitter.com/emulenews/status/612988348400070656?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  id=&#34;figure-as-for-figure-2-we-show-the-probability-of-edge-configurations-as-chevron-maps-for-two-databases-man-made-animal-here-we-show-the-ratio-of-histogram-counts-relative-to-that-of-the-non-animal-natural-image-dataset-deeper-and-deeper-red-circles-indicate-configurations-that-are-more-and-more-likely-and-blue-respectively-less-likely-with-respect-to-the-histogram-computed-for-non-animal-images-in-the-left-plot-the-animal-images-exhibit-relatively-more-circular-continuations-and-converging-angles-red-chevrons-in-the-central-vertical-axis-relative-to-non-animal-natural-images-at-the-expense-of-co-linear-parallel-and-orthogonal-configurations-blue-circles-along-the-middle-horizontal-axis-the-man-made-images-have-strikingly-more-co-linear-features-central-circle-which-reflects-the-prevalence-of-long-straight-lines-in-the-cage-images-in-that-dataset-we-use-this-representation-to-categorize-images-from-these-different-categories-in-figure4&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.&#34; srcset=&#34;
               /publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_ceddedcb37188dd127b36a6b63cae82a.png 400w,
               /publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_d04c20f2fe173e8e859f786e0ab5862f.png 760w,
               /publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_ceddedcb37188dd127b36a6b63cae82a.png&#34;
               width=&#34;760&#34;
               height=&#34;469&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-classification-results-to-quantify-the-difference-in-low-level-feature-statistics-across-categories-see-figure3-we-used-a-standard-support-vector-machine-svm-classifier-to-measure-how-each-representation-affected-the-classifiers-reliability-for-identifying-the-image-category-for-each-individual-image-we-constructed-a-vector-of-features-as-either-fo-the-histogram-of-first-order-statistics-as-the-histogram-of-edges-orientations-cm-the-chevron-map-subset-of-the-second-order-statistics-ie-the-two-dimensional-histogram-of-relative-orientation-and-azimuth-see-figure-2--or-so-the-full-four-dimensional-histogram-of-second-order-statistics-ie-all-parameters-of-the-edge-co-occurrences-we-gathered-these-vectors-for-each-different-class-of-images-and-report-here-the-results-of-the-svm-classifier-using-an-f1-score-50-represents-chance-level-while-it-was-expected-that-differences-would-be-clear-between-non-animal-natural-images-versus-laboratory-man-made-images-results-are-still-quite-high-for-classifying-animal-images-versus-non-animal-natural-images-and-are-in-the-range-reported-bycitetserre07-f1-score-of-80-for-human-observers-and-82-for-their-model-even-using-the-cm-features-alone-we-further-extend-this-results-to-the-psychophysical-results-of-serre-et-al-2007-in-figure-5&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&amp;#39;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&amp;#39; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50\% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80\% for human observers and 82\% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.&#34; srcset=&#34;
               /publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_9a22e6cc9ec8c1fce8a0b247e703dafe.png 400w,
               /publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_9ae1fe1b95f13e9e3ef840ed20f2f2d8.png 760w,
               /publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_9a22e6cc9ec8c1fce8a0b247e703dafe.png&#34;
               width=&#34;476&#34;
               height=&#34;294&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&amp;rsquo;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&#39; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80% for human observers and 82% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-to-see-whether-the-patterns-of-errors-made-by-humans-are-consistent-with-our-model-we-studied-the-second-order-statistics-of-the-50-non-animal-images-that-human-subjects-in-serre-et-al-2007-most-commonly-falsely-reported-as-having-an-animal-we-call-this-set-of-images-the-false-alarm-image-dataset-left-this-chevron-map-plot-shows-the-ratio-between-the-second-order-statistics-of-the-false-alarm-images-and-the-full-non-animal-natural-image-dataset-computed-as-in-figure-3-left-just-as-for-the-images-that-actually-do-contain-animals-figure3-left-the-images-falsely-reported-as-having-animals-have-more-co-circular-and-converging-red-chevrons-and-fewer-collinear-and-orthogonal-configurations-blue-chevrons-right-to-quantify-this-similarity-we-computed-the-kullback-leibler-distance-between-the-histogram-of-each-of-these-images-from-the-false-alarm-image-dataset-and-the-average-histogram-of-each-class-the-difference-between-these-two-distances-gives-a-quantitative-measure-of-how-close-each-image-is-to-the-average-histograms-for-each-class-consistent-with-the-idea-that-humans-are-using-edge-co-occurences-to-do-rapid-image-categorization-the-50-non-animal-images-that-were-worst-classified-are-biased-toward-the-animal-histogram-d--104-while-the-550-best-classified-non-animal-images-are-closer-to-the-non-animal-histogram&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&amp;#39; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram. &#34; srcset=&#34;
               /publication/perrinet-bednar-15/figure_FA_hu9158f735a2d21afb5c10a78c8717ec4e_575661_a86112321c2757eb5ea951c1ea192f3c.png 400w,
               /publication/perrinet-bednar-15/figure_FA_hu9158f735a2d21afb5c10a78c8717ec4e_575661_75e53ca2231998c2f5254d83a09d53f6.png 760w,
               /publication/perrinet-bednar-15/figure_FA_humans_hu9158f735a2d21afb5c10a78c8717ec4e_575661_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_FA_hu9158f735a2d21afb5c10a78c8717ec4e_575661_a86112321c2757eb5ea951c1ea192f3c.png&#34;
               width=&#34;760&#34;
               height=&#34;470&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&#39; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active inference, eye movements and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</link>
      <pubDate>Tue, 16 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</guid>
      <description>&lt;h1 id=&#34;active-inference-tracking-eye-movements-and-oculomotor-delays&#34;&gt;Active Inference, tracking eye movements and oculomotor delays&lt;/h1&gt;
&lt;p&gt;Tracking eye movements face a difficult task: they have to be fast while they suffer inevitable delays. If we focus on area MT of humans for instance as it is crucial for detecting the motion of visual objects, sensory information coming to this area is already lagging some 35 milliseconds behind operational time ‚Äì that is, it reflects some past information. Still the fastest action that may be done there is only able to reach the effector muscles of the eyes some 40 milliseconds later ‚Äì that is, in the future. The tracking eye movement system is however able to respond swiftly and even to anticipate repetitive movements (e.g. Barnes et al, 2000 ‚Äì refs in manuscript). In that case, it means that information in a cortical area is both predicted from the past sensory information but also anticipated to give an optimal response in the future. Even if numerous models have been described to model different mechanisms to account for delays, no theoretical approach has tackled the whole problem explicitly. In several areas of vision research, authors have proposed models at different levels of abstractions from biomechanical models, to neurobiological implementations (e.g. Robinson, 1986) or Bayesian models. This study is both novel and important because ‚Äì using a neurobiologically plausible hierarchical Bayesian model ‚Äì it demonstrates that using generalized coordinates to finesse the prediction of a target&amp;rsquo;s motion, the model can reproduce characteristic properties of tracking eye movements in the presence of delays. Crucially, the different refinements to the model that we propose ‚Äì pursuit initiation, smooth pursuit eye movements, and anticipatory response ‚Äì are consistent with the different types of tracking eye movements that may be observed experimentally.














&lt;figure  id=&#34;figure-a-this-figure-reports-the-response-of-predictive-processing-during-the-simulation-of-pursuit-initiation-using-a-single-sweep-of-a-visual-target-while-compensating-for-sensory-motor-delays-here-we-see-horizontal-excursions-of-oculomotor-angle-red-line-one-can-see-clearly-the-initial-displacement-of-the-target-that-is-suppressed-by-action-after-a-few-hundred-milliseconds-additionally-we-illustrate-the-effects-of-assuming-wrong-sensorimotor-delays-on-pursuit-initiation-under-pure-sensory-delays-blue-dotted-line-one-can-see-clearly-the-delay-in-sensory-predictions-in-relation-to-the-true-inputs-with-pure-motor-delays-blue-dashed-line-and-with-combined-sensorimotor-delays-blue-line-there-is-a-failure-of-optimal-control-with-oscillatory-fluctuations-in-oculomotor-trajectories-which-may-become-unstable-b-this-figure-reports-the-simulation-of-smooth-pursuit-when-the-target-motion-is-hemi-sinusoidal-as-would-happen-for-a-pendulum-that-would-be-stopped-at-each-half-cycle-left-of-the-vertical-broken-black-lines-in-the-lower-right-panel-we-report-the-horizontal-excursions-of-oculomotor-angle-the-generative-model-used-here-has-been-equipped-with-a-second-hierarchical-level-that-contains-hidden-states-modeling-latent-periodic-behavior-of-the-hidden-causes-of-target-motion-with-this-addition-the-improvement-in-pursuit-accuracy-apparent-at-the-onset-of-the-second-cycle-of-motion-is-observed-pink-shaded-area-similar-to-psychophysical-experimentss&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;**(A)** This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. **(B)** This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.&#34; srcset=&#34;
               /publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_543276db98d0a4743451eed80bc2b08b.png 400w,
               /publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_ceb3d0a2860ec222e4853d61e1c22982.png 760w,
               /publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_543276db98d0a4743451eed80bc2b08b.png&#34;
               width=&#34;760&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;(A)&lt;/strong&gt; This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. &lt;strong&gt;(B)&lt;/strong&gt; This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction explains the role of tracking in motion extrapolation</title>
      <link>https://laurentperrinet.github.io/publication/khoei-13-jpp/</link>
      <pubDate>Mon, 25 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-13-jpp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2017).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;li&gt;Based on &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;Khoei et al, 2017&lt;/a&gt;














&lt;figure  id=&#34;figure-figure-1-the-problem-of-fragmented-trajectories-and-motion-extrapolation-as-an-object-moves-in-visual-space-as-represented-here-for-commodity-by-the-red-trajectory-of-a-tennis-ball-in-a-spacetime-diagram-with-a-one-dimensional-space-on-the-vertical-axis-the-sensory-flux-may-be-interrupted-by-a-sudden-and-transient-blank-as-denoted-by-the-vertical-gray-area-and-the-dashed-trajectory-how-can-the-instantaneous-position-of-the-dot-be-estimated-at-the-time-of-reappearance-this-mechanism-is-the-basis-of-motion-extrapolation-and-is-rooted-on-the-prior-knowledge-on-the-coherency-of-trajectories-in-natural-images-we-show-below-the-typical-eye-velocity-profile-that-is-observed-during-smooth-pursuit-eye-movements-spem-as-a-prototypical-sensory-response-it-consists-of-three-phases-first-a-convergence-of-the-eye-velocity-toward-the-physical-speed-second-a-drop-of-velocity-during-the-blank-and-finally-a-sudden-catch-up-of-speed-at-reappearance-becker-and-fuchs-1985&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Figure 1: The problem of fragmented trajectories and motion extrapolation. As an object moves in visual space (as represented here for commodity by the red trajectory of a tennis ball in a space‚Äìtime diagram with a one-dimensional space on the vertical axis), the sensory flux may be interrupted by a sudden and transient blank (as denoted by the vertical, gray area and the dashed trajectory). How can the instantaneous position of the dot be estimated at the time of reappearance? This mechanism is the basis of motion extrapolation and is rooted on the prior knowledge on the coherency of trajectories in natural images. We show below the typical eye velocity profile that is observed during Smooth Pursuit Eye Movements (SPEM) as a prototypical sensory response. It consists of three phases: first, a convergence of the eye velocity toward the physical speed, second, a drop of velocity during the blank and finally, a sudden catch-up of speed at reappearance (Becker and Fuchs, 1985).&#34; srcset=&#34;
               /publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_067ef234dcd8442b6e2603ff0bfb5021.jpg 400w,
               /publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_c15a7adba7034cadfa2267f76a87d0b5.jpg 760w,
               /publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_067ef234dcd8442b6e2603ff0bfb5021.jpg&#34;
               width=&#34;293&#34;
               height=&#34;223&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: The problem of fragmented trajectories and motion extrapolation. As an object moves in visual space (as represented here for commodity by the red trajectory of a tennis ball in a space‚Äìtime diagram with a one-dimensional space on the vertical axis), the sensory flux may be interrupted by a sudden and transient blank (as denoted by the vertical, gray area and the dashed trajectory). How can the instantaneous position of the dot be estimated at the time of reappearance? This mechanism is the basis of motion extrapolation and is rooted on the prior knowledge on the coherency of trajectories in natural images. We show below the typical eye velocity profile that is observed during Smooth Pursuit Eye Movements (SPEM) as a prototypical sensory response. It consists of three phases: first, a convergence of the eye velocity toward the physical speed, second, a drop of velocity during the blank and finally, a sudden catch-up of speed at reappearance (Becker and Fuchs, 1985).
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Anisotropic connectivity implements motion-based prediction in a spiking neural network</title>
      <link>https://laurentperrinet.github.io/publication/kaplan-13/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kaplan-13/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;li&gt;see  follow-up on motion extrapolation: 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2013).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Motion-based prediction explains the role of tracking in motion extrapolation&lt;/a&gt;.
  &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/khoei-13-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/khoei-13-jpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2013.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2017).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  


lication/khoei-13-jpp&amp;quot; view=&amp;ldquo;4&amp;rdquo; &amp;gt;}}&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Advances in Texture Analysis for Emphysema Classification</title>
      <link>https://laurentperrinet.github.io/publication/nava-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/nava-13/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;relies on log-Gabor filters: 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/sylvain-fischer/&#34;&gt;Sylvain Fischer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/filip-sroubek/&#34;&gt;Filip ≈†roubek&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/rafael-redondo/&#34;&gt;Rafael Redondo&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/gabriel-cristobal/&#34;&gt;Gabriel Crist√≥bal&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2007).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/fischer-07-cv/&#34;&gt;Self-Invertible 2D Log-Gabor Wavelets&lt;/a&gt;.
  &lt;em&gt;International Journal of Computer Vision&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/fischer-07-cv/fischer-07-cv.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fischer-07-cv/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/bicv/LogGabor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1007/s11263-006-0026-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Smooth Pursuit and Visual Occlusion: Active Inference and Oculomotor Control in Schizophrenia</title>
      <link>https://laurentperrinet.github.io/publication/adams-12/</link>
      <pubDate>Fri, 26 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/adams-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;adams-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The behavioral receptive field underlying motion integration for primate tracking eye movements</title>
      <link>https://laurentperrinet.github.io/publication/masson-12/</link>
      <pubDate>Wed, 21 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/masson-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;masson-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motion Clouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception</title>
      <link>https://laurentperrinet.github.io/publication/sanz-12/</link>
      <pubDate>Wed, 14 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/sanz-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;sanz-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Complex dynamics in recurrent cortical networks based on spatially realistic connectivities</title>
      <link>https://laurentperrinet.github.io/publication/voges-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;voges-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Based on 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/nicole-voges/&#34;&gt;Nicole Voges&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2010).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/voges-10-jpp/&#34;&gt;Phase space analysis of networks based on biologically realistic parameters&lt;/a&gt;.
  &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/voges-10-jpp/voges-10-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/voges-10-jpp/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2009.11.004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>More is not always better: dissociation between perception and action explained by adaptive gain control</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;simoncini-12.png&#34; alt=&#34;header&#34;&gt;














&lt;figure  id=&#34;figure-band-pass-motion-stimuli-for-perception-and-action-tasks-a-in-the-space-representing-temporal-against-spatial-frequency-each-line-going-through-the-origin-corresponds-to-stimuli-moving-at-the-same-speed-a-simple-drifting-grating-is-a-single-point-in-this-space-our-moving-texture-stimuli-had-their-energy-distributed-within-an-ellipse-elongated-along-a-given-speed-line-keeping-constant-the-mean-spatial-and-temporal-frequencies-the-spatio-temporal-bandwidth-was-manipulated-by-co-varying-bsf-and-btf-as-illustrated-by-the-xyt-examples-human-performance-was-measured-for-two-different-tasks-run-in-parallel-blocks-b-for-ocular-tracking-motion-stimuli-were-presented-for-a-short-duration-200ms-in-the-wake-of-a-centering-saccade-to-control-both-attention-and-fixation-states-c-for-speed-discrimination-test-and-reference-stimuli-were-presented-successively-for-the-same-duration-and-subjects-were-instructed-to-indicate-whether-the-test-stimulus-was-perceived-as-slower-or-faster-than-reference&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;*Band-pass motion stimuli for perception and action tasks.* (a) In the space representing temporal against spatial frequency, each line going through the origin corresponds to stimuli moving at the same speed. A simple drifting grating is a single point in this space. Our moving texture stimuli had their energy distributed within an ellipse elongated along a given speed line, keeping constant the mean spatial and temporal frequencies. The spatio-temporal bandwidth was manipulated by co-varying Bsf and Btf as illustrated by the (x,y,t) examples. Human performance was measured for two different tasks, run in parallel blocks. (b) For ocular tracking, motion stimuli were presented for a short duration (200ms) in the wake of a centering saccade to control both attention and fixation states. (c) For speed discrimination, test and reference stimuli were presented successively for the same duration and subjects were instructed to indicate whether the test stimulus was perceived as slower or faster than reference. &#34; srcset=&#34;
               /publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_bdca28c638f584fe1353860d3a8d0382.gif 400w,
               /publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_b26689c102db40d72f5450100b2b51c9.gif 760w,
               /publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_1200x1200_fit_lanczos.gif 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_bdca28c638f584fe1353860d3a8d0382.gif&#34;
               width=&#34;80%&#34;
               height=&#34;256&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;Band-pass motion stimuli for perception and action tasks.&lt;/em&gt; (a) In the space representing temporal against spatial frequency, each line going through the origin corresponds to stimuli moving at the same speed. A simple drifting grating is a single point in this space. Our moving texture stimuli had their energy distributed within an ellipse elongated along a given speed line, keeping constant the mean spatial and temporal frequencies. The spatio-temporal bandwidth was manipulated by co-varying Bsf and Btf as illustrated by the (x,y,t) examples. Human performance was measured for two different tasks, run in parallel blocks. (b) For ocular tracking, motion stimuli were presented for a short duration (200ms) in the wake of a centering saccade to control both attention and fixation states. (c) For speed discrimination, test and reference stimuli were presented successively for the same duration and subjects were instructed to indicate whether the test stimulus was perceived as slower or faster than reference.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-12-pred/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-12-pred/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-12-pred.png&#34; alt=&#34;header&#34;&gt;














&lt;figure  id=&#34;figure-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-receptive-field-of-a-neuron-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-introducing-predictive-coding-resolves-the-aperture-problem&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.&#34; srcset=&#34;
               /publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_3a37ec9fc8a9a9bc8e6d67c8917f17b8.gif 400w,
               /publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_57bf0b80a55c659be6b53aa588cfcb2c.gif 760w,
               /publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_1200x1200_fit_lanczos.gif 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_3a37ec9fc8a9a9bc8e6d67c8917f17b8.gif&#34;
               width=&#34;80%&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-1-a-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-dotted-circle-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-due-to-the-limited-size-of-receptive-fields-in-sensory-cortical-areas-such-as-shown-by-the-dotted-white-circle-such-problem-is-faced-by-local-populations-of-neurons-that-visually-estimate-the-motion-of-objects-a-inset-on-a-polar-representation-of-possible-velocity-vectors-the-cross-in-the-center-corresponds-to-the-null-velocity-the-outer-circle-corresponding-to-twice-the-amplitude-of-physical-speed-we-plot-the-empirical-histogram-of-detected-velocity-vectors-this-representation-gives-a-quantification-of-the-aperture-problem-in-the-velocity-domain-at-the-onset-of-motion-detection-information-is-concentrated-along-an-elongated-constraint-line-whitehigh-probability-blackzero-probability-b-we-use-the-prior-knowledge-that-in-natural-scenes-motion-as-defined-by-its-position-and-velocity-is-following-smooth-trajectories-quantitatively-it-means-that-velocity-is-approximately-conserved-and-that-position-is-transported-according-to-the-known-velocity-we-show-here-such-a-transition-on-position-and-velocity-respectively-x_t-and-v_t-from-time-t-to-t--dt-with-the-perturbation-modeling-the-smoothness-of-prediction-in-position-and-velocity-respectively-n_x-and-n_v-c-applying-such-a-prior-on-a-dynamical-system-detecting-motion-we-show-that-motion-converges-to-the-physical-motion-after-approximately-one-spatial-period-the-line-moved-by-twice-its-height-c-inset-the-read-out-of-the-system-converged-to-the-physical-motion-motion-based-prediction-is-sufficient-to-resolve-the-aperture-problem-d-as-observed-at-the-perceptual-level-castet-et-al-1993-pei-et-al-2010-size-and-duration-of-the-tracking-angle-bias-decreased-with-respect-to-the-height-of-the-line-height-was-measured-relative-to-a-spatial-period-respectively-60-40-and-20-here-we-show-the-average-tracking-angle-red-out-from-the-probabilistic-representation-as-a-function-of-time-averaged-over-20-trials-error-bars-show-one-standard-deviation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Figure 1: *(A)* The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. *(A-inset)* On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). *(B)* We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t &amp;#43; dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). *(C)* Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). *(C-Inset)* The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. *(D)* As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_657da2821018ad03fa48e296624edb8a.jpg 400w,
               /publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_8642e0010f13cd06d9649c1e9e5a9b56.jpg 760w,
               /publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_657da2821018ad03fa48e296624edb8a.jpg&#34;
               width=&#34;80%&#34;
               height=&#34;717&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: &lt;em&gt;(A)&lt;/em&gt; The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. &lt;em&gt;(A-inset)&lt;/em&gt; On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). &lt;em&gt;(B)&lt;/em&gt; We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t + dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). &lt;em&gt;(C)&lt;/em&gt; Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). &lt;em&gt;(C-Inset)&lt;/em&gt; The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. &lt;em&gt;(D)&lt;/em&gt; As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-2-architecture-of-the-model-the-model-is-constituted-by-a-classical-measurement-stage-and-of-a-predictive-coding-layer-the-measurement-stage-consists-of-a-inferring-from-two-consecutive-frames-of-the-input-flow-b-a-likelihood-distribution-of-motion-this-layer-interacts-with-the-predictive-layer-which-consists-of-c-a-prediction-stage-that-infers-from-the-current-estimate-and-the-transition-prior-the-upcoming-state-estimate-and-d-an-estimation-stage-that-merges-the-current-prediction-of-motion-with-the-likelihood-measured-at-the-same-instant-in-the-previous-layer-b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_79b923828f8e98e870b0687f27406cb6.jpg 400w,
               /publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_9b6a135ecfc13a6f8fbed33bbb033b0a.jpg 760w,
               /publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_79b923828f8e98e870b0687f27406cb6.jpg&#34;
               width=&#34;80%&#34;
               height=&#34;695&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-3-to-explore-the-state-space-of-the-dynamical-system-we-simulated-motion-based-prediction-for-a-simple-small-dot-size-25-of-a-spatial-period-moving-horizontally-from-the-left-to-the-right-of-the-screen-we-tested-different-levels-of-sensory-noise-with-respect-to-different-levels-of-internal-noise-that-is-to-different-values-of-the-strength-of-prediction-right-results-show-the-emergence-of-different-states-for-different-prediction-precisions-a-regime-when-prediction-is-weak-and-which-shows-high-tracking-error-and-variability-no-tracking---nt-a-phase-for-intermediate-values-of-prediction-strength-as-in-figure-1-exhibiting-a-low-tracking-error-and-low-variability-in-the-tracking-phase-true-tracking---tt-and-finally-a-phase-corresponding-to-higher-precisions-with-relatively-efficient-mean-detection-but-high-variability-false-tracking---ft-we-give-3-representative-examples-of-the-emerging-states-at-one-contrast-level-c--01-with-starting-red-and-ending-blue-points-and-respectively-nt-tt-and-ft-by-showing-inferred-trajectories-for-each-trial-left-we-define-tracking-error-as-the-ratio-between-detected-speed-and-target-speed-and-we-plot-it-with-respect-to-the-stimulus-contrast-as-given-by-the-inverse-of-sensory-noise-error-bars-give-the-variability-in-tracking-error-as-averaged-over-20-trials-as-prediction-strength-increases-there-is-a-transition-from-smooth-contrast-response-function-nt-to-more-binary-responses-tt-and-ft&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. *(Right)* Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. *(Left)* We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_edcfcdaf6cfbae5a805ec192c57e9fed.jpg 400w,
               /publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_ca12c826175ee98f9dc9450e54ac50c6.jpg 760w,
               /publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_edcfcdaf6cfbae5a805ec192c57e9fed.jpg&#34;
               width=&#34;80%&#34;
               height=&#34;483&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. &lt;em&gt;(Right)&lt;/em&gt; Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. &lt;em&gt;(Left)&lt;/em&gt; We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-4-top-prediction-implements-a-competition-between-different-trajectories-here-we-focus-on-one-step-of-the-algorithm-by-testing-different-trajectories-at-three-key-positions-of-the-segment-stimulus-the-two-edges-and-the-center-dashed-circles-compared-to-the-pure-sensory-velocity-likelihood-left-insets-in-grayscale-prediction-modulates-response-as-shown-by-the-velocity-vectors-direction-coded-as-hue-as-in-figure-1-and-by-the-ratio-of-velocity-probabilities-log-ratio-in-bits-right-insets-there-is-no-change-for-the-middle-of-the-segment-yellow-tone-but-trajectories-that-are-predicted-out-of-the-line-are-explained-away-navy-tone-while-others-may-be-amplified-orange-tone-notice-the-asymmetry-between-both-edges-the-upper-edge-carrying-a-suppressive-predictive-information-while-the-bottom-edge-diffuses-coherent-motion-bottom-finally-the-aperture-problem-is-solved-due-to-the-repeated-application-of-this-spatio-temporal-contextual-information-modulation-to-highlight-the-anisotropic-diffusion-of-information-over-the-rest-of-the-line-we-plot-as-a-function-of-time-horizontal-axis-the-histogram-of-the-detected-motion-marginalized-over-horizontal-positions-vertical-axis-while-detected-direction-of-velocity-is-given-by-the-distribution-of-hues-blueish-colors-correspond-to-the-direction-perpendicular-to-the-diagonal-while-a-green-color-represents-a-disambiguated-motion-to-the-right-as-in-figure-1-the-plot-shows-that-motion-is-disambiguated-by-progressively-explaining-away-incoherent-motion-note-the-asymmetry-in-the-propagation-of-coherent-information&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;Figure 4: *(Top)* Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are ‚Äúexplained away‚Äù (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. *(Bottom)* Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_b4b0b2437b231deace63fa498f130d7f.jpg 400w,
               /publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_4d6f2469da439d61a981c7cb3ed99edf.jpg 760w,
               /publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_b4b0b2437b231deace63fa498f130d7f.jpg&#34;
               width=&#34;80%&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 4: &lt;em&gt;(Top)&lt;/em&gt; Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are ‚Äúexplained away‚Äù (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. &lt;em&gt;(Bottom)&lt;/em&gt; Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Perceptions as Hypotheses: Saccades as Experiments</title>
      <link>https://laurentperrinet.github.io/publication/friston-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/friston-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;friston-12.png&#34; alt=&#34;header&#34;&gt;














&lt;figure  id=&#34;figure-this-schematic-shows-the-dependencies-among-various-quantities-that-are-assumed-when-modeling-the-exchanges-of-a-self-organizing-system-like-the-brain-with-the-environment-the-top-panel-describes-the-states-of-the-environment-and-the-system-or-agent-in-terms-of-a-probabilistic-dependency-graph-where-connections-denote-directed-dependencies-the-quantities-are-described-within-the-nodes-of-this-graph-with-exemplar-forms-for-their-dependencies-on-other-variables-see-main-text-here-hidden-and-internal-states-are-separated-by-action-and-sensory-states-both-action-and-internal-states-encoding-a-conditional-density-minimize-free-energy-while-internal-states-encoding-prior-beliefs-maximize-salience-both-free-energy-and-salience-are-defined-in-terms-of-a-generative-model-that-is-shown-as-fictive-dependency-graph-in-the-lower-panel-note-that-the-variables-in-the-real-world-and-the-form-of-their-dynamics-are-different-from-that-assumed-by-the-generative-model-this-is-why-external-states-are-in-bold-furthermore-note-that-action-is-a-state-in-the-model-of-the-brain-but-is-replaced-by-hidden-controls-in-the-brains-model-of-its-world-this-means-that-the-agent-is-not-aware-of-action-but-has-beliefs-about-hidden-causes-in-the-world-that-action-can-fulfill-through-minimizing-free-energy-these-beliefs-correspond-to-prior-expectations-that-sensory-states-will-be-sampled-in-a-way-that-optimizes-conditional-confidence-or-salience&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.frontiersin.org/files/Articles/21922/fpsyg-03-00151-r4/image_m/fpsyg-03-00151-g001.jpg&#34; alt=&#34;**This schematic shows the dependencies among various quantities that are assumed when modeling the exchanges of a self organizing system like the brain with the environment.** The top panel describes the states of the environment and the system or agent in terms of a probabilistic dependency graph, where connections denote directed dependencies. The quantities are described within the nodes of this graph with exemplar forms for their dependencies on other variables (see main text). Here, hidden and internal states are separated by action and sensory states. Both action and internal states encoding a conditional density minimize free energy, while internal states encoding prior beliefs maximize salience. Both free energy and salience are defined in terms of a generative model that is shown as fictive dependency graph in the lower panel. Note that the variables in the real world and the form of their dynamics are different from that assumed by the generative model; this is why external states are in bold. Furthermore, note that action is a state in the model of the brain but is replaced by hidden controls in the brain‚Äôs model of its world. This means that the agent is not aware of action but has beliefs about hidden causes in the world that action can fulfill through minimizing free energy. These beliefs correspond to prior expectations that sensory states will be sampled in a way that optimizes conditional confidence or salience.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;This schematic shows the dependencies among various quantities that are assumed when modeling the exchanges of a self organizing system like the brain with the environment.&lt;/strong&gt; The top panel describes the states of the environment and the system or agent in terms of a probabilistic dependency graph, where connections denote directed dependencies. The quantities are described within the nodes of this graph with exemplar forms for their dependencies on other variables (see main text). Here, hidden and internal states are separated by action and sensory states. Both action and internal states encoding a conditional density minimize free energy, while internal states encoding prior beliefs maximize salience. Both free energy and salience are defined in terms of a generative model that is shown as fictive dependency graph in the lower panel. Note that the variables in the real world and the form of their dynamics are different from that assumed by the generative model; this is why external states are in bold. Furthermore, note that action is a state in the model of the brain but is replaced by hidden controls in the brain‚Äôs model of its world. This means that the agent is not aware of action but has beliefs about hidden causes in the world that action can fulfill through minimizing free energy. These beliefs correspond to prior expectations that sensory states will be sampled in a way that optimizes conditional confidence or salience.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Qui cr√©era le premier ordinateur intelligent?</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-doc-sciences/</link>
      <pubDate>Mon, 20 Jun 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-doc-sciences/</guid>
      <description>&lt;h1 id=&#34;qui-cr√©era-le-premier-ordinateur-intelligent&#34;&gt;Qui cr√©era le premier ordinateur intelligent?&lt;/h1&gt;
&lt;p&gt;Les ordinateurs classiques sont de plus en plus puissants, mais restent toujours aussi ¬´ stupides ¬ª. Impossible d‚Äôen trouver un avec lequel on puisse dialoguer de fa√ßon naturelle. Aucun syst√®me visuel artificiel ne voit aussi bien que nous, ou qu‚Äôune mouche ! Alors qui inventera le premier calculateur intelligent ?
&lt;img src=&#34;featured.jpg&#34; alt=&#34;Code neural&#34;&gt;
Le code neural (En haut : ¬© F. Chavane, en bas : ¬© T. Bal).
Le code neural est mieux compris gr√¢ce aux techniques d‚Äôimagerie r√©centes. Les neurosciences computationnelles permettent d‚Äô√©tudier les propri√©t√©s des r√©seaux de neurones.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pursuing motion illusions: a realistic oculomotor framework for Bayesian inference</title>
      <link>https://laurentperrinet.github.io/publication/bogadhi-11/</link>
      <pubDate>Fri, 22 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/bogadhi-11/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;bogadhi-11.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See a followup in 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Saccadic foveation of a moving visual target in the rhesus monkey</title>
      <link>https://laurentperrinet.github.io/publication/fleuriet-11/</link>
      <pubDate>Tue, 01 Feb 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fleuriet-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Phase space analysis of networks based on biologically realistic parameters</title>
      <link>https://laurentperrinet.github.io/publication/voges-10-jpp/</link>
      <pubDate>Wed, 10 Nov 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-10-jpp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;voges-10-jpp.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;see  follow-up : 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/nicole-voges/&#34;&gt;Nicole Voges&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/voges-12/&#34;&gt;Complex dynamics in recurrent cortical networks based on spatially realistic connectivities&lt;/a&gt;.
  &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/voges-12/voges-12.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/voges-12/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3389/fncom.2012.00041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Role of homeostasis in learning sparse representations</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-shl/</link>
      <pubDate>Sat, 17 Jul 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-shl/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-10-shl.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functional consequences of correlated excitatory and inhibitory conductances in cortical networks</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-10-jcns/</link>
      <pubDate>Fri, 25 Jun 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-10-jcns/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;kremkow-10-jcns.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computational Neuroscience, from Multiple Levels to Multi-level</title>
      <link>https://laurentperrinet.github.io/publication/dauce-10/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dauce-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamics of distributed 1D and 2D motion representations for short-latency ocular following</title>
      <link>https://laurentperrinet.github.io/publication/barthelemy-08/</link>
      <pubDate>Mon, 25 Feb 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/barthelemy-08/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;barthelemy-08.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyNN: A Common Interface for Neuronal Network Simulators</title>
      <link>https://laurentperrinet.github.io/publication/davison-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/davison-08/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;davison-08.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian modeling of dynamic motion integration</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-07/</link>
      <pubDate>Thu, 25 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-07/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;montagnini-07.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Self-Invertible 2D Log-Gabor Wavelets</title>
      <link>https://laurentperrinet.github.io/publication/fischer-07-cv/</link>
      <pubDate>Sat, 13 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-07-cv/</guid>
      <description>













&lt;figure  id=&#34;figure-figure-1-multiresolution-schemes-a-schematic-contours-of-the-log-gabor-filters-in-the-fourier-domain-with-5-scales-and-8-orientations-only-the-contours-at-78-of-the-filter-maximum-are-drawn-b-the-real-part-of-the-corresponding-filters-is-drawn-in-the-spatial-domain-the-two-first-scales-are-drawn-at-the-bottom-magnified-by-a-factor-of-4-for-a-better-visualization-the-different-scales-are-arranged-in-rows-and-the-orientations-in-columns-the-low-pass-filter-is-drawn-in-the-upper-left-part-c-the-corresponding-imaginary-parts-of-the-filters-are-shown-in-the-same-arrangement-note-that-the-low-pass-filter-does-not-have-imaginary-part-insets-b-and-c-show-the-final-filters-built-through-all-the-processes-described-in-section-2-d-in-the-proposed-scheme-the-elongation-of-log-gabor-wavelets-increases-with-the-number-of-orientations-nt--here-the-real-parts-left-column-and-imaginary-parts-right-column-are-drawn-for-the-3-4-6-8-10-12-and-16-orientation-schemes-e-as-a-comparison-orthogonal-wavelet-filters-db4-are-shown-horizontal-vertical-and-diagonal-wavelets-are-arranged-on-columns-low-pass-on-top-f-as-a-second-comparison-steerable-pyramid-filters-portilla-et-al-2003-are-shown-the-arrangement-over-scales-and-orientations-is-the-same-as-for-the-log-gabor-scheme&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;**Figure 1** Multiresolution schemes. (a) Schematic contours of the log-Gabor filters in the Fourier domain with 5 scales and 8 orientations (only the contours at 78% of the filter maximum are drawn). (b) The real part of the corresponding filters is drawn in the spatial domain. The two first scales are drawn at the bottom magnified by a factor of 4 for a better visualization. The different scales are arranged in rows and the orientations in columns. The low-pass filter is drawn in the upper-left part. (c) The corresponding imaginary parts of the filters are shown in the same arrangement. Note that the low-pass filter does not have imaginary part. Insets (b) and (c) show the final filters built through all the processes described in Section 2. (d) In the proposed scheme the elongation of log-Gabor wavelets increases with the number of orientations nt . Here the real parts (left column) and imaginary parts (right column) are drawn for the 3, 4, 6, 8, 10, 12 and 16 orientation schemes. (e) As a comparison orthogonal wavelet filters ‚ÄòDb4‚Äô are shown. Horizontal, vertical and diagonal wavelets are arranged on columns (low-pass on top). (f) As a second comparison, steerable pyramid filters (Portilla et al., 2003) are shown. The arrangement over scales and orientations is the same as for the log-Gabor scheme.&#34; srcset=&#34;
               /publication/fischer-07-cv/figure1_hu855fde1d10292610ea30d8b3a201517d_94218_ca50317b746c53be844e916b4fc0bd38.png 400w,
               /publication/fischer-07-cv/figure1_hu855fde1d10292610ea30d8b3a201517d_94218_859ea299e154f3e519f0e2a6b3c5b77f.png 760w,
               /publication/fischer-07-cv/figure1_hu855fde1d10292610ea30d8b3a201517d_94218_1200x1200_fit_lanczos_2.png 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/fischer-07-cv/figure1_hu855fde1d10292610ea30d8b3a201517d_94218_ca50317b746c53be844e916b4fc0bd38.png&#34;
               width=&#34;80%&#34;
               height=&#34;392&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Figure 1&lt;/strong&gt; Multiresolution schemes. (a) Schematic contours of the log-Gabor filters in the Fourier domain with 5 scales and 8 orientations (only the contours at 78% of the filter maximum are drawn). (b) The real part of the corresponding filters is drawn in the spatial domain. The two first scales are drawn at the bottom magnified by a factor of 4 for a better visualization. The different scales are arranged in rows and the orientations in columns. The low-pass filter is drawn in the upper-left part. (c) The corresponding imaginary parts of the filters are shown in the same arrangement. Note that the low-pass filter does not have imaginary part. Insets (b) and (c) show the final filters built through all the processes described in Section 2. (d) In the proposed scheme the elongation of log-Gabor wavelets increases with the number of orientations nt . Here the real parts (left column) and imaginary parts (right column) are drawn for the 3, 4, 6, 8, 10, 12 and 16 orientation schemes. (e) As a comparison orthogonal wavelet filters ‚ÄòDb4‚Äô are shown. Horizontal, vertical and diagonal wavelets are arranged on columns (low-pass on top). (f) As a second comparison, steerable pyramid filters (Portilla et al., 2003) are shown. The arrangement over scales and orientations is the same as for the log-Gabor scheme.
    &lt;/figcaption&gt;&lt;/figure&gt;
</description>
    </item>
    
    <item>
      <title>Modeling spatial integration in the ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-07-neurocomp.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Approximation of Images Inspired from the Functional Architecture of the Primary Visual Areas</title>
      <link>https://laurentperrinet.github.io/publication/fischer-07/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-07/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;relies on log-Gabor filters: 






  
    

&lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/sylvain-fischer/&#34;&gt;Sylvain Fischer&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/filip-sroubek/&#34;&gt;Filip ≈†roubek&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/rafael-redondo/&#34;&gt;Rafael Redondo&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/gabriel-cristobal/&#34;&gt;Gabriel Crist√≥bal&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2007).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/fischer-07-cv/&#34;&gt;Self-Invertible 2D Log-Gabor Wavelets&lt;/a&gt;.
  &lt;em&gt;International Journal of Computer Vision&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/fischer-07-cv/fischer-07-cv.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/fischer-07-cv/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/bicv/LogGabor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1007/s11263-006-0026-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  
















&lt;figure  id=&#34;figure-schematic-structure-of-the-primary-visual-cortex-implemented-in-the-present-study-simple-cortical-cells-are-modeled-through-log-gabor-functions-they-are-organized-in-pairs-in-quadrature-of-phase-dark-gray-circles-for-each-position-the-set-of-different-orientations-compose-a-pinwheel-large-light-gray-circles-the-retinotopic-organization-induces-that-adjacent-spatial-positions-are-arranged-in-adjacent-pinwheels-inhibition-interactions-occur-towards-the-closest-adjacent-positions-which-are-in-the-direc-tions-perpendicular-to-the-cell-preferred-orientation-and-toward-adjacent-orientations-light-red-connections-facilitation-occurs-to-wards-co-aligned-cells-up-to-a-larger-distance-dark-blue-connections&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; alt=&#34;Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections). &#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Coding static natural images using spiking event times: do neurons cooperate?</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-03-ieee/</link>
      <pubDate>Sat, 25 Sep 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-03-ieee/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-03-ieee.png&#34; alt=&#34;header&#34;&gt;














&lt;figure  id=&#34;figure-progressive-reconstruction-of-a-static-image-using-spikes-in-a-multi-scale-oriented-representation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;*Progressive reconstruction of a static image using spikes in a multi-scale oriented representation.*&#34; srcset=&#34;
               /publication/perrinet-03-ieee/v1_tiger_hue35305ea0fde0004c7038403208fa3b1_2047419_d497c01de804286ee4e54a784523f307.gif 400w,
               /publication/perrinet-03-ieee/v1_tiger_hue35305ea0fde0004c7038403208fa3b1_2047419_d64198da9191161a1914e4acb3914858.gif 760w,
               /publication/perrinet-03-ieee/v1_tiger_hue35305ea0fde0004c7038403208fa3b1_2047419_1200x1200_fit_lanczos.gif 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-03-ieee/v1_tiger_hue35305ea0fde0004c7038403208fa3b1_2047419_d497c01de804286ee4e54a784523f307.gif&#34;
               width=&#34;320&#34;
               height=&#34;240&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;Progressive reconstruction of a static image using spikes in a multi-scale oriented representation.&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature detection using spikes : the greedy approach</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-04-tauc/</link>
      <pubDate>Sun, 25 Jul 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-04-tauc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-02-sparse/</link>
      <pubDate>Thu, 25 Mar 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-02-sparse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Finding Independent Components using spikes : a natural result of Hebbian learning in a sparse spike coding scheme</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-04/</link>
      <pubDate>Sun, 25 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-04/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emergence of filters from natural scenes in a sparse spike coding scheme</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-03/</link>
      <pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-03/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coherence detection in a spiking neuron via Hebbian learning</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-02-stdp/</link>
      <pubDate>Tue, 25 Jun 2002 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-02-stdp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-02-stdp.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Network of integrate-and-fire neurons using Rank Order Coding A: how to implement spike timing dependant plasticity</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-01/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-01/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Network of integrate-and-fire neurons using Rank Order Coding B: spike timing dependant plasticity and emergence of orientation selectivity</title>
      <link>https://laurentperrinet.github.io/publication/delorme-01/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/delorme-01/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
