<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  
  









  




  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Laurent U Perrinet" />

  
  
  
    
  
  <meta name="description" content="Researcher in Computational Neuroscience" />

  
  <link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/author/jean-nicolas-jeremie/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.426e7de31186c4ced1e2b198ab5aa3ff.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140381649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-140381649-1', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  
    <link rel="alternate" href="/author/jean-nicolas-jeremie/index.xml" type="application/rss+xml" title="Novel visual computations" />
  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://laurentperrinet.github.io/author/jean-nicolas-jeremie/" />

  
  
  
  
  
  
  
  
    
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@laurentperrinet" />
    <meta property="twitter:creator" content="@laurentperrinet" />
  
  <meta property="og:site_name" content="Novel visual computations" />
  <meta property="og:url" content="https://laurentperrinet.github.io/author/jean-nicolas-jeremie/" />
  <meta property="og:title" content="Jean-Nicolas Jérémie | Novel visual computations" />
  <meta property="og:description" content="Researcher in Computational Neuroscience" /><meta property="og:image" content="https://laurentperrinet.github.io/author/jean-nicolas-jeremie/avatar_hua40b5f042c40f2b2bbad89397dde279b_69684_270x270_fill_q75_lanczos_center.jpg" />
    <meta property="twitter:image" content="https://laurentperrinet.github.io/author/jean-nicolas-jeremie/avatar_hua40b5f042c40f2b2bbad89397dde279b_69684_270x270_fill_q75_lanczos_center.jpg" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2021-10-15T00:00:00&#43;00:00" />
    
  

  



  

  





  <title>Jean-Nicolas Jérémie | Novel visual computations</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="e4fa79ffe4edef44a752f2d1b3d2458b" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.2da3b1fa37e894630bf6de39b1b694b3.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Novel visual computations</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Featured</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Events</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#grants"><span>Grants</span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    




<section id="profile-page" class="pt-5">
  <div class="container">
    
    
      
      
      
      




  










<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">

      
      
      <img class="avatar avatar-circle"
           width="270" height="270"
           src="/author/jean-nicolas-jeremie/avatar_hua40b5f042c40f2b2bbad89397dde279b_69684_270x270_fill_q75_lanczos_center.jpg" alt="Jean-Nicolas Jérémie">
      

      <div class="portrait-title">
        <h2>Jean-Nicolas Jérémie</h2>
        <h3>Phd candidate in Computational Neuroscience</h3>

        
      </div>

      <ul class="network-icon" aria-hidden="true">
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://github.com/JNJER/" target="_blank" rel="noopener" aria-label="github">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://twitter.com/JnJerem" target="_blank" rel="noopener" aria-label="twitter">
            <i class="fab fa-twitter big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://www.linkedin.com/in/jeremie-jean-nicolas-91306a1a1/" target="_blank" rel="noopener" aria-label="linkedin">
            <i class="fab fa-linkedin big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8">

    
    

    <div class="article-style">
      <h1 id="phd-student-2021-10--2024-09-bio-mimetic-agile-aerial-robots-flying-in-real-life-conditions">PhD Student (2021-10 / 2024-09): Bio-mimetic agile aerial robots flying in real-life conditions</h1>
<ul>
<li>
<p>this fellowship is part of the <a href="https://laurentperrinet.github.io/grant/anr-anb/" target="_blank" rel="noopener">AgileNeuroBot project</a></p>
</li>
<li>
<p>Institut des Neurosciences de la Timone, Aix-Marseille Université / CNRS</p>
</li>
<li>
<p>Thesis direction: <a href="https://laurentperrinet.github.io/author/laurent-u-perrinet/" target="_blank" rel="noopener">Laurent Perrinet</a> and co-direction: <a href="https://laurentperrinet.github.io/author/emmanuel-dauce/" target="_blank" rel="noopener">Emmanuel Daucé</a></p>
</li>
</ul>














<figure  id="figure-a-miniature-event-based-atis-sensor-contrary-to-a-classical-frame-based-camera-for-which-a-full-dense-image-representation-is-given-at-discrete-regularly-spaced-timings-the-event-based-camera-provides-with-events-at-the-micro-second-resolution-these-are-sparse-as-they-represent-luminance-increments-or-decrements-on-and-off-events-respectively">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/grant/anr-anb/event_driven_computations.png" alt="A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively)." loading="lazy" data-zoomable /></div>
  </div><figcaption data-pre="Figure&nbsp;" data-post=":&nbsp;" class="numbered">
      A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively).
    </figcaption></figure>
<h2 id="projet-1--fast--curious-modèles-ultra-rapides-de-recherche-visuelle">Projet #1 : &lsquo;Fast &amp; Curious&rsquo;: Modèles ultra-rapides de recherche visuelle</h2>
<h2 id="project-1--fast--curious-models-for-ultra-fast-visual-search">Project #1 : <em>&lsquo;Fast &amp; Curious&rsquo;: Models for ultra-fast visual search</em></h2>
<h3 id="mots-clés---keywords">Mots clés - Keywords</h3>
<blockquote>
<p>recherche visuelle, vision active, apprentissage profond, neurosciences computationnelles, comportement, sacades
<em>visual search, active vision, deep learning, computational neuroscience, behavior, sacades</em></p>
</blockquote>
<h3 id="description-de-la-problématique-de-recherche---project-description">Description de la problématique de recherche - Project description</h3>
<p>La détection d&rsquo;informations visuelles pertinentes dans une image nécessite un traitement ultra-rapide couvrant l&rsquo;ensemble du champ visuel. Ces tâches comprennent, par exemple, les décisions concernant la présence ou non d&rsquo;un animal dans la scène, ou la présence de proies ou de prédateurs. Il a été démontré qu&rsquo;un tel traitement ultra-rapide est effectivement à l&rsquo;œuvre, notamment dans le système visuel des primates (Thorpe et al., 1996) avec des réponses motrices de l&rsquo;ordre de 150 ms chez l&rsquo;homme (Kirchner et Thorpe, 2006). Une application de ces principes neuroscientifiques peut être établie en ce qui concerne la vision par ordinateur et appliquée en particulier aux systèmes embarqués tels que les robots aériens. Un drone, par exemple, doit être capable de distinguer une cible d&rsquo;un obstacle, une tâche qui est actuellement impossible avec une latence rapide (Gallego et al., 2019). Cependant, avec le changement de paradigme provoqué par la nouvelle révolution de l&rsquo;intelligence artificielle, et en particulier l&rsquo;apprentissage profond, nous avons accès à de nouveaux outils et méthodes. Il est relativement facile de localiser des objets d&rsquo;une certaine catégorie dans une image et donc de sélectionner un endroit par rapport à un autre. On peut citer par exemple les modèles VGG sur les bases de données Imagenet et sur les techniques de localisation telles que YOLO ou SSD. Cependant, ces techniques nécessitent l&rsquo;utilisation d&rsquo;infrastructures lourdes de type GPU qui sont difficilement transposables aux calculs embarqués. De plus, ces techniques sont basées sur le traitement statique des images alors que la plupart des flux naturels sont dynamiques et peuvent changer rapidement. Le but de la thèse est de développer de nouveaux algorithmes bio-mimétiques de recherche visuelle ultra-rapide.</p>
<p><em>Detecting relevant visual information in an image requires ultra-fast processing covering the entire visual field. These tasks include, for example, decisions about the presence or absence of an animal in the scene, or the presence of prey or predators. It has been shown that such ultra-fast processing is indeed at work, particularly in the visual system of primates (Thorpe et al., 1996) with motor responses of the order of 150 ms in humans (Kirchner and Thorpe, 2006). An application of these neuroscience principles can be established with respect to computer vision and applied in particular to embedded systems such as aerial robots. A drone, for example, must be able to distinguish a target from an obstacle, a task that is currently impossible with rapid latency (Gallego et al., 2019). However, with the paradigm shift brought about by the new artificial intelligence revolution, and in particular deep learning, we have access to new tools and methods. It is relatively easy to locate objects of a certain category in an image and thus to select one location relative to another. Examples are the VGG models on Imagenet databases and localization techniques such as YOLO or SSD. However, these techniques require the use of heavy GPU-type infrastructures that are difficult to transpose to on-board calculations. Moreover, these techniques are based on static image processing whereas most natural flows are dynamic and can change rapidly. The goal of the thesis is to develop new bio-mimetic algorithms for ultra-fast visual search.</em></p>
<h3 id="thématique--domaine--contexte">Thématique / Domaine / Contexte</h3>
<p>Pour résoudre ce problème vision par ordinateur, les neurosciences offrent de nouvelles perspectives autour d&rsquo;une approche générique de vision active. En effet le fonctionnement du système visuel des mammifères repose sur une rétine non homogène (fovéale) souligne l’importance du traitement central de l&rsquo;information visuelle. L’identité des objets dans l’environnement semble en effet nécessiter un centrage précis, réalisé à l’aide de saccades oculaires vers des cibles visuelles. Ce centrage nécessite un double traitement qui, d’un côté, traite la totalité du champ visuel périphérique pour localiser les cibles visuelles potentielles (voie du Where), et de l’autre analyse de manière détaillée le contenu visuel situé au centre de la rétine (voie du What) - (Daucé &amp; Perrinet, 2020). La localisation des cibles périphériques et l’identification du champ visuel central sont donc deux tâches distinctes, mais complémentaires, qui doivent être combinées pour analyser de manière efficace l’ensemble de la scène visuelle sous les contraintes de ressources fortes communes aux systèmes biologiques et aux robots.</p>
<p>Le domaine de la thèse croise donc vision par ordinateur, apprentissage machine (deep learning), apports de la psychophysique expérimentale et de la neurophysiolgie du système visuel.</p>
<p>Un aspect crucial de la thèse est le traitement central dû à l’organisation log-polaire du champ visuel primaire. Celle-ci est constituée d&rsquo;une organisation radiale avec forte densité de capteurs visuels au centre et très faible densité à la périphérie, codage que l’on retrouve des aires primaires jusqu’aux aux aires oculomotrices. Il est montré qu’une telle disposition spatiale des champs récepteurs permet une meilleure invariance de la réponse aux changements d’échelle (zoom) et à la rotation subjective (tilt) des objets de l&rsquo;environnement.</p>
<h3 id="objectifs">Objectifs</h3>
<p>Tous ces caractéristiques combinées du traitement visuel des mammifères se distinguent fortement des approches classiques de la vision artificielle, basées sur une analyse uniforme des pixels de la scène visuelle et de nombreuses couches de convolution. Le but est donc de montrer que dans le cas d’un traitement visuel à large champ, en environnement dynamique, et sous des contraintes matérielles et énergétiques fortes, un traitement visuel qui combine recherche de cibles et traitement central log-polaire optimise le traitement des données visuelles (via une invariance native à la translation, au zoom et au tilt), tout en atteignant des capacités de reconnaissance égales, voire supérieures à celles des algorithme de vision artificielle traditionnels. Méthode Nous allons développer un modèle de recherche visuelle ultra-rapide en étendant un modèle existant basé sur le deep learning et appliqué à un flux d&rsquo;images naturelles. La première contrainte que nous allons inclure est la transformation de l&rsquo;entrée visuelle en une entrée log-polaire afin que la réduction de volume de données permette à cet algorithmique neuro-mimétique de fonctionner sur une carte de calcul embarquée (de type Jetson JTX2). En étendant un algorithme précédent (Daucé &amp; Perrinet, 2020 September) à une tâche écologique (détecter la présence d&rsquo;un animal ou d&rsquo;une classe arbitraire d&rsquo;objet), nous allons créer un système capable de produire une séquence de saccades. À chaque saccade sera associée une détection en vision centrale ainsi que la précision assignée à cette détection.</p>
<h3 id="objectifs-de-valorisation-des-travaux-de-recherche-du-doctorant--diffusion-publication-et-confidentialité-droit-à-la-propriété-intellectuelle">Objectifs de valorisation des travaux de recherche du doctorant : diffusion, publication et confidentialité, droit à la propriété intellectuelle,&hellip;</h3>
<p>Nous anticipons donc des retombées de ce genre d&rsquo;étude dans le domaine de la vision par ordinateur mais aussi en neurosciences.</p>
<p>Tout d&rsquo;abord, cet algorithme sera capable d&rsquo;analyser un flux d&rsquo;images en temps réel et notamment des flux videos. On peut prévoir des applications en notamment pour anticiper des scènes prédéfinies (comme celles potentiellement dangereuses pour un jeune public) ou en robotique pour piloter un drone de façon autonome, par exemple pour intercepter une cible ou éviter des obstacles. Ce genre d&rsquo;applications sera construit en coopération avec le consortium réunit autour de l&rsquo;ANR AgileNeuroBot (2021/2024, <a href="https://laurentperrinet.github.io/grant/anr-anb/" target="_blank" rel="noopener">https://laurentperrinet.github.io/grant/anr-anb/</a>).</p>
<p>Les résultats attendus relatifs aux neurosciences sont nombreux. Tout d&rsquo;abord nous attendons acquérir une meilleure compréhension des mécanismes pré attentifs, notamment la prédiction des scanpaths oculaire (chemin de recherche visuelle des yeux sur l&rsquo;image) mais aussi inversement de pouvoir décoder la tache visuelle depuis ce même scanpath. Ces modèles permettront notamment la génération de stimuli optimisés par rapport aux taches afin de mieux quantifier la réponse comportementale (collaboration Valérie Goffaux, UC Louvain). Ces stimuli pourront être notamment être utilisés en neurophysiologie et nous développons actuellement un générateur de paréidolie basés sur l&rsquo;inversion des réseaux de catégorisation. Classiquement, différentes taches visuelles cognitives sont associées à différents chemins anatomiques distincts (par exemple voie dorsale versus ventrale). Notre modélisation permettra de mettre en évidence les conditions nécessaire à l&rsquo;émergence de différentes voies de traitement et ainsi une meilleure description du fonctionnement macroscopique du système visuel. Ces méthodes pourront avantageusement être reliées au traitement dans d&rsquo;autres modalités (comme la vocalisation telle qu&rsquo;elle est étudiée dans l&rsquo;équipe Banco @ INT). Collaborations envisagées C Casanova (UdeM, Canada), N Priebe (Austin, USA). Ouverture Internationale: Le projet APROVIS3D fait intervenir une collaboration en Europe (Grèce, Suisse, Espagne).</p>
<h3 id="références-bibliographiques">Références bibliographiques</h3>
<ul>
<li>Thorpe, S., Fize, D., &amp; Marlot, C. (1996). Speed of processing in the human visual system. Nature, 381 (6582), 520-522.\</li>
<li>Kirchner, H., &amp; Thorpe, S. J. (2006). Ultra-rapid object detection with saccadic eye movements: Visual processing speed revisited. Vision research, 46(11), 1762-1776.\</li>
<li>Daucé, E., Albiges, P., &amp; Perrinet, L. U. (2020). A dual foveal-peripheral visual processing model implements efficient saccade selection. Journal of Vision, 20(8), 22-22. <a href="https://hal.archives-ouvertes.fr/hal-02947410/" target="_blank" rel="noopener">https://hal.archives-ouvertes.fr/hal-02947410/</a>\</li>
<li>Daucé, E., &amp; Perrinet, L. (2020, September). Visual Search as Active Inference. In International Workshop on Active Inference (pp. 165-178). Springer, Cham. <a href="https://hal.archives-ouvertes.fr/hal-03084758/" target="_blank" rel="noopener">https://hal.archives-ouvertes.fr/hal-03084758/</a>\</li>
<li>Gallego, G., Delbruck, T., Orchard, G., Bartolozzi, C., Taba, B., Censi, A., &hellip; &amp; Scaramuzza, D. (2019). Event-based vision: A survey. arXiv preprint arXiv:1904.08405.</li>
</ul>

    </div>

    <div class="row">

      

      
      <div class="col-md-7">
        <div class="section-subheading">Education</div>
        <ul class="ul-edu fa-ul mb-0">
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Phd candidate in Computational Neuroscience, 2024</p>
              <p class="institution">Aix-Marseille Université</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Master in Neuroscience, 2021</p>
              <p class="institution">Aix-Marseille Université</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>

    

    
    
    
    <div class="article-widget content-widget-hr">
      <h3>Latest</h3>
      <ul>
        
        <li>
          <a href="/publication/jeremie-21-crs/">Ultra-fast categorization of images containing animals in vivo and in computo</a>
        </li>
        
      </ul>
    </div>
    
  </div>
</section>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  
  <p class="powered-by">
    This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&rsquo;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License</a>
Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared.
  </p>
  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.7cd6ec29d281a73c92a2958a1584aadc.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
