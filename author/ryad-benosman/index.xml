<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ryad Benosman | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/author/ryad-benosman/</link>
      <atom:link href="https://laurentperrinet.github.io/author/ryad-benosman/index.xml" rel="self" type="application/rss+xml" />
    <description>Ryad Benosman</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Thu, 13 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/author/ryad-benosman/avatar_hu2d5c77d2b84468d5dedad740d610465b_7790_270x270_fill_q75_lanczos_center.jpg</url>
      <title>Ryad Benosman</title>
      <link>https://laurentperrinet.github.io/author/ryad-benosman/</link>
    </image>
    
    <item>
      <title>A Robust Event-Driven Approach to Always-on Object Recognition</title>
      <link>https://laurentperrinet.github.io/publication/grimaldi-22-pami/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/grimaldi-22-pami/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From event-based computations to a bio-plausible Spiking Neural Network</title>
      <link>https://laurentperrinet.github.io/publication/grimaldi-21-crs/</link>
      <pubDate>Thu, 14 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/grimaldi-21-crs/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/aIt5OAleMR8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;this proceedings paper follows up the poster presented at CBMI : 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/sio-hoi-ieng/&#34;&gt;Sio-Hoi Ieng&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/ryad-benosman/&#34;&gt;Ryad Benosman&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2021).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-cbmi/&#34;&gt;A homeostatic gain control mechanism to improve event-driven object recognition&lt;/a&gt;.
   &lt;em&gt;Content-Based Multimedia Indexing (CBMI) 2021&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-03336554&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-cbmi/grimaldi-21-cbmi.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-21-cbmi/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.youtube.com/watch?v=KxX4pZKexCo&amp;amp;t=3335s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Video
 &lt;/a&gt;
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/CBMI50038.2021.9461901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;read the follow-up paper : 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/sio-hoi-ieng/&#34;&gt;Sio-Hoi Ieng&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/ryad-benosman/&#34;&gt;Ryad Benosman&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-pami/&#34;&gt;A Robust Event-Driven Approach to Always-on Object Recognition&lt;/a&gt;.
   &lt;em&gt;TechRxiv preprint&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-pami/grimaldi-22-pami.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-22-pami/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.36227/techrxiv.18003077.v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
   
   
   
     
   
   
   
   
   
     
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.techrxiv.org/articles/preprint/A_robust_event-driven_approach_to_always-on_object_recognition/18003077/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
     URL&lt;/a&gt;
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A homeostatic gain control mechanism to improve event-driven object recognition</title>
      <link>https://laurentperrinet.github.io/publication/grimaldi-21-cbmi/</link>
      <pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/grimaldi-21-cbmi/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;to be presented at the &lt;a href=&#34;https://cbmi2021.univ-lille.fr/call-for-contributions#callforpapersspecialbioinspired&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bio-inspired circuits, systems and algorithms for multimedia&lt;/a&gt; special session of the &lt;a href=&#34;https://cbmi2021.univ-lille.fr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Content-Based Multimedia Indexing (CBMI) 2021&lt;/a&gt; conference that you can &lt;a href=&#34;https://www.youtube.com/watch?v=KxX4pZKexCo&amp;amp;t=3335s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;watch on Youtube&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;this proceedings paper follows up he poster presented in : 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/sio-hoi-ieng/&#34;&gt;Sio-Hoi Ieng&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/ryad-benosman/&#34;&gt;Ryad Benosman&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2021).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-cosyne/&#34;&gt;A robust bio-inspired approach to event-driven object recognition&lt;/a&gt;.
   &lt;em&gt;Computational and Systems Neuroscience (Cosyne) 2021&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-cosyne/grimaldi-21-cosyne.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-21-cosyne/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;this proceedings paper was followed by the poster presented at CRS : 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/sio-hoi-ieng/&#34;&gt;Sio-Hoi Ieng&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/ryad-benosman/&#34;&gt;Ryad Benosman&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2021).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-crs/&#34;&gt;From event-based computations to a bio-plausible Spiking Neural Network&lt;/a&gt;.
   &lt;em&gt;Champalimaud Research Symposium (CRS21)&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-crs/grimaldi-21-crs.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-21-crs/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.youtube.com/watch?v=aIt5OAleMR8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Video
 &lt;/a&gt;
 
 
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;read the follow-up paper : 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/sio-hoi-ieng/&#34;&gt;Sio-Hoi Ieng&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/ryad-benosman/&#34;&gt;Ryad Benosman&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-pami/&#34;&gt;A Robust Event-Driven Approach to Always-on Object Recognition&lt;/a&gt;.
   &lt;em&gt;TechRxiv preprint&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-pami/grimaldi-22-pami.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-22-pami/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.36227/techrxiv.18003077.v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
   
   
   
     
   
   
   
   
   
     
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.techrxiv.org/articles/preprint/A_robust_event-driven_approach_to_always-on_object_recognition/18003077/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
     URL&lt;/a&gt;
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;Antoine Grimaldi and Laurent Perrinet received funding from the European Union ERA-NET CHIST-ERA 2018 research and innovation program under grant agreement No ANR-19-CHR3-0008-03.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A robust bio-inspired approach to event-driven object recognition</title>
      <link>https://laurentperrinet.github.io/publication/grimaldi-21-cosyne/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/grimaldi-21-cosyne/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Tomorrow Antoine Grimaldi will present our joint work on &amp;quot;A robust bio-inspired approach to event-driven object recognition&amp;quot; at &lt;a href=&#34;https://twitter.com/hashtag/cosyne2021?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#cosyne2021&lt;/a&gt; check-out the poster now &lt;a href=&#34;https://t.co/DUNQPcv1mx&#34;&gt;https://t.co/DUNQPcv1mx&lt;/a&gt; or meet him tomorrow during the poster session ! &lt;a href=&#34;https://t.co/wKTJPZbR6B&#34;&gt;pic.twitter.com/wKTJPZbR6B&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1364962423120265218?ref_src=twsrc%5Etfw&#34;&gt;February 25, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/grimaldi-21-cosyne/poster_hu42b2ccf5f39928908036dc7d8a4b9d24_7115951_74547e5c5bfc250f1c766a5b12fd761b.webp 400w,
               /publication/grimaldi-21-cosyne/poster_hu42b2ccf5f39928908036dc7d8a4b9d24_7115951_a12dc585f10bdf8434fd3c54162119d8.webp 760w,
               /publication/grimaldi-21-cosyne/poster_hu42b2ccf5f39928908036dc7d8a4b9d24_7115951_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-cosyne/poster_hu42b2ccf5f39928908036dc7d8a4b9d24_7115951_74547e5c5bfc250f1c766a5b12fd761b.webp&#34;
               width=&#34;100%&#34;
               height=&#34;555&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;see the poster online on the &lt;a href=&#34;https://app.hopin.com/events/cosyne-2021/expo/377631&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopin platform&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see a follow-up in: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/sio-hoi-ieng/&#34;&gt;Sio-Hoi Ieng&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/ryad-benosman/&#34;&gt;Ryad Benosman&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2021).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-cbmi/&#34;&gt;A homeostatic gain control mechanism to improve event-driven object recognition&lt;/a&gt;.
   &lt;em&gt;Content-Based Multimedia Indexing (CBMI) 2021&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-03336554&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-21-cbmi/grimaldi-21-cbmi.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-21-cbmi/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.youtube.com/watch?v=KxX4pZKexCo&amp;amp;t=3335s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Video
 &lt;/a&gt;
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1109/CBMI50038.2021.9461901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;read also the follow-up paper : 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/sio-hoi-ieng/&#34;&gt;Sio-Hoi Ieng&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/ryad-benosman/&#34;&gt;Ryad Benosman&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-pami/&#34;&gt;A Robust Event-Driven Approach to Always-on Object Recognition&lt;/a&gt;.
   &lt;em&gt;TechRxiv preprint&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-pami/grimaldi-22-pami.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-22-pami/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.36227/techrxiv.18003077.v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
   
   
   
     
   
   
   
   
   
     
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.techrxiv.org/articles/preprint/A_robust_event-driven_approach_to_always-on_object_recognition/18003077/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
     URL&lt;/a&gt;
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;Antoine Grimaldi and Laurent Perrinet received funding from the European Union ERA-NET CHIST-ERA 2018 research and innovation program under grant agreement No ANR-19-CHR3-0008-03.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ANR AgileNeuRobot (2021/2024)</title>
      <link>https://laurentperrinet.github.io/grant/anr-anr/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-anr/</guid>
      <description>&lt;!-- youtube-dl https://www.youtube.com/watch\?v\=36CTDiJjQ8I --&gt;














&lt;figure  id=&#34;figure-an-unmanned-aerial-vehicle-uav-flying-autonomously-in-a-cluttered-environment-would-require-the-agility-to-navigate-rapidly-by-detecting-as-fast-as-possible-potential-obstacles-as-represented-here-by-the-collision-zone-given-a-cruising-speed-associated-to-slow-or-fast-latencies-respectively-red-and-blue-shaded-areas-this-project-will-provide-with-a-novel-neuromorphic-architecture-designed-to-meet-these-requirements-thanks-to-an-event-based-two-way-processing&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An Unmanned aerial vehicle (UAV) flying autonomously in a cluttered environment would require the agility to navigate rapidly by detecting as fast as possible potential obstacles, as represented here by the collision zone, given a cruising speed, associated to slow or fast latencies (respectively red and blue shaded areas). This project will provide with a novel neuromorphic architecture designed to meet these requirements thanks to an event-based, two-way processing.&#34; srcset=&#34;
               /grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_11989bf1509db296ce4e56c0a5512eaf.webp 400w,
               /grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_bd10ea7c74a93e97c1d5129c215dafeb.webp 760w,
               /grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_11989bf1509db296ce4e56c0a5512eaf.webp&#34;
               width=&#34;760&#34;
               height=&#34;243&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      An Unmanned aerial vehicle (UAV) flying autonomously in a cluttered environment would require the agility to navigate rapidly by detecting as fast as possible potential obstacles, as represented here by the collision zone, given a cruising speed, associated to slow or fast latencies (respectively red and blue shaded areas). This project will provide with a novel neuromorphic architecture designed to meet these requirements thanks to an event-based, two-way processing.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;fiche-didentité&#34;&gt;Fiche d&amp;rsquo;identité&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Acronyme : AgileNeuRobot (ANR-20-CE23-0021)&lt;/li&gt;
&lt;li&gt;Titre : Robots aériens agiles bio-mimetiques pour le vol en conditions réelles&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Title : Bio-mimetic agile aerial robots flying in real-life conditions&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;CES : CE23 - Intelligence Artificielle / Instrument de financement : Projet de recherche collaborative (PRC) / Catégorie R&amp;amp;D : Recherche fondamentale&lt;/li&gt;
&lt;li&gt;Coordinateur Scientifique : PERRINET Laurent (UMR7289)&lt;/li&gt;
&lt;li&gt;Durée: 3 ans, à partir du 1er mars 2021 - 1er décembre 2024&lt;/li&gt;
&lt;li&gt;Budget total: 435 k€&lt;/li&gt;
&lt;li&gt;Responsables Scientifiques : Stéphane Viollet (BioRobotique, Inst Sciences Mouvement), Ryad Benosman (Inst de la Vision ) | Laurent Perrinet (NeOpTo, Inst Neurosciences de la Timone, coordinateur)&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-a-miniature-event-based-atis-sensor-contrary-to-a-classical-frame-based-camera-for-which-a-full-dense-image-representation-is-given-at-discrete-regularly-spaced-timings-the-event-based-camera-provides-with-events-at-the-micro-second-resolution-these-are-sparse-as-they-represent-luminance-increments-or-decrements-on-and-off-events-respectively&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively).&#34; srcset=&#34;
               /grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_72244f8a28585f1ccab85885fec7300e.webp 400w,
               /grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_18ec5d453856fc5dda035b77ef2a226c.webp 760w,
               /grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_72244f8a28585f1ccab85885fec7300e.webp&#34;
               width=&#34;760&#34;
               height=&#34;217&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;résumé&#34;&gt;Résumé&lt;/h2&gt;
&lt;p&gt;Des robots aériens autonomes seraient des outils essentiels dans les opérations de recherche et de sauvetage. Toutefois, voler dans des environnements complexes exige un haut niveau d&amp;rsquo;agilité, ce qui implique par exemple la capacité de déclencher des manœuvres agressives pour esquiver les obstacles: Les caméras et algorithmes d&amp;rsquo;intelligence artificielle conventionnels n&amp;rsquo;ont pas ces capacités. Dans ce projet, nous proposerons une solution associant de manière bio-inspirée une dynamique rapide de détection visuelle et de stabilisation. Nous intégrerons ces différents aspects dans un système neuromorphique événementiel de bout en bout. La clé de cette approche est l&amp;rsquo;optimisation des délais du système par traitement prédictif. Ceci permettra de voler indépendamment, sans aucune intervention de l&amp;rsquo;utilisateur. Notre objectif à plus long terme est de satisfaire ces besoins avec un minimum d&amp;rsquo;énergie et de fournir des solutions novatrices aux défis des algorithmes traditionnels d&amp;rsquo;IA.&lt;/p&gt;














&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-atis-until-the-rotors-the-camera-processor-and-motor-units-each-represents-respectively-multi-channel-feature-maps-c_i-an-estimate-of-the-depth-of-field-p-and-a-navigation-map-for-instance-time-of-contacts-on-a-polar-map-m-compared-to-a-discrete-time-pipeline-we-will-design-an-integrated-back-to-back-event-driven-system-based-on-a-fast-two-way-processing-between-the-c-p-and-m-units-event-driven-feed-forward-and-feed-back-communications-are-denoted-respectively-in-yellow-black-and-red-notice-the-attention-module-a-from-p-to-c-and-the-feed-back-of-navigation-information-from-m-and-the-imu-to-p&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Our system is divided into 3 units to process visual inputs (ATIS) until the rotors: the ***C***amera, ***P***rocessor and ***M***otor units. Each represents respectively multi-channel feature maps ($C_i$), an estimate of the depth-of-field ($P$) and a navigation map, for instance time-of-contacts on a polar map ($M$). Compared to a discrete-time pipeline, we will design an integrated, back-to-back event-driven system based on a fast, two-way processing between the ***C***, ***P*** and ***M*** units. Event-driven, feed-forward and feed-back communications are denoted respectively in yellow, black and red. Notice the attention module $A$ from $P$ to $C$ and the feed-back of navigation information from $M$ and the IMU to $P$.&#34; srcset=&#34;
               /grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_f200ed0437603f140c29fa06aa052552.webp 400w,
               /grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_b8c0f7df6c48c1940e9a5c9a53765cfa.webp 760w,
               /grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_f200ed0437603f140c29fa06aa052552.webp&#34;
               width=&#34;760&#34;
               height=&#34;226&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Our system is divided into 3 units to process visual inputs (ATIS) until the rotors: the &lt;em&gt;&lt;strong&gt;C&lt;/strong&gt;&lt;/em&gt;amera, &lt;em&gt;&lt;strong&gt;P&lt;/strong&gt;&lt;/em&gt;rocessor and &lt;em&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/em&gt;otor units. Each represents respectively multi-channel feature maps ($C_i$), an estimate of the depth-of-field ($P$) and a navigation map, for instance time-of-contacts on a polar map ($M$). Compared to a discrete-time pipeline, we will design an integrated, back-to-back event-driven system based on a fast, two-way processing between the &lt;em&gt;&lt;strong&gt;C&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;P&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/em&gt; units. Event-driven, feed-forward and feed-back communications are denoted respectively in yellow, black and red. Notice the attention module $A$ from $P$ to $C$ and the feed-back of navigation information from $M$ and the IMU to $P$.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Autonomous aerial robots would be essential tools in search and rescue operations. But flying in complex environments requires a high level of agility, which implies the ability to initiate aggressive maneuvers to avoid obstacles: Conventional AI cameras and algorithms do not have these capabilities. In this project, we propose a solution that will integrate bio-inspired rapid visual detection and stabilization dynamics into an end-to-end event based neuromorphic system. The key to this approach will be the optimization of delays through predictive processing. This will allow these robots to fly independently, without any user intervention. Our longer-term goal is to meet the requirements with very little power and provide innovative solutions to the challenges of traditional AI algorithms.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work was supported by ANR project &amp;ldquo;AgileNeuRobot&amp;rdquo; N° ANR-20-CE23-0021.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
