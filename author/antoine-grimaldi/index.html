<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
    <meta name="google-site-verification" content="6jjjboKLYp2QzywShNDvZkAfb0Yc8SDtfFzOonP3o0o" />
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Laurent U Perrinet" />

  
  
  
    
  
  <meta name="description" content="Scientific website of Laurent Udo Perrinet." />

  
  <link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/author/antoine-grimaldi/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.8722fd0f5cb8e5cb38d3b73367786cc1.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140381649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-140381649-1', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  


  




  
  
  

  
    <link rel="alternate" href="/author/antoine-grimaldi/index.xml" type="application/rss+xml" title="Novel visual computations" />
  

  

  <link rel="icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://laurentperrinet.github.io/author/antoine-grimaldi/" />

  
  
  
  
  
  
  
  
    
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@laurentperrinet" />
    <meta property="twitter:creator" content="@laurentperrinet" />
  
  <meta property="og:site_name" content="Novel visual computations" />
  <meta property="og:url" content="https://laurentperrinet.github.io/author/antoine-grimaldi/" />
  <meta property="og:title" content="Antoine Grimaldi | Novel visual computations" />
  <meta property="og:description" content="Scientific website of Laurent Udo Perrinet." /><meta property="og:image" content="https://laurentperrinet.github.io/author/antoine-grimaldi/avatar_hu85406bb2d5f7db2dce1cab01b4e48063_27520_270x270_fill_q75_lanczos_center.jpg" />
    <meta property="twitter:image" content="https://laurentperrinet.github.io/author/antoine-grimaldi/avatar_hu85406bb2d5f7db2dce1cab01b4e48063_27520_270x270_fill_q75_lanczos_center.jpg" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2023-06-15T00:00:00&#43;00:00" />
    
  

  



  

  <link rel="me" href="https://neuromatch.social/@laurentperrinet" />


  <title>Antoine Grimaldi | Novel visual computations</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="c2f0a6a942593f19f40fafaac178a1ee" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Featured</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Events</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#people"><span>People</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#grants"><span>Grants</span></a>
          </li>

          
          

          

          
          
          
            
              
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    




<section id="profile-page" class="pt-5">
  <div class="container">
    
    
      
      
      
      




  










<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">

      
      
      <img class="avatar avatar-circle"
           width="270" height="270"
           src="/author/antoine-grimaldi/avatar_hu85406bb2d5f7db2dce1cab01b4e48063_27520_270x270_fill_q75_lanczos_center.jpg" alt="Antoine Grimaldi">
      

      <div class="portrait-title">

        <h2>Antoine Grimaldi</h2>

        <h3>Phd candidate in Computational Neuroscience</h3>

        
      </div>

      <ul class="network-icon" aria-hidden="true">
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://github.com/AntoineGrimaldi" target="_blank" rel="noopener" aria-label="github">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="http://antoinegrimaldi.fr/" target="_blank" rel="noopener" aria-label="external-link-alt">
            <i class="fas fa-external-link-alt big-icon"></i>
          </a>
        </li>
        
        
        
        
        
        
        
        
          
        
        <li>
          <a href="https://www.researchgate.net/profile/Antoine-Grimaldi-2" target="_blank" rel="noopener" aria-label="researchgate">
            <i class="ai ai-researchgate big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://twitter.com/A_Grismaldi" target="_blank" rel="noopener" aria-label="twitter">
            <i class="fab fa-twitter big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8">

    
    

    <div class="article-style">
      <h1 id="ultra-fast-vision-using-spiking-neural-networks-phd-position-2020-09--2024-03">&ldquo;Ultra-fast vision using Spiking Neural Networks&rdquo; (PhD position, 2020-09 / 2024-03)</h1>
<ul>
<li>
<p><a href="https://laurentperrinet.github.io/post/2020-06-30_phd-position/" target="_blank" rel="noopener">APROVIS3D grant (ANR-19-CHR3-0008-03)</a> at Aix-Marseille Université</p>
</li>
<li>
<p>Keywords: Vision, Spiking Neural Networks, Bio-Inspired Computer Vision</p>
</li>
<li>
<p>Thesis director: Dr. Laurent PERRINET, Institut de Neurosciences de la Timone (INT)</p>
</li>
</ul>
<h2 id="main-publications">Main publications</h2>
<ul>
<li>









  


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2023).
  <a href="/publication/grimaldi-23-bc/">Learning heterogeneous delays in a layer of spiking neurons for fast motion detection</a>.
  <em>Submitted</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-23-bc/grimaldi-23-bc.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-23-bc/cite.bib">
  Cite
</a>














</p>

  
  
</div>


</li>
<li>









  


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/amelie-gruel/">Amélie Gruel</a></span>, <span >
      <a href="/author/camille-besnainou/">Camille Besnainou</a></span>, <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/jean-martinet/">Jean Martinet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2022).
  <a href="/publication/grimaldi-22-polychronies/">Precise spiking motifs in neurobiological and neuromorphic data</a>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-22-polychronies/grimaldi-22-polychronies.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-22-polychronies/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3390/brainsci13010068" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>


</li>
<li>









  


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/sio-hoi-ieng/">Sio-Hoi Ieng</a></span>, <span >
      <a href="/author/ryad-benosman/">Ryad Benosman</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2022).
  <a href="/publication/grimaldi-23/">A Robust Event-Driven Approach to Always-on Object Recognition</a>.
  <em>TechRxiv preprint</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-23/grimaldi-23.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-23/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.36227/techrxiv.18003077.v1" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.techrxiv.org/articles/preprint/A_robust_event-driven_approach_to_always-on_object_recognition/18003077/1" target="_blank" rel="noopener">
    URL</a>

</p>

  
  
</div>


</li>
</ul>
<h2 id="detailed-description-ultra-fast-vision-using-spiking-neural-networks">Detailed description: &ldquo;Ultra-fast vision using Spiking Neural Networks&rdquo;</h2>
<p>Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. Crucially, given an equal constraint on energy consumption, these algorithms are relatively slow compared to biological vision. It is believed that one major factor of this rapidity is the fact that visual information is represented by short pulses (spikes) at analog – not discrete – times (<a href="#Paugam12">Paugam and Bohte, 2012</a>). However, most classical computer vision algorithms rely on such frame-based approaches. One solution to overcome their limitations is to use event-based representations, but these still lack in practice, and their high potential is largely underexploited. Inspired by biology, the project addresses the scientific question of developing a low-power sensing architecture for the processing of visual scenes, able to function on analog devices without a central clock and aimed at being validated in real-life situations. More specifically, the project will develop new paradigms for biologically inspired computer vision (<a href="#Cristobal15">Cristobal, Keil and Perrinet, 2015</a>), from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes.</p>
<p><strong>In this doctoral project, we propose to address major limitations of classical computer vision by implementing specific dynamical features of cortical circuits: <em>spiking neural networks</em> (<a href="#Perrinet04">Perrinet, Thorpe and Samuelides, 2004</a>; <a href="#Lagorce16">Lagorce et al., 2018</a>), <em>lateral diffusion of neural information</em> (<a href="#Chavane2000">Chavane et al., 2011</a>; <a href="#muller2018cortical">Muller et al., 2018</a>) and <em>dynamic neuronal association fields</em> (<a href="#Fr%c3%a9gnac2012">Frégnac et al., 2012</a>; <a href="#Fr%c3%a9gnac2016">Frégnac et al., 2016</a>; <a href="#gerard2016synaptic">Gerard-Mercier et al., 2016</a>)</strong>. One starting point is to use event-based cameras <a href="#Dupeyroux18">(Dupeyroux et al., 2018)</a> and to extend results of self-supervised learning that we have obtained on static, natural images (<a href="#BoutinFranciosiniChavaneRuffierPerrinet20">Boutin et al., 2020</a>) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the &ldquo;association field&rdquo; described at the psychophysical (<a href="#Field1993">Field et al., 1993</a>), spiking (<a href="#Li2002">Li and Gilbert, 2002</a>) and synaptic (<a href="#gerard2016synaptic">Gerard-Mercier et al., 2016</a>) levels. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (<a href="#Voges12">Voges and Perrinet, 2012</a>). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). It is not well understood, but probably decisive for ultra-fast vision, how recurrent cortico-cortical loops add a level of distributed top-down complexity in the feed-forward stream of information which participates to the ultra-fast integration of sensory input and perceptual context (<a href="#Keller2019">Keller et al., 2019</a>). Coupled with the dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for defining ultra-fast vision algorithms.</p>
<h2 id="research-context">Research context</h2>
<p>The thesis will be carried out in the team &ldquo;NEuronal OPerations in visual TOpographic maps&rdquo; (NeOpTo) within the <a href="http://www.int.univ-amu.fr/?lang=en" target="_blank" rel="noopener">Institut de Neurosciences de la Timone</a> in <a href="https://en.wikipedia.org/wiki/Marseille" target="_blank" rel="noopener">Marseille</a>, a welcoming and lively town by the Mediterranean sea in the south of France. The research team is led by F. Chavane (DR2, CNRS) and currently hosts 4 permanent staff, 3 post-docs and 4 PhD students. The research themes of the team are focused on neuronal operations within visual cortical maps. Indeed, along the cortical hierarchy, low-level features such as the position and orientation of the visual stimulus (but also auditory tone, somatosensory touch, etc&hellip;) but also higher-level features (such as faces, viewpoints of objects, etc&hellip;) are represented topographically on the cortical surface.</p>
<p>This work will be conducted in direct collaboration with <a href="http://i3s.unice.fr/jmartinet/en" target="_blank" rel="noopener">Jean Martinet</a> who will co-supervise the thesis. We will develop these algorithms in collaboration with <a href="https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;hl=fr" target="_blank" rel="noopener">Ryad Benosman</a> (Université Pierre et Marie Curie) and <a href="https://scholar.google.com/citations?user=iIGoymcAAAAJ" target="_blank" rel="noopener">Stéphane Viollet</a> (équipe biorobotique, Institut des Sciences du Mouvement).</p>
<h2 id="fr-description-du-sujet-de-thèse">FR: Description du sujet de thèse</h2>
<p>La vision biologique est étonnamment efficace. Pour tirer parti de cette efficacité, l&rsquo;apprentissage profond et les réseaux neuronaux convolutionnels (CNN) ont récemment permis de réaliser de grandes avancées en matière de vision artificielle par ordinateur. Cependant, ces algorithmes sont aujourd&rsquo;hui confrontés à de multiples défis : les architectures apprises sont souvent peu interprétables, sont démesurément gourmandes en énergie, n&rsquo;intègrent généralement pas les informations contextuelles qui semblent parfaitement adaptées à la vision biologique et à la perception humaine. Aussi ces algorithmes sont relativement lents -à consommation énergétique égale- par rapport à la vision biologique. On pense qu&rsquo;un facteur majeur de cette rapidité est le fait que l&rsquo;information est représentée par de courtes impulsions à des moments analogiques - et non discrets. Toutefois, les algorithmes de vision par ordinateur utilisant une telle représentation dans des réseaux de neurones impulsionnels font encore défaut dans la pratique, et son important potentiel est largement sous-exploité. Ce projet, qui est inspiré de la biologie, aborde la question scientifique du développement d&rsquo;une architecture ultra-rapide de détection et de traitement de scènes visuelles, fonctionnant sur des appareils sans horloge centrale, et visant à valider ce genre d&rsquo;algorithmes événementiels dans des situations réelles. Plus spécifiquement, le projet développera de nouveaux paradigmes pour une vision d&rsquo;inspiration biologique, de la détection au traitement, afin d&rsquo;aider des machines telles que les robots aériens autonomes (UAV), les véhicules autonomes ou les robots à acquérir une compréhension de haut niveau des scènes visuelles.</p>
<h2 id="fr-contexte-de-travail">FR: Contexte de travail</h2>
<p>La thèse sera effectuée dans l&rsquo;équipe &ldquo;NEuronal OPerations in visual TOpographic maps&rdquo; (NeOpTo) au sein de l&rsquo;Institut de Neurosciences de la Timone (INT). L&rsquo;équipe de recherche est dirigée par F. Chavane (DR2, CNRS) et accueille actuellement 4 personnels permanents, 3 post-doctorants et 4 doctorants. Les thématiques de recherche de l&rsquo;équipe sont centrées sur les opérations neuronales au sein de cartes corticales visuelles. En effet, le long de la hiérarchie corticale, les caractéristiques de bas niveau telles que la position, l’orientation du stimulus visuel (mais aussi la tonalité auditive, le toucher somatosensoriel, etc&hellip;) mais aussi les caractéristiques de niveau supérieur (telles que les visages, les points de vue d’objets, etc&hellip;) sont représentées topographiquement sur la surface corticale.</p>
<p>Cette thèse sera menée en collaboration directe avec <a href="http://i3s.unice.fr/jmartinet/en" target="_blank" rel="noopener">Jean Martinet</a> qui co-supervisera cette thèse. Nous développerons ces algorithmes en collaboration avec <a href="https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;hl=fr" target="_blank" rel="noopener">Ryad Benosman</a> (Université Pierre et Marie Curie) et <a href="https://scholar.google.com/citations?user=iIGoymcAAAAJ" target="_blank" rel="noopener">Stéphane Viollet</a> (équipe biorobotique, Institut des Sciences du Mouvement).</p>
<h1 id="references">References</h1>
<ul>
<li>
<p><a name="BoutinFranciosiniChavaneRuffierPerrinet20">Boutin, Victor, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, and Laurent U Perrinet. (2019). </a> &ldquo;<a href="https://arxiv.org/abs/1902.07651" target="_blank" rel="noopener">Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.</a>&rdquo; <em>arXiv</em></p>
</li>
<li>
<p><a name="Dupeyroux18">Julien Dupeyroux, Victor Boutin, Julien R Serres, Laurent U Perrinet, Stéphane Viollet. (2018). </a> &ldquo;<a href="https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/" target="_blank" rel="noopener">M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation.</a>&rdquo; <em>ISCAS</em></p>
</li>
<li>
<p><a name="Chavane2011">Chavane, F., Sharon, D., Jancke, D., Marre, O., Frégnac, Y. and Grinvald, A. (2011). </a> &ldquo;<a href="https://doi.org/10.1016/S0928-4257%2800%2901096-2" target="_blank" rel="noopener">Lateral spread of orientation selectivity in V1 is controlled by intracortical cooperativity.</a>&rdquo; <em>Journal of Physiology Paris</em> 94 (5-6): 333&ndash;42.</p>
</li>
<li>
<p><a name="Cristobal15">Gabriel Cristóbal, Laurent U Perrinet, Matthias S Keil (2015). </a> &ldquo;<a href="https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/" target="_blank" rel="noopener">Biologically Inspired Computer Vision.</a>&rdquo; <em>Wiley</em>.</p>
</li>
<li>
<p><a name="Field1993">Field, D.J., Hayes, A. and Hess, R.F. (1993). </a> &ldquo;<a href="https://doi.org/10.1016/0042-6989%2893%2990156-Q" target="_blank" rel="noopener">Contour integration by the human visual system: Evidence for a local “association field”.</a>&rdquo; <em>Vision Research</em> 33 (2), pp. 173-193.</p>
</li>
<li>
<p><a name="gerard2016synaptic">Gerard-Mercier, Florian, Pedro V Carelli, Marc Pananceau, Xoana G Troncoso, and Yves Frégnac. (2016). </a> &ldquo;<a href="https://www.jneurosci.org/content/36/14/3925" target="_blank" rel="noopener">Synaptic Correlates of Low-Level Perception in V1.</a>&rdquo; <em>Journal of Neuroscience</em> 36 (14): 3925&ndash;42.</p>
</li>
<li>
<p><a name="Keller2019">Keller, A., Roth, M.M. and Scanziani, M. (2019). </a> 2019. &ldquo;<a href="https://www.abstractsonline.com/pp8/#!/7883/presentation/65856" target="_blank" rel="noopener">The feedback receptive field of neurons in the mammalian primary visual cortex.</a>&rdquo; <em>American Society for Neuroscience Abstracts</em>, 403.13. Chicago.</p>
</li>
<li>
<p><a name="Lagorce16">Lagorce, X., Orchard, G., Galluppi, F., Shi, B. E., &amp; Benosman, R. B.</a> (2016). &ldquo;<a href="https://www.neuromorphic-vision.com/public/publications/1/publication.pdf" target="_blank" rel="noopener">HOTS: a hierarchy of event-based time-surfaces for pattern recognition.</a>&rdquo; <em>IEEE transactions on pattern analysis and machine intelligence</em>.</p>
</li>
<li>
<p><a name="Li2002">Li W, Piëch V, Gilbert CD</a> (2006). &ldquo;<a href="http://www.paper.edu.cn/scholar/showpdf/MUz2UN2INTA0eQxeQh" target="_blank" rel="noopener">Contour saliency in primary visual cortex.</a>&rdquo; <em>Neuron</em>, 50(6):951–962.</p>
</li>
<li>
<p><a name="muller2018cortical">Muller, Lyle, Frédéric Chavane, John Reynolds, and Terrence J Sejnowski. </a> (2018). &ldquo;<a href="https://papers.cnl.salk.edu/PDFs/Cortical%20travelling%20waves_%20mechanisms%20and%20computational%20principles.%202018-4515.pdf" target="_blank" rel="noopener">Cortical Travelling Waves: Mechanisms and Computational Principles.</a>&rdquo; <em>Nature Reviews Neuroscience</em> 19 (5): 255.</p>
</li>
<li>
<p><a name="Paugam12">Hélène Paugam-Moisy, Sander M. Bohte. </a> (2012). &ldquo;Computing with Spiking Neuron Networks.&rdquo; <em>Handbook of Natural Computing</em>, Springer-Verlag, pp.335-376, 2012</p>
</li>
<li>
<p><a name="Perrinet04">Laurent U Perrinet, Manuel Samuelides, Simon J Thorpe. </a> (2004). <a href="https://laurentperrinet.github.io/publication/perrinet-03-ieee/" target="_blank" rel="noopener">&ldquo;Coding static natural images using spiking event times: do neurons cooperate?&rdquo;</a> <em>IEEE Transactions on Neural Networks</em>.</p>
</li>
<li>
<p><a name="Tang18">Tang, Hanlin, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. </a> (2018). &ldquo;<a href="https://doi.org/10.1073/pnas.1719397115" target="_blank" rel="noopener">Recurrent computations for visual pattern completion.</a>&rdquo; <em>Proceedings of the National Academy of Sciences</em> 115 (35) 8835-8840.</p>
</li>
<li>
<p><a name="Voges12">Voges, Nicole, and Laurent U Perrinet.</a> (2012). &ldquo;<a href="https://doi.org/10.3389/fncom.2012.00041" target="_blank" rel="noopener">Complex Dynamics in Recurrent Cortical Networks Based on Spatially Realistic Connectivities.</a>&rdquo; <em>Frontiers in Computational Neuroscience</em> 6.</p>
</li>
</ul>

    </div>

    <div class="row">

      

      
      <div class="col-md-7">
        <div class="section-subheading">Education</div>
        <ul class="ul-edu fa-ul mb-0">
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Phd candidate in Computational Neuroscience, 2023</p>
              <p class="institution">Aix-Marseille Université</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>

    

    
    
    
    <div class="article-widget content-widget-hr">
      <h3>Latest</h3>
      <ul>
        
        <li>
          <a href="/publication/grimaldi-23-bc/">Learning heterogeneous delays in a layer of spiking neurons for fast motion detection</a>
        </li>
        
        <li>
          <a href="/publication/gruel-23-bc/">Stakes of Neuromorphic Foveation: a promising future for embedded event cameras</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-23-gdr/">Learning heterogeneous delays of spiking neurons for motion detection</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-22-polychronies/">Precise spiking motifs in neurobiological and neuromorphic data</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-22-icip/">Learning heterogeneous delays of spiking neurons for motion detection</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-22-fens/">Learning heterogeneous delays of spiking neurons for motion detection</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-22-areadne/">Decoding spiking motifs using neurons with heterogeneous delays</a>
        </li>
        
        <li>
          <a href="/talk/2022-06-19-neuro-vision-heterogeneous/">Learning heterogeneous delays of Spiking Neurons for motion detection</a>
        </li>
        
        <li>
          <a href="/talk/2022-05-19-centuri-day/">Polychrony detection using heterogeneous delays</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-23/">A Robust Event-Driven Approach to Always-on Object Recognition</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-21-crs/">From event-based computations to a bio-plausible Spiking Neural Network</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-21-cbmi/">A homeostatic gain control mechanism to improve event-driven object recognition</a>
        </li>
        
        <li>
          <a href="/publication/grimaldi-21-cosyne/">A robust bio-inspired approach to event-driven object recognition</a>
        </li>
        
      </ul>
    </div>
    
  </div>
</section>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2023 <a rel="me" href="https://neuromatch.social/@laurentperrinet">Laurent U Perrinet</a>. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.5a3a4e7cbc7b4e121b2d29312cf8ad59.js"></script>

    
    
    
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    
      <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d66c8b3b4ad0f66a62428f6bc7cf477d.js"></script>

    
    
      <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js" type="module"></script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
