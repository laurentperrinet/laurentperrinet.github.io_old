<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent &amp; Upcoming Talks | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/talk/</link>
      <atom:link href="https://laurentperrinet.github.io/talk/index.xml" rel="self" type="application/rss+xml" />
    <description>Recent &amp; Upcoming Talks</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Sun, 19 Jun 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_512x512_fill_lanczos_center_3.png</url>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://laurentperrinet.github.io/talk/</link>
    </image>
    
    <item>
      <title>Learning heterogeneous delays of Spiking Neurons for motion detection</title>
      <link>https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-heterogeneous/</link>
      <pubDate>Sun, 19 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-heterogeneous/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;During the &lt;a href=&#34;https://twitter.com/hashtag/CVPR2022?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#CVPR2022&lt;/a&gt;-&lt;a href=&#34;https://twitter.com/hashtag/NeuroVision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#NeuroVision&lt;/a&gt; workshop about ¬´¬†What can computer vision learn from visual neuroscience?¬†¬ª, &lt;a href=&#34;https://twitter.com/A_Grismaldi?ref_src=twsrc%5Etfw&#34;&gt;@A_Grismaldi&lt;/a&gt; will talk today about ¬´¬†Learning hetero-synaptic delays of Spiking Neurons for motion detection¬†¬ª shows how to learn spike motifs !&lt;a href=&#34;https://t.co/95HGSGUOUy&#34;&gt;https://t.co/95HGSGUOUy&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1538417555911720963?ref_src=twsrc%5Etfw&#34;&gt;June 19, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;for a follow-up, check out  









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/camille-besnainou/&#34;&gt;Camille Besnainou&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/hugo-ladret/&#34;&gt;Hugo Ladret&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2022).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-icip/&#34;&gt;Learning heterogeneous delays of spiking neurons for motion detection&lt;/a&gt;.
  &lt;em&gt;Proceedings of ICIP 2022&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-icip/grimaldi-22-icip.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/grimaldi-22-icip/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














  
  
  
    
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://2022.ieeeicip.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    URL&lt;/a&gt;

&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Retinotopic mapping improves the reliability of image classification</title>
      <link>https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-retinotopic/</link>
      <pubDate>Sun, 19 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-retinotopic/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Follows a previous work 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/pierre-albiges/&#34;&gt;Pierre Albig√®s&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34;&gt;A dual foveal-peripheral visual processing model implements efficient saccade selection&lt;/a&gt;.
  &lt;em&gt;Journal of Vision&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/725879v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/dauce-20/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/WhereIsMyMNIST&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1167/jov.20.8.22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-retinotopic/2022-06-10_Jeremie-etal-NeuroVision_video-abstract.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;ul&gt;
&lt;li&gt;for a follow-up, check out  









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/jean-nicolas-jeremie/&#34;&gt;Jean-Nicolas J√©r√©mie&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2022).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/jeremie-22-fens/&#34;&gt;Ultra-rapid visual search in natural images using active deep learning&lt;/a&gt;.
  &lt;em&gt;Proceedings of the FENS Forum 2022&lt;/em&gt;.
  
  &lt;p&gt;








  





&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/jeremie-22-fens/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Contributions of neuroscience to the detection and localization of objects in visual inputs</title>
      <link>https://laurentperrinet.github.io/talk/2022-06-14-mir-symposium/</link>
      <pubDate>Tue, 14 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2022-06-14-mir-symposium/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;for visual search see: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/pierre-albiges/&#34;&gt;Pierre Albig√®s&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2020).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34;&gt;A dual foveal-peripheral visual processing model implements efficient saccade selection&lt;/a&gt;.
   &lt;em&gt;Journal of Vision&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/725879v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/dauce-20/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/WhereIsMyMNIST&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1167/jov.20.8.22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for retinotopy, see: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/jean-nicolas-jeremie/&#34;&gt;Jean-Nicolas J√©r√©mie&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-retinotopic/&#34;&gt;Retinotopic mapping improves the reliability of image classification&lt;/a&gt;.
   &lt;em&gt;NeuroVision Workshop in conjunction with CVPR 2022&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-retinotopic/2022-06-19-neuro-vision-retinotopic.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/talk/2022-06-19-neuro-vision-retinotopic/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
   
   
     
   
   
   
   
   
     
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://sites.google.com/uci.edu/neurovision2022/schedule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
     URL&lt;/a&gt;
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for event-based computations, see: 
 
 
 
 
 
 
 
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Polychrony detection using heterosynaptic delays</title>
      <link>https://laurentperrinet.github.io/talk/2022-05-19-centuri-day/</link>
      <pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2022-05-19-centuri-day/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Follow this future presentations 









 
 
 
 
 
 
 
 
 
 
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Glad to meet the &lt;a href=&#34;https://twitter.com/centuri_ls?ref_src=twsrc%5Etfw&#34;&gt;@centuri_ls&lt;/a&gt; crowd at the &lt;a href=&#34;https://twitter.com/hashtag/CENTURIday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#CENTURIday&lt;/a&gt; ! With &lt;a href=&#34;https://twitter.com/A_Grismaldi?ref_src=twsrc%5Etfw&#34;&gt;@A_Grismaldi&lt;/a&gt; &lt;a href=&#34;https://t.co/r4633Vzg4F&#34;&gt;https://t.co/r4633Vzg4F&lt;/a&gt; &lt;a href=&#34;https://t.co/GbOGKhB6zA&#34;&gt;https://t.co/GbOGKhB6zA&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1527604282043813888?ref_src=twsrc%5Etfw&#34;&gt;May 20, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>R√©seaux de neurones artificiels et apprentissage machine appliqu√©s √† la compr√©hension de la vision</title>
      <link>https://laurentperrinet.github.io/talk/2022-03-23-ue-neurosciences-computationnelles/</link>
      <pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2022-03-23-ue-neurosciences-computationnelles/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;O√π: Salle PHY51 - Marseille (France)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quoi: &lt;a href=&#34;https://ametice.univ-amu.fr/course/view.php?id=89069&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master 1 Neurosciences et Sciences Cognitives&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;R√©seaux neuronaux artificiels pour la vision&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Mercredi 23/03/2022 de 9h-12h&lt;/li&gt;
&lt;li&gt;Introduction aux Neurosciences de la Vision&lt;/li&gt;
&lt;li&gt;R√©seaux de neurones artificiels et apprentissage machine&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;em&gt;Neurones impulsionnels et mod√®les des fonctions visuelles&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Mercredi 23/03/2022 de 13h30-16h30&lt;/li&gt;
&lt;li&gt;TP via notebook&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Statistics of the sparse representations of natural images</title>
      <link>https://laurentperrinet.github.io/talk/2022-03-22-siam-is-22/</link>
      <pubDate>Tue, 22 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2022-03-22-siam-is-22/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see previous work: &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mini-symposium-learning-from-vision-efficient-representation-sparse-coding-and-modelling&#34;&gt;Mini-Symposium &amp;ldquo;Learning from vision: Efficient representation, sparse coding, and modelling&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;Although recent years have seen a striking improvement in imaging techniques, there are many tasks for which human interaction is still essential, as color gamut correction in the cinema industry. This suggests that a better understanding of the mechanisms underlying the visual system is instrumental to advances in imaging techniques.
Along these lines, various ideas from computational neurosciences have found application in imaging, from pattern recognition to image inpainting. A promising line of investigation is built on methods based on models of the primary visual cortex and on neural coding, in particular via the efficient representation principle. These methods have recently allowed to define new artificial neural networks paradigms and to reproduce complex visual illusions.
In this mini-symposium we aim to gather together experts working in the field of mathematical neuroscience and imaging, with a focus on these methods. In particular, the speakers will present recent results based on sparse coding and models of the visual system.&lt;/p&gt;
&lt;h3 id=&#34;organizer-dario-prandi&#34;&gt;Organizer: Dario Prandi&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;12:40-1:05 &lt;em&gt;The intrinsically nonlinear nature of receptive fields in vision: implications for imaging, vision science and artificial neural networks&lt;/em&gt; Marcelo Bertalm√≠o, Spanish National Research Council, Spain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1:10-1:35 &lt;em&gt;ChebLieNet: Invariant Spectral Graph Nns Turned Equivariant by Sub-Riemannian Geometry on Lie Groups&lt;/em&gt; Erik Bekkers, University of Amsterdam, Netherlands&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;1:40-2:05 &lt;em&gt;Deep Predictive Coding for More Robust and Human-Like Vision&lt;/em&gt; Rufin VanRullen, Centre de Recherche Cerveau et Cognition (CerCo), France&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;2:10-2:35 &lt;em&gt;Statistics of the Sparse Representations of Natural Images&lt;/em&gt; Hugo Ladret and Laurent U. Perrinet, CNRS &amp;amp; Aix-Marseille Universit√©, Marseille, France&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More on &lt;a href=&#34;https://meetings.siam.org/sess/dsp_programsess.cfm?sessioncode=73028&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://meetings.siam.org/sess/dsp_programsess.cfm?sessioncode=73028&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Des illusions aux hallucinations visuelles: une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/talk/2022-01-12-neuro-cercle/</link>
      <pubDate>Wed, 12 Jan 2022 18:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2022-01-12-neuro-cercle/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Nous aurons le plaisir d‚Äô√©changer avec notre conf√©rencier Laurent Perrinet et nous vous esp√©rons nombreux. Pour situer le conf√©rencier : &lt;a href=&#34;https://laurentperrinet.github.io/2019-05_illusions-visuelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/2019-05_illusions-visuelles/&lt;/a&gt;
¬´ C&amp;rsquo;est toujours fascinant de voir ou de revoir des illusions visuelles. C&amp;rsquo;est encore plus fascinant de plonger dans leurs explications. ¬ª&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Dynamical processing of orientation precision in the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/talk/2021-08-27-ddxl/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2021-08-27-ddxl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;This is 40th edition of Dynamicsdays&lt;/li&gt;
&lt;li&gt;Nice, 23-27 August 2021 - &lt;a href=&#34;https://dynamicsdays2021.univ-cotedazur.fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dynamicsdays2021.univ-cotedazur.fr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;check out the &lt;a href=&#34;https://dynamicsdays2021.univ-cotedazur.fr/assets/dynamicsdays_nice_2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;book of abstracts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In this talk, we will present the following paper : 
 
 
 
 
 
 
 
 
 
&lt;/li&gt;
&lt;li&gt;Preliminary Program:
&lt;ul&gt;
&lt;li&gt;Bruno Cessac, &lt;em&gt;The Retina as a Dynamical System&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Hugo Ladret &amp;amp; Laurent Perrinet, &lt;em&gt;Dynamics of the processing of orientation precision in the primary visual cortex&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Gianluigi Mongillo, &lt;em&gt;Glassy phase in dynamically balanced networks&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Romain Veltz, &lt;em&gt;Spatial and color hallucinations in a mathematical model of primary visual cortex&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Pooling in a predictive model of V1 explains functional and structural diversity across species</title>
      <link>https://laurentperrinet.github.io/talk/2021-06-15-smb/</link>
      <pubDate>Tue, 15 Jun 2021 11:15:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2021-06-15-smb/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Presenting my poster tonight at 8:00p &lt;a href=&#34;https://twitter.com/hashtag/cosyne2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#cosyne2020&lt;/a&gt;, a work developed using Sparse Deep Predictive Coding (SDPC) during my PhD &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; &lt;a href=&#34;https://twitter.com/NeuroSchool_mrs?ref_src=twsrc%5Etfw&#34;&gt;@NeuroSchool_mrs&lt;/a&gt; &lt;a href=&#34;https://t.co/LtUEBnlPNt&#34;&gt;pic.twitter.com/LtUEBnlPNt&lt;/a&gt;&lt;/p&gt;&amp;mdash; AF (@Angelo_RDN) &lt;a href=&#34;https://twitter.com/Angelo_RDN/status/1233458739220504578?ref_src=twsrc%5Etfw&#34;&gt;February 28, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is from:&lt;br&gt;Koray Kavukcuoglu, Marc&amp;#39;Aurelio Ranzato, Rob Fergus and Yann LeCun: Learning Invariant Features through Topographic Filter Maps, Proc. International Conference on Computer Vision and Pattern Recognition (CVPR&amp;#39;09), IEEE, 2009 &lt;a href=&#34;https://t.co/4gH6L3dmaJ&#34;&gt;pic.twitter.com/4gH6L3dmaJ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Yann LeCun (@ylecun) &lt;a href=&#34;https://twitter.com/ylecun/status/1384940135419101187?ref_src=twsrc%5Etfw&#34;&gt;April 21, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;poster.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this talk, I will present the following paper : 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/frederic-chavane/&#34;&gt;Fr√©d√©ric Chavane&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/franciosini-21/&#34;&gt;Pooling in a predictive model of V1 explains functional and structural diversity across species&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/2021.04.19.440444v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/franciosini-21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/franciosini-21/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1010270&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;see a follow-up in: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/frederic-chavane/&#34;&gt;Fr√©d√©ric Chavane&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/franck-ruffier/&#34;&gt;Franck Ruffier&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2021).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34;&gt;Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1902.07651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/boutin-franciosini-chavane-ruffier-perrinet-20.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/boutin-franciosini-chavane-ruffier-perrinet-20/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/VictorBoutin/InteractionMap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1008629&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;more about the role of top-down connections: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/franck-ruffier/&#34;&gt;Franck Ruffier&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2020).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/&#34;&gt;Effect of top-down connections in Hierarchical Sparse Coding&lt;/a&gt;.
   &lt;em&gt;Neural Computation&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.00892&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/boutin-franciosini-ruffier-perrinet-20-feedback.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/boutin-franciosini-ruffier-perrinet-20-feedback/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1162/neco_a_01325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dynamical processing of orientation precision in the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/talk/2021-05-20-neuro-france/</link>
      <pubDate>Thu, 20 May 2021 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2021-05-20-neuro-france/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;As presented during the &lt;a href=&#34;https://www.neurosciences.asso.fr/SN21/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroFrance 2021&lt;/a&gt; meeting
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;now (2:00 PM - 3:00 PM CEST on Thursday, May 20), you can hear Hugo Ladret &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; present his poster P4.47 &amp;quot;Processing of orientation precision in the primary visual cortex&amp;quot;&lt;a href=&#34;https://t.co/vuT6gegwtO&#34;&gt;https://t.co/vuT6gegwtO&lt;/a&gt;&lt;br&gt;Neurofrance 2021 &lt;a href=&#34;https://twitter.com/hashtag/NF2021?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#NF2021&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/NeuroFrance2021?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#NeuroFrance2021&lt;/a&gt; &lt;a href=&#34;https://twitter.com/SocNeuro_Tweets?ref_src=twsrc%5Etfw&#34;&gt;@SocNeuro_Tweets&lt;/a&gt; &lt;a href=&#34;https://t.co/YQNF9FiB6m&#34;&gt;pic.twitter.com/YQNF9FiB6m&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1395351843035828224?ref_src=twsrc%5Etfw&#34;&gt;May 20, 2021&lt;/a&gt;&lt;/blockquote&gt;
 &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/li&gt;
&lt;li&gt;get the &lt;a href=&#34;https://www.professionalabstracts.com/nf2021/programme-nf2021.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;abstract book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;In this talk, we will present the following paper : 
 
 
 
 
 
 
 
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Understanding natural vision using deep predictive coding</title>
      <link>https://laurentperrinet.github.io/talk/2020-09-25-irphe/</link>
      <pubDate>Fri, 25 Sep 2020 15:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-09-25-irphe/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;What:: talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2020-09-25-irphe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S√©minaire √† l&amp;rsquo;Institut de Recherche sur les Ph√©nom√®nes Hors √âquilibre (IRPH√â)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Who:: Perrinet, Laurent U&lt;/li&gt;
&lt;li&gt;Where: Marseille (France), see &lt;a href=&#34;https://laurentperrinet.github.io/talk/2020-09-25-irphe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2020-09-25-irphe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;When: 25/09/2020, time: 15:45:00-16:30:00&lt;/li&gt;
&lt;li&gt;What:
&lt;ul&gt;
&lt;li&gt;Slides @ &lt;a href=&#34;https://laurentperrinet.github.io/2020-09-25_IRPHE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/2020-09-25_IRPHE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code for slides @ &lt;a href=&#34;https://github.com/laurentperrinet/2020-09-25_IRPHE/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/laurentperrinet/2020-09-25_IRPHE/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Abstract: Building models which efficiently process images is a great source of inspiration to better understand the processes which underly our visual perception. I will present some classical models stemming from the Machine Learning community and propose some extensions inspired by Nature. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computations at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision (CNNs in the terminology  of deep learning) take into account neurophysiological observations and provide as of today the most successful framework for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. We will here introduce a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective and we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational levels. These results from the SDPC model additionally demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Visual search as active inference</title>
      <link>https://laurentperrinet.github.io/talk/2020-09-14-iwai/</link>
      <pubDate>Mon, 14 Sep 2020 18:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-09-14-iwai/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see proceedings paper: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2020).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/dauce-20-iwai/&#34;&gt;Visual search as active inference&lt;/a&gt;.
   &lt;em&gt;IWAI 2020&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/dauce-20-iwai/dauce-20-iwai.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/dauce-20-iwai/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/2020-09-14_IWAI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Slides
 &lt;/a&gt;
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1007/978-3-030-64919-7_17&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;the mathematical details are described as a talk the 1st International WS on &lt;a href=&#34;https://twitter.com/hashtag/ActiveInference?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ActiveInference&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/IWAI2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#IWAI2020&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/ECMLPKDD?ref_src=twsrc%5Etfw&#34;&gt;@ECMLPKDD&lt;/a&gt; &lt;a href=&#34;https://t.co/4s7gHbMxiT&#34;&gt;https://t.co/4s7gHbMxiT&lt;/a&gt; and paper &amp;quot;Visual search as active inference&amp;quot; &lt;a href=&#34;https://t.co/yNCOFHf7FS&#34;&gt;https://t.co/yNCOFHf7FS&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1305488089989754883?ref_src=twsrc%5Etfw&#34;&gt;September 14, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;What:: talk @ &lt;a href=&#34;https://iwaiworkshop.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1st International Workshop on Active Inference (IWAI 2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Who:: Emmanuel Dauc√© and Laurent Perrinet&lt;/li&gt;
&lt;li&gt;Where: Ghent (Belgium), gone virtual, see &lt;a href=&#34;https://laurentperrinet.github.io/talk/2020-09-14-iwai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2020-09-14-iwai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;When: 14/09/2020, time: 12:20:00-12:40:00&lt;/li&gt;
&lt;li&gt;What:
&lt;ul&gt;
&lt;li&gt;Slides @ &lt;a href=&#34;https://laurentperrinet.github.io/2020-09-14_IWAI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/2020-09-14_IWAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code for slides @ &lt;a href=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/laurentperrinet/2020-09-14_IWAI/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Abstract: Visual search is an essential cognitive ability, offering a prototypical control problem to be addressed with Active Inference. Under a Naive Bayes assumption, the maximisation of the information gain objective is consistent with the separation of the visual sensory flow in two independent pathways, namely the &amp;ldquo;What&amp;rdquo; and the &amp;ldquo;Where&amp;rdquo; pathways. On the &amp;ldquo;What&amp;rdquo; side, the processing of the central part of the visual field (the fovea) provides the current interpretation of the scene, here the category of the target. On the &amp;ldquo;Where&amp;rdquo; side, the processing of the full visual field (at lower resolution) is expected to provide hints about future central foveal processing given the potential realisation of saccadic movements. A map of the classification accuracies, as obtained by such counterfactual saccades, defines a utility function on the motor space, whose maximal argument prescribes the next saccade. The comparison of the foveal and the peripheral predictions finally forms an estimate of the future information gain, providing a simple and resource-efficient way to implement information gain seeking policies in active vision. This dual-pathway information processing framework is found efficient on a synthetic visual search task and we show here quantitatively the role of the precision encoded within the accuracy map. More importantly, it is expected to draw connections toward a more general actor-critic principle in action selection, with the accuracy of the central processing taking the role of a value (or intrinsic reward) of the previous saccade.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>From the retina to action: Understanding visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2020-04-ue-neurosciences-computationnelles/</link>
      <pubDate>Fri, 03 Apr 2020 16:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-04-ue-neurosciences-computationnelles/</guid>
      <description>&lt;h1 id=&#34;2020-04_ue-neurosciences-computationnelles-mat√©riel-pour-le-cours-de-mod√©lisation&#34;&gt;2020-04_UE-neurosciences-computationnelles, mat√©riel pour le cours de mod√©lisation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;O√π: Marseille (France)&lt;/li&gt;
&lt;li&gt;Quoi: Master Neurosciences et Sciences Cognitives&lt;/li&gt;
&lt;li&gt;But de ce travail: lire un article scientifique, pouvoir le reproduire avec des simulations d&amp;rsquo;un neurone et afin d&amp;rsquo;am√©liorer sa compr√©hension.&lt;/li&gt;
&lt;li&gt;Modalit√©s: les √©tudiants s&amp;rsquo;organisent seuls, en binome ou en trinome pour fournir un m√©moire sous forme de &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt; compl√©t√© √† partir &lt;a href=&#34;https://raw.githubusercontent.com/laurentperrinet/2020-04_UE-neurosciences-computationnelles/master/MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;du mod√®le qui est fourni&lt;/a&gt;. Suivez les balises &lt;code&gt;TODO&lt;/code&gt; dans le notebook pour vous guider dans cette r√©daction. Les commentaires doivent √™tre fait en fran√ßais (ou en anglais si n√©cessaire) dans le notebook (n&amp;rsquo;oubliez-pas de sauver vos changements) et envoy√© par e-mail √† mailto:laurent.perrinet@univ-amu.fr une fois votre travail fini (de pr√©f√©rence avant le 31 avri).&lt;/li&gt;
&lt;li&gt;Outils n√©cessaires: &lt;a href=&#34;https://jupyter.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter&lt;/a&gt;, avec &lt;a href=&#34;https://numpy.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numpy&lt;/a&gt; et &lt;a href=&#34;https://matplotlib.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matplotlib&lt;/a&gt;. Ce sont des outils standard et qui sont facilement installables sur toute plateforme. Si vous avez des probl√®mes, me joindre par e-mail ou sur le &lt;a href=&#34;https://spik.xyz/nc/index.php/call/xuswegwv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;forum&lt;/a&gt; üëá&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Des illusions aux hallucinations visuelles: une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/talk/2020-01-20-atelier-sciences-cinema/</link>
      <pubDate>Mon, 20 Jan 2020 10:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-01-20-atelier-sciences-cinema/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Watch ‚Äú√áA TOURNE‚Äù on &lt;a href=&#34;https://twitter.com/hashtag/Vimeo?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Vimeo&lt;/a&gt; &lt;a href=&#34;https://t.co/nzUAEwjTeD&#34;&gt;https://t.co/nzUAEwjTeD&lt;/a&gt; (english subs) r√©alis√© par Camille Goujon et par les √©l√®ves du Lyc√©e Professionnel Domaine Eguille, Ved√®ne (France) - &lt;a href=&#34;https://t.co/EhVz0YZdPs&#34;&gt;https://t.co/EhVz0YZdPs&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/StopMotion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#StopMotion&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/vision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#vision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuroscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuroscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/outreach?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#outreach&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1284791644240347138?ref_src=twsrc%5Etfw&#34;&gt;July 19, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/398661322&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;√áA TOURNE a √©t√© s√©lectionn√© pour participer √† la comp√©tition du ¬´ Alexandre Trauner ART/Film Festival ¬ª (Szolnok, Hongrie) : &lt;a href=&#34;http://www.ataff.hu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ataff.hu/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;visible aux Soir√©e Courts M√©trages Cin√© Rencontre de la Ville de Berre l&amp;rsquo;√âtang &lt;a href=&#34;https://www.berreletang.fr/soiree-courts-metrages?periode=2021-04-30%2017%3A23%3A26&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.berreletang.fr/soiree-courts-metrages?periode=2021-04-30%2017%3A23%3A26&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;√áA TOURNE de Camille Goujon, a √©t√© s√©lectionn√© au 27√®me Festival national du film d&amp;rsquo;animation de Rennes M√©tropole, dans la cat√©gorie Autoproductions du 7 au 11 octobre 2021 &lt;a href=&#34;http://festival-film-animation.fr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://festival-film-animation.fr/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;le court-m√©trage a √©t√© s√©lectionn√© pour participer au ¬´ Happy Valley Animation Festival ¬ª (Pennsylvanie, USA) : &lt;a href=&#34;https://happyvalleyanimationfestival.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://happyvalleyanimationfestival.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Le film &amp;ldquo;√áA TOURNE&amp;rdquo; a √©t√© s√©lectionn√© pour faire partie de la comp√©tition cat√©gorie ¬´FILMS SCOLAIRES&amp;quot; diffus√©e du 4 au 7 novembre 2020 dans le cadre du festival &amp;ldquo;7√®me Art Jeunes Talent! : &lt;a href=&#34;http://www.festivaltournezjeunesse.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.festivaltournezjeunesse.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dans le cadre d&amp;rsquo;un projet R√©gion (APERLA) les √©l√®ves de seconde Bac Pro Menuisiers agenceurs ont con√ßu ce film sous la direction de leur professeur Mme Bomont et  sous la direction artistique de &lt;a href=&#34;https://www.youtube.com/user/camillegoujon1/videos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Camille Goujon&lt;/a&gt;, artiste et cin√©aste d&amp;rsquo;animation &lt;a href=&#34;https://www.domaine-eguilles.fr/realisation-collective-de-lyceens-sous-la-direction-artistique-de-camille-goujon-artiste-et-cineaste-d-animation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.domaine-eguilles.fr/realisation-collective-de-lyceens-sous-la-direction-artistique-de-camille-goujon-artiste-et-cineaste-d-animation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ce court m√©trage fait partie des 7 films r√©alis√©s dans le cadre des ¬´ Ateliers de r√©alisation Cin√©sciences ¬ª propos√©s par l‚Äôassociation Polly Maggoo &lt;a href=&#34;http://festivalrisc.org/films-dateliers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://festivalrisc.org/films-dateliers/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ref sur &lt;a href=&#34;http://www.lussasdoc.org/film-ca_tourne-1,53288.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.lussasdoc.org/film-ca_tourne-1,53288.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Le texte de cette pr√©sentation est reprise dans cet article de &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; (&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lien direct&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Voir la @ &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34;&gt;pr√©sentation au NeuroStories&lt;/a&gt; sur un th√®me similaire&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learning where to look: a foveated visuomotor control model</title>
      <link>https://laurentperrinet.github.io/talk/2019-07-15-cns/</link>
      <pubDate>Mon, 15 Jul 2019 12:20:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-07-15-cns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;download a &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-07-15-cns/2019-07-15-cns.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preliminary PDF&lt;/a&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Emmanuel Dauc√© @ &lt;a href=&#34;https://twitter.com/hashtag/CNS2019Barcelona?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#CNS2019Barcelona&lt;/a&gt; speaks about our joint work on ¬´¬†Learning where to look: a foveated visuomotor control model¬†¬ª more info @ &lt;a href=&#34;https://t.co/HREjuIgNCn&#34;&gt;https://t.co/HREjuIgNCn&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNSorg?ref_src=twsrc%5Etfw&#34;&gt;@CNSorg&lt;/a&gt; &lt;a href=&#34;https://t.co/GbbXhWL1k1&#34;&gt;pic.twitter.com/GbbXhWL1k1&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1150713758643380226?ref_src=twsrc%5Etfw&#34;&gt;July 15, 2019&lt;/a&gt;&lt;/blockquote&gt;
 &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;















&lt;figure  id=&#34;figure-problem-setting-in-generic-ecological-settings-the-visual-system-faces-a-tricky-problem-when-searching-for-one-target-from-a-class-of-targets-in-a-cluttered-environment-a-it-is-synthesized-in-the-following-experiment-after-a-fixation-period-of-200-ms-an-observer-is-presented-with-a-luminous-display--showing-a-single-target-from-a-known-class-here-digits-and-at-a-random-position-the-display-is-presented-for-a-short-period-of-500-ms-light-shaded-area-in-b-that-is-enough-to-perform-at-most-one-saccade-here-successful-on-the-potential-target-finally-the-observer-has-to-identify-the-digit-by-a-keypress-b-prototypical-trace-of-a-saccadic-eye-movement-to-the-target-position-in-particular-we-show-the-fixation-window-and-the-temporal-window-during-which-a-saccade-is-possible-green-shaded-area-c-simulated-reconstruction-of-the-visual-information-from-the-interoceptive-retinotopic-map-at-the-onset-of-the-display-and-after-a-saccade-the-dashed-red-box-indicating-the-visual-area-of-the-what-pathway-in-contrast-to-an-exteroceptive-representation-see-a-this-demonstrates-that-the-position-of-the-target-has-to-be-inferred-from-a-degraded-sampled-image-in-particular-the-configuration-of-the-display-is-such-that-by-adding-clutter-and-reducing-the-size-of-the-digit-it-may-become-necessary-to-perform-a-saccade-to-be-able-to-identify-the-digit-the-computational-pathway-mediating-the-action-has-to-infer-the-location-of-the-target-emphbefore-seeing-it-that-is-before-being-able-to-actually-identify-the-targets-category-from-a-central-fixation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/SpikeAI/2019-07-15_CNS/master/figures/fig_intro.jpg&#34; alt=&#34;Problem setting: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. **A)** It is synthesized in the following experiment: After a fixation period of 200 ms, an observer is presented with a luminous display  showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of 500 ms (light shaded area in B), that is enough to perform at most one saccade (here, successful) on the potential target. Finally, the observer has to identify the digit by a keypress. **B)** Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window and the temporal window during which a saccade is possible (green shaded area). **C)** Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display and after a saccade, the dashed red box indicating the visual area of the ``what&amp;#39;&amp;#39; pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target&amp;#39;s category from a central fixation. &#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Problem setting: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. &lt;strong&gt;A)&lt;/strong&gt; It is synthesized in the following experiment: After a fixation period of 200 ms, an observer is presented with a luminous display  showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of 500 ms (light shaded area in B), that is enough to perform at most one saccade (here, successful) on the potential target. Finally, the observer has to identify the digit by a keypress. &lt;strong&gt;B)&lt;/strong&gt; Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window and the temporal window during which a saccade is possible (green shaded area). &lt;strong&gt;C)&lt;/strong&gt; Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display and after a saccade, the dashed red box indicating the visual area of the ``what&amp;rsquo;&amp;rsquo; pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target&amp;rsquo;s category from a central fixation.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-results-success&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-20.png&#34; alt=&#34;Results: success&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Results: success
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-results-failure-to-classify&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-32.png&#34; alt=&#34;Results: failure to classify&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Results: failure to classify
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-results-failure-to-locate&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-47.png&#34; alt=&#34;Results: failure to locate&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Results: failure to locate
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Should I stay or should I go? Humans adapt to the volatility of visual motion properties, and know about it</title>
      <link>https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/</link>
      <pubDate>Thu, 23 May 2019 01:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/chloe-pasturel/&#34;&gt;Chlo√© Pasturel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/anna-montagnini/&#34;&gt;Anna Montagnini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-02394142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/pasturel-montagnini-perrinet-20/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;This is part of the &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-05-23-neurofrance/&#34;&gt;Active Inference symposium&lt;/a&gt; @ &lt;a href=&#34;https://www.neurosciences.asso.fr/V2/colloques/SN19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroFrance&lt;/a&gt; SYMPOSIUM, Room 7
23.05.2019, 11:00 &amp;ndash; 13:00&lt;/li&gt;
&lt;li&gt;in french: Principes et psychophysique de l¬¥Inf√©rence Active dans l&amp;rsquo;estimation d&amp;rsquo;un biais dynamique et volatile de probabilit√©&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Des illusions aux hallucinations visuelles: une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/talk/2019-04-18-jnlf/</link>
      <pubDate>Thu, 18 Apr 2019 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-04-18-jnlf/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Le texte de cette pr√©sentation est reprise dans cet article de &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; (&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lien direct&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Voir la @ &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34;&gt;pr√©sentation au NeuroStories&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Should I stay or should I go? Adaption of human observers to the volatility of visual inputs</title>
      <link>https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/</link>
      <pubDate>Fri, 05 Apr 2019 15:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/chloe-pasturel/&#34;&gt;Chlo√© Pasturel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/anna-montagnini/&#34;&gt;Anna Montagnini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-02394142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/pasturel-montagnini-perrinet-20/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>From the retina to action: Understanding visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2019-04-03-a-course-on-vision-and-modelization/</link>
      <pubDate>Wed, 03 Apr 2019 16:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-04-03-a-course-on-vision-and-modelization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From the retina to action: Predictive processing in the visual system</title>
      <link>https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/</link>
      <pubDate>Mon, 25 Mar 2019 14:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a follow-up in: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/frederic-chavane/&#34;&gt;Fr√©d√©ric Chavane&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/franck-ruffier/&#34;&gt;Franck Ruffier&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2021).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34;&gt;Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1902.07651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/boutin-franciosini-chavane-ruffier-perrinet-20.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/boutin-franciosini-chavane-ruffier-perrinet-20/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/VictorBoutin/InteractionMap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1008629&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;more about the role of top-down connections: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/franck-ruffier/&#34;&gt;Franck Ruffier&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2020).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/&#34;&gt;Effect of top-down connections in Hierarchical Sparse Coding&lt;/a&gt;.
   &lt;em&gt;Neural Computation&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/2002.00892&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/boutin-franciosini-ruffier-perrinet-20-feedback.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/boutin-franciosini-ruffier-perrinet-20-feedback/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1162/neco_a_01325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Should I stay or should I go? Adaption of human observers to the volatility of visual inputs</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-18-laconeu/</link>
      <pubDate>Fri, 18 Jan 2019 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-18-laconeu/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/chloe-pasturel/&#34;&gt;Chlo√© Pasturel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/anna-montagnini/&#34;&gt;Anna Montagnini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-02394142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/pasturel-montagnini-perrinet-20/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Role of dynamics in neural computations underlying visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-17-laconeu/</link>
      <pubDate>Thu, 17 Jan 2019 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-17-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient coding of visual information in neural computations</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-16-laconeu/</link>
      <pubDate>Wed, 16 Jan 2019 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-16-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling spiking neural networks using Brian, Nest and pyNN</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-14-laconeu/</link>
      <pubDate>Mon, 14 Jan 2019 11:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-14-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rencontre avec les coll√©giens marseillais</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-10-polly-maggoo/</link>
      <pubDate>Thu, 10 Jan 2019 09:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-10-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;cin√©ma-et-sciences--rencontre-avec-les-coll√©giens-marseillais&#34;&gt;Cin√©ma et sciences : rencontre avec les coll√©giens marseillais&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; met en place
tout le long de l‚Äôann√©e, des actions de culture scientifique et
artistique en direction du grand public et des lyc√©es, au cours
desquelles l&amp;rsquo;association programme des films √† caract√®re scientifique.
Les projections se d√©roulent en pr√©sence de chercheurs et/ou de
cin√©astes dans la perspective d‚Äôun d√©veloppement de la culture
cin√©matographique et scientifique en direction des publics scolaires.
Le jeudi 10 janvier 2019, je suis venu √©changer au c√¥t√© de Serge Dentin
autour de films traitant du rapport fiction/r√©el, des illusion visuelles
(&amp;quot; Qu‚Äôest ce qu‚Äôune image? &amp;ldquo;), des rapports d‚Äô√©chelles, de la
perception, &amp;hellip; et qui sont projet√©s lors de la s√©ance, avec les √©l√®ves
de deux classes de 4√®me. Une occasion aussi de parler du m√©tier de
chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
10 janvier 2019&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
coll√®ge Andr√© Malraux, Marseille&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&amp;ldquo;LAZARUS MIRAGES : T√âL√âPATHIE √Ä L&amp;rsquo;UNIVERSIT√â DE SHANGAI&amp;rdquo; de Patric
JEAN et Henry BROCH (France, 2012, documentaire, 3&#39;21)
/&amp;ldquo;CARLITOPOLIS&amp;rdquo; / / &amp;ldquo;BIG DATA, BIG BUSINESS&amp;rdquo; / &amp;ldquo;&lt;a href=&#34;https://www.youtube.com/watch?v=RVeHxUVkW4w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Centrifuge
Brain Project, A Short Film by Till
Nowak&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>La mod√©lisation biomorphique de la perception visuelle</title>
      <link>https://laurentperrinet.github.io/talk/2018-10-11-bio-morphisme/</link>
      <pubDate>Thu, 11 Oct 2018 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-10-11-bio-morphisme/</guid>
      <description>&lt;h1 id=&#34;la-mod√©lisation-biomorphique-de-la-perception-visuelle&#34;&gt;La mod√©lisation biomorphique de la perception visuelle&lt;/h1&gt;
&lt;h2 id=&#34;in-la-mod√©lisation-de-la-gen√®se-physico-math√©matique-du-vivant&#34;&gt;in &amp;ldquo;La mod√©lisation de la gen√®se physico-math√©matique du vivant&amp;rdquo;&lt;/h2&gt;
&lt;h2 id=&#34;biomorphisme-et-creation-artistique-session-3&#34;&gt;BIOMORPHISME ET CREATION ARTISTIQUE¬†‚Äì Session 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
11 Octobre 2018&lt;/li&gt;
&lt;li&gt;Atelier&lt;br&gt;
S√©minaire/workshop organis√© dans le cadre du projet Biomorphisme.
Approches sensibles et conceptuelles des formes du vivant
&lt;a href=&#34;http://lesa.univ-amu.fr/?q=node/391&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lesa.univ-amu.fr/?q=node/391&lt;/a&gt; &lt;a href=&#34;http://centregranger.cnrs.fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://centregranger.cnrs.fr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
B√¢timent Egger, dans la salle E 215 (2√®me √©tage c√¥t√© voie ferr√©e) -
3 avenue R. Schuman - Aix-en-Provence&lt;/li&gt;
&lt;li&gt;Visuels&lt;br&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2018-10-11_BioMorphisme.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Organisation&lt;br&gt;
Jean Arnaud, PR arts plastiques au LESA-AMU¬†; Julien Bernard, MCF
philosophe des sciences au Centre GG Granger-AMU¬†; Sylvie Pic,
artiste&lt;/li&gt;
&lt;li&gt;R√©sum√©&lt;br&gt;
La vision utilise un faisceau d&amp;rsquo;informations de diff√©rentes qualit√©s
pour atteindre une perception unifi√©e du monde environnant. Elle
interagit avec lui en cr√©ant son propre mod√®le g√©n√©ratif de sa
structure physico-math√©matique. Avec &lt;a href=&#34;https://laurentperrinet.github.io/LaurentPerrinet/EtienneRey&#34;&gt;Etienne
Rey&lt;/a&gt; de l&amp;rsquo;atelier Ondes Parall√®les,
nous avons utilis√© lors de plusieurs projets art-science (voir
&lt;a href=&#34;https://github.com/NaturalPatterns&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/NaturalPatterns&lt;/a&gt;) des installations permettant
de manipuler explicitement des composantes de ce flux d&amp;rsquo;information
et de r√©v√©ler des ambiguit√©s dans notre perception. Dans
l&amp;rsquo;installation
&lt;a href=&#34;https://github.com/NaturalPatterns/Tropique&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique&lt;/a&gt;, des
faisceaux de lames lumineuses sont arrang√©s dans l&amp;rsquo;espace assombri
de l&amp;rsquo;installation. Les spectateurs les observent gr√¢ce √† leur
interaction avec une brume invisible qui est diffus√©e dans l&amp;rsquo;espace.
L&amp;rsquo;ensemble des faisceaux √©volue comme autant de lames lumineuses √†
partir de 6 video-projecteurs plac√©s dans l&amp;rsquo;espace de
l&amp;rsquo;installation, suivant une dynamique autonome. En m√™me temps, la
position des spectateurs est capt√©e et permet d&amp;rsquo;alterner entre une
vision de ces sculptures d&amp;rsquo;un point de vue introceptif √† un point de
vue exteroceptif. Dans ¬´&lt;a href=&#34;https://github.com/NaturalPatterns/elasticite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trame
√âlasticit√©&lt;/a&gt;¬ª, 25
parall√©l√©pip√®des de miroirs (3m de haut) sont arrang√©s verticalement
sur une ligne horizontale. Ces lames sont rotatives et leurs
mouvements est synchronis√©. Suivant la dyamique qui est impos√© √† ces
lames, la perception de l‚Äôespace environnent fluctue conduisant √†
recomposer l‚Äôespace de la concentration √† l‚Äôexpansion, ou encore √†
g√©n√©rer un surface semblant transparente ou inverser la visons de
ce qui est situ√©e devant et derri√®re l‚Äôobservateur. Enfin, dans
¬´&lt;a href=&#34;https://github.com/NaturalPatterns/TRAMES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trames&lt;/a&gt;¬ª, nous
explorons l&amp;rsquo;interaction de s√©ries p√©riodiques de points plac√©es sur
des surfaces transparentes. √Ä partir de premi√®res exp√©rimentations
utilisant une technique novatrice de s√©rigraphie, ces trames de
points sont plac√©es afin de faire √©merger des structures selon le
point de vue du spectateur. Ce qui est en jeu ici c‚Äôest l‚Äô√©mergence
de l‚Äôapparition de motifs virtuels r√©sultat de la relation entre une
r√©alit√© physique, la grandeur et l‚Äôordonnancement de trames et notre
physiologie qui conduit √† cette √©tat de perception. Lorsqu‚Äôon est
fasse √† ces motifs ce qui saute au yeux plus que le motif r√©el c‚Äôest
sa r√©sultante, instable et √©ph√©m√®re qui fait apparaitre une richesse
de figures g√©om√©triques qui se transforment et √©voluent en fonction
du temps d‚Äôobservation et du point de vue. Sur ce principe de
dispositif optique, le travail de chacun des motifs, li√© √† un
s√©quen√ßage de trames conduit √† faire apparaitre une composition et
des √©mergences de formes sp√©cifiques. L‚Äôexp√©rience de perception de
chacun des motifs explore les notions d‚Äôinstabilit√©, de flux,
d‚Äô√©mergences ‚Ä¶ dont l‚Äôexp√©rience donne √† entrevoir des formes que
l‚Äôon retrouve dans la nature ou les ph√©nom√®nes naturels: le dessin
du pelage d‚Äôun z√®bre, une accumulation de bulles de savons, ou plus
g√©n√©ralement dans les compositions chimiques issue de la th√©orie de
la morphog√©n√®se de Turing. De mani√®re g√©n√©rale, nous montrerons ici
les diff√©rentes m√©thodes utilis√©es, comme l&amp;rsquo;utilisation des limites
perceptives, et aussi les r√©sultats apport√©s par une telle
collaboration.&lt;/li&gt;
&lt;li&gt;Mots-Cl√©s&lt;br&gt;
art cin√©tique¬†;¬†science¬†;¬†vision¬†;¬†perception¬†;¬†mod√®le interne&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Intervention f√™te de la science 2018</title>
      <link>https://laurentperrinet.github.io/talk/2018-10-10-polly-maggoo/</link>
      <pubDate>Wed, 10 Oct 2018 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-10-10-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;f√™te-de-la-science-2018--alcazar--merlan&#34;&gt;F√äTE DE LA SCIENCE 2018 : Alcazar / MERLAN&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; met en place
tout le long de l‚Äôann√©e, des actions de culture scientifique et
artistique en direction du grand public et des lyc√©es, au cours
desquelles l&amp;rsquo;association programme des films √† caract√®re scientifique.
Les projections se d√©roulent en pr√©sence de chercheurs et/ou de
cin√©astes dans la perspective d‚Äôun d√©veloppement de la culture
cin√©matographique et scientifique en direction des publics scolaires.
Le samedi 6 octobre et le mercredi 10 octobre, je suis venu √©changer au
c√¥t√© de Serge Dentin autour de films traitant du rapport fiction/r√©el,
des illusion visuelles (&amp;quot; Qu‚Äôest ce qu‚Äôune image? &amp;ldquo;), des rapports
d‚Äô√©chelles, de la perception, &amp;hellip; et qui sont projet√©s lors de la
s√©ance, avec tout public (samedi) ou des √©l√®ves de lyc√©e (mercredi).
Une occasion aussi de parler du m√©tier de chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
6 octobre 2018&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
biblioth√®que de l&amp;rsquo;Alcazar (BMVR), Marseille&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&lt;em&gt;SAMSUNG GALAXY&lt;/em&gt; de Romain CHAMPALAUNE (France, 2015),
documentaire-fiction, 7‚Ä≤ / &lt;em&gt;LA DR√îLE DE GUERRE D‚ÄôALAN TURING&lt;/em&gt; de
Denis VAN WAEREBEKE (France, 2014), documentaire, 60‚Äô&lt;/li&gt;
&lt;li&gt;URL&lt;br&gt;
&lt;a href=&#34;http://pollymaggoo.org/fete-de-la-science-2018-alcazar-bmvr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pollymaggoo.org/fete-de-la-science-2018-alcazar-bmvr/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Date&lt;br&gt;
10 octobre 2018&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
biblioth√®que du Merlan, Marseille&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&lt;em&gt;SAMSUNG GALAXY&lt;/em&gt; de Romain CHAMPALAUNE (France, 2015),
documentaire-fiction, 7‚Ä≤ / &amp;ldquo;JE TE SUIS (JAG F√ñLJER DIG)&amp;rdquo; / &amp;ldquo;OS
Love_EN&amp;rdquo; / &amp;ldquo;BIG DATA, BIG BUSINESS&amp;rdquo; / COPIER-CLONER / et en bonus
&amp;ldquo;&lt;a href=&#34;https://www.youtube.com/watch?v=RVeHxUVkW4w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Centrifuge Brain Project, A Short Film by Till
Nowak&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Principles and psychophysics of Active Inference in anticipating a dynamic, switching probabilistic bias</title>
      <link>https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/</link>
      <pubDate>Thu, 05 Apr 2018 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/chloe-pasturel/&#34;&gt;Chlo√© Pasturel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/anna-montagnini/&#34;&gt;Anna Montagnini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-02394142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/pasturel-montagnini-perrinet-20/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Probabilities, Bayes and the Free-energy principle</title>
      <link>https://laurentperrinet.github.io/talk/2018-03-26-cours-neuro-comp-fep/</link>
      <pubDate>Mon, 26 Mar 2018 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-03-26-cours-neuro-comp-fep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/chloe-pasturel/&#34;&gt;Chlo√© Pasturel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/anna-montagnini/&#34;&gt;Anna Montagnini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-02394142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/pasturel-montagnini-perrinet-20/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;previous talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Exp√©riences autour de la perception de la forme en art et science</title>
      <link>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</link>
      <pubDate>Thu, 25 Jan 2018 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</guid>
      <description>&lt;h1 id=&#34;meetup-art-et-neurosciences&#34;&gt;Meetup Art et Neurosciences&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Quoi&lt;br&gt;
Meetup Art et Neurosciences&lt;/li&gt;
&lt;li&gt;Qui&lt;br&gt;
&lt;a href=&#34;https://www.facebook.com/events/211121069456116/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Association
NeuroNautes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quand&lt;br&gt;
25 Janvier 2018&lt;/li&gt;
&lt;li&gt;O√π&lt;br&gt;
Salle des voutes campus Saint Charles&lt;/li&gt;
&lt;li&gt;Support visuel&lt;br&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html&lt;/a&gt;
(notes: la pr√©sentation peut mettre un certain temps
√† charger. Une fois que le titre apparait, appuyer sur la touche &amp;ldquo;F&amp;rdquo;
pour mettre en plein √©cran)
















&lt;figure  id=&#34;figure-elasticit√©-dynamique-est-compos√©e-des-pi√®ces-expansion-trame-et-lignes-sonores-volume-hexagonal-en-miroir-de-7-m√®tres-de-diam√®tre-expansion-fonctionne-comme-une-chambre-d√©cho-a-lint√©rieur-de-ce-volume-se-situe-trame-constitu√©e-de-25-lames-de-miroir-en-rotation-cette-pi√®ce-r√©oriente-continuellement-le-regard-quant-√†-lignes-sonores-elle-est-form√©e-de-quatre-monolithes-orient√©s-vers-expansion-et-√©met-des-sons-qui-se-r√©orientent-en-fonction-du-mouvement-des-lames--etienne-rey-adagp-paris&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.lafriche.org/public_data/diapo/resident/1454686884/desk/2._elasticite_dynamique-etienne_rey-photoquentin_chevrier_pour_art2m_et_arcadi_ile_de_france.jpg&#34; alt=&#34;Elasticit√©&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Elasticit√© dynamique est compos√©e des pi√®ces Expansion, Trame et Lignes sonores. Volume hexagonal en miroir de 7 m√®tres de diam√®tre, Expansion fonctionne comme une chambre d&amp;rsquo;√©cho. A l&amp;rsquo;int√©rieur de ce volume se situe Trame. Constitu√©e de 25 lames de miroir en rotation, cette pi√®ce r√©oriente continuellement le regard. Quant √† Lignes sonores, elle est form√©e de quatre monolithes orient√©s vers Expansion et √©met des sons qui se r√©orientent en fonction du mouvement des lames. (¬© Etienne Rey, Adagp Paris
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised learning applied to robotic vision</title>
      <link>https://laurentperrinet.github.io/talk/2017-11-24-neurosciences-robotique/</link>
      <pubDate>Fri, 24 Nov 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-11-24-neurosciences-robotique/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a related work describing SDPC in: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/victor-boutin/&#34;&gt;Victor Boutin&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/angelo-franciosini/&#34;&gt;Angelo Franciosini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/frederic-chavane/&#34;&gt;Fr√©d√©ric Chavane&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/franck-ruffier/&#34;&gt;Franck Ruffier&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2021).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34;&gt;Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1902.07651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/boutin-franciosini-chavane-ruffier-perrinet-20.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/boutin-franciosini-chavane-ruffier-perrinet-20/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/VictorBoutin/InteractionMap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1008629&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Participation au jury</title>
      <link>https://laurentperrinet.github.io/talk/2017-11-17-festival-interferences/</link>
      <pubDate>Fri, 17 Nov 2017 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-11-17-festival-interferences/</guid>
      <description>&lt;h1 id=&#34;festival-interf√©rences&#34;&gt;FESTIVAL INTERF√âRENCES‚Äã&lt;/h1&gt;
&lt;h2 id=&#34;cin√©ma-documentaire-et-d√©bat-public&#34;&gt;Cin√©ma Documentaire et D√©bat Public&lt;/h2&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-festival-interf√©rences&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://static.wixstatic.com/media/e37617_35d8c5b48dd340a481db5f711aeaa35a~mv2_d_1772_2480_s_2.jpg/v1/fill/w_600,h_797,al_c,q_85,usm_0.66_1.00_0.01/e37617_35d8c5b48dd340a481db5f711aeaa35a~mv2_d_1772_2480_s_2.jpg&#34; alt=&#34;FESTIVAL INTERF√âRENCES‚Äã&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      FESTIVAL INTERF√âRENCES‚Äã
    &lt;/figcaption&gt;&lt;/figure&gt;

Le collectif Sc√®nes Publiques compos√© de citoyens, chercheurs et
cin√©astes, organise la deuxi√®me √©dition du Festival Interf√©rences du 8
au 18 novembre 2017 √† Lyon. J&amp;rsquo;ai eu la chance de pouvoir participer au
jury autour de documentaires avec un regard scientifiques. Une occasion
aussi de parler du m√©tier de chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
17 et 18 Novembre 2017&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
Lyon&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&lt;a href=&#34;http://www.lacitedoc.com/interferences-programmation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.lacitedoc.com/interferences-programmation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What dynamic neural codes for efficient visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2017-11-15-colloque-master/</link>
      <pubDate>Wed, 15 Nov 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-11-15-colloque-master/</guid>
      <description>&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;unsupervised learning : &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-10-shl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet (2010)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biologically inspired computer vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;supervised learning : &lt;a href=&#34;https://www.nature.com/articles/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nature.com/articles/srep11400&lt;/a&gt; (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more info&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;dynamics: Khoei et al (2017) - &lt;a href=&#34;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068&lt;/a&gt; ( &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more info&lt;/a&gt; )&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Back to the present: dealing with delays in biological and neuromorphic systems</title>
      <link>https://laurentperrinet.github.io/talk/2017-06-28-telluride/</link>
      <pubDate>Wed, 28 Jun 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-06-28-telluride/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tutorial on predictive coding</title>
      <link>https://laurentperrinet.github.io/talk/2017-06-30-telluride/</link>
      <pubDate>Wed, 28 Jun 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-06-30-telluride/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tutorial: Active inference for eye movements: Bayesian methods, neural inference, dynamics</title>
      <link>https://laurentperrinet.github.io/talk/2017-01-20-laconeu/</link>
      <pubDate>Fri, 20 Jan 2017 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-01-20-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tutorial: Sparse optimization in neural computations</title>
      <link>https://laurentperrinet.github.io/talk/2017-01-19-laconeu/</link>
      <pubDate>Thu, 19 Jan 2017 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-01-19-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Back to the present: how neurons deal with delays</title>
      <link>https://laurentperrinet.github.io/talk/2017-01-18-laconeu/</link>
      <pubDate>Wed, 18 Jan 2017 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-01-18-laconeu/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://laconeu.cl/wp-content/uploads/2018/04/Valparaiso-3.jpg&#34; alt=&#34;Chile&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Participation au jury et entretien avec Clara Delmon</title>
      <link>https://laurentperrinet.github.io/talk/2016-11-20-polly-maggoo/</link>
      <pubDate>Sun, 20 Nov 2016 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-11-20-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;rencontres-internationales-sciences--cin√©mas&#34;&gt;RENCONTRES INTERNATIONALES SCIENCES &amp;amp; CIN√âMAS&lt;/h1&gt;
&lt;h2 id=&#34;cin√©ma-les-vari√©t√©s&#34;&gt;cin√©ma les Vari√©t√©s&lt;/h2&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-httppollymaggooorgwp-contentuploads201610risc2016_a3-724x1024jpg&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg&#34; alt=&#34;http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; programme la
10e √©dition des RENCONTRES INTERNATIONALES SCIENCES &amp;amp; CIN√âMAS (RISC) √†
Marseille, au cours desquelles l&amp;rsquo;association programme des films √†
caract√®re scientifique. Les projections se d√©roulent en pr√©sence de
chercheurs et/ou de cin√©astes dans la perspective d‚Äôun d√©veloppement de
la culture cin√©matographique et scientifique en direction des publics
scolaires.
Ce dimanche 20 novembre, je suis venu √©changer au c√¥t√© de Serge Dentin
et Caroline Renard (Ma√Ætre de conf√©rences en √©tudes cin√©matographiques √†
Aix-Marseille Universit√©), autour de films traitant du rapport
fiction/r√©el, de la m√©moire, et du temps. Une occasion aussi de parler
du m√©tier de chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
25 Avril 2016&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
cin√©ma les Vari√©t√©s&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&amp;ldquo;addendum&amp;rdquo; court m√©trage de J√©r√¥me Lefdup et &amp;ldquo;Po√©tique du cerveau&amp;rdquo;
long m√©trage de Nurith Aviv&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;entretien-avec-clara-delmon&#34;&gt;entretien avec Clara Delmon&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;occasion aussi d&amp;rsquo;un entretien avec Clara Delmon dans le cadre de son
m√©moire de DSAA (Dipl√¥me Sup√©rieur d‚ÄôArts Appliqu√©s) mention Design
Graphique √† Marseille, disponible sur
&lt;a href=&#34;http://www.tonerkebab.fr/wiki/doku.php/wiki:proto-memoires:clara-delmon:clara-delmon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.tonerkebab.fr/wiki/doku.php/wiki:proto-memoires:clara-delmon:clara-delmon&lt;/a&gt;
et &lt;a href=&#34;https://www.behance.net/claradelmon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.behance.net/claradelmon&lt;/a&gt; &lt;a href=&#34;http://www.tonerkebab.fr/wiki/lib/exe/fetch.php/wiki:proto-memoires:clara-delmon:clara_synthe_se.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;L‚Äô√©chec de la
perception&amp;rdquo;&lt;/a&gt;.
Entretien avec Laurent PERRINET, rencontr√© √† la 10e √©dition des RISC
(Rencontres Internationales de la Science et du Cin√©ma) chercheur au
CNRS (Centre National de la Recherche Scientifque) √† l‚ÄôInstitut de
Neurosciences de la Timone √† Marseille, sp√©cialis√© en perception
visuelle.&lt;/p&gt;
&lt;h2 id=&#34;entretien&#34;&gt;Entretien&lt;/h2&gt;
&lt;p&gt;Entretien avec Laurent PERRINET, rencontr√© √† la 10e √©dition des RISC
(Rencontres Internationales de la Science et du Cin√©ma)chercheur au CNRS
(Centre National de la Recherche Scientifque) √† l‚ÄôInstitut de
Neurosciences de la Timone √† Marseille, sp√©cialis√© en perception
visuelle. Ôªø
&lt;strong&gt;1 / Vous faites les Rencontres Internationales de la Science et du
CineÃÅma depuis quelques anneÃÅes deÃÅjaÃÄ, la science est de plus en plus
preÃÅsente dans les arts, comme avec certains courants artistiques comme
l‚ÄôArt CineÃÅtique ou l‚ÄôArt Optique, pourquoi pensez-vous qu‚Äôune telle
interaction est preÃÅsente aÃÄ notre eÃÅpoque ? J‚Äôai la sensation qu‚Äôil y a
un inteÃÅreÃÇt grandissant pour l‚ÄôeÃÅtude du cerveau dans le domaine des
arts et de la communication. AÃÄ votre avis, pourquoi un tel besoin de
donner de la poeÃÅsie au cerveau, (ou du cerveau aÃÄ la poeÃÅsie) ?&lt;/strong&gt;
En effet, je participe aux Rencontres Internationales de la Science et
du CineÃÅma depuis deÃÅjaÃÄ deux ans deÃÅjaÃÄ. Le but est simplement de
rentrer en contact avec le grand public et partager ma passion pour
l‚ÄôeÃÅtude de la perception visuelle et du cerveau plus geÃÅneÃÅralement.
J‚Äôattache beaucoup d‚Äôimportance aÃÄ ces rencontres car elle nous
permettent aussi de mieux comprendre l‚ÄôinteÃÅreÃÇt public pour le cerveau
dans son fonctionnement normal mais aussi dans ses dysfonctions. C‚Äôest
aussi une source d‚Äôinspiration pour savoir dans quelle direction il est
important de plus creuser nos recherches.
&lt;strong&gt;2 / Vous travaillez notamment avec Etienne Rey sur des installations
interactives, ouÃÄ la place et le ressenti du spectateur font l‚Äô≈ìuvre. La
vue est alors votre outil de travail essentiel, pourquoi ce sens est-il
plus sensiblement exposeÃÅ aÃÄ l‚ÄôexpeÃÅrience de l‚Äôillusion ? Qu‚Äôapporte
l‚ÄôexpeÃÅrience perceptive au spectateur ?&lt;/strong&gt;
En effet, en paralleÃÄle de ces actions de partage avec le public, je
travaille aussi avec &lt;em&gt;EÃÅtienne Rey&lt;/em&gt;, un artiste plasticien reÃÅsidant aÃÄ
la Friche Belle de mai aÃÄ Marseille. Notre travail s‚Äôarticule autour de
l‚ÄôambiguiÃàteÃÅ de l‚ÄôexpeÃÅrience perceptive du spectateur.
Est-il en train de se regarder lui-meÃÇme dans un miroir ou le miroir
est-il lui-meÃÇme une ≈ìuvre d‚Äôart ?
&lt;strong&gt;3 / Les graphistes d‚Äôaujourd‚Äôhui ont tendance aÃÄ brouiller les codes,
deÃÅformer, rendre illisible, en bref utiliser la complexiteÃÅ de l‚Äôimage
pour en complexifier la lecture. Pensez-vous qu‚Äôune image ouÃÄ on ne voit
rien puisse en dire plus ? C‚Äôest-aÃÄ-dire, pensez-vous qu‚Äôen accentuant
l‚Äôacte de lecture, le designer graphique ameÃÄne aÃÄ son lecteur une
activiteÃÅ qui consisterait non plus seulement aÃÄ deÃÅchiffrer un message
(preÃÅsentation d‚Äôun eÃÅveÃÄnement, publiciteÃÅ&amp;hellip;) mais aÃÄ s‚Äôobserver
lui-meÃÇme en tant que lecteur ?&lt;/strong&gt;
Le travail du systeÃÄme visuel est de deÃÅcoder les messages ambigus qui
lui sont deÃÅlivreÃÅs par la reÃÅtine. En creÃÅant des oeuvres graphiques
qui brouillent les codes et en les deÃÅformants, on oblige le cerveau aÃÄ
avoir une deÃÅmarche plus active par rapport au deÃÅcodage du message
fourni.
Tout le travail du graphiste consiste donc aÃÄ indiquer ce processus
actif tout en conservant l‚ÄôinteÃÅgriteÃÅ du message.
&lt;strong&gt;4 / Ces images utilisent le plus souvent des trames, des rayures, des
distorsions qui captent notre attention. Pourquoi notre ≈ìil est plus
attireÃÅ par ce qui est en mouvement ?&lt;/strong&gt;
Notre oeil est attireÃÅ par tout ce qui est surprenant. Cela inclut donc
tout ce qui ne peut pas arriver par hasard comme des bouts de lignes
aligneÃÅs. Mais notre oeil est aussi attireÃÅ par ce qu‚Äôil trouve
surprenant de ne pas pouvoir preÃÅdire, comme par exemple des lignes qui
sont leÃÅgeÃÄrement deÃÅcaleÃÅes ou un objet qui est en mouvement. Un
processus actif s‚ÄôeÃÅtablit alors pour comprendre cette stimulation avec
de nouvelles hypotheÃÄses.
&lt;strong&gt;5 / Il semblerait que notre ≈ìil soit attireÃÅ par des formes, des
couleurs, des objets particuliers qui diffeÃÄrent pour chacun d‚Äôentre
nous. Il y a dans la perception visuelle des notions de pulsions, de
deÃÅsirs, un besoin de voir, comment expliquez-vous que le cerveau soit
sans cesse en queÃÇte et en attente d‚Äôimages ?&lt;/strong&gt;
Pour moi la perception visuelle n‚Äôest pas juste un cineÃÅma aÃÄ
l‚ÄôinteÃÅrieur du cerveau !
C‚Äôest un processus vital qui sert aÃÄ mieux interagir avec
l‚Äôenvironnement. AÃÄ ce titre il est toujours en queÃÇte de nouvelles
images pour ameÃÅliorer ce rapport au monde que l‚Äôon construit sans
cesse. Il faut voir par exemple comment un enfant manipule des objets.
Il le fait pour mieux comprendre les images de ces objets et la facÃßon
dont il peut interagir avec le monde.
&lt;strong&gt;6 / On l‚Äôa vu notamment dans le film PoeÃÅtique du Cerveau de Nurith
Aviv diffuseÃÅ aÃÄ cette 10e eÃÅdition du RISC, la meÃÅmoire et
l‚ÄôexpeÃÅrience visuelle de chacun influent sur notre perception. Vous
avez parleÃÅ d‚Äô ¬´ autopoiÃàeÃÄse ¬ª, cela signifie-t-il que nous voyons tous
les choses diffeÃÅremment ? Est-ce qu‚Äôun systeÃÄme de donneÃÅes
preÃÅ-eÃÅtablies est formeÃÅ par notre cerveau au cours de nos anneÃÅes de
vie et sert de ¬´ lunettes ¬ª pour voir le monde ?&lt;/strong&gt;
La perception visuelle est un processus actif de compreÃÅhension d‚Äôune
repreÃÅsentation du monde. Elle est donc propre aÃÄ chacun car elle se
construit avec notre expeÃÅrience et la facÃßon dont nous interagissons
avec le monde visuel. Mais ce monde est le meÃÇme pour chaque individu et
nous partageons les meÃÇmes codes et les meÃÇmes systeÃÄmes pour apprendre
aÃÄ nous repreÃÅsenter ce monde.
Nos ¬´ lunettes ¬ª sont donc propres aÃÄ notre expeÃÅrience mais elles ont
suÃÇrement beaucoup en commun entre individus.
&lt;strong&gt;7 / Peut-on enlever ces lunettes? Des expeÃÅrimentations optiques comme
celles d‚ÄôEtienne Rey ou celles de designers graphiques conduisants une
reÃÅflexion sur notre vision peuvent-elles amener une nouvelle
expeÃÅrience visuelle remettant en question notre activiteÃÅ
perceptive?&lt;/strong&gt;
On ne pourra jamais enlever ses lunettes ! Pour voir, on est obligeÃÅ
d‚Äôinteragir avec le monde. Toute perception est une interpreÃÅtation et
ne pourra jamais eÃÇtre absolue : le monde physique nous est ¬´ cacheÃÅ ¬ª
par la meÃÅdiation avec nos sens, qui par essence sont toujours ambigus.
Par contre, ces expeÃÅrimentations optiques permettent de mieux
comprendre les limites de cet aspect de notre perception visuelle et
ainsi de donner un acceÃÄs plus direct avec cette conscience du monde
visuel.
&lt;strong&gt;8 / Le meÃÅcanisme d‚Äôanticipation mis aÃÄ l‚Äô≈ìuvre dans notre cerveau
faisant intervenir notre meÃÅmoire et notre imagination dans la
constitution d‚Äôune image stable ne nous eÃÅloigne-t-il pas trop de la
reÃÅaliteÃÅ ? Il y a une ¬´ imagination anticipative ¬ª et une confirmation
de ce reÃÅel par la mise en tension de nos projections avec la situation
preÃÅsente, ce systeÃÄme n‚Äôest-il pas proche de celui de l‚Äôillusion
d‚Äôoptique ?&lt;/strong&gt;
Au contraire je pense que ces meÃÅcanismes d‚Äôanticipation sont plus
proches de la reÃÅaliteÃÅ que celle qu‚Äôon imagine eÃÇtre la ¬´ vraie ¬ª
reÃÅaliteÃÅ. Par exemple on ne voit que dans un spectre de lumieÃÄre treÃÄs
deÃÅfini alors que les objets visuels existent potentiellement par
exemple dans la lumieÃÄre ultraviolette. Cette reÃÅaliteÃÅ laÃÄ n‚Äôest
visible qu‚Äôavec des appareils speÃÅcialiseÃÅs.
Pour moi la seule reÃÅaliteÃÅ qui vaille, c‚Äôest la reÃÅaliteÃÅ de la
construction qui est opeÃÅreÃÅe dans la perception visuelle et non la
reÃÅaliteÃÅ geÃÅneÃÅralement eÃÅtablie du monde physique externe aÃÄ nos
sens.
En comprenant mieux les meÃÅcanismes qui nous permettent de simuler cette
reÃÅaliteÃÅ physique externe, nous sommes plus objectifs par rapport aux
limites de notre connaissance du monde.
AÃÄ ce titre je pense que ces meÃÅcanismes d‚Äôanticipation sont donc plus
proches de la reÃÅaliteÃÅ par rapport aÃÄ une reÃÅaliteÃÅ objective telle
qu‚Äôon se la repreÃÅsente traditionnellement.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The flash-lag effect as a motion-based predictive shift</title>
      <link>https://laurentperrinet.github.io/talk/2016-11-03-sigma/</link>
      <pubDate>Thu, 03 Nov 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-11-03-sigma/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt; and &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;Khoei et al, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement contingencies modulate anticipatory smooth eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2016-11-03-gdr/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-11-03-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biologically-inspired characterization of sparseness in natural images</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/</link>
      <pubDate>Wed, 26 Oct 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/</link>
      <pubDate>Wed, 26 Oct 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/james-a-bednar/&#34;&gt;James A Bednar&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2015).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;/a&gt;.
  &lt;em&gt;Scientific Reports&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01202447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;http://www.nature.com/articles/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-bednar-15/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PerrinetBednar15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1038/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Eye movements as a model for active inference</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-13-law/</link>
      <pubDate>Thu, 13 Oct 2016 10:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-13-law/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/chloe-pasturel/&#34;&gt;Chlo√© Pasturel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/anna-montagnini/&#34;&gt;Anna Montagnini&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;.
  &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal.archives-ouvertes.fr/hal-02394142&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/pasturel-montagnini-perrinet-20/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modelling the dynamics of cognitive processes: from the Bayesian brain to particles</title>
      <link>https://laurentperrinet.github.io/talk/2016-07-07-edp-proba/</link>
      <pubDate>Thu, 07 Jul 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-07-07-edp-proba/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau</title>
      <link>https://laurentperrinet.github.io/talk/2016-04-28-mejanes/</link>
      <pubDate>Thu, 28 Apr 2016 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-04-28-mejanes/</guid>
      <description>&lt;h1 id=&#34;les-illusions-visuelles-un-r√©v√©lateur-du-fonctionnement-de-notre-cerveau&#34;&gt;Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau&lt;/h1&gt;
&lt;h2 id=&#34;cycle-de-conf√©rences-tous-connect√©s-biblioth√®que-de-m√©janes&#34;&gt;Cycle de conf√©rences &amp;ldquo;Tous connect√©s&amp;rdquo;, Biblioth√®que de M√©janes&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;conf√©rence tout public √† la Biblioth√®que de M√©janes (Aix-en-Provence, Avril 2016)&#34; srcset=&#34;
               /talk/2016-04-28-mejanes/featured_hu6f6d5fb2faa25e28f30d4d4a74ac9231_213872_04215fbe6c12bc576e0d31f88551d1d4.webp 400w,
               /talk/2016-04-28-mejanes/featured_hu6f6d5fb2faa25e28f30d4d4a74ac9231_213872_d2f3144e0144aad08e7f37eb6cfe2353.webp 760w,
               /talk/2016-04-28-mejanes/featured_hu6f6d5fb2faa25e28f30d4d4a74ac9231_213872_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/talk/2016-04-28-mejanes/featured_hu6f6d5fb2faa25e28f30d4d4a74ac9231_213872_04215fbe6c12bc576e0d31f88551d1d4.webp&#34;
               width=&#34;570&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
28 Avril 2016&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
Biblioth√®que de M√©janes&lt;/li&gt;
&lt;li&gt;Visuels&lt;br&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2016-04-28_mejanes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau</title>
      <link>https://laurentperrinet.github.io/talk/2016-04-25-polly-maggoo/</link>
      <pubDate>Mon, 25 Apr 2016 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-04-25-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;les-illusions-visuelles-un-r√©v√©lateur-du-fonctionnement-de-notre-cerveau&#34;&gt;Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau&lt;/h1&gt;
&lt;h2 id=&#34;cin√©sciences-coll√®ge-clair-soleil&#34;&gt;Cin√©sciences, coll√®ge Clair Soleil&lt;/h2&gt;
&lt;p&gt;L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; met en place
tout le long de l‚Äôann√©e, des actions de culture scientifique et
artistique en direction des coll√®ges et des lyc√©es, les &lt;em&gt;Cin√©sciences&lt;/em&gt;,
au cours desquelles l&amp;rsquo;association programme des films √† caract√®re
scientifique, au sein d‚Äô√©tablissements scolaires. Les projections se
d√©roulent en pr√©sence de chercheurs et/ou de cin√©astes dans la
perspective d‚Äôun d√©veloppement de la culture cin√©matographique et
scientifique en direction des publics scolaires.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
25 Avril 2016&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
coll√®ge Clair Soleil, Marseille&lt;/li&gt;
&lt;li&gt;Visuels&lt;br&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2016-04-25_pollymagoo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction with neuromorphic hardware</title>
      <link>https://laurentperrinet.github.io/talk/2015-11-05-chile/</link>
      <pubDate>Thu, 05 Nov 2015 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2015-11-05-chile/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motion-based prediction with neuromorphic hardware</title>
      <link>https://laurentperrinet.github.io/talk/2015-10-07-gdr-bio-comp/</link>
      <pubDate>Wed, 07 Oct 2015 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2015-10-07-gdr-bio-comp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</link>
      <pubDate>Fri, 25 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see &lt;a href=&#34;https://laurentperrinet.github.io/publication/kaplan-khoei-14/&#34;&gt;Kaplan and al, 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>WP5 - Demo 1.3 : Spiking model of motion-based prediction</title>
      <link>https://laurentperrinet.github.io/talk/2014-03-20-manchester/</link>
      <pubDate>Thu, 20 Mar 2014 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-03-20-manchester/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Axonal delays and on-time control of eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Demo 1, Task4: Implementation of models showing emergence of cortical fields and maps</title>
      <link>https://laurentperrinet.github.io/talk/2013-11-26-brain-scales-demos/</link>
      <pubDate>Tue, 26 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2013-11-26-brain-scales-demos/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Together with Bernhard Kaplan, we talked about how we aim at &amp;ldquo;compiling&amp;rdquo; a predictive motion-based approach as a spiking neural networks and then as a parallel wafer systems in the BrainscaleS project (Demo 1, Task4).&lt;/li&gt;
&lt;li&gt;(private to the consortium: &lt;a href=&#34;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showMeetingInfoPage&amp;amp;meetingID=52&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showMeetingInfoPage&amp;meetingID=52&lt;/a&gt;  &lt;a href=&#34;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showAgenda&amp;amp;meetingID=52&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showAgenda&amp;meetingID=52&lt;/a&gt;  including copies of the slides)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge co-occurrences and categorizing natural images</title>
      <link>https://laurentperrinet.github.io/talk/2013-07-05-cerco/</link>
      <pubDate>Fri, 05 Jul 2013 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2013-07-05-cerco/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/james-a-bednar/&#34;&gt;James A Bednar&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2015).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;/a&gt;.
  &lt;em&gt;Scientific Reports&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01202447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;http://www.nature.com/articles/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-bednar-15/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PerrinetBednar15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1038/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why methods and tools are the key to artificial brain-like systems</title>
      <link>https://laurentperrinet.github.io/talk/2013-03-21-marseille/</link>
      <pubDate>Thu, 21 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2013-03-21-marseille/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see also: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/andrew-p-davison/&#34;&gt;Andrew P Davison&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/daniel-bruderle/&#34;&gt;Daniel Bruderle&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/jochen-eppler/&#34;&gt;Jochen Eppler&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/jens-kremkow/&#34;&gt;Jens Kremkow&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/eilif-muller/&#34;&gt;Eilif Muller&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/dejan-pecevski/&#34;&gt;Dejan Pecevski&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/pierre-yger/&#34;&gt;Pierre Yger&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2008).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/davison-08/&#34;&gt;PyNN: A Common Interface for Neuronal Network Simulators&lt;/a&gt;.
   &lt;em&gt;Frontiers in Neuroinformatics&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-00586786&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/davison-08/davison-08.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/davison-08/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/project/open-science/&#34;&gt;
     Project
   &lt;/a&gt;
   
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3389/neuro.11.011.2008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2012-05-10-itwist/</link>
      <pubDate>Thu, 10 May 2012 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-05-10-itwist/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/james-a-bednar/&#34;&gt;James A Bednar&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2015).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;/a&gt;.
  &lt;em&gt;Scientific Reports&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01202447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;http://www.nature.com/articles/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-bednar-15/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PerrinetBednar15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1038/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Apparent motion in V1 - Probabilistic approaches</title>
      <link>https://laurentperrinet.github.io/talk/2012-03-23-juelich/</link>
      <pubDate>Fri, 23 Mar 2012 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-03-23-juelich/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MotionClouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception</title>
      <link>https://laurentperrinet.github.io/talk/2012-03-22-juelich/</link>
      <pubDate>Thu, 22 Mar 2012 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-03-22-juelich/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Grabbing, tracking and sniffing as models for motion detection and eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-27-fil/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-27-fil/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-24-edinburgh/</link>
      <pubDate>Tue, 24 Jan 2012 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-24-edinburgh/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/james-a-bednar/&#34;&gt;James A Bednar&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2015).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;/a&gt;.
  &lt;em&gt;Scientific Reports&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01202447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;http://www.nature.com/articles/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-bednar-15/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PerrinetBednar15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1038/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</link>
      <pubDate>Thu, 12 Jan 2012 17:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2011-11-15-sfn/</link>
      <pubDate>Tue, 15 Nov 2011 08:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-11-15-sfn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Abstract Control Number: 17671&lt;/li&gt;
&lt;li&gt;Presentation Number: 530.04&lt;/li&gt;
&lt;li&gt;Presentation Time: 8:45am - 9:00am&lt;/li&gt;
&lt;li&gt;session:&lt;/li&gt;
&lt;li&gt;Session Type: Nanosymposium&lt;/li&gt;
&lt;li&gt;Session Number: 530&lt;/li&gt;
&lt;li&gt;Session Title: Development of Motor and Sensory Systems&lt;/li&gt;
&lt;li&gt;See a followup in 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/james-a-bednar/&#34;&gt;James A Bednar&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2015).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;/a&gt;.
  &lt;em&gt;Scientific Reports&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01202447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;http://www.nature.com/articles/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-bednar-15/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PerrinetBednar15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1038/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Demo 1, Task4: Implementation of models showing emergence of cortical fields and maps</title>
      <link>https://laurentperrinet.github.io/talk/2011-10-05-brain-scales-ess/</link>
      <pubDate>Wed, 05 Oct 2011 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-10-05-brain-scales-ess/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2011-09-28-ermites/</link>
      <pubDate>Wed, 28 Sep 2011 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-09-28-ermites/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/james-a-bednar/&#34;&gt;James A Bednar&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2015).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;/a&gt;.
  &lt;em&gt;Scientific Reports&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01202447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;http://www.nature.com/articles/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-bednar-15/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/PerrinetBednar15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1038/srep11400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Propri√©t√©s √©mergentes d&#39;un mod√®le de pr√©diction probabiliste utilisant un champ neural</title>
      <link>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</link>
      <pubDate>Sat, 02 Jul 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</guid>
      <description>&lt;p&gt;La finalit√© de cette manifestation est de permettre √† nos chercheurs de se r√©unir en groupes de travail et en ateliers afin de d√©couvrir la th√©matique des neurosciences et son interdisciplinarit√©. La manifestation se tient dans le cadre des activit√©s du laboratoire LAMS, de ABC MATHINFO, du GDRI NeurO et du r√©seau m√©diterran√©en &lt;a href=&#34;http://www.neuromedproject.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroMed&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;SPIE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic models of the low-level visual system: the role of prediction in detecting motion</title>
      <link>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</link>
      <pubDate>Fri, 17 Dec 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</guid>
      <description>&lt;p&gt;An event ranging &amp;ldquo;From Mathematical Image Analysis to Neurogeometry of the Brain&amp;rdquo; &lt;a href=&#34;http://www.conftauc.cnrs-gif.fr/programme.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LADISLAV TAUC &amp;amp; GDR MSPC NEUROSCIENCES CONFERENCE&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication from Mina Khoei @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;TAUC 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Diffraction monochromatique, spectre audiographique</title>
      <link>https://laurentperrinet.github.io/talk/2010-04-14-ondes-paralleles/</link>
      <pubDate>Wed, 14 Apr 2010 19:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-04-14-ondes-paralleles/</guid>
      <description>&lt;h1 id=&#34;diffraction-monochromatique-spectre-audiographique&#34;&gt;Diffraction monochromatique, spectre audiographique&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;ttp://ondesparalleles.org/wp-content/uploads/2014/02/cloche_fiche_a.jpg&#34; alt=&#34;Diffraction&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diffraction est une sculpture en suspension compos√©e d‚Äôune multitude de plaques de mati√®re transparente et r√©fl√©chissante. L‚Äôinstallation met en jeu notre perception de l‚Äôespace par des ph√©nom√®nes de r√©sonance et de r√©flection de la lumi√®re. Chaque lieu d‚Äôexposition donne √† exp√©rimenter et √† √©laborer, in situ, de nouvelles formes. A Seconde Nature, Etienne Rey abordera la relation entre le volume et le son en prenant comme base de construction un spectre audio, en collaboration avec l‚Äôartiste sonore Mathias Delplanque.&lt;/li&gt;
&lt;li&gt;Live de Mathias Delplanque et rencontre autour de Diffraction, le Mercredi 14 avril 2010: A l‚Äôoccasion de cette rencontre publique, quatre chercheurs sp√©cialistes de l‚Äôarchitecture, de la perception, du son, et de la lumi√®re exposeront depuis leurs domaines de recherches les processus engag√©s autour de Diffraction.`&lt;/li&gt;
&lt;li&gt;Farid Ameziane, Ecole Nationale Sup√©rieure d‚ÄôArchitecture de Marseille Luminy (EAML), Directeur de l‚ÄôInsARTis, Marseille&lt;/li&gt;
&lt;li&gt;Guillaume Bonello, Charg√© de mission, POPsud, co/OAMP, Marseille&lt;/li&gt;
&lt;li&gt;Fabrice Mortessagne, Directeur du laboratoire de Physique de la Mati√®re Condens√©e (LPMC), Nice-Sophia Antipolis&lt;/li&gt;
&lt;li&gt;Laurent Perrinet, Chercheur √† l‚ÄôInstitut de Neurosciences Cognitives de M√©diterran√©e, Equipe DyVA, Marseille&lt;/li&gt;
&lt;li&gt;Mod√©ratrice : Colette Tron, Fondatrice d‚ÄôAlphabetville, Marseille&lt;/li&gt;
&lt;li&gt;Entr√©e libre &amp;amp; gratuite - 19h, dur√©e 2h.&lt;/li&gt;
&lt;li&gt;Renseignements pratiques :&lt;/li&gt;
&lt;li&gt;Espace Sextius investi par Seconde Nature  :&lt;/li&gt;
&lt;li&gt;27bis rue du 11 novembre,&lt;/li&gt;
&lt;li&gt;13100 Aix-en-Provence&lt;/li&gt;
&lt;li&gt;(!) visitez le site de Seconde Nature&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notes-de-lintervention-de-laurent-perrinet&#34;&gt;notes de l&amp;rsquo;intervention de Laurent Perrinet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Qu&amp;rsquo;est-ce que voir?&lt;/strong&gt; En perception, les neurones ¬´ parlent ¬ª tous
en m√™me temps par de br√®ves impulsions √©lectrochimiques, g√©n√©rant un
m√©lange de signaux, un bruit. Pourtant c&amp;rsquo;est par eux que nous
pensons, voyons, sentons. Les ordinateurs sont diff√©rents, plus
rapides. Ils sont construits avec pour mod√®le la grammaire humaine
autour d‚Äôune unit√© centrale, car on imaginait la cognition sous cet
angle √† leur invention. Le bit est le quantum d‚Äôun &lt;strong&gt;algorithme
m√©canique&lt;/strong&gt; (th√®se de Church-Turing). Une th√©orie tranche par
rapport √† la pr√©c√©dente, propos√©e par ¬´von Neumann¬ª : beaucoup
d‚Äôunit√©s sont pr√©sentes dans le cerveau. Compar√©e √† la cha√Æne
logique du langage, dans cet algorithme, beaucoup d‚Äôautres cha√Ænes
et logiques se m√™lent. Comment vont-elles ¬´ parler ¬ª entre elles ?
Existe-t-il des &lt;strong&gt;algorithmes biologiques&lt;/strong&gt; ?
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;OUCHI&#34; srcset=&#34;
               /talk/2010-04-14-ondes-paralleles/ouchi_hu23e83ca23d06ab08652e13fad64505cb_81281_8ff78dfe2d44fb916dc27432b14b91de.webp 400w,
               /talk/2010-04-14-ondes-paralleles/ouchi_hu23e83ca23d06ab08652e13fad64505cb_81281_edd499645a692baddc73acb499cea6af.webp 760w,
               /talk/2010-04-14-ondes-paralleles/ouchi_hu23e83ca23d06ab08652e13fad64505cb_81281_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/talk/2010-04-14-ondes-paralleles/ouchi_hu23e83ca23d06ab08652e13fad64505cb_81281_8ff78dfe2d44fb916dc27432b14b91de.webp&#34;
               width=&#34;405&#34;
               height=&#34;332&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

D√©finir ce ¬´ langage ¬ª, c&amp;rsquo;est comprendre comment une &lt;strong&gt;somme
d‚Äôinformations locales&lt;/strong&gt; peut produire une &lt;strong&gt;perception globale&lt;/strong&gt;.
Comment en jouant avec les atomes du code, en les superposant, les ¬´
cassant ¬ª pour les mettre en r√©sonance, les neurosciences et l&amp;rsquo;artiste
questionnent le langage de notre pens√©e ? Quel est le code utilis√© par
les neurones pour communiquer (code neuronal ? existe-t-il un m√™me
&lt;strong&gt;vocabulaire&lt;/strong&gt; au sens homomorphique ?). En pratique, on apprend par
exemple la s√©lectivit√© √† l&amp;rsquo;orientation. Les ph√©nom√®nes d‚Äôorientation
sont radicaux √† la fin de l‚Äôexp√©rience, ¬´ gelant ¬ª son √©volution. Un
lien √©vident avec l‚Äôinstallation &lt;em&gt;Phytosph√®re&lt;/em&gt; d‚ÄôEtienne Rey.
L‚Äôinformation dans le cerveau se propage &lt;strong&gt;par diffusion, par
diffraction&lt;/strong&gt; (contamination des informations entre neurones pour
occuper l‚Äôespace), en &lt;strong&gt;lien avec le travail sur la lumi√®re d‚ÄôEtienne
Rey.&lt;/strong&gt; L&amp;rsquo;image a besoin de 30 millisecondes pour se diffuser de l‚Äô≈ìil
vers l‚Äôarri√®re du cr√¢ne et 85 millisecondes pour produire un r√©flexe
oculaire. Les neurosciences cherchent √† savoir comment comprendre la
&lt;strong&gt;globalit√© par l&amp;rsquo;√©mergence&lt;/strong&gt;.
Il y a donc une &lt;strong&gt;superposition d‚Äô√©tats&lt;/strong&gt;, comme dans la &lt;em&gt;diffraction&lt;/em&gt;
d‚ÄôEtienne Rey.
En perception, le m√©canisme
neuronal cherche √† &lt;strong&gt;sortir de l‚Äôambigu√Øt√©&lt;/strong&gt; premi√®re quand il conna√Æt
une image : il &lt;strong&gt;superpose&lt;/strong&gt; des particules √©l√©mentaires d&amp;rsquo;information,
les diffuse pour les prendre toutes. Ce qui √©merge est non lin√©aire. Le
cerveau interf√®re ces particules, donc les met en comp√©tition, en
coop√©ration (voir exp√©rience plus haut avec les neurones rouges et
bleus), dans une dynamique o√π ces particules se r√©orientent elles-m√™mes.
Elles cr√©ent des ph√©nom√®nes d‚Äôorganisation, se collent, deviennent plus
lumineuses. &lt;strong&gt;La perception n‚Äôest donc pas s√©quentielle mais fluide&lt;/strong&gt; et
la sortie de l&amp;rsquo;ambiguit√© depuis l&amp;rsquo;image pixel vient de l&amp;rsquo;introduction de
ces contraintes. Ainsi quand nous voyons un objet, nous le ¬´ capturons
¬ª. Quand nous sommes vus, nous cherchons √† nous s√©parer de cette
capture.
Un probl√®me classique est l&amp;rsquo;ambiguit√© du monde sensible. Une couleur que
l‚Äôon ne voit pas va appara√Ætre visuellement. &lt;strong&gt;L‚Äôinpainting&lt;/strong&gt; cr√©√© une
≈ìuvre qui correspond √† un m√©canisme neuronal, cherchant √† reproduire
toujours une m√™me structure. La m√©moire iconique du monde ext√©rieur va
impr√©gner le cerveau, s‚Äôy figer. Tout le probl√®me de la perception pour
les neurosciences repose sur deux dialectiques. La premi√®re pr√©sente une
analogie avec les images informatiques par pixels : ce serait en
neurosciences une m√©taphore de la sensation pure. La seconde rappelle
l‚Äôimage vectoris√©e : pour s‚Äôextraire de la sensation pure, le cerveau
retiendra des r√®gles proches des algorithmes. En cognition, il permet de
mettre en lumi√®re le symptome d**&amp;lsquo;autisme**. Dans un sch√©ma montrant un
bloc derri√®re un arbre, d√©passant des deux c√¥t√©s, sera d√©coup√©
visuellement par l‚Äôautiste en plusieurs morceaux distincts. Il ne
g√©n√©ralise pas l‚Äôinformation.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;diffractionFriche_0134.jpg&#34; srcset=&#34;
               /talk/2010-04-14-ondes-paralleles/featured_huf8e22210a5b712d30ed9259f43294b7e_139525_d1fa9440610b842ec7074fd6a4ecd60c.webp 400w,
               /talk/2010-04-14-ondes-paralleles/featured_huf8e22210a5b712d30ed9259f43294b7e_139525_94e53d4e3aaf3d5247c46c6c3c03ea17.webp 760w,
               /talk/2010-04-14-ondes-paralleles/featured_huf8e22210a5b712d30ed9259f43294b7e_139525_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/talk/2010-04-14-ondes-paralleles/featured_huf8e22210a5b712d30ed9259f43294b7e_139525_d1fa9440610b842ec7074fd6a4ecd60c.webp&#34;
               width=&#34;760&#34;
               height=&#34;505&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Comment √™tre s√ªr d‚Äôune perception globale
en d√©signant les modules de l‚Äôinstallation d‚ÄôEtienne Rey, ou signifiants
des atomes, dans ce passage du local au global ? Les modules ne se
voient pas forc√©ment dans l‚Äôinstallation, mais d‚Äôautres aspects sont
per√ßus. La relation √† l‚Äôatome, m√™me si elle n‚Äôest pas signifiante pour
le public, n‚Äôest pas primordiale. Le public voit une accumulation de ¬´
choses ¬ª, car par principe quand un ph√©nom√®ne est concentr√© ¬´ il se
passe des choses ¬ª par jeu de contraste. Le fait de bouger face √†
l‚Äôinstallation rend unique √† l&amp;rsquo;individu la perception et r√©alise la
globalit√© de l‚Äô≈ìuvre: on a alors passage de l‚Äôatome √† la forme globale.
Cette r√©solution rejoint Giotto et les d√©buts de la perspective en art
pictural. Il a r√©v√©l√© la question du point de vue, par positionnement et
d√©placement. En effet, les personnes penchent la t√™te dans
l‚Äôinstallation s&lt;em&gt;pirale&lt;/em&gt; en container, d‚ÄôEtienne Rey, pour le festival
Ozosph√®re √† Strasbourg. Ce ph√©nom√®ne est √† rattach√© aux th√©ories sur la
perception.
&lt;strong&gt;Biographie&lt;/strong&gt; Laurent Perrinet, chercheur √† l‚ÄôInstitut de Neurosciences
Cognitives de la M√©diterran√©e √† Marseille, unit√© mixte du CNRS, aime
citer ¬´ La vie de Brian ¬ª des Monty Python : (Brian:) &amp;ldquo;You have to work
it out for yourselves!&amp;rdquo; (Crowd:) &amp;ldquo;Yes, we have to work it out for
ourselves&amp;hellip; (silence) Tell us more!&amp;rdquo;. L‚Äôindividualit√© et la perception
du monde‚Ä¶ Dans l‚Äô√©quipe DyVA (pour Dynamique de la perception visuelle
et de l&amp;rsquo;action), Laurent Perrinet s&amp;rsquo;int√©resse aux neurones impulsionnels
et au codage neuronal, ainsi qu‚Äô√† la perception des mouvements
spatio-temporels. Ces processus d√©finis comme des algorithmes, la
repr√©sentation du flux vid√©o mod√©lise via l‚Äôinformatique ces
interactions au niveau cellulaire (colonnes corticales) et au niveau
cognitif (aires corticales). Il cherche √† comprendre le fonctionnement
des calculs corticaux dans le syst√®me visuel. Cette recherche fournit
des r√©ponses aux probl√®mes cognitifs. Apr√®s un dipl√¥me d&amp;rsquo;ing√©nieur de
traitement du signal et de mod√©lisation stochastique de l&amp;rsquo;√©cole
d‚Äôa√©ronautique Supa√©ro √† Toulouse et des √©tudes √† San Diego et √†
Pasadena (Californie) pour la Nasa, Laurent Perrinet obtient un doctorat
de Sciences Cognitives. R√©pondant aux questions ¬´ Peut-on parler
d‚Äôintelligence m√©canique ? ¬ª, ¬´ Pourquoi une grenouille gobe mieux une
mouche qu‚Äôun robot ? ¬ª ou ¬´ Quelle est la diff√©rence entre intelligence
et algorithme ? ¬ª, il intervient en 2009 au colloque marseillais ¬´ Les
chemins de l‚Äôintelligence ¬ª. Parmi ses publications : &lt;em&gt;Role of
homeostasis in learning sparse representations&lt;/em&gt;, et sa th√®se &lt;em&gt;Comment
d√©chiffrer le code impulsionnel de la vision ? √âtude du flux parall√®le,
asynchrone et √©pars dans le traitement visuel ultra-rapide&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Models of low-level vision: linking probabilistic models and neural masses</title>
      <link>https://laurentperrinet.github.io/talk/2010-01-08-facets/</link>
      <pubDate>Fri, 08 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-01-08-facets/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Reading out the dynamics of lateral interactions in the primary visual cortex from VSD data</title>
      <link>https://laurentperrinet.github.io/talk/2009-11-30-vss/</link>
      <pubDate>Mon, 30 Nov 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-11-30-vss/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent poster @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-09-vss/&#34;&gt;VSS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Peut-on parler d&#39;intelligence m√©canique?</title>
      <link>https://laurentperrinet.github.io/talk/2009-11-24-intelligence-mecanique/</link>
      <pubDate>Tue, 24 Nov 2009 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-11-24-intelligence-mecanique/</guid>
      <description>&lt;p&gt;Nous parlerons de cette partie &amp;ldquo;m√©canique&amp;rdquo; du cerveau animal ou humain qui permet de percevoir les mouvements et de &amp;hellip; survivre au sein de l&amp;rsquo;environnement. On verra, par exemple, que notre cerveau peut-√™tre &lt;a href=&#34;http://interstices.info/classificateur&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;plus rapide que nous&lt;/a&gt;, qu&amp;rsquo;il y a des solutions &amp;ldquo;stupides&amp;rdquo; qui marchent remarquablement bien pour &lt;a href=&#34;http://interstices.info/generation-trajectoires&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sortir d&amp;rsquo;un labyrinthe&lt;/a&gt;, et qui si la grenouille sait &lt;a href=&#34;http://interstices.info/grenouille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gober une mouche bien mieux qu&amp;rsquo;un robot&lt;/a&gt; &amp;hellip; elle n&amp;rsquo;est pas plus maligne ! Parce que ce qu&amp;rsquo;il ne faut pas confondre ici c&amp;rsquo;est &lt;a href=&#34;https://interstices.info/calculer-penser/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;la diff√©rence entre calculer et penser&lt;/a&gt;, entre &lt;a href=&#34;http://interstices.info/algo-mode-emploi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intelligence et algorithmes&lt;/a&gt;. En comprenant cela, avec &lt;a href=&#34;http://fr.wikipedia.org/wiki/Alan_Turing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alan Mathison Turing&lt;/a&gt;, le Gutenberg du XX√®me si√®cle, l&amp;rsquo;humanit√© a bascul√© des temps modernes √† l&amp;rsquo;√®re du num√©rique.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(!) visitez le &lt;a href=&#34;https://interstices.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;site d&amp;rsquo;interstices&lt;/a&gt;!&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Control of the temporal interplay between excitation and inhibition by the statistics of visual input</title>
      <link>https://laurentperrinet.github.io/talk/2009-07-18-kremkow-09-cnstalk/</link>
      <pubDate>Sat, 18 Jul 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-07-18-kremkow-09-cnstalk/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this subsequent paper in the &lt;a href=&#34;https://laurentperrinet.github.io/publication/kremkow-10-jcns/&#34;&gt;Journal of Computational Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Decoding low-level neural information to track visual motion</title>
      <link>https://laurentperrinet.github.io/talk/2009-04-01-int/</link>
      <pubDate>Wed, 01 Apr 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-04-01-int/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Decoding the population dynamics underlying ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/talk/2008-06-01-ulm/</link>
      <pubDate>Sun, 01 Jun 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2008-06-01-ulm/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;related publications  @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-06-fens/&#34;&gt;FENS 2006&lt;/a&gt;,   @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-neurocomp/&#34;&gt;NeuroComp 2008&lt;/a&gt; and   @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-areadne/&#34;&gt;AREADNE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>From neural activity to behavior: computational neuroscience as a synthetic approach for understanding the neural code.</title>
      <link>https://laurentperrinet.github.io/talk/2008-04-01-incm/</link>
      <pubDate>Tue, 01 Apr 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2008-04-01-incm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling of spikes, sparseness and adaptation in the primary visual cortex: applications to imaging</title>
      <link>https://laurentperrinet.github.io/talk/2008-02-01-toledo/</link>
      <pubDate>Fri, 01 Feb 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2008-02-01-toledo/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;related publications  @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-06-fens/&#34;&gt;FENS 2006&lt;/a&gt;,   @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-neurocomp/&#34;&gt;NeuroComp 2008&lt;/a&gt; and   @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-areadne/&#34;&gt;AREADNE 2008&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What efficient code for adaptive spiking representations?</title>
      <link>https://laurentperrinet.github.io/talk/2007-12-01-rankprize/</link>
      <pubDate>Sat, 01 Dec 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2007-12-01-rankprize/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Codes for Adaptive Sparse Representations of Natural Images</title>
      <link>https://laurentperrinet.github.io/talk/2007-09-01-mipm/</link>
      <pubDate>Sat, 01 Sep 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2007-09-01-mipm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/talk/2006-01-01-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2006-01-01-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;related publication @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-spie/&#34;&gt;SPIE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
