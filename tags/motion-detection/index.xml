<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>motion detection | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/tags/motion-detection/</link>
      <atom:link href="https://laurentperrinet.github.io/tags/motion-detection/index.xml" rel="self" type="application/rss+xml" />
    <description>motion detection</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Thu, 24 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/img/hulk.png</url>
      <title>motion detection</title>
      <link>https://laurentperrinet.github.io/tags/motion-detection/</link>
    </image>
    
    <item>
      <title>Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli</title>
      <link>https://laurentperrinet.github.io/publication/ravello-19/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ravello-19/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www4.cnrs-dir.fr/insb/recherche/parutions/articles2019/l-perrinet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;d√®s-la-r√©tine-le-syst√®me-visuel-pr√©f√®re-des-images-naturelles&#34;&gt;D√®s la r√©tine, le syst√®me visuel pr√©f√®re des images naturelles&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Dans la r√©tine, au premier √©tage du traitement de l&amp;rsquo;image visuelle, on peut obtenir des repr√©sentations extr√™mement fines. Une collaboration entre des chercheurs fran√ßais et chiliens a permis de mettre en √©vidence que, dans la r√©tine de rongeurs, une repr√©sentation de la vitesse de l&amp;rsquo;image visuelle est pr√©cis√©ment cod√©e. Dans cette collaboration pluridisciplinaire, l&amp;rsquo;utilisation d&amp;rsquo;un mod√®le du fonctionnement de la r√©tine a permis de g√©n√©rer un nouveau type de stimuli visuels qui a r√©v√©l√© des r√©sultats exp√©rimentaux surprenants.&lt;/em&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Travail collaboratif et multi-disciplinaire entre &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar et &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; librement disponible sur &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; - merci √†  l&amp;#39;&lt;a href=&#34;https://twitter.com/AgenceRecherche?ref_src=twsrc%5Etfw&#34;&gt;@AgenceRecherche&lt;/a&gt; pour l&amp;#39;aide financi√®re et √† &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; + &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; pour l&amp;#39;&lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://t.co/YixRfpCrT3&#34;&gt;https://t.co/YixRfpCrT3&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1092139540788244480?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

La r√©tine est la premi√®re √©tape du traitement visuel, aux capacit√©s √©tonnantes. √Ä la diff√©rence d&amp;rsquo;un simple capteur comme ceux qu‚Äôon trouve dans les appareils photographiques num√©riques, ce mince tissu neuronal est un syst√®me complexe et encore largement m√©connu. Une meilleure connaissance de cette structure est essentielle pour la construction de capteurs du futur efficaces et √©conomes -par exemple ceux qui √©quiperont les futures voitures autonomes- mais aussi pour mieux comprendre des pathologies comme la D√©ficience Maculaire Li√©e √† l&amp;rsquo;Age (DMLA). Une des facettes m√©connues de la r√©tine est sa capacit√© √† d√©tecter des mouvements et cet article permet de mieux comprendre une partie des m√©canismes en jeu.
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New study on speed selectivity in the &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; showing that a majority of neurons prefer natural-like stimuli. Collaborative and multi-disciplinary work with &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar and &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; available with &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; at &lt;a href=&#34;https://t.co/Vb7GoRxjoT&#34;&gt;https://t.co/Vb7GoRxjoT&lt;/a&gt;&lt;/p&gt;&amp;mdash; Adrian Palacios (@APalacio_s) &lt;a href=&#34;https://twitter.com/APalacio_s/status/1092200890377879552?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Retinal cell preference for natural-like stimuli. Very elegant work by &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; et al. &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuroscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuroscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/decoding?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#decoding&lt;/a&gt; &lt;a href=&#34;https://t.co/3xNWaZd5x6&#34;&gt;https://t.co/3xNWaZd5x6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andres Canales-Johnson (@canalesjohnson) &lt;a href=&#34;https://twitter.com/canalesjohnson/status/1092211339311923201?ref_src=twsrc%5Etfw&#34;&gt;February 4, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Conciliant mod√©lisation et neurophysiologie, cette √©tude a permis de faire des pr√©dictions sur le traitement de l&amp;rsquo;information r√©tinienne et en particulier de g√©n√©rer des textures synth√©tiques qui sont optimales pour ces mod√®les (voir film). Les enregistrements effectu√©s sur la r√©tine de rongeurs diurnes Octodon degus ont ensuite permis de mesurer la s√©lectivit√© √† la vitesse mais aussi de valider une nouvelle fois ces mod√®les en reconstruisant l&amp;rsquo;image d&amp;rsquo;entr√©e √† partir de l&amp;rsquo;activit√© neurale.
Le r√©sultat le plus inattendu est la diff√©rence de s√©lectivit√© de certaines classes de neurones r√©tiniens par rapport √† la complexit√© du stimulus pr√©sent√©. En effet, la repr√©sentation de la vitesse est relativement peu pr√©cise si on utilise des r√©seaux de lignes (&amp;ldquo;Grating&amp;rdquo;), comme cela est d&amp;rsquo;habitude r√©alis√© dans la plupart des exp√©riences neurophysiologiques. Au contraire, elle devient plus pr√©cise si on utilise comme signaux visuels des textures artificielles ressemblant √† des nuages en mouvement (&amp;ldquo;MC Narrow&amp;rdquo;). En particulier, plus cette texture est complexe, plus la repr√©sentation est pr√©cise (&amp;ldquo;MC Broad&amp;rdquo;).
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/R%C3%A9sultatScientifique?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#R√©sultatScientifique&lt;/a&gt; üîç| D√®s la &lt;a href=&#34;https://twitter.com/hashtag/r%C3%A9tine?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#r√©tine&lt;/a&gt;, le syst√®me &lt;a href=&#34;https://twitter.com/hashtag/visuel?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#visuel&lt;/a&gt; pr√©f√®re des images naturelles&lt;br&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://t.co/BBY2IpGum6&#34;&gt;https://t.co/BBY2IpGum6&lt;/a&gt;&lt;br&gt;üìï &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; | &lt;a href=&#34;https://t.co/5mULuWTp3N&#34;&gt;https://t.co/5mULuWTp3N&lt;/a&gt;&lt;br&gt;ü§ù &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/LaurentPerrinet?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#LaurentPerrinet&lt;/a&gt; &lt;a href=&#34;https://t.co/34R1URHUic&#34;&gt;pic.twitter.com/34R1URHUic&lt;/a&gt;&lt;/p&gt;&amp;mdash; Biologie au CNRS (@INSB_CNRS) &lt;a href=&#34;https://twitter.com/INSB_CNRS/status/1091392027848294401?ref_src=twsrc%5Etfw&#34;&gt;February 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli. Beautiful article in Scientific Reports on an original animal model, the diurnal rodent Octodon degus. &lt;a href=&#34;https://t.co/BdzyzEVYnX&#34;&gt;https://t.co/BdzyzEVYnX&lt;/a&gt; (open access) &lt;a href=&#34;https://t.co/1UaoMYTFd2&#34;&gt;pic.twitter.com/1UaoMYTFd2&lt;/a&gt;&lt;/p&gt;&amp;mdash; St√©phane Deny (@StephaneDeny) &lt;a href=&#34;https://twitter.com/StephaneDeny/status/1090452532223045632?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Ces textures complexes sont plus proches des images naturellement observ√©es et ces r√©sultats montrent donc que d√®s la r√©tine, le syst√®me visuel est particuli√®rement adapt√© √† des stimulations naturelles. Ce r√©sultat devrait pouvoir s&#39;√©tendre √† des textures encore plus complexes et encore plus proches d&amp;rsquo;images naturelles, mais aussi pouvoir se g√©n√©raliser √† d&amp;rsquo;autres aires visuelles plus complexes, comme le cortex visuel primaire, et √† d&amp;rsquo;autres esp√®ces.





  
  











&lt;figure id=&#34;figure-pour-une-cellule-repr√©sentative-on-montre-ici-la-r√©ponse-au-cours-du-temps-sous-forme-dimpulsions-pour-diff√©rentes-pr√©sentations-trial-ainsi-que-la-moyenne-de-cette-r√©ponse-firing-rate-les-diff√©rentes-colonnes-repr√©sentent-diff√©rentes-vitesses-des-stimulations-sur-la-r√©tine-les-diff√©rentes-lignes-sont-diff√©rentes-stimulations-en-bleu-une-stimulation-classique-sous-forme-de-r√©seaux-de-lignes--grating--en-vert-et-orange-la-r√©ponse-√†-une-texture-progressivement-plus-complexe-de--mc-narrow--√†--mc-broad--si-les-r√©ponses-aux-diff√©rents-stimulations-sont-en-moyenne-similaires-elles-sont-variables-dessai-en-essai-et-une-analyse-statistique-a-permis-de-montrer-que-dans-la-majorit√©-des-cellules-les-r√©ponses-sont-dautant-plus-pr√©cises-que-la-stimulation-est-complexe--cesar-ravello-&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Pour une cellule repr√©sentative, on montre ici la r√©ponse au cours du temps sous forme d&amp;rsquo;impulsions pour diff√©rentes pr√©sentations (Trial) ainsi que la moyenne de cette r√©ponse (Firing rate). Les diff√©rentes colonnes repr√©sentent diff√©rentes vitesses des stimulations sur la r√©tine. Les diff√©rentes lignes sont diff√©rentes stimulations. En bleu, une stimulation classique sous forme de r√©seaux de lignes (¬´ Grating ¬ª). En vert et Orange, la r√©ponse √† une texture progressivement plus complexe (de ¬´ Mc Narrow ¬ª √† ¬´ MC Broad ¬ª). Si les r√©ponses aux diff√©rents stimulations sont en moyenne similaires, elles sont variables d‚Äôessai en essai et une analyse statistique a permis de montrer que dans la majorit√© des cellules, les r√©ponses sont d&amp;rsquo;autant plus pr√©cises que la stimulation est complexe. ¬© Cesar Ravello&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;540&#34; height=&#34;416&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Pour une cellule repr√©sentative, on montre ici la r√©ponse au cours du temps sous forme d&amp;rsquo;impulsions pour diff√©rentes pr√©sentations (Trial) ainsi que la moyenne de cette r√©ponse (Firing rate). Les diff√©rentes colonnes repr√©sentent diff√©rentes vitesses des stimulations sur la r√©tine. Les diff√©rentes lignes sont diff√©rentes stimulations. En bleu, une stimulation classique sous forme de r√©seaux de lignes (¬´ Grating ¬ª). En vert et Orange, la r√©ponse √† une texture progressivement plus complexe (de ¬´ Mc Narrow ¬ª √† ¬´ MC Broad ¬ª). Si les r√©ponses aux diff√©rents stimulations sont en moyenne similaires, elles sont variables d‚Äôessai en essai et une analyse statistique a permis de montrer que dans la majorit√© des cellules, les r√©ponses sont d&amp;rsquo;autant plus pr√©cises que la stimulation est complexe. ¬© Cesar Ravello
  &lt;/figcaption&gt;


&lt;/figure&gt;













  


&lt;video controls &gt;
  &lt;source src=&#34;video_perrinet.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
Cette vid√©o montre les trois classes de stimulations utilis√©es dans cette √©tude. En plus des r√©seaux sinuso√Ødaux (‚ÄúGrating‚Äù) qui sont classiquement utilis√©s en neurosciences, cette √©tude a utilis√© des textures al√©atoires (Motion Clouds (MC)) qui sont inspir√©es de mod√®les du traitement visuel. Ils permettent en particulier de manipuler des param√®tres visuels critiques comme la vari√©t√© de fr√©quences spatiales qui sont superpos√©es: soit unique (‚ÄúGrating‚Äù), fine (‚ÄúMC Narrow‚Äù), soit plus large (‚ÄúMC Broad‚Äù). Ces vid√©os ont √©t√© directement projet√©es sur des r√©tines pos√©es sur des grilles d‚Äô√©lectrodes qui permettent de mesurer l‚Äôactivit√© neurale (voir figure). ¬© Laurent Perrinet / Cesar Ravello&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures</title>
      <link>https://laurentperrinet.github.io/publication/vacher-16/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speed uncertainty and motion perception with naturalistic random textures</title>
      <link>https://laurentperrinet.github.io/publication/mansour-18-vss/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-18-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-17-gdr/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-17-gdr/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How the dynamics of human smooth pursuit is influenced by speed uncertainty</title>
      <link>https://laurentperrinet.github.io/publication/mansour-17-ecvp/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-17-ecvp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-17-gdr/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-17-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-16-ecvp/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-16-ecvp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-16-gdr/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-16-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-16-sfn/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-16-sfn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active inference, eye movements and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</link>
      <pubDate>Tue, 16 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</guid>
      <description>&lt;h1 id=&#34;active-inference-tracking-eye-movements-and-oculomotor-delays&#34;&gt;Active Inference, tracking eye movements and oculomotor delays&lt;/h1&gt;
&lt;p&gt;Tracking eye movements face a difficult task: they have to be fast while they suffer inevitable delays. If we focus on area MT of humans for instance as it is crucial for detecting the motion of visual objects, sensory information coming to this area is already lagging some 35 milliseconds behind operational time ‚Äì that is, it reflects some past information. Still the fastest action that may be done there is only able to reach the effector muscles of the eyes some 40 milliseconds later ‚Äì that is, in the future. The tracking eye movement system is however able to respond swiftly and even to anticipate repetitive movements (e.g. Barnes et al, 2000 ‚Äì refs in manuscript). In that case, it means that information in a cortical area is both predicted from the past sensory information but also anticipated to give an optimal response in the future. Even if numerous models have been described to model different mechanisms to account for delays, no theoretical approach has tackled the whole problem explicitly. In several areas of vision research, authors have proposed models at different levels of abstractions from biomechanical models, to neurobiological implementations (e.g. Robinson, 1986) or Bayesian models. This study is both novel and important because ‚Äì using a neurobiologically plausible hierarchical Bayesian model ‚Äì it demonstrates that using generalized coordinates to finesse the prediction of a target&amp;rsquo;s motion, the model can reproduce characteristic properties of tracking eye movements in the presence of delays. Crucially, the different refinements to the model that we propose ‚Äì pursuit initiation, smooth pursuit eye movements, and anticipatory response ‚Äì are consistent with the different types of tracking eye movements that may be observed experimentally.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-a-this-figure-reports-the-response-of-predictive-processing-during-the-simulation-of-pursuit-initiation-using-a-single-sweep-of-a-visual-target-while-compensating-for-sensory-motor-delays-here-we-see-horizontal-excursions-of-oculomotor-angle-red-line-one-can-see-clearly-the-initial-displacement-of-the-target-that-is-suppressed-by-action-after-a-few-hundred-milliseconds-additionally-we-illustrate-the-effects-of-assuming-wrong-sensorimotor-delays-on-pursuit-initiation-under-pure-sensory-delays-blue-dotted-line-one-can-see-clearly-the-delay-in-sensory-predictions-in-relation-to-the-true-inputs-with-pure-motor-delays-blue-dashed-line-and-with-combined-sensorimotor-delays-blue-line-there-is-a-failure-of-optimal-control-with-oscillatory-fluctuations-in-oculomotor-trajectories-which-may-become-unstable-b-this-figure-reports-the-simulation-of-smooth-pursuit-when-the-target-motion-is-hemi-sinusoidal-as-would-happen-for-a-pendulum-that-would-be-stopped-at-each-half-cycle-left-of-the-vertical-broken-black-lines-in-the-lower-right-panel-we-report-the-horizontal-excursions-of-oculomotor-angle-the-generative-model-used-here-has-been-equipped-with-a-second-hierarchical-level-that-contains-hidden-states-modeling-latent-periodic-behavior-of-the-hidden-causes-of-target-motion-with-this-addition-the-improvement-in-pursuit-accuracy-apparent-at-the-onset-of-the-second-cycle-of-motion-is-observed-pink-shaded-area-similar-to-psychophysical-experimentss&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;(A) This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. (B) This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2657&#34; height=&#34;1417&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;(A)&lt;/strong&gt; This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. &lt;strong&gt;(B)&lt;/strong&gt; This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Motion-based prediction model for flash lag effect</title>
      <link>https://laurentperrinet.github.io/publication/khoei-14-vss/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-14-vss/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt; and 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;Khoei et al, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The characteristics of microsaccadic eye movements varied with the change of strategy in a match-to-sample task</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-14-vss/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-14-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</link>
      <pubDate>Fri, 25 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/kaplan-khoei-14/&#34;&gt;Kaplan and al, 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Axonal delays and on-time control of eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/publication/kaplan-khoei-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kaplan-khoei-14/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;















&lt;figure id=&#34;figure-figure-4-rasterplot-of-input-and-output-spikes-the-raster-plot-from-excitatory-neurons-is-ordered-according-to-their-position-each-input-spike-is-a-blue-dot-and-each-output-spike-is-a-black-dot-while-input-is-scattered-during-blanking-periods-figure-1-the-network-output-shows-shows-some-tuned-activity-during-the-blank-compare-with-the-activity-before-visual-stimulation-to-decode-such-patterns-of-activity-we-used-a-maximum-likelihood-estimation-technique-based-on-the-tuning-curve-of-the-neurons&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.frontiersin.org/files/Articles/53894/fncom-07-00112-r2/image_m/fncom-07-00112-g003.jpg&#34; data-caption=&#34;Figure 4: Rasterplot of input and output spikes. The raster plot from excitatory neurons is ordered according to their position. Each input spike is a blue dot and each output spike is a black dot. While input is scattered during blanking periods (Figure 1), the network output shows shows some tuned activity during the blank (compare with the activity before visual stimulation). To decode such patterns of activity we used a maximum-likelihood estimation technique based on the tuning curve of the neurons.&#34;&gt;


  &lt;img src=&#34;https://www.frontiersin.org/files/Articles/53894/fncom-07-00112-r2/image_m/fncom-07-00112-g003.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 4: &lt;em&gt;Rasterplot of input and output spikes.&lt;/em&gt; The raster plot from excitatory neurons is ordered according to their position. Each input spike is a blue dot and each output spike is a black dot. While input is scattered during blanking periods (Figure 1), the network output shows shows some tuned activity during the blank (compare with the activity before visual stimulation). To decode such patterns of activity we used a maximum-likelihood estimation technique based on the tuning curve of the neurons.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction explains the role of tracking in motion extrapolation</title>
      <link>https://laurentperrinet.github.io/publication/khoei-13-jpp/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-13-jpp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;Khoei et al, 2017&lt;/a&gt;





  
  











&lt;figure id=&#34;figure-figure-1-the-problem-of-fragmented-trajectories-and-motion-extrapolation-as-an-object-moves-in-visual-space-as-represented-here-for-commodity-by-the-red-trajectory-of-a-tennis-ball-in-a-spacetime-diagram-with-a-one-dimensional-space-on-the-vertical-axis-the-sensory-flux-may-be-interrupted-by-a-sudden-and-transient-blank-as-denoted-by-the-vertical-gray-area-and-the-dashed-trajectory-how-can-the-instantaneous-position-of-the-dot-be-estimated-at-the-time-of-reappearance-this-mechanism-is-the-basis-of-motion-extrapolation-and-is-rooted-on-the-prior-knowledge-on-the-coherency-of-trajectories-in-natural-images-we-show-below-the-typical-eye-velocity-profile-that-is-observed-during-smooth-pursuit-eye-movements-spem-as-a-prototypical-sensory-response-it-consists-of-three-phases-first-a-convergence-of-the-eye-velocity-toward-the-physical-speed-second-a-drop-of-velocity-during-the-blank-and-finally-a-sudden-catch-up-of-speed-at-reappearance-becker-and-fuchs-1985&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Figure 1: The problem of fragmented trajectories and motion extrapolation. As an object moves in visual space (as represented here for commodity by the red trajectory of a tennis ball in a space‚Äìtime diagram with a one-dimensional space on the vertical axis), the sensory flux may be interrupted by a sudden and transient blank (as denoted by the vertical, gray area and the dashed trajectory). How can the instantaneous position of the dot be estimated at the time of reappearance? This mechanism is the basis of motion extrapolation and is rooted on the prior knowledge on the coherency of trajectories in natural images. We show below the typical eye velocity profile that is observed during Smooth Pursuit Eye Movements (SPEM) as a prototypical sensory response. It consists of three phases: first, a convergence of the eye velocity toward the physical speed, second, a drop of velocity during the blank and finally, a sudden catch-up of speed at reappearance (Becker and Fuchs, 1985).&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;293&#34; height=&#34;223&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 1: The problem of fragmented trajectories and motion extrapolation. As an object moves in visual space (as represented here for commodity by the red trajectory of a tennis ball in a space‚Äìtime diagram with a one-dimensional space on the vertical axis), the sensory flux may be interrupted by a sudden and transient blank (as denoted by the vertical, gray area and the dashed trajectory). How can the instantaneous position of the dot be estimated at the time of reappearance? This mechanism is the basis of motion extrapolation and is rooted on the prior knowledge on the coherency of trajectories in natural images. We show below the typical eye velocity profile that is observed during Smooth Pursuit Eye Movements (SPEM) as a prototypical sensory response. It consists of three phases: first, a convergence of the eye velocity toward the physical speed, second, a drop of velocity during the blank and finally, a sudden catch-up of speed at reappearance (Becker and Fuchs, 1985).
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Anisotropic connectivity implements motion-based prediction in a spiking neural network</title>
      <link>https://laurentperrinet.github.io/publication/kaplan-13/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kaplan-13/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-13-vss/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-13-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motion-based prediction and development of the response to an &#39;on the way&#39; stimulus</title>
      <link>https://laurentperrinet.github.io/publication/khoei-13-cns/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-13-cns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Smooth Pursuit and Visual Occlusion: Active Inference and Oculomotor Control in Schizophrenia</title>
      <link>https://laurentperrinet.github.io/publication/adams-12/</link>
      <pubDate>Fri, 26 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/adams-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;adams-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The behavioral receptive field underlying motion integration for primate tracking eye movements</title>
      <link>https://laurentperrinet.github.io/publication/masson-12/</link>
      <pubDate>Wed, 21 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/masson-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;masson-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grabbing, tracking and sniffing as models for motion detection and eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-27-fil/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-27-fil/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</link>
      <pubDate>Thu, 12 Jan 2012 17:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Effect of image statistics on fixational eye movements</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-12-vss/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-12-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception.</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-12-coding/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-12-coding/</guid>
      <description></description>
    </item>
    
    <item>
      <title>More is not always better: dissociation between perception and action explained by adaptive gain control</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;simoncini-12.png&#34; alt=&#34;header&#34;&gt;





  
  











&lt;figure id=&#34;figure-band-pass-motion-stimuli-for-perception-and-action-tasks-a-in-the-space-representing-temporal-against-spatial-frequency-each-line-going-through-the-origin-corresponds-to-stimuli-moving-at-the-same-speed-a-simple-drifting-grating-is-a-single-point-in-this-space-our-moving-texture-stimuli-had-their-energy-distributed-within-an-ellipse-elongated-along-a-given-speed-line-keeping-constant-the-mean-spatial-and-temporal-frequencies-the-spatio-temporal-bandwidth-was-manipulated-by-co-varying-bsf-and-btf-as-illustrated-by-the-xyt-examples-human-performance-was-measured-for-two-different-tasks-run-in-parallel-blocks-b-for-ocular-tracking-motion-stimuli-were-presented-for-a-short-duration-200ms-in-the-wake-of-a-centering-saccade-to-control-both-attention-and-fixation-states-c-for-speed-discrimination-test-and-reference-stimuli-were-presented-successively-for-the-same-duration-and-subjects-were-instructed-to-indicate-whether-the-test-stimulus-was-perceived-as-slower-or-faster-than-reference-&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_2000x2000_fit_lanczos.gif&#34; data-caption=&#34;Band-pass motion stimuli for perception and action tasks. (a) In the space representing temporal against spatial frequency, each line going through the origin corresponds to stimuli moving at the same speed. A simple drifting grating is a single point in this space. Our moving texture stimuli had their energy distributed within an ellipse elongated along a given speed line, keeping constant the mean spatial and temporal frequencies. The spatio-temporal bandwidth was manipulated by co-varying Bsf and Btf as illustrated by the (x,y,t) examples. Human performance was measured for two different tasks, run in parallel blocks. (b) For ocular tracking, motion stimuli were presented for a short duration (200ms) in the wake of a centering saccade to control both attention and fixation states. (c) For speed discrimination, test and reference stimuli were presented successively for the same duration and subjects were instructed to indicate whether the test stimulus was perceived as slower or faster than reference.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;256&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Band-pass motion stimuli for perception and action tasks.&lt;/em&gt; (a) In the space representing temporal against spatial frequency, each line going through the origin corresponds to stimuli moving at the same speed. A simple drifting grating is a single point in this space. Our moving texture stimuli had their energy distributed within an ellipse elongated along a given speed line, keeping constant the mean spatial and temporal frequencies. The spatio-temporal bandwidth was manipulated by co-varying Bsf and Btf as illustrated by the (x,y,t) examples. Human performance was measured for two different tasks, run in parallel blocks. (b) For ocular tracking, motion stimuli were presented for a short duration (200ms) in the wake of a centering saccade to control both attention and fixation states. (c) For speed discrimination, test and reference stimuli were presented successively for the same duration and subjects were instructed to indicate whether the test stimulus was perceived as slower or faster than reference.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-11-vss/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-11-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-11-pattern/</link>
      <pubDate>Fri, 23 Sep 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-11-pattern/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Propri√©t√©s √©mergentes d&#39;un mod√®le de pr√©diction probabiliste utilisant un champ neural</title>
      <link>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</link>
      <pubDate>Sat, 02 Jul 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</guid>
      <description>&lt;p&gt;La finalit√© de cette manifestation est de permettre √† nos chercheurs de se r√©unir en groupes de travail et en ateliers afin de d√©couvrir la th√©matique des neurosciences et son interdisciplinarit√©. La manifestation se tient dans le cadre des activit√©s du laboratoire LAMS, de ABC MATHINFO, du GDRI NeurO et du r√©seau m√©diterran√©en 
&lt;a href=&#34;http://www.neuromedproject.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroMed&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;SPIE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Saccadic foveation of a moving visual target in the rhesus monkey</title>
      <link>https://laurentperrinet.github.io/publication/fleuriet-11/</link>
      <pubDate>Tue, 01 Feb 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fleuriet-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic models of the low-level visual system: the role of prediction in detecting motion</title>
      <link>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</link>
      <pubDate>Fri, 17 Dec 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</guid>
      <description>&lt;p&gt;An event ranging &amp;ldquo;From Mathematical Image Analysis to Neurogeometry of the Brain&amp;rdquo; 
&lt;a href=&#34;http://www.conftauc.cnrs-gif.fr/programme.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LADISLAV TAUC &amp;amp; GDR MSPC NEUROSCIENCES CONFERENCE&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication from Mina Khoei @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;TAUC 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Models of low-level vision: linking probabilistic models and neural masses</title>
      <link>https://laurentperrinet.github.io/talk/2010-01-08-facets/</link>
      <pubDate>Fri, 08 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-01-08-facets/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Different pooling of motion information for perceptual speed discrimination and behavioral speed estimation</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-10-vss/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-10-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical emergence of a neural solution for motion integration</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-areadne/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic models of the low-level visual system: the role of prediction in detecting motion</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-tauc/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-tauc/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Decoding center-surround interactions in population of neurons for the ocular following response</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-09-cosyne/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-09-cosyne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inferring monkey ocular following responses from V1 population dynamics using a probabilistic model of motion integration</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-09-vss/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-09-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamics of distributed 1D and 2D motion representations for short-latency ocular following</title>
      <link>https://laurentperrinet.github.io/publication/barthelemy-08/</link>
      <pubDate>Sun, 03 Feb 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/barthelemy-08/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;barthelemy-08.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decoding the population dynamics underlying ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-areadne/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling spatial integration in the ocular following response to center-surround stimulation using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-a/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical Neural Networks: modeling low-level vision at short latencies</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07/</link>
      <pubDate>Sat, 03 Mar 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07/</guid>
      <description>&lt;p&gt;Dynamical Neural Networks (DyNNs) are a class of models for networks of neurons where particular focus is put on the role of time in the emergence of functional computational properties. The definition and study of these models involves the cooperation of a large range of scientific fields from statistical physics, probabilistic modelling, neuroscience and psychology to control theory. It focuses on the mechanisms that may be relevant for studying cognition by hypothesizing that information is distributed in the activity of the neurons in the system and that the timing helps in maintaining this information to lastly form decisions or actions. The system responds at best to the constraints of the outside world and learning strategies tune this internal dynamics to achieve optimal performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic inference for motion tracking</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-07-a/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-07-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling spatial integration in the ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-07-neurocomp.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-fens/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-fens/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dynamics of motion representation in short-latency ocular following: A two-pathways Bayesian model</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-05-a/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-05-a/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
