<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>motion anticipation | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/tags/motion-anticipation/</link>
      <atom:link href="https://laurentperrinet.github.io/tags/motion-anticipation/index.xml" rel="self" type="application/rss+xml" />
    <description>motion anticipation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Thu, 26 Sep 2019 00:00:00 +0200</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/img/hulk.png</url>
      <title>motion anticipation</title>
      <link>https://laurentperrinet.github.io/tags/motion-anticipation/</link>
    </image>
    
    <item>
      <title>Humans adapt their anticipatory eye movements to the volatility of visual motion properties</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;get a &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v1&#34; target=&#34;_blank&#34;&gt;preprint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;source code for &lt;a href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2019&#34; target=&#34;_blank&#34;&gt;paper&lt;/a&gt; and for the &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM&#34; target=&#34;_blank&#34;&gt;framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A low-cost, accessible eye tracking framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-18-gdr/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-18-gdr/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;poster presented @ [[&lt;a href=&#34;https://gdrvision2018.sciencesconf.org/|GDR&#34; target=&#34;_blank&#34;&gt;https://gdrvision2018.sciencesconf.org/|GDR&lt;/a&gt; vision, Paris]].&lt;/li&gt;
&lt;li&gt;program : &lt;a href=&#34;https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf&#34; target=&#34;_blank&#34;&gt;https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;poster : &lt;a href=&#34;https://github.com/laurentperrinet/Perrinet18gdr/raw/master/Perrinet18gdr.pdf&#34; target=&#34;_blank&#34;&gt;https://github.com/laurentperrinet/Perrinet18gdr/raw/master/Perrinet18gdr.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;poster (code) :  &lt;a href=&#34;https://github.com/laurentperrinet/Perrinet18gdr/&#34; target=&#34;_blank&#34;&gt;https://github.com/laurentperrinet/Perrinet18gdr/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;source code for this framework: &lt;a href=&#34;https://github.com/laurentperrinet/CatchTheEye&#34; target=&#34;_blank&#34;&gt;https://github.com/laurentperrinet/CatchTheEye&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ANEMO: Quantitative tools for the ANalysis of Eye MOvements</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-18-anemo/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-18-anemo/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;ldquo;&lt;/li&gt;
&lt;li&gt;as presented at &lt;a href=&#34;https://eyemovements.sciencesconf.org/&#34; target=&#34;_blank&#34;&gt;https://eyemovements.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;get the &lt;a href=&#34;https://github.com/invibe/ANEMO/raw/master/2018-05-04_Poster_Grenoble/Pasturel_etal2018_grenoble.pdf&#34; target=&#34;_blank&#34;&gt;poster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code : &lt;a href=&#34;https://github.com/invibe/ANEMO/&#34; target=&#34;_blank&#34;&gt;https://github.com/invibe/ANEMO/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-18-grenoble/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-18-grenoble/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;ldquo;&lt;/li&gt;
&lt;li&gt;as presented at &lt;a href=&#34;https://eyemovements.sciencesconf.org/&#34; target=&#34;_blank&#34;&gt;https://eyemovements.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;get the &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/raw/master/Poster/2018-06-05_Poster_Workshop_Grenoble/Pasturel_etal2018grenoble.pdf&#34; target=&#34;_blank&#34;&gt;poster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code : &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/&#34; target=&#34;_blank&#34;&gt;https://github.com/chloepasturel/AnticipatorySPEM/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-18/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;ldquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Expériences autour de la perception de la forme en art et science</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-17-gdr/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-17-gdr/</guid>
      <description>

&lt;h1 id=&#34;expériences-autour-de-la-perception-de-la-forme-en-art-et-science&#34;&gt;Expériences autour de la perception de la forme en art et science&lt;/h1&gt;

&lt;p&gt;La vision utilise un faisceau d&amp;rsquo;informations de différentes qualités pour atteindre une perception unifiée du monde environnant. Nous avons utilisé lors de plusieurs projets art-science (voir &lt;a href=&#34;https://github.com/NaturalPatterns&#34; target=&#34;_blank&#34;&gt;https://github.com/NaturalPatterns&lt;/a&gt;) des installations permettant de manipuler explicitement des composantes de ce flux d&amp;rsquo;information et de révéler des ambiguités dans notre perception.
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_b.jpg&#34; alt=&#34;Tropique&#34; /&gt;
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_a.jpg&#34; alt=&#34;Tropique&#34; /&gt;
Dans l&amp;rsquo;installation «Tropique», des faisceaux de lames lumineuses sont arrangés dans l&amp;rsquo;espace assombri de l&amp;rsquo;installation. Les spectateurs les observent grâce à leur interaction avec une brume invisible qui est diffusée dans l&amp;rsquo;espace. Dans «Trame Élasticité», 25 parallélépipèdes de miroirs (3m de haut) sont arrangés verticalement sur une ligne horizontale. Ces lames sont rotatives et leurs mouvements est synchronisé. Suivant la dyamique qui est imposé à ces lames, la perception de l’espace environnent fluctue conduisant à recomposer l’espace de la concentration à l’expansion, ou encore à générer un surface semblant transparente ou inverser la visons de ce qui est située devant et derrière l’observateur. Enfin, dans «Trame instabilité», nous explorons l&amp;rsquo;interaction de séries périodiques de points placées sur des surfaces transparentes. À partir de premières expérimentations utilisant une technique novatrice de sérigraphie, ces trames de points sont placées afin de faire émerger des structures selon le point de vue du spectateur. De manière générale, nous montrerons ici les différentes méthodes utilisées, comme l&amp;rsquo;utilisation des limites perceptives, et aussi les résultats apportés par une telle collaboration.
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2017/01/EtienneRey-TRAME-Vasarely-B.jpg&#34; alt=&#34;Elasticité&#34; /&gt;
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2017/01/EtienneRey-TRAME-Vasarely-D.jpg&#34; alt=&#34;Elasticité&#34; /&gt;
 - poster présenté au &lt;a href=&#34;https://gdrvision2017.sciencesconf.org/&#34; target=&#34;_blank&#34;&gt;GDR vision 2017, Lille&lt;/a&gt;.
 - abstract: &lt;a href=&#34;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017abstract_168363.pdf&#34; target=&#34;_blank&#34;&gt;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017abstract_168363.pdf&lt;/a&gt;
 - poster : &lt;a href=&#34;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017poster.pdf&#34; target=&#34;_blank&#34;&gt;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017poster.pdf&lt;/a&gt;
 - poster (code) : &lt;a href=&#34;https://github.com/NaturalPatterns/2017-10-12_GDR/blob/master/2017-10-12_PerrinetRey2017poster.ipynb&#34; target=&#34;_blank&#34;&gt;https://github.com/NaturalPatterns/2017-10-12_GDR/blob/master/2017-10-12_PerrinetRey2017poster.ipynb&lt;/a&gt;
 - more code : &lt;a href=&#34;https://github.com/NaturalPatterns&#34; target=&#34;_blank&#34;&gt;https://github.com/NaturalPatterns&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
