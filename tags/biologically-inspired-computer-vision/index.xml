<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Biologically Inspired Computer Vision | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/tags/biologically-inspired-computer-vision/</link>
      <atom:link href="https://laurentperrinet.github.io/tags/biologically-inspired-computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Biologically Inspired Computer Vision</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Thu, 25 Jan 2018 18:30:00 +0100</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/img/hulk.png</url>
      <title>Biologically Inspired Computer Vision</title>
      <link>https://laurentperrinet.github.io/tags/biologically-inspired-computer-vision/</link>
    </image>
    
    <item>
      <title>Expériences autour de la perception de la forme en art et science</title>
      <link>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</link>
      <pubDate>Thu, 25 Jan 2018 18:30:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</guid>
      <description>&lt;h1 id=&#34;meetup-art-et-neurosciences&#34;&gt;Meetup Art et Neurosciences&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Quoi&lt;br&gt;
Meetup Art et Neurosciences&lt;/li&gt;
&lt;li&gt;Qui&lt;br&gt;
&lt;a href=&#34;https://www.facebook.com/events/211121069456116/&#34;&gt;Association
NeuroNautes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quand&lt;br&gt;
25 Janvier 2018&lt;/li&gt;
&lt;li&gt;Où&lt;br&gt;
Salle des voutes campus Saint Charles&lt;/li&gt;
&lt;li&gt;Support visuel&lt;br&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html&#34;&gt;https://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html&lt;/a&gt;
(notes: la présentation peut mettre un certain temps
à charger. Une fois que le titre apparait, appuyer sur la touche &amp;ldquo;F&amp;rdquo;
pour mettre en plein écran)
&lt;img src=&#34;http://www.lafriche.org/public_data/diapo/resident/1454686884/desk/2._elasticite_dynamique-etienne_rey-photoquentin_chevrier_pour_art2m_et_arcadi_ile_de_france.jpg&#34; alt=&#34;Elasticité&#34; title=&#34;Elasticité dynamique est composée des pièces Expansion, Trame et Lignes sonores. Volume hexagonal en miroir de 7 mètres de diamètre, Expansion fonctionne comme une chambre d&#39;écho. A l&#39;intérieur de ce volume se situe Trame. Constituée de 25 lames de miroir en rotation, cette pièce réoriente continuellement le regard. Quant à Lignes sonores, elle est formée de quatre monolithes orientés vers Expansion et émet des sons qui se réorientent en fonction du mouvement des lames. (© Etienne Rey, Adagp Paris&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation</title>
      <link>https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tutorial: Sparse optimization in neural computations</title>
      <link>https://laurentperrinet.github.io/talk/2017-01-19-laconeu/</link>
      <pubDate>Thu, 19 Jan 2017 10:45:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-01-19-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differential response of the retinal neural code with respect to the sparseness of natural images</title>
      <link>https://laurentperrinet.github.io/publication/ravello-16-droplets/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ravello-16-droplets/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biologically-inspired characterization of sparseness in natural images</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/</link>
      <pubDate>Wed, 26 Oct 2016 13:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/</link>
      <pubDate>Wed, 26 Oct 2016 13:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2016-10-26 : EUVIP BICV</title>
      <link>https://laurentperrinet.github.io/post/2016-10-26_euvip-bicv/</link>
      <pubDate>Wed, 26 Oct 2016 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/post/2016-10-26_euvip-bicv/</guid>
      <description>&lt;h1 id=&#34;20161026--euvip-special-session-on-biologically-inspired-computer-vision&#34;&gt;2016-10-26 : EUVIP Special Session on &lt;em&gt;Biologically Inspired Computer Vision&lt;/em&gt;&lt;/h1&gt;
&lt;h2 id=&#34;description-of-the-session&#34;&gt;description of the session&lt;/h2&gt;
&lt;p&gt;Recent advances in imaging technologies have yielded scientific data at
unprecedented detail and volume, leading to the need of a shift of
paradigm in image processing and computer vision. Beyond the usual
classical von Neumann architecture, one strategy that is emerging in
order to process and interpret this amount of data follows from the
architecture of biological organisms and shows for instance
computational paradigms implementing asynchronous communication with a
high degree of local connectivity in sensors or brain tissues. This
session aims at bringing together researchers from different fields of
Biologically Inspired Computer Vision to present latest results in the
field, from fundamental to more specialized topics, including visual
analysis based on a computational level, hardware implementation, and
the design of new more advanced vision sensors. It is expected to
provide a comprehensive overview in the computer area of biologically
motivated vision. On the one hand, biological organisms can provide a
source of inspiration for new computationally efficient and robust
vision models and on the other hand machine vision approaches can
provide new insights for understanding biological visual systems. This
session covers a wide range of topics from fundamental to more
specialized topics, including visual analysis based on a computational
level, hardware implementation, and the design of new more advanced
vision sensors. In particular, we expect to provide an overview of a few
representative applications and current state of the art of the research
in this area.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;URL
&lt;a href=&#34;http://www-l2ti.univ-paris13.fr/euvip2016/index.php/86-euvip2016/129-tentative-technical-program-in-detail&#34;&gt;http://www-l2ti.univ-paris13.fr/euvip2016/index.php/86-euvip2016/129-tentative-technical-program-in-detail&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date
October 26th, 2016&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Location
Ecole Centrale Marseille&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Address
&lt;a href=&#34;https://www.centrale-marseille.fr/fr/acces-0&#34;&gt;38 rue Frédéric Joliot-Curie 13013 Marseille,
France&lt;/a&gt; Phone : +33
(0)4 91 05 45 45&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Programme&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;13.50  &lt;a href=&#34;http://ieeexplore.ieee.org/document/7764586/&#34;&gt;Visual System Inspired Algorithm For Contours, Corner And T Junction Detection&lt;/a&gt;, Antoni Buades, &lt;em&gt;Rafael Grompone Von Gioi&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;13.50  &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/&#34;&gt;Biologically-inspired characterization of sparseness in natural images&lt;/a&gt;, &lt;em&gt;Laurent Perrinet&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.10 &lt;a href=&#34;http://david.alleysson.free.fr/Publications/JIST0224reprint.pdf&#34;&gt;Color filter array imitating the random nature of color arrangement in the human cone mosaic&lt;/a&gt;, Prakhar Amba, &lt;em&gt;David Alleysson&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.30 &lt;a href=&#34;http://ieeexplore.ieee.org/document/7764601/&#34;&gt;An Illuminant-Independent Analysis Of Reflectance As Sensed By Humans, And Its Applicability To Computer Vision&lt;/a&gt;, Alban Flachot, Phelma, J.Kevin O&#39;Regan, &lt;em&gt;Edoardo Provenzi&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.50 &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/&#34;&gt;Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor&lt;/a&gt;, Lionel Fillatre, Michel Barlaud, &lt;em&gt;Laurent Perrinet&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Biologically-inspired characterization of sparseness in natural images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-16-euvip/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-16-euvip/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv-chap-1/</link>
      <pubDate>Fri, 20 Nov 2015 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv-chap-1/</guid>
      <description>












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;mindmap.png&#34; data-caption=&#34;Mindmap of the book contents. Cross-links between chapters have been indicated as thin lines.&#34;&gt;
&lt;img src=&#34;mindmap.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Mindmap of the book contents. Cross-links between chapters have been indicated as thin lines.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Sparse Models for Computer Vision</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-15-bicv/</link>
      <pubDate>Fri, 20 Nov 2015 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-15-bicv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visual motion processing and human tracking behavior</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-15-bicv/</link>
      <pubDate>Fri, 20 Nov 2015 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-15-bicv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biologically Inspired Computer Vision</title>
      <link>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/</link>
      <pubDate>Wed, 07 Oct 2015 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/</guid>
      <description>












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;header.jpg&#34; data-caption=&#34;Biologically Inspired Computer vision&#34;&gt;
&lt;img src=&#34;header.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Biologically Inspired Computer vision
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h1 id=&#34;biologically-inspired-computer-vision&#34;&gt;Biologically Inspired Computer Vision&lt;/h1&gt;
&lt;p&gt;As state-of-the-art imaging technologies becomes more and more advanced, yielding scientific data at unprecedented detail and volume, the need to process and interpret all the data has made image processing and computer vision also increasingly important. Sources of data that have to be routinely dealt with today applications include video transmission, wireless communication, automatic fingerprint processing, massive databanks, non-weary and accurate automatic airport screening, robust night vision to name a few. Multidisciplinary inputs from other disciplines such as computational neuroscience, cognitive science, mathematics, physics and biology will have a fundamental impact in the progress of imaging and vision sciences. One of the advantages of the study of biological organisms is to devise very diﬀerent type of computational paradigms beyond the usual von Neumann e.g. by implementing a neural network with a high degree of local connectivity.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;header.jpg&#34; &gt;
&lt;img src=&#34;header.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

This is a comprehensive and rigorous reference in the area of biologically motivated vision sensors. The study of biologically visual systems can be considered as a two way avenue. On the one hand, biological organisms can provide a source of inspiration for new computational efficient and robust vision models and on the other hand machine vision approaches can provide new insights for understanding biological visual systems. Along the different chapters, this book covers a wide range of topics from fundamental to more specialized topics, including visual analysis based on a computational level, hardware implementation, and the design of new more advanced vision sensors. The last two sections of the book provide an overview of a few representative applications and current state of the art of the research in this area. This makes it a valuable book for graduate, Master, PhD students and also researchers in the field.
This book contains 17 chapters that have been organized in four different parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fundamentals&lt;/li&gt;
&lt;li&gt;Sensing&lt;/li&gt;
&lt;li&gt;Modeling&lt;/li&gt;
&lt;li&gt;Applications
See the &lt;a href=&#34;http://bicv.github.io/toc/&#34;&gt;Table of contents&lt;/a&gt;.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;mindmap.png&#34; &gt;
&lt;img src=&#34;mindmap.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Coding Of Natural Images Using A Prior On Edge Co-Occurences</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-15-eusipco/</link>
      <pubDate>Thu, 20 Aug 2015 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-15-eusipco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge co-occurrences can account for rapid categorization of natural versus animal images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-bednar-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-bednar-15/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html&#34;&gt;communiqué de presse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.nature.com/article-assets/npg/srep/2015/150622/srep11400/extref/srep11400-s1.pdf&#34;&gt;supplementary information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;hhttps://invibe.net/LaurentPerrinet/Publications/PerrinetBednar15?action=AttachFile&amp;amp;do=get&amp;amp;target=PerrinetBednar15supplementary.pdf&#34;&gt;supplementary material&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-study-of-how-people-can-quickly-spot-animals-by-sight-is-helping-uncover-the-workings-of-the-human-brain&#34;&gt;A study of how people can quickly spot animals by sight is helping uncover the workings of the human brain.&lt;/h1&gt;
&lt;p&gt;Scientists examined why volunteers who were shown hundreds of pictures - some with animals and some without - were able to detect animals in as little as one-tenth of a second.
They found that one of the first parts of the brain to process visual information - the primary visual cortex - can control this fast response.
More complex parts of the brain are not required at this stage, contrary to what was previously thought.
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New info published on how the human brain processes visual information from &lt;a href=&#34;https://twitter.com/EdinburghUni?ref_src=twsrc%5Etfw&#34;&gt;@EdinburghUni&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/uniamu?ref_src=twsrc%5Etfw&#34;&gt;@uniamu&lt;/a&gt; stuidy &lt;a href=&#34;http://t.co/KUicugL8P7&#34;&gt;http://t.co/KUicugL8P7&lt;/a&gt;&lt;/p&gt;&amp;mdash; EdinUniNeuro (@EdinUniNeuro) &lt;a href=&#34;https://twitter.com/EdinUniNeuro/status/613011086829162497?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure_model.jpg&#34; data-caption=&#34;Edge co-occurrences (A) An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. (B) The relationship between a reference edge A and another edge B can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license). This is used to compute the chevron map in Figure~2.&#34;&gt;
&lt;img src=&#34;figure_model.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Edge co-occurrences &lt;strong&gt;(A)&lt;/strong&gt; An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. &lt;strong&gt;(B)&lt;/strong&gt; The relationship between a reference edge &lt;em&gt;A&lt;/em&gt; and another edge &lt;em&gt;B&lt;/em&gt; can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Elephant_/%28Loxodonta_Africana/%29_05.jpg&#34;&gt;Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license&lt;/a&gt;). This is used to compute the chevron map in Figure~2.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;動物か否かの見分け方。&lt;a href=&#34;http://t.co/TTY8MwZGoO&#34;&gt;http://t.co/TTY8MwZGoO&lt;/a&gt;　引用されてるけど、Thorpe (1996)の150msで区別されてるって話(なつかしい)と関係ありそう。&lt;/p&gt;&amp;mdash; Makito Oku (@okumakito) &lt;a href=&#34;https://twitter.com/okumakito/status/613128456637841408?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure_chevrons.png&#34; data-caption=&#34;The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&amp;rsquo; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.&#34;&gt;
&lt;img src=&#34;figure_chevrons.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&amp;rsquo; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;a href=&#34;http://t.co/NY9HapBx2S&#34;&gt;http://t.co/NY9HapBx2S&lt;/a&gt; &lt;a href=&#34;http://t.co/rKQ8I5i6Ty&#34;&gt;pic.twitter.com/rKQ8I5i6Ty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Francis Villatoro (@emulenews) &lt;a href=&#34;https://twitter.com/emulenews/status/612988348400070656?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure_chevrons2.png&#34; data-caption=&#34;As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.&#34;&gt;
&lt;img src=&#34;figure_chevrons2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure_results.png&#34; data-caption=&#34;Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&amp;#39;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&amp;rsquo; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80% for human observers and 82% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.&#34;&gt;
&lt;img src=&#34;figure_results.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&#39;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&amp;rsquo; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80% for human observers and 82% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;figure_FA_humans.png&#34; data-caption=&#34;To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&amp;rsquo; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram.&#34;&gt;
&lt;img src=&#34;figure_FA_humans.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&amp;rsquo; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Active inference, eye movements and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</link>
      <pubDate>Tue, 16 Dec 2014 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</guid>
      <description>&lt;h1 id=&#34;active-inference-tracking-eye-movements-and-oculomotor-delays&#34;&gt;Active Inference, tracking eye movements and oculomotor delays&lt;/h1&gt;
&lt;p&gt;Tracking eye movements face a difficult task: they have to be fast while they suffer inevitable delays. If we focus on area MT of humans for instance as it is crucial for detecting the motion of visual objects, sensory information coming to this area is already lagging some 35 milliseconds behind operational time – that is, it reflects some past information. Still the fastest action that may be done there is only able to reach the effector muscles of the eyes some 40 milliseconds later – that is, in the future. The tracking eye movement system is however able to respond swiftly and even to anticipate repetitive movements (e.g. Barnes et al, 2000 – refs in manuscript). In that case, it means that information in a cortical area is both predicted from the past sensory information but also anticipated to give an optimal response in the future. Even if numerous models have been described to model different mechanisms to account for delays, no theoretical approach has tackled the whole problem explicitly. In several areas of vision research, authors have proposed models at different levels of abstractions from biomechanical models, to neurobiological implementations (e.g. Robinson, 1986) or Bayesian models. This study is both novel and important because – using a neurobiologically plausible hierarchical Bayesian model – it demonstrates that using generalized coordinates to finesse the prediction of a target&#39;s motion, the model can reproduce characteristic properties of tracking eye movements in the presence of delays. Crucially, the different refinements to the model that we propose – pursuit initiation, smooth pursuit eye movements, and anticipatory response – are consistent with the different types of tracking eye movements that may be observed experimentally.













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;featured.png&#34; data-caption=&#34;(A) This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. (B) This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.&#34;&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;(A)&lt;/strong&gt; This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. &lt;strong&gt;(B)&lt;/strong&gt; This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Edge co-occurrences are sufficient to categorize natural versus animal images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-bednar-14-vss/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-bednar-14-vss/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</link>
      <pubDate>Fri, 25 Apr 2014 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see &lt;a href=&#34;https://laurentperrinet.github.io/publication/kaplan-khoei-14/&#34;&gt;Kaplan and al, 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge co-occurrences and categorizing natural images</title>
      <link>https://laurentperrinet.github.io/talk/2013-07-05-cerco/</link>
      <pubDate>Fri, 05 Jul 2013 13:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2013-07-05-cerco/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Advances in Texture Analysis for Emphysema Classification</title>
      <link>https://laurentperrinet.github.io/publication/nava-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/nava-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adaptive Sparse Spike Coding : applications of Neuroscience to the compression of natural images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-spie/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-spie/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-Invertible 2D Log-Gabor Wavelets</title>
      <link>https://laurentperrinet.github.io/publication/fischer-07-cv/</link>
      <pubDate>Sat, 13 Jan 2007 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-07-cv/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;fischer-07-cv.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Approximation of Images Inspired from the Functional Architecture of the Primary Visual Areas</title>
      <link>https://laurentperrinet.github.io/publication/fischer-07/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-07/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse Gabor wavelets by local operations</title>
      <link>https://laurentperrinet.github.io/publication/fischer-05-a/</link>
      <pubDate>Wed, 29 Jun 2005 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-05-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient representation of natural images using local cooperation</title>
      <link>https://laurentperrinet.github.io/publication/fischer-05/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0100</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-05/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coding static natural images using spiking event times: do neurons cooperate?</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-03-ieee/</link>
      <pubDate>Mon, 20 Sep 2004 00:00:00 +0200</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-03-ieee/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-03-ieee.png&#34; alt=&#34;header&#34;&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;v1_tiger.gif&#34; data-caption=&#34;Progressive reconstruction of a static image using spikes in a multi-scale oriented representation.&#34;&gt;
&lt;img src=&#34;v1_tiger.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Progressive reconstruction of a static image using spikes in a multi-scale oriented representation.&lt;/em&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
