<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Novel visual computations on Novel visual computations</title>
    <link>https://laurentperrinet.github.io/</link>
    <description>Recent content in Novel visual computations on Novel visual computations</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.
&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/3.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;http://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png&#34; /&gt;&lt;/a&gt;&lt;br /&gt;This work is licensed under a &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by-nc-sa/3.0/&#34;&gt;Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License&lt;/a&gt;
Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared.
</copyright>
    <lastBuildDate>Fri, 22 Feb 2019 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Meaningful representations emerge from Sparse Deep Predictive Coding</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-19/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli</title>
      <link>https://laurentperrinet.github.io/publication/ravello-19/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/publication/ravello-19/</guid>
      <description>

&lt;h1 id=&#34;d√®s-la-r√©tine-le-syst√®me-visuel-pr√©f√®re-des-images-naturelles&#34;&gt;D√®s la r√©tine, le syst√®me visuel pr√©f√®re des images naturelles&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Dans la r√©tine, au premier √©tage du traitement de l&amp;rsquo;image visuelle, on peut obtenir des repr√©sentations extr√™mement fines. Une collaboration entre des chercheurs fran√ßais et chiliens a permis de mettre en √©vidence que, dans la r√©tine de rongeurs, une repr√©sentation de la vitesse de l&amp;rsquo;image visuelle est pr√©cis√©ment cod√©e. Dans cette collaboration pluridisciplinaire, l&amp;rsquo;utilisation d&amp;rsquo;un mod√®le du fonctionnement de la r√©tine a permis de g√©n√©rer un nouveau type de stimuli visuels qui a r√©v√©l√© des r√©sultats exp√©rimentaux surprenants.&lt;/em&gt;&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Travail collaboratif et multi-disciplinaire entre &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar et &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; librement disponible sur &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; - merci √†  l&amp;#39;&lt;a href=&#34;https://twitter.com/AgenceRecherche?ref_src=twsrc%5Etfw&#34;&gt;@AgenceRecherche&lt;/a&gt; pour l&amp;#39;aide financi√®re et √† &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; + &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; pour l&amp;#39;&lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://t.co/YixRfpCrT3&#34;&gt;https://t.co/YixRfpCrT3&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1092139540788244480?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;p&gt;La r√©tine est la premi√®re √©tape du traitement visuel, aux capacit√©s √©tonnantes. √Ä la diff√©rence d&amp;rsquo;un simple capteur comme ceux qu‚Äôon trouve dans les appareils photographiques num√©riques, ce mince tissu neuronal est un syst√®me complexe et encore largement m√©connu. Une meilleure connaissance de cette structure est essentielle pour la construction de capteurs du futur efficaces et √©conomes -par exemple ceux qui √©quiperont les futures voitures autonomes- mais aussi pour mieux comprendre des pathologies comme la D√©ficience Maculaire Li√©e √† l&amp;rsquo;Age (DMLA). Une des facettes m√©connues de la r√©tine est sa capacit√© √† d√©tecter des mouvements et cet article permet de mieux comprendre une partie des m√©canismes en jeu.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New study on speed selectivity in the &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; showing that a majority of neurons prefer natural-like stimuli. Collaborative and multi-disciplinary work with &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar and &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; available with &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; at &lt;a href=&#34;https://t.co/Vb7GoRxjoT&#34;&gt;https://t.co/Vb7GoRxjoT&lt;/a&gt;&lt;/p&gt;&amp;mdash; Adrian Palacios (@APalacio_s) &lt;a href=&#34;https://twitter.com/APalacio_s/status/1092200890377879552?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Retinal cell preference for natural-like stimuli. Very elegant work by &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; et al. &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuroscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuroscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/decoding?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#decoding&lt;/a&gt; &lt;a href=&#34;https://t.co/3xNWaZd5x6&#34;&gt;https://t.co/3xNWaZd5x6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andres Canales-Johnson (@canalesjohnson) &lt;a href=&#34;https://twitter.com/canalesjohnson/status/1092211339311923201?ref_src=twsrc%5Etfw&#34;&gt;February 4, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Conciliant mod√©lisation et neurophysiologie, cette √©tude a permis de faire des pr√©dictions sur le traitement de l&amp;rsquo;information r√©tinienne et en particulier de g√©n√©rer des textures synth√©tiques qui sont optimales pour ces mod√®les (voir film). Les enregistrements effectu√©s sur la r√©tine de rongeurs diurnes Octodon degus ont ensuite permis de mesurer la s√©lectivit√© √† la vitesse mais aussi de valider une nouvelle fois ces mod√®les en reconstruisant l&amp;rsquo;image d&amp;rsquo;entr√©e √† partir de l&amp;rsquo;activit√© neurale.&lt;/p&gt;

&lt;p&gt;Le r√©sultat le plus inattendu est la diff√©rence de s√©lectivit√© de certaines classes de neurones r√©tiniens par rapport √† la complexit√© du stimulus pr√©sent√©. En effet, la repr√©sentation de la vitesse est relativement peu pr√©cise si on utilise des r√©seaux de lignes (&amp;ldquo;Grating&amp;rdquo;), comme cela est d&amp;rsquo;habitude r√©alis√© dans la plupart des exp√©riences neurophysiologiques. Au contraire, elle devient plus pr√©cise si on utilise comme signaux visuels des textures artificielles ressemblant √† des nuages en mouvement (&amp;ldquo;MC Narrow&amp;rdquo;). En particulier, plus cette texture est complexe, plus la repr√©sentation est pr√©cise (&amp;ldquo;MC Broad&amp;rdquo;).&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/R%C3%A9sultatScientifique?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#R√©sultatScientifique&lt;/a&gt; üîç| D√®s la &lt;a href=&#34;https://twitter.com/hashtag/r%C3%A9tine?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#r√©tine&lt;/a&gt;, le syst√®me &lt;a href=&#34;https://twitter.com/hashtag/visuel?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#visuel&lt;/a&gt; pr√©f√®re des images naturelles&lt;br&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://t.co/BBY2IpGum6&#34;&gt;https://t.co/BBY2IpGum6&lt;/a&gt;&lt;br&gt;üìï &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; | &lt;a href=&#34;https://t.co/5mULuWTp3N&#34;&gt;https://t.co/5mULuWTp3N&lt;/a&gt;&lt;br&gt;ü§ù &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/LaurentPerrinet?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#LaurentPerrinet&lt;/a&gt; &lt;a href=&#34;https://t.co/34R1URHUic&#34;&gt;pic.twitter.com/34R1URHUic&lt;/a&gt;&lt;/p&gt;&amp;mdash; Biologie au CNRS (@INSB_CNRS) &lt;a href=&#34;https://twitter.com/INSB_CNRS/status/1091392027848294401?ref_src=twsrc%5Etfw&#34;&gt;February 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli. Beautiful article in Scientific Reports on an original animal model, the diurnal rodent Octodon degus. &lt;a href=&#34;https://t.co/BdzyzEVYnX&#34;&gt;https://t.co/BdzyzEVYnX&lt;/a&gt; (open access) &lt;a href=&#34;https://t.co/1UaoMYTFd2&#34;&gt;pic.twitter.com/1UaoMYTFd2&lt;/a&gt;&lt;/p&gt;&amp;mdash; St√©phane Deny (@StephaneDeny) &lt;a href=&#34;https://twitter.com/StephaneDeny/status/1090452532223045632?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Ces textures complexes sont plus proches des images naturellement observ√©es et ces r√©sultats montrent donc que d√®s la r√©tine, le syst√®me visuel est particuli√®rement adapt√© √† des stimulations naturelles. Ce r√©sultat devrait pouvoir s&amp;rsquo;√©tendre √† des textures encore plus complexes et encore plus proches d&amp;rsquo;images naturelles, mais aussi pouvoir se g√©n√©raliser √† d&amp;rsquo;autres aires visuelles plus complexes, comme le cortex visuel primaire, et √† d&amp;rsquo;autres esp√®ces.&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;featured.jpg&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Pour une cellule repr√©sentative, on montre ici la r√©ponse au cours du temps sous forme d&amp;rsquo;impulsions pour diff√©rentes pr√©sentations (Trial) ainsi que la moyenne de cette r√©ponse (Firing rate). Les diff√©rentes colonnes repr√©sentent diff√©rentes vitesses des stimulations sur la r√©tine. Les diff√©rentes lignes sont diff√©rentes stimulations. En bleu, une stimulation classique sous forme de r√©seaux de lignes (¬´ Grating ¬ª). En vert et Orange, la r√©ponse √† une texture progressivement plus complexe (de ¬´ Mc Narrow ¬ª √† ¬´ MC Broad ¬ª). Si les r√©ponses aux diff√©rents stimulations sont en moyenne similaires, elles sont variables d‚Äôessai en essai et une analyse statistique a permis de montrer que dans la majorit√© des cellules, les r√©ponses sont d&amp;rsquo;autant plus pr√©cises que la stimulation est complexe. ¬© Cesar Ravello&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;











  


&lt;video controls &gt;
  &lt;source src=&#34;video_perrinet.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
Cette vid√©o montre les trois classes de stimulations utilis√©es dans cette √©tude. En plus des r√©seaux sinuso√Ødaux (‚ÄúGrating‚Äù) qui sont classiquement utilis√©s en neurosciences, cette √©tude a utilis√© des textures al√©atoires (Motion Clouds (MC)) qui sont inspir√©es de mod√®les du traitement visuel. Ils permettent en particulier de manipuler des param√®tres visuels critiques comme la vari√©t√© de fr√©quences spatiales qui sont superpos√©es: soit unique (‚ÄúGrating‚Äù), fine (‚ÄúMC Narrow‚Äù), soit plus large (‚ÄúMC Broad‚Äù). Ces vid√©os ont √©t√© directement projet√©es sur des r√©tines pos√©es sur des grilles d‚Äô√©lectrodes qui permettent de mesurer l‚Äôactivit√© neurale (voir figure). ¬© Laurent Perrinet / Cesar Ravello&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Role of dynamics in neural computations underlying visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-17-laconeu/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2019-01-17-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient coding of visual information in neural computations</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-16-laconeu/</link>
      <pubDate>Wed, 16 Jan 2019 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2019-01-16-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling spiking neural networks using Brian, Nest and pyNN</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-14-laconeu/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2019-01-14-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reinforcement effects in anticipatory smooth eye movements</title>
      <link>https://laurentperrinet.github.io/publication/damasse-18/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0200</pubDate>
      
      <guid>https://laurentperrinet.github.io/publication/damasse-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>https://laurentperrinet.github.io/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>https://laurentperrinet.github.io/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey V1</title>
      <link>https://laurentperrinet.github.io/publication/chemla-18/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0200</pubDate>
      
      <guid>https://laurentperrinet.github.io/publication/chemla-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Principles and psychophysics of Active Inference</title>
      <link>https://laurentperrinet.github.io/talk/2018-04-05-active-inference/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0200</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2018-04-05-active-inference/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Principles and psychophysics of Active Inference in anticipating a dynamic, switching probabilistic bias</title>
      <link>https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0200</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilities, Bayes and the Free-energy principle</title>
      <link>https://laurentperrinet.github.io/talk/2018-03-26-cours-neuro-comp-fep/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0200</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2018-03-26-cours-neuro-comp-fep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From biological vision to unsupervised hierarchical sparse coding</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-itwist/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-itwist/</guid>
      <description>

&lt;h1 id=&#34;from-biological-vision-to-unsupervised-hierarchical-sparse-coding&#34;&gt;From biological vision to unsupervised hierarchical sparse coding&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;accepted submission @ &lt;a href=&#34;https://sites.google.com/view/itwist18&#34; class=&#34;https&#34;&gt;iTWIST: international Traveling Workshop on Interactions between low-complexity data models and Sensing Techniques&lt;/a&gt;, 21 - 23 November‚Äã, 2018&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://sites.google.com/view/itwist18/program#h.p_9OOcrreKb--s&#34; class=&#34;https&#34;&gt;poster session&lt;/a&gt; scheduled on Thursday, November 22th, from 10h30 till 12h00.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CIRM, Marseille, France. &lt;span id=&#34;line-10&#34; class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;get full proceedings @ &lt;a href=&#34;https://arxiv.org/html/1812.00648&#34;&gt;https://arxiv.org/html/1812.00648&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Poster &lt;a href=&#34;https://invibe.net/LaurentPerrinet/Publications/BoutinFranciosiniRuffierPerrinet18itwist?action=AttachFile&amp;do=get&amp;target=BoutinFranciosiniRuffierPerrinet18itwist.pdf&#34; title=&#34;PDF&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation</title>
      <link>https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exp√©riences autour de la perception de la forme en art et science</title>
      <link>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0100</pubDate>
      
      <guid>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
