<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Novel visual computations</title>
    <link>https://laurentperrinet.github.io/</link>
      <atom:link href="https://laurentperrinet.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Novel visual computations</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Sun, 27 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/media/hulk.png</url>
      <title>Novel visual computations</title>
      <link>https://laurentperrinet.github.io/</link>
    </image>
    
    <item>
      <title>Modelling Complex-cells and topological structure in the visual cortex of mammals using Sparse Predictive Coding</title>
      <link>https://laurentperrinet.github.io/publication/franciosini-20-cosyne/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/franciosini-20-cosyne/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Presenting my poster tonight at 8:00p &lt;a href=&#34;https://twitter.com/hashtag/cosyne2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#cosyne2020&lt;/a&gt;, a work developed using Sparse Deep Predictive Coding (SDPC) during my PhD &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; &lt;a href=&#34;https://twitter.com/NeuroSchool_mrs?ref_src=twsrc%5Etfw&#34;&gt;@NeuroSchool_mrs&lt;/a&gt; &lt;a href=&#34;https://t.co/LtUEBnlPNt&#34;&gt;pic.twitter.com/LtUEBnlPNt&lt;/a&gt;&lt;/p&gt;&amp;mdash; Angelo Franciosini (@RaguDellaNonna) &lt;a href=&#34;https://twitter.com/RaguDellaNonna/status/1233458739220504578?ref_src=twsrc%5Etfw&#34;&gt;February 28, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;







  



  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/franciosini-20-cosyne/poster_hu869adeac44b17d85c28add0c262e1474_334775_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/franciosini-20-cosyne/poster_hu869adeac44b17d85c28add0c262e1474_334775_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;100%&#34; height=&#34;1080&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding natural vision using deep predictive coding</title>
      <link>https://laurentperrinet.github.io/talk/2020-09-25-irphe/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-09-25-irphe/</guid>
      <description>&lt;h1 id=&#34;understanding-natural-vision-br-using-deep-predictive-coding&#34;&gt;Understanding natural vision &lt;BR&gt; using deep predictive coding&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What:: talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2020-09-25-irphe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S√©minaire √† l&amp;rsquo;Institut de Recherche sur les Ph√©nom√®nes Hors √âquilibre (IRPH√â)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Who:: Perrinet, Laurent U&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Where: Marseille (France), see &lt;a href=&#34;https://laurentperrinet.github.io/talk/2020-09-25-irphe&#34;&gt;https://laurentperrinet.github.io/talk/2020-09-25-irphe&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When: 25/09/2020, time: 15:45:00-16:30:00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slides @ &lt;a href=&#34;https://laurentperrinet.github.io/2020-09-25_IRPHE&#34;&gt;https://laurentperrinet.github.io/2020-09-25_IRPHE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code for slides @ &lt;a href=&#34;https://github.com/laurentperrinet/2020-09-25_IRPHE/&#34;&gt;https://github.com/laurentperrinet/2020-09-25_IRPHE/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Abstract: Building models which efficiently process images is a great source of inspiration to better understand the processes which underly our visual perception. I will present some classical models stemming from the Machine Learning community and propose some extensions inspired by Nature. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computations at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision (CNNs in the terminology  of deep learning) take into account neurophysiological observations and provide as of today the most successful framework for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. We will here introduce a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective and we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational levels. These results from the SDPC model additionally demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Visual search as active inference</title>
      <link>https://laurentperrinet.github.io/talk/2020-09-14-iwai/</link>
      <pubDate>Mon, 14 Sep 2020 18:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-09-14-iwai/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;the mathematical details are described as a talk the 1st International WS on &lt;a href=&#34;https://twitter.com/hashtag/ActiveInference?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ActiveInference&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/IWAI2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#IWAI2020&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/ECMLPKDD?ref_src=twsrc%5Etfw&#34;&gt;@ECMLPKDD&lt;/a&gt; &lt;a href=&#34;https://t.co/4s7gHbMxiT&#34;&gt;https://t.co/4s7gHbMxiT&lt;/a&gt; and paper &amp;quot;Visual search as active inference&amp;quot; &lt;a href=&#34;https://t.co/yNCOFHf7FS&#34;&gt;https://t.co/yNCOFHf7FS&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1305488089989754883?ref_src=twsrc%5Etfw&#34;&gt;September 14, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; &gt;


  &lt;img src=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>PhD offer &#34;Ultra-fast vision using Spiking Neural Networks&#34;</title>
      <link>https://laurentperrinet.github.io/post/2020-06-30_phd-position/</link>
      <pubDate>Tue, 30 Jun 2020 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2020-06-30_phd-position/</guid>
      <description>&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    THE POSITION HAS BEEN FILLED.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Dear colleagues,&lt;/p&gt;
&lt;p&gt;Applications are welcome for a fully funded doctoral position at 
&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;INT&lt;/a&gt; in 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, France. Your mission will be to build ultra-fast vision algorithms using event-based cameras and spiking neural networks. The project is funded by the 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/aprovis-3-d/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;APROVIS3D&lt;/a&gt; grant (ANR-19-CHR3-0008-03) and will be coordinated by 
&lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;. The work will be carried out in collaboration with a leading computer science institute at Universit√© C√¥te d‚ÄôAzur (Sophia Antipolis, France), the Laboratoire d&amp;rsquo;Informatique, Signaux et Syst√®mes de Sophia-Antipolis (I3S, UMR7271 - UNS CNRS), that will be part of the supervision team. We are seeking candidates with a strong background in machine learning, computer vision and computational neuroscience.&lt;/p&gt;
&lt;p&gt;To obtain further information, please visit 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2020-06-30_phd-position&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/post/2020-06-30_phd-position&lt;/a&gt; or contact me @ 
&lt;a href=&#34;mailto:Laurent.Perrinet@univ-amu.fr&#34;&gt;Laurent.Perrinet@univ-amu.fr&lt;/a&gt;. To candidate, follow instructions on the dedicated 
&lt;a href=&#34;https://bit.ly/3igRji4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;server from the CNRS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The starting date is set to October 1st, 2020 and the appointment is for 36 month. Applications are welcome immediately.&lt;/p&gt;
&lt;p&gt;Thanks for distributing this announcement to potential candidates!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;CD Doctorant &amp;quot;Vision ultra-rapide utilisant des R√©seaux de neurones impulsionnels&amp;quot; H/F (MARSEILLE) (MARSEILLE 05) &lt;a href=&#34;https://t.co/I5CXWxR3zi&#34;&gt;https://t.co/I5CXWxR3zi&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Emploi?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Emploi&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/OffreEmploi?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#OffreEmploi&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Recrutement?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Recrutement&lt;/a&gt;&lt;/p&gt;&amp;mdash; EmploiCNRS (@EmploiCNRS) &lt;a href=&#34;https://twitter.com/EmploiCNRS/status/1277872035700539392?ref_src=twsrc%5Etfw&#34;&gt;June 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;detailed-description-ultra-fast-vision-using-spiking-neural-networks&#34;&gt;Detailed description: &amp;ldquo;Ultra-fast vision using Spiking Neural Networks&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. Crucially, given an equal constraint on energy consumption, these algorithms are relatively slow compared to biological vision. It is believed that one major factor of this rapidity is the fact that visual information is represented by short pulses (spikes) at analog ‚Äì not discrete ‚Äì times (
&lt;a href=&#34;#Paugam12&#34;&gt;Paugam and Bohte, 2012&lt;/a&gt;). However, most classical computer vision algorithms rely on such frame-based approaches. One solution to overcome their limitations is to use event-based representations, but these still lack in practice, and their high potential is largely underexploited. Inspired by biology, the project addresses the scientific question of developing a low-power sensing architecture for the processing of visual scenes, able to function on analog devices without a central clock and aimed at being validated in real-life situations. More specifically, the project will develop new paradigms for biologically inspired computer vision (
&lt;a href=&#34;#Cristobal15&#34;&gt;Cristobal, Keil and Perrinet, 2015&lt;/a&gt;), from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this doctoral project, we propose to address major limitations of classical computer vision by implementing specific dynamical features of cortical circuits: &lt;em&gt;spiking neural networks&lt;/em&gt; (
&lt;a href=&#34;#Perrinet04&#34;&gt;Perrinet, Thorpe and Samuelides, 2004&lt;/a&gt;; 
&lt;a href=&#34;#Lagorce16&#34;&gt;Lagorce et al., 2018&lt;/a&gt;), &lt;em&gt;lateral diffusion of neural information&lt;/em&gt; (
&lt;a href=&#34;#Chavane2000&#34;&gt;Chavane et al., 2011&lt;/a&gt;; 
&lt;a href=&#34;#muller2018cortical&#34;&gt;Muller et al., 2018&lt;/a&gt;) and &lt;em&gt;dynamic neuronal association fields&lt;/em&gt; (
&lt;a href=&#34;#Fr%c3%a9gnac2012&#34;&gt;Fr√©gnac et al., 2012&lt;/a&gt;; 
&lt;a href=&#34;#Fr%c3%a9gnac2016&#34;&gt;Fr√©gnac et al., 2016&lt;/a&gt;; 
&lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;)&lt;/strong&gt;. One starting point is to use event-based cameras 
&lt;a href=&#34;#Dupeyroux18&#34;&gt;(Dupeyroux et al., 2018)&lt;/a&gt; and to extend results of self-supervised learning that we have obtained on static, natural images (
&lt;a href=&#34;#BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt;Boutin et al., 2020&lt;/a&gt;) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the &amp;ldquo;association field&amp;rdquo; described at the psychophysical (
&lt;a href=&#34;#Field1993&#34;&gt;Field et al., 1993&lt;/a&gt;), spiking (
&lt;a href=&#34;#Li2002&#34;&gt;Li and Gilbert, 2002&lt;/a&gt;) and synaptic (
&lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;) levels. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (
&lt;a href=&#34;#Voges12&#34;&gt;Voges and Perrinet, 2012&lt;/a&gt;). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). It is not well understood, but probably decisive for ultra-fast vision, how recurrent cortico-cortical loops add a level of distributed top-down complexity in the feed-forward stream of information which participates to the ultra-fast integration of sensory input and perceptual context (
&lt;a href=&#34;#Keller2019&#34;&gt;Keller et al., 2019&lt;/a&gt;). Coupled with the dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for defining ultra-fast vision algorithms.&lt;/p&gt;
&lt;h2 id=&#34;expected-profile-of-the-candidate&#34;&gt;Expected profile of the candidate&lt;/h2&gt;
&lt;p&gt;Candidates should have experience in the domain of computational neuroscience, physics, engineering or related, and a solid training in machine learning and computer vision.&lt;/p&gt;
&lt;p&gt;The candidate has to show good skills in computer science (programming skills, architecture understanding, git versioning, &amp;hellip;), and in image processing methods. Good command of programming tools (Python scripting) is required. Multidisciplinary background would be strongly appreciated and in particular an advanced knowledge in mathematics, for a deep understanding of signal processing methods, along with strong computational skills. The candidate needs to show a keen interest in neuroscience. It is a bonus if the candidate is curious about neuroscience and visual perception.&lt;/p&gt;
&lt;p&gt;The candidate has to fluently speak English to understand publications and to attend international conferences and workshops and pro-actively interact with partners in France, Switzerland, Spain and Greece. The preferred candidate will have the ability to work autonomously, and needs to be flexible to comply with the working method of the supervisors.&lt;/p&gt;
&lt;h2 id=&#34;research-context&#34;&gt;Research context&lt;/h2&gt;
&lt;p&gt;The thesis will be carried out in the team &amp;ldquo;NEuronal OPerations in visual TOpographic maps&amp;rdquo; (NeOpTo) within the 
&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; in 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, a welcoming and lively town by the Mediterranean sea in the south of France. The research team is led by F. Chavane (DR2, CNRS) and currently hosts 4 permanent staff, 3 post-docs and 4 PhD students. The research themes of the team are focused on neuronal operations within visual cortical maps. Indeed, along the cortical hierarchy, low-level features such as the position and orientation of the visual stimulus (but also auditory tone, somatosensory touch, etc&amp;hellip;) but also higher-level features (such as faces, viewpoints of objects, etc&amp;hellip;) are represented topographically on the cortical surface.&lt;/p&gt;
&lt;p&gt;This work will be conducted in direct collaboration with 
&lt;a href=&#34;http://i3s.unice.fr/jmartinet/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jean Martinet&lt;/a&gt; who will co-supervise the thesis. We will develop these algorithms in collaboration with 
&lt;a href=&#34;https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;amp;hl=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryad Benosman&lt;/a&gt; (Universit√© Pierre et Marie Curie) and 
&lt;a href=&#34;https://scholar.google.co.uk/citations?user=iIGoymcAAAAJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;St√©phane Viollet&lt;/a&gt; (√©quipe biorobotique, Institut des Sciences du Mouvement).&lt;/p&gt;
&lt;h2 id=&#34;fr-description-du-sujet-de-th√®se&#34;&gt;FR: Description du sujet de th√®se&lt;/h2&gt;
&lt;p&gt;La vision biologique est √©tonnamment efficace. Pour tirer parti de cette efficacit√©, l&amp;rsquo;apprentissage profond et les r√©seaux neuronaux convolutionnels (CNN) ont r√©cemment permis de r√©aliser de grandes avanc√©es en mati√®re de vision artificielle par ordinateur. Cependant, ces algorithmes sont aujourd&amp;rsquo;hui confront√©s √† de multiples d√©fis : les architectures apprises sont souvent peu interpr√©tables, sont d√©mesur√©ment gourmandes en √©nergie, n&amp;rsquo;int√®grent g√©n√©ralement pas les informations contextuelles qui semblent parfaitement adapt√©es √† la vision biologique et √† la perception humaine. Aussi ces algorithmes sont relativement lents -√† consommation √©nerg√©tique √©gale- par rapport √† la vision biologique. On pense qu&amp;rsquo;un facteur majeur de cette rapidit√© est le fait que l&amp;rsquo;information est repr√©sent√©e par de courtes impulsions √† des moments analogiques - et non discrets. Toutefois, les algorithmes de vision par ordinateur utilisant une telle repr√©sentation dans des r√©seaux de neurones impulsionnels font encore d√©faut dans la pratique, et son important potentiel est largement sous-exploit√©. Ce projet, qui est inspir√© de la biologie, aborde la question scientifique du d√©veloppement d&amp;rsquo;une architecture ultra-rapide de d√©tection et de traitement de sc√®nes visuelles, fonctionnant sur des appareils sans horloge centrale, et visant √† valider ce genre d&amp;rsquo;algorithmes √©v√©nementiels dans des situations r√©elles. Plus sp√©cifiquement, le projet d√©veloppera de nouveaux paradigmes pour une vision d&amp;rsquo;inspiration biologique, de la d√©tection au traitement, afin d&amp;rsquo;aider des machines telles que les robots a√©riens autonomes (UAV), les v√©hicules autonomes ou les robots √† acqu√©rir une compr√©hension de haut niveau des sc√®nes visuelles.&lt;/p&gt;
&lt;h2 id=&#34;fr-contexte-de-travail&#34;&gt;FR: Contexte de travail&lt;/h2&gt;
&lt;p&gt;La th√®se sera effectu√©e dans l&amp;rsquo;√©quipe &amp;ldquo;NEuronal OPerations in visual TOpographic maps&amp;rdquo; (NeOpTo) au sein de l&amp;rsquo;Institut de Neurosciences de la Timone (INT). L&amp;rsquo;√©quipe de recherche est dirig√©e par F. Chavane (DR2, CNRS) et accueille actuellement 4 personnels permanents, 3 post-doctorants et 4 doctorants. Les th√©matiques de recherche de l&amp;rsquo;√©quipe sont centr√©es sur les op√©rations neuronales au sein de cartes corticales visuelles. En effet, le long de la hi√©rarchie corticale, les caract√©ristiques de bas niveau telles que la position, l‚Äôorientation du stimulus visuel (mais aussi la tonalit√© auditive, le toucher somatosensoriel, etc&amp;hellip;) mais aussi les caract√©ristiques de niveau sup√©rieur (telles que les visages, les points de vue d‚Äôobjets, etc&amp;hellip;) sont repr√©sent√©es topographiquement sur la surface corticale.&lt;/p&gt;
&lt;p&gt;Cette th√®se sera men√©e en collaboration directe avec 
&lt;a href=&#34;http://i3s.unice.fr/jmartinet/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jean Martinet&lt;/a&gt; qui co-supervisera cette th√®se. Nous d√©velopperons ces algorithmes en collaboration avec 
&lt;a href=&#34;https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;amp;hl=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryad Benosman&lt;/a&gt; (Universit√© Pierre et Marie Curie) et 
&lt;a href=&#34;https://scholar.google.co.uk/citations?user=iIGoymcAAAAJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;St√©phane Viollet&lt;/a&gt; (√©quipe biorobotique, Institut des Sciences du Mouvement).&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt;Boutin, Victor, Angelo Franciosini, Fr√©d√©ric Chavane, Franck Ruffier, and Laurent U Perrinet. (2019). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://arxiv.org/abs/1902.07651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.&lt;/a&gt;&amp;rdquo; &lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Dupeyroux18&#34;&gt;Julien Dupeyroux, Victor Boutin, Julien R Serres, Laurent U Perrinet, St√©phane Viollet. (2018). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation.&lt;/a&gt;&amp;rdquo; &lt;em&gt;ISCAS&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chavane2011&#34;&gt;Chavane, F., Sharon, D., Jancke, D., Marre, O., Fr√©gnac, Y. and Grinvald, A. (2011). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.1016/S0928-4257%2800%2901096-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lateral spread of orientation selectivity in V1 is controlled by intracortical cooperativity.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Physiology Paris&lt;/em&gt; 94 (5-6): 333&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Cristobal15&#34;&gt;Gabriel Crist√≥bal, Laurent U Perrinet, Matthias S Keil (2015). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biologically Inspired Computer Vision.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Wiley&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Field1993&#34;&gt;Field, D.J., Hayes, A. and Hess, R.F. (1993). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.1016/0042-6989%2893%2990156-Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour integration by the human visual system: Evidence for a local ‚Äúassociation field‚Äù.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Vision Research&lt;/em&gt; 33 (2), pp. 173-193.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;gerard2016synaptic&#34;&gt;Gerard-Mercier, Florian, Pedro V Carelli, Marc Pananceau, Xoana G Troncoso, and Yves Fr√©gnac. (2016). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://www.jneurosci.org/content/36/14/3925&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synaptic Correlates of Low-Level Perception in V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 36 (14): 3925&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Keller2019&#34;&gt;Keller, A., Roth, M.M. and Scanziani, M. (2019). &lt;/a&gt; 2019. &amp;ldquo;
&lt;a href=&#34;https://www.abstractsonline.com/pp8/#!/7883/presentation/65856&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The feedback receptive field of neurons in the mammalian primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;American Society for Neuroscience Abstracts&lt;/em&gt;, 403.13. Chicago.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Lagorce16&#34;&gt;Lagorce, X., Orchard, G., Galluppi, F., Shi, B. E., &amp;amp; Benosman, R. B.&lt;/a&gt; (2016). &amp;ldquo;
&lt;a href=&#34;https://www.neuromorphic-vision.com/public/publications/1/publication.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOTS: a hierarchy of event-based time-surfaces for pattern recognition.&lt;/a&gt;&amp;rdquo; &lt;em&gt;IEEE transactions on pattern analysis and machine intelligence&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Li2002&#34;&gt;Li W, Pi√´ch V, Gilbert CD&lt;/a&gt; (2006). &amp;ldquo;
&lt;a href=&#34;http://www.paper.edu.cn/scholar/showpdf/MUz2UN2INTA0eQxeQh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour saliency in primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Neuron&lt;/em&gt;, 50(6):951‚Äì962.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;muller2018cortical&#34;&gt;Muller, Lyle, Fr√©d√©ric Chavane, John Reynolds, and Terrence J Sejnowski. &lt;/a&gt; (2018). &amp;ldquo;
&lt;a href=&#34;https://papers.cnl.salk.edu/PDFs/Cortical%20travelling%20waves_%20mechanisms%20and%20computational%20principles.%202018-4515.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cortical Travelling Waves: Mechanisms and Computational Principles.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Nature Reviews Neuroscience&lt;/em&gt; 19 (5): 255.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Paugam12&#34;&gt;H√©l√®ne Paugam-Moisy, Sander M. Bohte. &lt;/a&gt; (2012). &amp;ldquo;Computing with Spiking Neuron Networks.&amp;rdquo; &lt;em&gt;Handbook of Natural Computing&lt;/em&gt;, Springer-Verlag, pp.335-376, 2012&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Perrinet04&#34;&gt;Laurent U Perrinet, Manuel Samuelides, Simon J Thorpe. &lt;/a&gt; (2004). 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-03-ieee/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Coding static natural images using spiking event times: do neurons cooperate?&amp;quot;&lt;/a&gt; &lt;em&gt;IEEE Transactions on Neural Networks&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Tang18&#34;&gt;Tang, Hanlin, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. &lt;/a&gt; (2018). &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.1073/pnas.1719397115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recurrent computations for visual pattern completion.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 115 (35) 8835-8840.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Voges12&#34;&gt;Voges, Nicole, and Laurent U Perrinet.&lt;/a&gt; (2012). &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.3389/fncom.2012.00041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Complex Dynamics in Recurrent Cortical Networks Based on Spatially Realistic Connectivities.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt; 6.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2020-09-11 : Feedforward and feedback processes in visual recognition (T Serre)</title>
      <link>https://laurentperrinet.github.io/post/2020-09-11_seminaire-thomas-serre/</link>
      <pubDate>Tue, 16 Jun 2020 06:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2020-09-11_seminaire-thomas-serre/</guid>
      <description>&lt;h1 id=&#34;2020-09-11--feedforward-and-feedback-processes-in-visual-recognition-by-thomas-serre&#34;&gt;2020-09-11 : &amp;ldquo;Feedforward and feedback processes in visual recognition&amp;rdquo; by Thomas Serre&lt;/h1&gt;
&lt;p&gt;During a seminar at the Institute of Neurosciences Timone in Marseille, 
&lt;a href=&#34;http://serre-lab.clps.brown.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Serre&lt;/a&gt; will present his recent work on &amp;ldquo;Feedforward and feedback processes in visual recognition&amp;rdquo;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching ‚Äì and sometimes even surpassing ‚Äì human accuracy on a variety of visual recognition tasks. In this talk, however, I will show that these neural networks and their recent extensions exhibit a limited ability to solve seemingly simple visual reasoning problems involving incremental grouping, similarity, and spatial relation judgments. Our group has developed a recurrent network model of classical and extra-classical receptive fields that is constrained by the anatomy and physiology of the visual cortex. The model was shown to account for diverse visual illusions providing computational evidence for a novel canonical circuit that is shared across visual modalities. I will show that this computational neuroscience model can be turned into a modern end-to-end trainable deep recurrent network architecture that addresses some of the shortcomings exhibited by state-of-the-art feedforward networks for solving complex visual reasoning tasks. This suggests that neuroscience may contribute powerful new ideas and approaches to computer science and artificial intelligence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Dr. &lt;a href=&#34;http://serre-lab.clps.brown.edu/&#34;&gt;Thomas Serre&lt;/a&gt; is an Associate Professor in Cognitive Linguistic &amp;amp; Psychological Sciences and an affiliate of the Carney Institute for Brain Science at Brown University. He received a Ph.D. in Neuroscience from MIT in 2006 and an MSc in EECS from T√©l√©com Bretagne (France) in 2000. His research seeks to understand the neural computations supporting visual perception and has been featured in the BBC series ‚ÄúVisions from the Future‚Äù and other news articles (The Economist, New Scientist, Scientific American, IEEE Computing in Science and Technology, Technology Review and Slashdot). Dr. Serre is the Faculty Director of the Center for Computation and Visualization and the Associate Director of the Initiative for Computation in Brain and Mind at Brown University. He also holds an International Chair in AI within the Artificial and Natural Intelligence Toulouse Institute (France). Dr. Serre has served as an area chair and a senior program committee member for top-tier machine learning and computer vision conferences including AAAI, CVPR, and NeurIPS. He is currently serving as a domain expert for IARPA‚Äôs Machine Intelligence from Cortical Networks (MICrONS) program and as a scientific advisor for Vium, Inc. He was the recipient of an NSF Early Career Award as well as DARPA‚Äôs Young Faculty Award and Director‚Äôs Award.
  &lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A dual foveal-peripheral visual processing model implements efficient saccade selection</title>
      <link>https://laurentperrinet.github.io/publication/dauce-20/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dauce-20/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New &lt;a href=&#34;https://twitter.com/ARVOJOV?ref_src=twsrc%5Etfw&#34;&gt;@ARVOJOV&lt;/a&gt; : &amp;quot;A dual foveal-peripheral visual processing model implements efficient saccade selection&amp;quot; &lt;a href=&#34;https://t.co/JqnpBM5bcd&#34;&gt;https://t.co/JqnpBM5bcd&lt;/a&gt; comes with code @ &lt;a href=&#34;https://t.co/5MoIh00Bb8&#34;&gt;https://t.co/5MoIh00Bb8&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/OpenAccess?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#OpenAccess&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/visionscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#visionscience&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1305488088412688385?ref_src=twsrc%5Etfw&#34;&gt;September 14, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; &gt;


  &lt;img src=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;presented during this 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/&#34;&gt;talk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Cours et tutoriels</title>
      <link>https://laurentperrinet.github.io/project/courses/</link>
      <pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/project/courses/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From the retina to action: Understanding visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2020-04-ue-neurosciences-computationnelles/</link>
      <pubDate>Fri, 03 Apr 2020 16:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-04-ue-neurosciences-computationnelles/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling Complex-cells and topological structure in the visual cortex of mammals using Sparse Predictive Coding</title>
      <link>https://laurentperrinet.github.io/publication/franciosini-20-sigma/</link>
      <pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/franciosini-20-sigma/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2020-03-13: Soutenance Victor Boutin</title>
      <link>https://laurentperrinet.github.io/post/2020-03-13_soutenance-victor-boutin/</link>
      <pubDate>Wed, 04 Mar 2020 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2020-03-13_soutenance-victor-boutin/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Int√©ress√©s par le &amp;quot;Sparse deep predictive coding&amp;quot; / &amp;quot;codage hi√©rarchique, √©pars et pr√©dictif&amp;quot; ? Victor Boutin (Equipe NeOpTo) soutiendra sa th√®se de doctorat intitul√©e Vendredi 13 mars √† 14h  &lt;a href=&#34;https://t.co/BIrciyiRUf&#34;&gt;https://t.co/BIrciyiRUf&lt;/a&gt;&lt;br&gt;ü§ù &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/regionpaca?ref_src=twsrc%5Etfw&#34;&gt;@regionpaca&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/INT?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#INT&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://twitter.com/FranckRUFFIER?ref_src=twsrc%5Etfw&#34;&gt;@FranckRUFFIER&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1235128290458951680?ref_src=twsrc%5Etfw&#34;&gt;March 4, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Date : Vendredi 13 mars √† 14h&lt;/p&gt;
&lt;p&gt;Lieu:  salle Henri Gastaut, au rez de chauss√©e de l&amp;rsquo;INT  (how to 
&lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;La th√®se sera suivie d‚Äôun pot au R+4 de l‚Äô
&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; (how to 
&lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;jury&#34;&gt;Jury&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;amp;hl=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryad Benosman&lt;/a&gt;, Universit√© Pierre et Marie Curie, Rapporteur&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://scholar.google.fr/citations?hl=fr&amp;amp;user=uR-7ex4AAAAJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simon Thorpe&lt;/a&gt;, CNRS, Rapporteur&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.i2m.univ-amu.fr/perso/sandrine.anthoine/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sandrine Anthoine&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://neuro-psi.cnrs.fr/spip.php?article934&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yves Fr√©gnac&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sidkouider.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sid Kouider&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;, CNRS, Directeur de th√®se&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.ism.univ-amu.fr/ruffier/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Franck Ruffier&lt;/a&gt;, CNRS, Co-directeur de th√®se&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.researchgate.net/profile/Mossadek_Talby&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mossadek Talby&lt;/a&gt;, AMU, Jury invit√©&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Building models to efficiently represent images is a central and difficult problem in the machine learning community. The neuroscientific study of the early visual cortical areas is a great source of inspiration to find economical and robust solutions. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computation at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision have accounted for neurophysiological evidence and provide the most successful frameworks for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. The presented thesis introduces a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called  Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective. In terms of computation, the recurrent connectivity introduced by the PC framework allows the SDPC to converge to lower prediction errors with a higher convergence rate. In addition, we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational level. At the neural organization level, the feedback signal of the model accounted for a reorganization of the V1 association fields that promotes contour integration. At the representational level, the SDPC exhibited significant denoising ability which is highly correlated with the strength of the feedback from V2 to V1. These results from the SDPC model demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.&lt;/p&gt;
&lt;h2 id=&#34;r√©sum√©&#34;&gt;R√©sum√©&lt;/h2&gt;
&lt;p&gt;La repr√©sentation concise et efficace de l&amp;rsquo;information est un probl√®me qui occupe une place centrale dans l&amp;rsquo;apprentissage machine. Le cerveau, et plus particuli√®rement le cortex visuel, ont depuis longtemps trouv√© des solutions performantes et robustes afin de r√©soudre un tel probl√®me. A l&amp;rsquo;√©chelle locale, le codage √©pars est l&amp;rsquo;un des m√©canismes les plus prometteurs pour mod√©liser le traitement de l&amp;rsquo;information au sein des populations de neurones dans le cortex visuel. Le codage √©pars introduit une comp√©tition entre les neurones afin de d√©crire un stimulus visuel en limitant le nombre de neurones actifs. A l&amp;rsquo;√©chelle structurelle, les mod√®les dits ascendants d√©crivent le cortex visuel comme une succession d&amp;rsquo;unit√©s de traitement dans lesquelles l&amp;rsquo;information se propage de la r√©tine vers les couches profondes du cortex. Ces mod√®les ont expliqu√© avec succ√®s un grand nombre de ph√©nom√®nes neuro-physiologiques et ont servi d&amp;rsquo;inspiration afin de construire des algorithmes de reconnaissance d&amp;rsquo;objets extr√™mement performants. N√©anmoins, les mod√®les ascendants n&amp;rsquo;expliquent pas le grand nombre de connections r√©currentes et descendantes que l&amp;rsquo;on trouve dans le cortex visuel. Ces connections sont connues pour moduler l&amp;rsquo;activit√© des neurones en incluant des details contextuels au flux d&amp;rsquo;information ascendant. La th√©orie du codage pr√©dictif a √©t√© sugg√©r√©e pour mod√©liser les connections ascendantes, r√©currentes, et descendantes que l&amp;rsquo;on retrouve entre les diff√©rentes r√©gions corticales. Cette th√®se propose de combiner codage √©pars et codage pr√©dictif au sein d&amp;rsquo;un mod√®le hi√©rarchique et convolutif. Nous avons entrain√© ce mod√®le sur diff√©rentes bases de donn√©es afin de l&amp;rsquo;analyser avec une perspective √† la fois computationnelle et biologique. D&amp;rsquo;un point de vue computationnel, nous d√©montrons que les connections descendantes, introduites par le codage pr√©dictif, permettent une convergence meilleure et plus rapide du mod√®le. De plus, nous analysons les effets des connections descendantes sur l&amp;rsquo;organisation des populations de neurones, ainsi que leurs cons√©quences sur la mani√®re dont notre algorithme se repr√©sente les images. Nous montrons que les connections descendantes r√©organisent les champs d&amp;rsquo;association de neurones dans V1 afin de permettre une meilleure int√©gration des contours. En outre, nous observons que ces connections permettent une meilleure reconstruction des images bruit√©es. Nos r√©sultats sugg√®rent que l&amp;rsquo;inspiration des neurosciences fournit un cadre prometteur afin de d√©velopper des algorithmes de vision artificielles plus performants et plus robustes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Anticipatory Responses along Motion Trajectories in Awake Monkey Area V1</title>
      <link>https://laurentperrinet.github.io/publication/benvenuti-20/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/benvenuti-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effect of top-down connections in Hierarchical Sparse Coding</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/</link>
      <pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Humans adapt their anticipatory eye movements to the volatility of visual motion properties</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/</guid>
      <description>&lt;h1 id=&#34;humans-adapt-their-anticipatory-eye-movements-to-the-volatility-of-visual-motion-properties&#34;&gt;&amp;ldquo;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&amp;rdquo;&lt;/h1&gt;














  


&lt;video controls &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h2 id=&#34;at-what-point-should-we-become-alarmed-when-faced-with-changes-in-the-environment-the-sensory-system-provides-an-effective-response&#34;&gt;At what point should we become alarmed? When faced with changes in the environment, the sensory system provides an effective response.&lt;/h2&gt;
&lt;p&gt;The current health situation has shown us how abruptly our environment can change from one state to another, tragically illustrating the volatility we can face. To understand this notion of volatility, let&amp;rsquo;s take the case of a doctor who, among the patients he receives, usually diagnoses one out of ten cases of flu. Suddenly, he gets 5 out of 10 patients who test positive. Is this an unfortunate coincidence or are we now sure that there is a switch to a flu episode? Recent events have shown us how difficult it is to make a rational decision in times of uncertainty, and in particular to decide &lt;em&gt;when&lt;/em&gt; to act. However, mathematical solutions exist that adapt our behavior by optimally combining the information explored recently with that exploited in the past. In an article published in PLoS Computational Biology, Pasturel, Montagnini and Perrinet show that our brain responds to changes in the sensory environment in the same way as this mathematical model.&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-by-manipulating-the-probability-bias-of-the-presentation-of-a-visual-target-on-a-screen-this-experiment-manipulates-the-volatility-of-the-environment-in-a-controlled-way-by-introducing-switches-in-the-probability-bias-these-switches-randomly-change-the-bias-among-different-degrees-of-probability-both-left-and-right-at-each-trial-the-bias-then-generates-a-realization-either-left-l-or-right-r--the-target-moves-in-blocks-of-50-trials-1-to-50-and-these-realizations-are-the-only-ones-to-be-observed-the-evolution-of-the-bias-and-its-shifts-remaining-hidden-from-the-observer-compared-to-the-floating-average-that-is-conventionally-used-a-mathematical-model-can-be-deduced-as-a-predictive-average-that-allows-to-better-follow-the-dynamics-of-the-probability-bias-thanks-to-psychophysical-experiments-we-have-shown-that-observers-preferentially-follow-the-predictive-mean-rather-than-the-floating-mean-both-in-explicit-judgements-predictive-betting-and-more-surprisingly-in-the-anticipatory-movements-of-the-eyes-that-are-carried-out-without-the-observers-being-aware-of-them&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis_hu2604416ccc807da517dbfcd6542c544b_578813_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;1790&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R).  The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;These theoretical and experimental results show that in this realistic situation in which the context changes at random moments throughout the experiment, our sensory system adapts to volatility in an adaptive manner over the course of the trials. In particular, the experiments show in two behavioural experiments that humans adapt to volatility at the early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive level, through explicit evaluations. These results thus suggest that humans (and future artificial systems) can use much richer adaptation strategies than previously assumed. They provide a better understanding of how humans adapt to changing environments in order to make judgements or plan responses based on information that varies over time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;read the 
&lt;a href=&#34;https://doi.org/10.1371/journal.pcbi.1007438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;publication&lt;/a&gt; (or in 
&lt;a href=&#34;https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1007438&amp;amp;type=printable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;get a 
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preprint&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/784116v3.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;supplementary info : &lt;a href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf&#34;&gt;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for paper: &lt;a href=&#34;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&#34;&gt;https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for framework: &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM&#34;&gt;https://github.com/chloepasturel/AnticipatorySPEM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for figures 
&lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/1_protocole.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 1&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2_raw-results.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 2&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/3_Results_1-theory_BBCP.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 3&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/4_Results_2_fitting_BBCP.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 4&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/5_Meta_analysis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figure 5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://raw.githubusercontent.com/chloepasturel/AnticipatorySPEM/master/2020-03_video-abstract/PasturelMontagniniPerrinet2020_video-abstract.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video abstract&lt;/a&gt; (and the 
&lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/blob/master/2020-03_video-abstract/2020-03-24_video-abstract.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; for generating the video abstract)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Des illusions aux hallucinations visuelles: une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/talk/2020-01-20-atelier-sciences-cinema/</link>
      <pubDate>Mon, 20 Jan 2020 10:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2020-01-20-atelier-sciences-cinema/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Watch ‚Äú√áA TOURNE‚Äù on &lt;a href=&#34;https://twitter.com/hashtag/Vimeo?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Vimeo&lt;/a&gt; &lt;a href=&#34;https://t.co/nzUAEwjTeD&#34;&gt;https://t.co/nzUAEwjTeD&lt;/a&gt; (english subs) r√©alis√© par Camille Goujon et par les √©l√®ves du Lyc√©e Professionnel Domaine Eguille, Ved√®ne (France) - &lt;a href=&#34;https://t.co/EhVz0YZdPs&#34;&gt;https://t.co/EhVz0YZdPs&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/StopMotion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#StopMotion&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/vision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#vision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuroscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuroscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/outreach?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#outreach&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1284791644240347138?ref_src=twsrc%5Etfw&#34;&gt;July 19, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/398661322&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Le film &amp;ldquo;√áA TOURNE&amp;rdquo; a √©t√© s√©lectionn√© pour faire partie de la comp√©tition cat√©gorie ¬´FILMS SCOLAIRES&amp;quot; diffus√©e du 4 au 7 novembre 2020 dans le cadre du festival &amp;ldquo;7√®me Art Jeunes Talent! : &lt;a href=&#34;http://www.festivaltournezjeunesse.com&#34;&gt;http://www.festivaltournezjeunesse.com&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Le texte de cette pr√©sentation est reprise dans cet article de 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; (
&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lien direct&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Voir la @ 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34;&gt;pr√©sentation au NeuroStories&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>From the retina to action: Dynamics of predictive processing in the visual system</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning dynamics in a neural network model of the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/ladret-20-aes/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ladret-20-aes/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See also 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/ladret-19-sfn/&#34;&gt;Ladret and Perrinet, 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Postdoc position on Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves</title>
      <link>https://laurentperrinet.github.io/post/2019-10-28_postdoc-position/</link>
      <pubDate>Mon, 21 Oct 2019 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-28_postdoc-position/</guid>
      <description>&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    THE POSITION HAS BEEN FILLED.
  &lt;/div&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Applications are welcome for a post-doctoral position at INT-Marseille, France exploring novel visual computations using spatio-temporal diffusion kernels and travelling waves. More info @ &lt;a href=&#34;https://t.co/f6tUR8XW6y&#34;&gt;https://t.co/f6tUR8XW6y&lt;/a&gt; &lt;a href=&#34;https://t.co/odzckjtloa&#34;&gt;pic.twitter.com/odzckjtloa&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1188940039293751297?ref_src=twsrc%5Etfw&#34;&gt;October 28, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Dear colleagues,&lt;/p&gt;
&lt;p&gt;Applications are welcome for a post-doctoral position at 
&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;INT&lt;/a&gt; in 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, France. Your mission will be to explore novel visual computations using spatio-temporal diffusion kernels and traveling waves. The project is funded by the 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-horizontal-v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANR Horizontal V1&lt;/a&gt; grant (ANR-17-CE37-0006) from the French National Research Agency (ANR) and will be coordinated by 
&lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;, in collaboration with 
&lt;a href=&#34;https://www.mullerlab.ca&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lyle Muller&lt;/a&gt; and 
&lt;a href=&#34;http://www.int.univ-amu.fr/spip.php?page=equipe&amp;amp;equipe=NeOpTo&amp;amp;lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fr√©d√©ric Chavane&lt;/a&gt; at INT and 
&lt;a href=&#34;http://neuro-psi.cnrs.fr/spip.php?article934&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yves Fr√©gnac&lt;/a&gt; and Jan Antolik at UNIC-NeuroPSI, Gif. We are seeking candidates with a strong background in machine learning, computer vision and computational neuroscience.&lt;/p&gt;
&lt;p&gt;For more information, visit 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-28_postdoc-position&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/post/2019-10-28_postdoc-position&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The starting date is set to January 6th, 2020 but can be flexibly extended. To obtain further information or send applications (including a full CV, a letter of motivation, 2 reference names), please contact: 
&lt;a href=&#34;mailto:Laurent.Perrinet@univ-amu.fr&#34;&gt;Laurent.Perrinet@univ-amu.fr&lt;/a&gt;. The appointment is for 18 month. Applications are welcome immediately and until the end of year 2019.&lt;/p&gt;
&lt;p&gt;Thanks for distributing this announcement to potential candidates!&lt;/p&gt;
&lt;h1 id=&#34;detailed-description-visual-computations-using-spatio-temporal-diffusion-kernels-and-traveling-waves&#34;&gt;Detailed description: Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves&lt;/h1&gt;
&lt;p&gt;Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. It is clear from recent advances in system and computational neuroscience that nonlinear, recurrent interactions in visual cortical networks are key to this efficiency¬†(
&lt;a href=&#34;#Tang18&#34;&gt;Tang et al., 2018&lt;/a&gt;; 
&lt;a href=&#34;#Kietzmann19&#34;&gt;Kietzmann et al., 2019&lt;/a&gt;). We will use inspiration from neurophysiology and brain imaging to resolve this apparent gap between traditional CNNs and biological visual systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this post-doctoral project, we propose to address these major limitations by focusing on specific dynamical features of cortical circuits: &lt;em&gt;lateral diffusion of sensory-evoked traveling waves&lt;/em&gt; (
&lt;a href=&#34;#Chavane2000&#34;&gt;Chavane et al., 2011&lt;/a&gt;; 
&lt;a href=&#34;#muller2018cortical&#34;&gt;Muller et al., 2018&lt;/a&gt;) and &lt;em&gt;dynamic neuronal association fields&lt;/em&gt; (
&lt;a href=&#34;#Fr%c3%a9gnac2012&#34;&gt;Fr√©gnac et al., 2012&lt;/a&gt;; 
&lt;a href=&#34;#Fr%c3%a9gnac2016&#34;&gt;Fr√©gnac et al., 2016&lt;/a&gt;; 
&lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;)&lt;/strong&gt;. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (
&lt;a href=&#34;#Voges12&#34;&gt;Voges and Perrinet, 2012&lt;/a&gt;). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). Less studied, but probably decisive in active vision, recurrent cortico-cortical loops add a level of distributed top-down complexity which participates to the lateral integration of sensory input and perceptual context (
&lt;a href=&#34;#Keller2019&#34;&gt;Keller et al., 2019&lt;/a&gt;). Coupled with the continuous time dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for generating information diffusion through traveling waves. Inspired by recent work in neuroscience uncovering the ubiquity of these waves during visual processing, we aim to design a self-supervised CNN that will exploit these dynamics for new applications in computer vision.&lt;/p&gt;
&lt;p&gt;The proposed work will be organized as a collaboration between two labs (INT, Marseille and UNIC, Gif) along three tasks to be integrated in a unified model:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The starting point will be to extend results of self-supervised learning that we have obtained on static, natural images (
&lt;a href=&#34;#BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt;Boutin et al., 2019&lt;/a&gt;) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the &amp;ldquo;association field&amp;rdquo; described at the psychophysical (
&lt;a href=&#34;#Field1993&#34;&gt;Field et al., 1993&lt;/a&gt;), spiking (
&lt;a href=&#34;#Li2002&#34;&gt;Li and Gilbert, 2002&lt;/a&gt;) and synaptic (
&lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;) levels.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The central aim will be to develop a dynamical version of this feedback/lateral kernel in the context of the 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-horizontal-v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANR Horizontal-V1&lt;/a&gt; project, linking the two labs and confronted to their recent electrophysiological data pointing to different classes of spatio-temporal diffusion and different degree of anisotropies during apparent and continuous motion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The implementation of this kernel inspired by CNN theory will be compared with a biologically realistic models of the early visual system (
&lt;a href=&#34;#Antolik2019&#34;&gt;Antolik et al., 2019&lt;/a&gt;), and simulations of the lateral diffusion kernel will be developed in collaboration with 
&lt;a href=&#34;http://antolik.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jan Antolik&lt;/a&gt;, external collaborator to the ANR grant.  In parallel, using tools linking neural activity to VSD imaging¬†(
&lt;a href=&#34;#muller2014stimulus&#34;&gt;Muller et al., 2014&lt;/a&gt;; 
&lt;a href=&#34;#Chemla2018&#34;&gt;Chemla et al., 2019&lt;/a&gt;), we will analyze at a more mesocopic level the role of observed traveling waves in forming efficient representations of the visual world.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;expected-profile-of-the-candidate&#34;&gt;Expected profile of the candidate&lt;/h2&gt;
&lt;p&gt;Candidates should have at least a PhD degree in the domain of computational neuroscience, physics, engineering or related, and a solid training in machine learning and computer vision.&lt;/p&gt;
&lt;p&gt;The candidate has to show good skills in computer science (programming skills, architecture understanding, git versioning, &amp;hellip;), and in image processing methods. Good command of programming tools (Python scripting) is required. Multidisciplinary background would be strongly appreciated and in particular an advanced knowledge in mathematics, for a deep understanding of signal processing methods, along with strong computational skills. The candidate needs to show a keen interest in neuroscience. It is a bonus if the candidate is curious about neuroscience and visual perception.&lt;/p&gt;
&lt;p&gt;The candidate has to fluently speak English to understand publications and to attend international conferences and workshops. The preferred candidate will have the ability to work autonomously, and needs to be flexible to comply with the working method of the supervisors.&lt;/p&gt;
&lt;h2 id=&#34;research-context&#34;&gt;Research context&lt;/h2&gt;
&lt;p&gt;This project is funded by the French National Research Agency (ANR) under the 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-horizontal-v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANR Horizontal V1&lt;/a&gt; grant (coordinator Y. Fr√©gnac) which aims at understanding the emergence of sensory predictions linking local shape attributes (orientation, contour) to global indices of movement (direction, speed, trajectory) at the earliest stage of cortical processing (primary visual cortex, i.e. V1). The cross-talk between physiological and theoretical approaches will be fostered by the close collaboration with the teams of Fr√©d√©ric Chavane at INT and Yves Fr√©gnac at UNIC. The theoretical work will be performed in close collaboration with 
&lt;a href=&#34;https://www.mullerlab.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lyle Muller&lt;/a&gt; (Western U) and Jan Antolik (Prague). The project will be primarily hosted at the 
&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; in 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, a lively town by the Mediterranean sea in the south of France, but the applicant will be asked also to show mobility to visit the other partner lab when needed.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Antolik2019&#34;&gt; Antolik, J, C Monier, Y Fr√©gnac, AP Davison. (2019). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/416156v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A comprehensive data-driven model of cat primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;BioRxiv&lt;/em&gt;, 416156.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt; Boutin, Victor, Angelo Franciosini, Fr√©d√©ric Chavane, Franck Ruffier, and Laurent U Perrinet. (2019). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://arxiv.org/abs/1902.07651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.&lt;/a&gt;&amp;rdquo; &lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chavane2000&#34;&gt; Chavane, F., C. Monier, V. Bringuier, P. Baudot, L. Borg-Graham, J. Lorenceau, and Y. Fr√©gnac. 2000. &lt;/a&gt; &amp;ldquo;The Visual Cortical Association Field: A Gestalt Concept or a Psychophysiological Entity?&amp;rdquo; &lt;em&gt;Frontiers in System Neuroscience&lt;/em&gt; 4(5): 1-26.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chavane2011&#34;&gt; Chavane, F., Sharon, D., Jancke, D., Marre, O., Fr√©gnac, Y. and Grinvald, A.  (2011). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.1016/S0928-4257%2800%2901096-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lateral spread of orientation selectivity in V1 is controlled by intracortical cooperativity.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Physiology Paris&lt;/em&gt; 94 (5-6): 333&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chemla2018&#34;&gt; Chemla, Sandrine, Alexandre Reynaud, Matteo diVolo, Yann Zerlaut, Laurent Perrinet, Alain Destexhe, and Fr√©d√©ric Chavane. &lt;/a&gt; (2018). &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.1523/JNEUROSCI.2792-18.2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Suppressive Waves Disambiguate the Representation of Long-Range Apparent Motion in Awake Monkey V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 39 (22) 4282-4298.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Field1993&#34;&gt; Field, D.J., Hayes, A. and Hess, R.F. (1993). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.1016/0042-6989%2893%2990156-Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour integration by the human visual system: Evidence for a local ‚Äúassociation field‚Äù.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Vision Research&lt;/em&gt; 33 (2), pp. 173-193.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Fr√©gnac2012&#34;&gt; Fr√©gnac, Y. (2012)  &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01685152/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reading out the synaptic echoes of low-level perception in V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;European Conference in Computer Vision&lt;/em&gt; 486-495. Springer, Berlin, Heidelberg.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Fr√©gnac2016&#34;&gt; Fr√©gnac, Y., Fournier, J., Gerard-Mercier, F., Monier, C., Carelli, P., , M., Troncoso, X. (2016).  &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://link-springer-com.insb.bib.cnrs.fr/content/pdf/10.1007%2F978-3-319-28802-4_4.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Visual Brain: Computing Through Multiscale Complexity.&lt;/a&gt;&amp;rdquo; In &lt;em&gt;Micro-, Meso- and Macro-Dynamics of the Brain&lt;/em&gt; pp 43-57.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;gerard2016synaptic&#34;&gt; Gerard-Mercier, Florian, Pedro V Carelli, Marc Pananceau, Xoana G Troncoso, and Yves Fr√©gnac. (2016). &lt;/a&gt; &amp;ldquo;
&lt;a href=&#34;https://www.jneurosci.org/content/36/14/3925&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synaptic Correlates of Low-Level Perception in V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 36 (14): 3925&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Keller2019&#34;&gt;Keller, A., Roth, M.M. and Scanziani, M. (2019).  &lt;/a&gt; 2019. &amp;ldquo;
&lt;a href=&#34;https://www.abstractsonline.com/pp8/#!/7883/presentation/65856&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The feedback receptive field of neurons in the mammalian primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;American Society for Neuroscience Abstracts&lt;/em&gt;,  403.13. Chicago.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Kietzmann19&#34;&gt;Kietzmann, Tim C., Courtney J. Spoerer, Lynn K. A. S√∂rensen, Radoslaw M. Cichy, Olaf Hauk, and Nikolaus Kriegeskorte. &lt;/a&gt; (2019). &amp;ldquo;
&lt;a href=&#34;https://doi.org/10/gf9j2t&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recurrence Is Required to Capture the Representational Dynamics of the Human Visual System.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;, October, 201905544.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Li2002&#34;&gt;Li W, Pi√´ch V, Gilbert CD&lt;/a&gt;  (2006). &amp;ldquo;
&lt;a href=&#34;http://www.paper.edu.cn/scholar/showpdf/MUz2UN2INTA0eQxeQh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour saliency in primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Neuron&lt;/em&gt;, 50(6):951‚Äì962.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;muller2014stimulus&#34;&gt;Muller, Lyle, Alexandre Reynaud, Fr√©d√©ric Chavane, and Alain Destexhe. &lt;/a&gt; (2014). &amp;ldquo;
&lt;a href=&#34;http://www.int.univ-amu.fr/IMG/pdf/Muller_Nature_Communications2014.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Stimulus-Evoked Population Response in Visual Cortex of Awake Monkey Is a Propagating Wave.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Nature Communications&lt;/em&gt; 5: 3675.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;muller2018cortical&#34;&gt; Muller, Lyle, Fr√©d√©ric Chavane, John Reynolds, and Terrence J Sejnowski. &lt;/a&gt; (2018). &amp;ldquo;
&lt;a href=&#34;https://papers.cnl.salk.edu/PDFs/Cortical%20travelling%20waves_%20mechanisms%20and%20computational%20principles.%202018-4515.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cortical Travelling Waves: Mechanisms and Computational Principles.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Nature Reviews Neuroscience&lt;/em&gt; 19 (5): 255.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Tang18&#34;&gt;Tang, Hanlin, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. &lt;/a&gt; (2018). &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.1073/pnas.1719397115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recurrent computations for visual pattern completion.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 115 (35) 8835-8840.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Voges12&#34;&gt; Voges, Nicole, and Laurent U Perrinet.&lt;/a&gt; (2012). &amp;ldquo;
&lt;a href=&#34;https://doi.org/10.3389/fncom.2012.00041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Complex Dynamics in Recurrent Cortical Networks Based on Spatially Realistic Connectivities.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt; 6.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2019-10-10: GDR vision 2019</title>
      <link>https://laurentperrinet.github.io/post/2019-10-10_gdrvision/</link>
      <pubDate>Thu, 10 Oct 2019 12:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-10_gdrvision/</guid>
      <description>&lt;p&gt;Avec Anna Montagnini, Manuel Vidal et Fran√ßoise Vitu, nous organisons cette ann√©e le GDR Vision √† Marseille les journ√©es du 10 et 11 octobre.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;plus d&amp;rsquo;infos sur &lt;a href=&#34;https://gdrvision2019.sciencesconf.org/&#34;&gt;https://gdrvision2019.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;nous aurons un atelier m√©thodologique le jeudi matin sur les apports possibles du Deep Learning pour les sciences de la vision: 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Utiliser l&amp;rsquo;apprentissage profond en vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;la session sp√©ciale du jeudi est sponsoris√©e par la 
&lt;a href=&#34;https://laurentperrinet.github.io/project/spikeai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;projet SpikeAI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;R√©unions pass√©es:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lille: &lt;a href=&#34;https://gdrvision2017.sciencesconf.org/&#34;&gt;https://gdrvision2017.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paris: &lt;a href=&#34;https://gdrvision2018.sciencesconf.org/&#34;&gt;https://gdrvision2018.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2019-10-10: Atelier Utiliser l&#39;apprentissage profond en vision</title>
      <link>https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/</link>
      <pubDate>Thu, 10 Oct 2019 09:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/</guid>
      <description>&lt;p&gt;Date : jeudi 10 octobre de 9h30 √† 12h30&lt;/p&gt;
&lt;p&gt;Intervenants : Laurent Perrinet et Chlo√© Pasturel&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://gdrvision2019.sciencesconf.org/resource/page/id/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;programme&lt;/a&gt;: Nous proposons dans cet atelier pratique de pr√©senter les nouveaux enjeux apport√©s par l&amp;rsquo;apprentissage profond et plus g√©n√©ralement par l&amp;rsquo;apprentissage machine. L&amp;rsquo;objectif est de montrer sous forme de simples exercises pratiques comment ces nouveaux outils permettent 1) de cat√©goriser des images 2) d&amp;rsquo;apprendre un tel mod√®les 3) de g√©n√©rer de nouvelles images √† partir d&amp;rsquo;une base existante.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SpikeAI/2019-10-10_ML-tutorial&#34;&gt;https://github.com/SpikeAI/2019-10-10_ML-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Atelier concoct√© en collaboration avec 
&lt;a href=&#34;https://github.com/chloepasturel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chlo√© Pasturel&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cet atelier fait partie du 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-10_gdrvision/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GDR vision 2019&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2019-10-07: Le temps des sens</title>
      <link>https://laurentperrinet.github.io/post/2019-10-07_neurostories/</link>
      <pubDate>Mon, 07 Oct 2019 18:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-07_neurostories/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Cette pr√©sentation lors des 
&lt;a href=&#34;http://neuroschool-stories.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroStories&lt;/a&gt; vise √† aborder la notion de temps dans le cerveau.&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/jJKTdlChefc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Chaque ann√©e, NeuroSchool nous raconte des histoires sur un th√®me √† la fois philosophique et scientifique. L‚Äôobjectif est de faire conna√Ætre, d‚Äôune mani√®re inventive, les recherches de pointe men√©es √† Marseille et ailleurs, dans le domaine des neurosciences. Le format inventif associe des NeuroStories et des causeries scientifiques.&amp;rdquo; &lt;a href=&#34;http://neuroschool-stories.com/&#34;&gt;http://neuroschool-stories.com/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Le texte de cette pr√©sentation est repris dans 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; (
&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lien direct&lt;/a&gt;) ainsi que dans 
&lt;a href=&#34;https://www.science-et-vie.com/paroles-d-experts/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-53387&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Science &amp;amp; Vie&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2019-10-07-neurostories-videos-of-my-talk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurostories: d&amp;rsquo;autres figures anim√©es du flash-lag effect&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A dynamic model for decoding direction and orientation in macaque primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-19-nccd/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-19-nccd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MesoCentre (2018/2022)</title>
      <link>https://laurentperrinet.github.io/grant/mesocentre/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/mesocentre/</guid>
      <description>&lt;p&gt;In the field of neuroscience, modeling has long made it possible to validate and predict theories of information processing in the neural networks that make up the brain. The recent emergence of solutions inherited from machine learning, in particular deep learning, has changed the field since 2012. One reason for the effectiveness of these methods is the amount of data analyzed but above all the ability to teach these algorithms on dedicated architectures, including graphics cards (GPUs). Indeed, this architecture allows an algorithm to be parallelized into a multitude of simple and independent subprograms that allow speed gains of around 6x to 10x to be achieved over traditional visual information processing architectures. We are now using these architectures excessively to test new image processing models - targeting applications in both neuroscience and machine learning.&lt;/p&gt;
&lt;p&gt;Dans le domaine des neurosciences, la mod√©lisation a longtemps permis d&amp;rsquo;effectuer des validations et pr√©dictions sur les th√©ories du traitement de l&amp;rsquo;information dans les r√©seaux de neurones qui constitue le cerveau. L&amp;rsquo;√©mergence r√©cente de solutions h√©rit√©es de l&amp;rsquo;apprentissage machine, en particulier l&amp;rsquo;apprentissage profond (Deep Learning) est venu bouleverser le champ depuis 2012. Une raison de l&amp;rsquo;efficacit√© de ces m√©thodes est la quantit√© de donn√©es analys√©es mais surtout la capacit√© de faire apprendre ces algorithmes sur des architectures d√©di√©es, notamment les cartes graphiques (GPU). En effet cette architecture permet de parall√©liser un algorithme en une multitude de sous programmes simples et ind√©pendants qui permettent d&amp;rsquo;atteindre des gains de vitesse de l&amp;rsquo;ordre de 6x √† 10x sur des architectures classiques de traitement de l&amp;rsquo;information visuelle. Nous utilisons maintenant excessivement ces architectures pour tester de nouveaux mod√®les de traitement des images - en visant des applications  aussi bien aux neurosciences qu&amp;rsquo;en apprentissage machine.&lt;/p&gt;
&lt;p&gt;Acknowledgement: This work was granted access to the HPC resources of Aix-Marseille Universit√© financed by the project Equip@Meso (ANR-10-EQPX-29-01) of the program ¬´ Investissements d‚ÄôAvenir ¬ª supervised by the Agence Nationale de la Recherche.¬ª&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>APROVIS3D: Analog PROcessing of bioinspired VIsion Sensors for 3D reconstruction (2019/2023)</title>
      <link>https://laurentperrinet.github.io/grant/aprovis-3-d/</link>
      <pubDate>Tue, 10 Sep 2019 10:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/aprovis-3-d/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Le projet APROVIS3D est laur√©at de l&#39;
&lt;a href=&#34;http://www.chistera.eu/projects/aprovis3d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;appel √† projets 2018 &lt;em&gt;CHIST-ERA&lt;/em&gt;&lt;/a&gt; :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;APROVIS3D project targets analog computing for artificial intelligence in the form of Spiking Neural Networks (SNNs) on a mixed analog and digital architecture. The project includes including field programmable analog array (FPAA) and SpiNNaker applied to a stereopsis system dedicated to coastal surveillance using an aerial robot. Computer vision systems widely rely on artificial intelligence and especially neural network based machine learning, which recently gained huge visibility. The training stage for deep convolutional neural networks is both time and energy consuming. In contrast, the human brain has the ability to perform visual tasks with unrivalled computational and energy efficiency. It is believed that one major factor of this efficiency is the fact that information is vastly represented by short pulses (spikes) at analog ‚Äì not discrete ‚Äì times. However, computer vision algorithms using such representation still lack in practice, and its high potential is largely underexploited. Inspired from biology, the project addresses the scientific question of developing a low-power, end-to-end analog sensing and processing architecture of 3D visual scenes, running on analog devices, without a central clock and aims to validate them in real-life situations. More specifically, the project will develop new paradigms for biologically inspired vision, from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes. The ambitious long-term vision of the project is to develop the next generation AI paradigm that will eventually compete with deep learning. We believe that neuromorphic computing, mainly studied in EU countries, will be a key technology in the next decade. It is therefore both a scientific and strategic challenge for the EU to foster this technological breakthrough. The consortium from four EU countries offers a unique combination of expertise that the project requires. SNNs specialists from various fields, such as visual sensors (IMSE, Spain), neural network architecture and computer vision (Uni. of Lille, France) and computational neuroscience (INT, France) will team up with robotics and automatic control specialists (NTUA, Greece), and low power integrated systems designers (ETHZ, Switzerland) to help geoinformatics researchers (UNIWA, Greece) build a demonstrator UAV for coastal surveillance (TRL5). Adding up to the shared interest regarding analog based computing and computer vision, all team members have a lot to offer given their different and complementary points of view and expertise. Key challenges of this project will be end-to-end analog system design (from sensing to AI-based control of the UAV and 3D coastal volumetric reconstruction), energy efficiency, and practical usability in real conditions. We aim to show that such a bioinspired analog design will bring large benefits in terms of power efficiency, adaptability and efficiency needed to make coastal surveillance with UAVs practical and more efficient than digital approaches.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type de contrat : Subvention / Aide&lt;/li&gt;
&lt;li&gt;Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE&lt;/li&gt;
&lt;li&gt;Objet : AAP 2019 - CHIST-ERA &amp;ldquo;Analog PROcessing of bioinspired VIsion Sensors for 3D reconstruction&amp;rdquo; ANR-19-CHR3-0008-03&lt;/li&gt;
&lt;li&gt;Responsable Scientifique INT : PERRINET Laurent (UMR7289)&lt;/li&gt;
&lt;li&gt;‚ÄúThis project has received funding from the European Union‚Äôs ERA-NET CHIST-ERA 2018 research and innovation programme under grant agreement No ANR-19-CHR3-0008-03‚Äù&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Learning where to look: a foveated visuomotor control model</title>
      <link>https://laurentperrinet.github.io/talk/2019-07-15-cns/</link>
      <pubDate>Mon, 15 Jul 2019 12:20:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-07-15-cns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;download a 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-07-15-cns/2019-07-15-cns.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preliminary PDF&lt;/a&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Emmanuel Dauc√© @ &lt;a href=&#34;https://twitter.com/hashtag/CNS2019Barcelona?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#CNS2019Barcelona&lt;/a&gt; speaks about our joint work on ¬´¬†Learning where to look: a foveated visuomotor control model¬†¬ª more info @ &lt;a href=&#34;https://t.co/HREjuIgNCn&#34;&gt;https://t.co/HREjuIgNCn&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNSorg?ref_src=twsrc%5Etfw&#34;&gt;@CNSorg&lt;/a&gt; &lt;a href=&#34;https://t.co/GbbXhWL1k1&#34;&gt;pic.twitter.com/GbbXhWL1k1&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1150713758643380226?ref_src=twsrc%5Etfw&#34;&gt;July 15, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



















&lt;figure id=&#34;figure-problem-setting-in-generic-ecological-settings-the-visual-system-faces-a-tricky-problem-when-searching-for-one-target-from-a-class-of-targets-in-a-cluttered-environment-a-it-is-synthesized-in-the-following-experiment-after-a-fixation-period-of-200-ms-an-observer-is-presented-with-a-luminous-display--showing-a-single-target-from-a-known-class-here-digits-and-at-a-random-position-the-display-is-presented-for-a-short-period-of-500-ms-light-shaded-area-in-b-that-is-enough-to-perform-at-most-one-saccade-here-successful-on-the-potential-target-finally-the-observer-has-to-identify-the-digit-by-a-keypress-b-prototypical-trace-of-a-saccadic-eye-movement-to-the-target-position-in-particular-we-show-the-fixation-window-and-the-temporal-window-during-which-a-saccade-is-possible-green-shaded-area-c-simulated-reconstruction-of-the-visual-information-from-the-interoceptive-retinotopic-map-at-the-onset-of-the-display-and-after-a-saccade-the-dashed-red-box-indicating-the-visual-area-of-the-what-pathway-in-contrast-to-an-exteroceptive-representation-see-a-this-demonstrates-that-the-position-of-the-target-has-to-be-inferred-from-a-degraded-sampled-image-in-particular-the-configuration-of-the-display-is-such-that-by-adding-clutter-and-reducing-the-size-of-the-digit-it-may-become-necessary-to-perform-a-saccade-to-be-able-to-identify-the-digit-the-computational-pathway-mediating-the-action-has-to-infer-the-location-of-the-target-emphbefore-seeing-it-that-is-before-being-able-to-actually-identify-the-targets-category-from-a-central-fixation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/SpikeAI/2019-07-15_CNS/master/figures/fig_intro.jpg&#34; data-caption=&#34;Problem setting: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. &amp;lt;strong&amp;gt;A)&amp;lt;/strong&amp;gt; It is synthesized in the following experiment: After a fixation period of 200 ms, an observer is presented with a luminous display  showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of 500 ms (light shaded area in B), that is enough to perform at most one saccade (here, successful) on the potential target. Finally, the observer has to identify the digit by a keypress. &amp;lt;strong&amp;gt;B)&amp;lt;/strong&amp;gt; Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window and the temporal window during which a saccade is possible (green shaded area). &amp;lt;strong&amp;gt;C)&amp;lt;/strong&amp;gt; Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display and after a saccade, the dashed red box indicating the visual area of the ``what&amp;#39;&amp;#39; pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target&amp;amp;rsquo;s category from a central fixation.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/SpikeAI/2019-07-15_CNS/master/figures/fig_intro.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Problem setting: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. &lt;strong&gt;A)&lt;/strong&gt; It is synthesized in the following experiment: After a fixation period of 200 ms, an observer is presented with a luminous display  showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of 500 ms (light shaded area in B), that is enough to perform at most one saccade (here, successful) on the potential target. Finally, the observer has to identify the digit by a keypress. &lt;strong&gt;B)&lt;/strong&gt; Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window and the temporal window during which a saccade is possible (green shaded area). &lt;strong&gt;C)&lt;/strong&gt; Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display and after a saccade, the dashed red box indicating the visual area of the ``what&#39;&#39; pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target&amp;rsquo;s category from a central fixation.
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-results-success&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-20.png&#34; data-caption=&#34;Results: success&#34;&gt;


  &lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-20.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results: success
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-results-failure-to-classify&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-32.png&#34; data-caption=&#34;Results: failure to classify&#34;&gt;


  &lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-32.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results: failure to classify
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-results-failure-to-locate&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-47.png&#34; data-caption=&#34;Results: failure to locate&#34;&gt;


  &lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-47.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results: failure to locate
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Illusions et hallucinations visuelles : une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/post/2019-06-06-theconversation/</link>
      <pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-06-06-theconversation/</guid>
      <description>&lt;p&gt;Publication d&amp;rsquo;un nouvel article g√©n√©raliste autour des &amp;ldquo;Illusions et hallucinations visuelles&amp;rdquo; √† d√©couvrir sur le site 
&lt;a href=&#34;https://theconversation.com/illusions-et-hallucinations-visuelles-une-porte-sur-la-perception-117389&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TheConversation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Illusions et hallucinations visuelles : une porte sur la perception &lt;a href=&#34;https://t.co/nOJFVRYDz7&#34;&gt;https://t.co/nOJFVRYDz7&lt;/a&gt; &lt;a href=&#34;https://t.co/HiBPHbKdSA&#34;&gt;pic.twitter.com/HiBPHbKdSA&lt;/a&gt;&lt;/p&gt;&amp;mdash; The Conversation France (@FR_Conversation) &lt;a href=&#34;https://twitter.com/FR_Conversation/status/1136743272024612886?ref_src=twsrc%5Etfw&#34;&gt;June 6, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Les objectifs sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mieux comprendre la fonction de la perception visuelle en explorant certaines limites ;&lt;/li&gt;
&lt;li&gt;mieux comprendre l‚Äôimportance de l‚Äôaspect dynamique de la perception ;&lt;/li&gt;
&lt;li&gt;mieux comprendre le r√¥le de l‚Äôaction dans la perception.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Les &lt;a href=&#34;https://twitter.com/hashtag/illusions?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#illusions&lt;/a&gt; et &lt;a href=&#34;https://twitter.com/hashtag/hallucinations?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#hallucinations&lt;/a&gt; nous informent sur les &lt;a href=&#34;https://twitter.com/hashtag/perceptions?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#perceptions&lt;/a&gt; mais aussi sur la compr√©hension des m√©canismes c√©r√©braux.&lt;br&gt;&lt;br&gt;Laurent Perrinet, &lt;a href=&#34;https://twitter.com/imera_amu?ref_src=twsrc%5Etfw&#34;&gt;@imera_amu&lt;/a&gt;, nous explique comment et pourquoi une image peut tromper nos sens.&lt;br&gt;&lt;br&gt;üå™Ô∏è&lt;a href=&#34;https://t.co/5QSaezZonz&#34;&gt;https://t.co/5QSaezZonz&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/neurosciences?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neurosciences&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/cerveau?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#cerveau&lt;/a&gt;üß† &lt;a href=&#34;https://t.co/t8ehsyoGt8&#34;&gt;pic.twitter.com/t8ehsyoGt8&lt;/a&gt;&lt;/p&gt;&amp;mdash; The Conversation France (@FR_Conversation) &lt;a href=&#34;https://twitter.com/FR_Conversation/status/1136989689867620353?ref_src=twsrc%5Etfw&#34;&gt;June 7, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Une version √©tendue est accessible sur le 
&lt;a href=&#34;https://laurentperrinet.github.io/2019-05_illusions-visuelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo GitHub&lt;/a&gt;, ainsi que les 
&lt;a href=&#34;https://github.com/laurentperrinet/2019-05_illusions-visuelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sources&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Should I stay or should I go? Humans adapt to the volatility of visual motion properties, and know about it</title>
      <link>https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/</link>
      <pubDate>Thu, 23 May 2019 01:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;See the final publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Pasturel, Montagnini and Perrinet (2020)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is part of the 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-05-23-neurofrance/&#34;&gt;Active Inference symposium&lt;/a&gt; @ 
&lt;a href=&#34;https://www.neurosciences.asso.fr/V2/colloques/SN19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroFrance&lt;/a&gt; SYMPOSIUM, Room 7
23.05.2019, 11:00 &amp;ndash; 13:00&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in french: Principes et psychophysique de l¬¥Inf√©rence Active dans l&amp;rsquo;estimation d&amp;rsquo;un biais dynamique et volatile de probabilit√©&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2019-05-20: Symposium on Active Inference at NeuroFrance 2019</title>
      <link>https://laurentperrinet.github.io/post/2019-05-23-neurofrance/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-05-23-neurofrance/</guid>
      <description>&lt;h2 id=&#34;active-inference-bridging-theoretical-and-experimental-neurosciences--inference-active-un-pont-entre-neurosciences-th√©oriques-et-exp√©rimentales&#34;&gt;Active Inference: Bridging theoretical and experimental neurosciences. / Inference Active: Un pont entre neurosciences th√©oriques et exp√©rimentales.&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.neurosciences.asso.fr/V2/colloques/SN19/index_en.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://neuro-marseille.org/wp-content/uploads/2018/07/capture-decran-2018-07-06-a-190423.png&#34; alt=&#34;Site NeuroFrance&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SYMPOSIUM S17&lt;/li&gt;
&lt;li&gt;When: 23.05.2019 11:00-13:00h&lt;/li&gt;
&lt;li&gt;When: Endoume 1+2&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;s171httpswwwprofessionalabstractscomnf2019iplannerpresentation1397-active-inference-and-brain-computer-interfaces--inf√©rence-active-et-interfaces-cerveau-machine&#34;&gt;
&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1397&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.1&lt;/a&gt; 	Active inference and Brain-Computer Interfaces / Inf√©rence active et interfaces cerveau-machine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mattout J. (Lyon, France), Mladenovic J. (Lyon, France), Frey J. (Bordeaux, France)3, Joffily M. (Lyon, France), Maby E. (Lyon, France), Lotte F. (Lyon, France)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Brain-Computer Interfaces (BCIs) devices bypass natural pathways to connect the brain with a machine, directly. They may rely on invasive or non-invasive measures of brain activity and applications cover a large domain, mostly but not restricted to clinical ones. A major objective is to restore communication and autonomy in heavily motor impaired patients.
However, no BCI has made its way to a routinely used clinical application yet. One lead for improvement is to endow the machine with learning abilities so that it can optimize its decisions and adapt to changes in the user signals over time1. Several approaches have been proposed but a generic framework is still lacking to foster the development of efficient adaptive BCIs2.
Initially proposed to model perception, learning and action by the brain, the Active Inference (AI) framework offers great promises in that aim3. It rests on an explicit generative model of the environment. In BCI, from the machine&amp;rsquo;s point of view, brain signals play the role of sensory inputs on which the machine&amp;rsquo;s perception of mental states will be based. Furthermore, the machine builds up decisions and trades between different actions such as: go on observing, deciding to decide, correcting its previous action or moving on.
In this talk, I will present an instantiation of AI in the context of the EEG-based P300-speller BCI for communication, showing it can flexibly combine complementary adaptive features pertaining to both perception and action, and yield significant improvements as shown on realistic simulations. We will discuss perspectives to further extend the current model and performance as well as the challenges ahead to implement this framework online.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Mattout, J. Brain-Computer Interfaces: A Neuroscience Paradigm of Social Interaction? A Matter of Perspective. Frontiers in Human Neuroscience 6, (2012).&lt;/li&gt;
&lt;li&gt;Mladenovic, J., Mattout, J. &amp;amp; Lotte, F. A Generic Framework for Adaptive EEG-Based BCI Training and Operation. in Brain-computer interfaces handbook: technological and theoretical advances (eds. Nam, C. S., Nijholt, A. &amp;amp; Lotte, F.) Chapter 31 (Taylor &amp;amp; Francis, CRC Press, 2018).&lt;/li&gt;
&lt;li&gt;Friston, K., Mattout, J. &amp;amp; Kilner, J. Action understanding and active inference. Biological Cybernetics 104, 137-160 (2011).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;s172httpswwwprofessionalabstractscomnf2019iplannerpresentation1398-comparing-active-inference-and-reinforcement-learning-models-of-a-go-nogo-task-and-their-relationships-to-striatal-dopamine-2-receptors-assessed-using-pet--comparaison-des-mod√®les-dinf√©rence-active-et-dapprentissage-par-renforcement-dans-une-t√¢che-go--nogo--relation-avec-les-r√©cepteurs-dopaminergiques-d2-striataux-√©valu√©s-par-tep&#34;&gt;
&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.2&lt;/a&gt; 	Comparing active inference and reinforcement learning models of a Go NoGo task and their relationships to striatal dopamine 2 receptors assessed using PET / Comparaison des mod√®les d&amp;rsquo;inf√©rence active et d&amp;rsquo;apprentissage par renforcement dans une t√¢che Go / NoGo : relation avec les r√©cepteurs dopaminergiques D2 striataux √©valu√©s par TEP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;R. Adams (London)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1398&#34;&gt;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1398&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Adaptive behaviour includes the ability to choose actions that result in advantageous outcomes. It is key to survival and a fundamental function of nervous systems. Active inference (AI) and reinforcement learning (RL) are two influential models of how the brain might achieve this. A key AI parameter is the precision of beliefs about policies. Precision controls the stochasticity of action selection - similar to decision temperature in RL - and is thought to be encoded by striatal dopamine. 75 healthy subjects performed a &amp;lsquo;go/no-go&amp;rsquo; task, and we measured striatal dopamine 2/3 receptor (D2/3R) availability in a subset of 25 using [11C]-(+)-PHNO positron emission tomography. In behavioural model comparison, RL performed best across the whole group but AI performed best in accurate subjects. D2/3R availability in the limbic striatum correlated with AI policy precision and also with RL irreducible decision &amp;lsquo;noise&amp;rsquo;. Limbic striatal D2/3R availability also correlated with AI Pavlovian prior beliefs - i.e. the respective probabilities of making or withholding actions in rewarding or loss-avoiding contexts - and the RL learning rate. These findings are consistent with the notion that occupancy of inhibitory striatal D2/3Rs controls the variability of action selection.&lt;/p&gt;
&lt;h3 id=&#34;s173httpswwwprofessionalabstractscomnf2019iplannerpresentation1399-principles-and-psychophysics-of-active-inference-in-anticipating-a-dynamic-switching-probabilistic-bias--principes-et-psychophysique-de-linf√©rence-active-dans-lestimation-dun-biais-dynamique-et-volatile-de-probabilit√©&#34;&gt;
&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1399&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.3&lt;/a&gt; 	Principles and psychophysics of active inference in anticipating a dynamic, switching probabilistic bias / Principes et psychophysique de l&amp;rsquo;inf√©rence active dans l¬¥estimation d&amp;rsquo;un biais dynamique et volatile de probabilit√©&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;L. Perrinet (Marseille)&lt;/li&gt;
&lt;li&gt;see more info on this 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;talk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;s174httpswwwprofessionalabstractscomnf2019iplannerpresentation1400-is-laziness-contagious-a-computational-approach-to-attitude-alignment--la-fain√©antise-est-elle-contagieuse-une-approche-computationnelle-de-lalignement-des-attitudes&#34;&gt;
&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.4&lt;/a&gt; 	Is laziness contagious? A computational approach to attitude alignment / La fain√©antise est-elle contagieuse? Une approche computationnelle de l¬¥alignement des attitudes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;J. Daunizeau (Paris)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What do people learn from observing others¬¥ attitudes, such as prudence, impatience or laziness? Rather than viewing these attitudes as examples of subjective and biologically entrenched personality traits, we assume that they derive from uncertain (and mostly implicit) beliefs about how to best weigh risks, delays and efforts in ensuing cost-benefit trade-offs. In this view, it is adaptive to update one¬¥s belief after having observed others¬¥ attitude, which provides valuable information regarding how to best behave in related difficult decision contexts. This is the starting point of our bayesian model of attitude alignment, which we derive in the light of recent neuroimaging findings. First, we disclose a few non-trivial predictions from this model. Second, we validate these predictions experimentally by profiling people¬¥s prudence, impatience and laziness both before and after guessing a series of cost-benefit arbitrages performed by calibrated artificial agents (which are impersonating human individuals). Third, we extend these findings and assess attitude alignment in autistic individuals. Finally, we discuss the relevance and implications of this work, with a particular emphasis on the assessment of biases of social cognition.&lt;/p&gt;
&lt;h3 id=&#34;s175httpswwwprofessionalabstractscomnf2019iplannerpresentation223-generative-bayesian-modeling-for-causal-inference-between-neural-activity-and-behavior-in-drosophila-larva&#34;&gt;
&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/223&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.5&lt;/a&gt; 	Generative Bayesian modeling for causal inference between neural activity and behavior in Drosophila larva&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;C. Barre (Paris) (TBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A fundamental property of the central nervous system is its ability to select appropriate behavioral patterns or sequences of behavioral patterns in response to sensory cues, but what are the biological mechanisms underlying decision making? The Drosophila larva is an ideal animal model for reverse-engineering the neural processes underlying behavior. The full connectome of the larva brain has been imaged at the individual-synapse level using electron microscopy.
The host of genetic techniques available for Drosophila allows us to optogenetically manipulate over 1,500 of its roughly 12,000 neurons individually in freely behaving larvae.
This enables us to establish causal relationships between neural activity, and behavior at the fundamental level of individual neurons and neural connections.
We have access to video record of the individual behavior of ~3,000,000 larvae. We have identified 6 stereotypical behavioral patterns using a combination of supervised and unsupervised machine learning. The behavioral identified for the larva: crawl, turn, stop, crawl backward, hunch (retract the head), and roll (lateral slide). Each realization of a behavioral pattern is characterized by a different duration, amplitude, and velocity.
Here we present a generative model that extracts the behavior of wildtype larvae using Bayesian inference, and interprets behavioral changes following neuron activation or inactivation from large-scale experimental screens. Fig. shows the average behavior of 10,000 larvae over time in a screen where a single neuron is activated at t=30s. A clear change in behavior is seen following activation is seen which is well captured by the model, illustrating its accuracy.
The generative model enables us to robustly detect behavioral modifications as significant deviations of the patterns in the larvae&amp;rsquo;s sequence of activities from their equilibrium behavior.&lt;/p&gt;
&lt;h3 id=&#34;neurofrance-marseille-capitale-des-neurosciences&#34;&gt;NeuroFrance: Marseille, capitale des neurosciences&lt;/h3&gt;
&lt;p&gt;Du  22 au 24 mai 2019 au Palais des congr√®s de Marseille (Parc Chanot), pr√®s de 1300 chercheurs, cliniciens et √©tudiants venus du monde entier partageront leurs travaux lors de NeuroFrance 2019, colloque international organis√© par la Soci√©t√© des Neurosciences.Au total, 8 conf√©rences pl√©ni√®res, 42 symposiums, 6 sessions sp√©cialis√©es, 525 communications affich√©es, ainsi qu‚Äôune exposition avec 42 entreprises et soci√©t√©s de biotechnologies, feront de ce colloque un moment exceptionnel pour mettre en lumi√®re les avanc√©es majeures scientifiques et technologiques sur le fonctionnement du cerveau. Vous pourrez aussi d√©couvrir le &amp;ldquo;Neurovillage&amp;rdquo; qui permettra de vous immerger au c≈ìur des innovations neuroscientifiques marseillaises, ainsi que  l‚Äôexposition  ¬´ L‚ÄôArt en t√™te ¬ª, compos√©e de cinq ≈ìuvres originales cr√©√©es par des artistes et des scientifiques. Plusieurs √©v√©nements seront √©galement propos√©s autour du colloque pour le grand public comme pour les chercheurs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Des illusions aux hallucinations visuelles: une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/talk/2019-04-18-jnlf/</link>
      <pubDate>Thu, 18 Apr 2019 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-04-18-jnlf/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Le texte de cette pr√©sentation est reprise dans cet article de 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; (
&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lien direct&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Voir la @ 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34;&gt;pr√©sentation au NeuroStories&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SpikeAI: laureat du D√©fi Biomim√©tisme (2019)</title>
      <link>https://laurentperrinet.github.io/grant/spikeai/</link>
      <pubDate>Mon, 15 Apr 2019 10:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/spikeai/</guid>
      <description>&lt;h1 id=&#34;description&#34;&gt;Description&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Le projet SpikeAI est laur√©at de l&#39;
&lt;a href=&#34;http://www.cnrs.fr/mi/spip.php?article1452&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;appel √† projets 2019 &lt;em&gt;Biomim√©tisme&lt;/em&gt;&lt;/a&gt; :&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The SpikeAI project targets analog computing for artificial intelligence in the form of Spiking Neural Networks (SNNs). Computer vision systems widely rely on artificial intelligence and especially neural network based machine learning, which recently gained huge visibility. The training stage for deep convolutional neural networks is time-consuming and yields enormous energy consumption. In contrast, the human brain has the ability to perform visual tasks with unrivaled computational and energy efficiency. It is believed that one major factor of this efficiency is the fact that information is vastly represented by short pulses (spikes) at analog ‚Äìnot discrete‚Äì times. However, computer vision algorithms using such representation still lack in practice, and its high potential is largely underexploited. Inspired from biology, the project addresses the scientific question of developing a low-power, end-to-end analog sensing and processing architecture. This will be applied on the particular context of a field programmable analog array (FPAA) applied to a stereovision system dedicated to coastal surveillance using an aerial robot of 3D visual scenes, running on analog devices, without a central clock and to validate them in real-life situations. The ambitious long-term vision of the project is to develop the next generation AI paradigm that will at term compete with deep learning. We believe that neuromorphic computing, mainly studied in EU countries, will be a key technology in the next decade. It is therefore both a scientific and strategic challenge for France and EU to foster this technological breakthrough. &lt;em&gt;This call will help kickstart collaboration within this European consortium to help leverage the chance to successfully apply to future large-scale grant proposals (e.g. ANR, CHIST-ERA, ERC).&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get 
&lt;a href=&#34;https://spikeai.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more information&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outcomes&#34;&gt;outcomes&lt;/h2&gt;
&lt;p&gt;The main goal was mainly to build a network of actors and to answer to relevant calls in the field of biomimetic research. We have communicated through the diffusion of computational frameworks and actions which are gathered online @ &lt;a href=&#34;https://github.com/SpikeAI&#34;&gt;https://github.com/SpikeAI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Summary of the actions taken:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We had a APROVIS3D FPP meeting April 23-24, 2019 in Lille with all partners. The call could support the travel of the 5 participants from outside Lille. During these two days, we had a first day to know each other better and a second day devoted to writing the grant proposal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With the help of this call, we could kickstart a collaboration within this European consortium which helped successfully achieve a large-scale grant proposal (CHIST-ERA : &lt;a href=&#34;https://laurentperrinet.github.io/grant/aprovis-3-d/&#34;&gt;https://laurentperrinet.github.io/grant/aprovis-3-d/&lt;/a&gt; ).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We organized a tutorial on deep learning during the GDR Vision, see &lt;a href=&#34;https://github.com/SpikeAI/2019-10-10_ML-tutorial&#34;&gt;https://github.com/SpikeAI/2019-10-10_ML-tutorial&lt;/a&gt; and &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/&#34;&gt;https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We could invite a major actor of the modeling of biomimetic computations, Ryad Benosman, to the GDR vision meeting. The call could support his travel and accommodation and was acknowledged in all communications (see &lt;a href=&#34;https://gdrvision2019.sciencesconf.org/resource/page/id/6)&#34;&gt;https://gdrvision2019.sciencesconf.org/resource/page/id/6)&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Should I stay or should I go? Adaption of human observers to the volatility of visual inputs</title>
      <link>https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/</link>
      <pubDate>Fri, 05 Apr 2019 15:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Pasturel, Montagnini and Perrinet (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>From the retina to action: Understanding visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2019-04-03-a-course-on-vision-and-modelization/</link>
      <pubDate>Wed, 03 Apr 2019 16:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-04-03-a-course-on-vision-and-modelization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>From the retina to action: Predictive processing in the visual system</title>
      <link>https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/</link>
      <pubDate>Mon, 25 Mar 2019 14:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;check-out our preprint on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34;&gt;SDPC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey V1</title>
      <link>https://laurentperrinet.github.io/publication/chemla-19/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/chemla-19/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</title>
      <link>https://laurentperrinet.github.io/publication/boutin-20-sigma/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-20-sigma/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;presented during this 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/&#34;&gt;talk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli</title>
      <link>https://laurentperrinet.github.io/publication/ravello-19/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ravello-19/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www4.cnrs-dir.fr/insb/recherche/parutions/articles2019/l-perrinet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;d√®s-la-r√©tine-le-syst√®me-visuel-pr√©f√®re-des-images-naturelles&#34;&gt;D√®s la r√©tine, le syst√®me visuel pr√©f√®re des images naturelles&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Dans la r√©tine, au premier √©tage du traitement de l&amp;rsquo;image visuelle, on peut obtenir des repr√©sentations extr√™mement fines. Une collaboration entre des chercheurs fran√ßais et chiliens a permis de mettre en √©vidence que, dans la r√©tine de rongeurs, une repr√©sentation de la vitesse de l&amp;rsquo;image visuelle est pr√©cis√©ment cod√©e. Dans cette collaboration pluridisciplinaire, l&amp;rsquo;utilisation d&amp;rsquo;un mod√®le du fonctionnement de la r√©tine a permis de g√©n√©rer un nouveau type de stimuli visuels qui a r√©v√©l√© des r√©sultats exp√©rimentaux surprenants.&lt;/em&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Travail collaboratif et multi-disciplinaire entre &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar et &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; librement disponible sur &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; - merci √†  l&amp;#39;&lt;a href=&#34;https://twitter.com/AgenceRecherche?ref_src=twsrc%5Etfw&#34;&gt;@AgenceRecherche&lt;/a&gt; pour l&amp;#39;aide financi√®re et √† &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; + &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; pour l&amp;#39;&lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://t.co/YixRfpCrT3&#34;&gt;https://t.co/YixRfpCrT3&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1092139540788244480?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

La r√©tine est la premi√®re √©tape du traitement visuel, aux capacit√©s √©tonnantes. √Ä la diff√©rence d&amp;rsquo;un simple capteur comme ceux qu‚Äôon trouve dans les appareils photographiques num√©riques, ce mince tissu neuronal est un syst√®me complexe et encore largement m√©connu. Une meilleure connaissance de cette structure est essentielle pour la construction de capteurs du futur efficaces et √©conomes -par exemple ceux qui √©quiperont les futures voitures autonomes- mais aussi pour mieux comprendre des pathologies comme la D√©ficience Maculaire Li√©e √† l&amp;rsquo;Age (DMLA). Une des facettes m√©connues de la r√©tine est sa capacit√© √† d√©tecter des mouvements et cet article permet de mieux comprendre une partie des m√©canismes en jeu.
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New study on speed selectivity in the &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; showing that a majority of neurons prefer natural-like stimuli. Collaborative and multi-disciplinary work with &lt;a href=&#34;https://twitter.com/cesarravello?ref_src=twsrc%5Etfw&#34;&gt;@cesarravello&lt;/a&gt; &lt;a href=&#34;https://twitter.com/laurentperrinet?ref_src=twsrc%5Etfw&#34;&gt;@laurentperrinet&lt;/a&gt; Mar√≠a Jos√© Escobar and &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; available with &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; at &lt;a href=&#34;https://t.co/Vb7GoRxjoT&#34;&gt;https://t.co/Vb7GoRxjoT&lt;/a&gt;&lt;/p&gt;&amp;mdash; Adrian Palacios (@APalacio_s) &lt;a href=&#34;https://twitter.com/APalacio_s/status/1092200890377879552?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Retinal cell preference for natural-like stimuli. Very elegant work by &lt;a href=&#34;https://twitter.com/APalacio_s?ref_src=twsrc%5Etfw&#34;&gt;@APalacio_s&lt;/a&gt; et al. &lt;a href=&#34;https://twitter.com/hashtag/retina?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#retina&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuroscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuroscience&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/decoding?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#decoding&lt;/a&gt; &lt;a href=&#34;https://t.co/3xNWaZd5x6&#34;&gt;https://t.co/3xNWaZd5x6&lt;/a&gt;&lt;/p&gt;&amp;mdash; Andres Canales-Johnson (@canalesjohnson) &lt;a href=&#34;https://twitter.com/canalesjohnson/status/1092211339311923201?ref_src=twsrc%5Etfw&#34;&gt;February 4, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Conciliant mod√©lisation et neurophysiologie, cette √©tude a permis de faire des pr√©dictions sur le traitement de l&amp;rsquo;information r√©tinienne et en particulier de g√©n√©rer des textures synth√©tiques qui sont optimales pour ces mod√®les (voir film). Les enregistrements effectu√©s sur la r√©tine de rongeurs diurnes Octodon degus ont ensuite permis de mesurer la s√©lectivit√© √† la vitesse mais aussi de valider une nouvelle fois ces mod√®les en reconstruisant l&amp;rsquo;image d&amp;rsquo;entr√©e √† partir de l&amp;rsquo;activit√© neurale.
Le r√©sultat le plus inattendu est la diff√©rence de s√©lectivit√© de certaines classes de neurones r√©tiniens par rapport √† la complexit√© du stimulus pr√©sent√©. En effet, la repr√©sentation de la vitesse est relativement peu pr√©cise si on utilise des r√©seaux de lignes (&amp;ldquo;Grating&amp;rdquo;), comme cela est d&amp;rsquo;habitude r√©alis√© dans la plupart des exp√©riences neurophysiologiques. Au contraire, elle devient plus pr√©cise si on utilise comme signaux visuels des textures artificielles ressemblant √† des nuages en mouvement (&amp;ldquo;MC Narrow&amp;rdquo;). En particulier, plus cette texture est complexe, plus la repr√©sentation est pr√©cise (&amp;ldquo;MC Broad&amp;rdquo;).
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/R%C3%A9sultatScientifique?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#R√©sultatScientifique&lt;/a&gt; üîç| D√®s la &lt;a href=&#34;https://twitter.com/hashtag/r%C3%A9tine?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#r√©tine&lt;/a&gt;, le syst√®me &lt;a href=&#34;https://twitter.com/hashtag/visuel?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#visuel&lt;/a&gt; pr√©f√®re des images naturelles&lt;br&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://t.co/BBY2IpGum6&#34;&gt;https://t.co/BBY2IpGum6&lt;/a&gt;&lt;br&gt;üìï &lt;a href=&#34;https://twitter.com/SciReports?ref_src=twsrc%5Etfw&#34;&gt;@SciReports&lt;/a&gt; | &lt;a href=&#34;https://t.co/5mULuWTp3N&#34;&gt;https://t.co/5mULuWTp3N&lt;/a&gt;&lt;br&gt;ü§ù &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/InstitutDeNeurosciencesDeLaTimone?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#InstitutDeNeurosciencesDeLaTimone&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/LaurentPerrinet?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#LaurentPerrinet&lt;/a&gt; &lt;a href=&#34;https://t.co/34R1URHUic&#34;&gt;pic.twitter.com/34R1URHUic&lt;/a&gt;&lt;/p&gt;&amp;mdash; Biologie au CNRS (@INSB_CNRS) &lt;a href=&#34;https://twitter.com/INSB_CNRS/status/1091392027848294401?ref_src=twsrc%5Etfw&#34;&gt;February 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli. Beautiful article in Scientific Reports on an original animal model, the diurnal rodent Octodon degus. &lt;a href=&#34;https://t.co/BdzyzEVYnX&#34;&gt;https://t.co/BdzyzEVYnX&lt;/a&gt; (open access) &lt;a href=&#34;https://t.co/1UaoMYTFd2&#34;&gt;pic.twitter.com/1UaoMYTFd2&lt;/a&gt;&lt;/p&gt;&amp;mdash; St√©phane Deny (@StephaneDeny) &lt;a href=&#34;https://twitter.com/StephaneDeny/status/1090452532223045632?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Ces textures complexes sont plus proches des images naturellement observ√©es et ces r√©sultats montrent donc que d√®s la r√©tine, le syst√®me visuel est particuli√®rement adapt√© √† des stimulations naturelles. Ce r√©sultat devrait pouvoir s&amp;rsquo;√©tendre √† des textures encore plus complexes et encore plus proches d&amp;rsquo;images naturelles, mais aussi pouvoir se g√©n√©raliser √† d&amp;rsquo;autres aires visuelles plus complexes, comme le cortex visuel primaire, et √† d&amp;rsquo;autres esp√®ces.






  



  
  











&lt;figure id=&#34;figure-pour-une-cellule-repr√©sentative-on-montre-ici-la-r√©ponse-au-cours-du-temps-sous-forme-dimpulsions-pour-diff√©rentes-pr√©sentations-trial-ainsi-que-la-moyenne-de-cette-r√©ponse-firing-rate-les-diff√©rentes-colonnes-repr√©sentent-diff√©rentes-vitesses-des-stimulations-sur-la-r√©tine-les-diff√©rentes-lignes-sont-diff√©rentes-stimulations-en-bleu-une-stimulation-classique-sous-forme-de-r√©seaux-de-lignes--grating--en-vert-et-orange-la-r√©ponse-√†-une-texture-progressivement-plus-complexe-de--mc-narrow--√†--mc-broad--si-les-r√©ponses-aux-diff√©rents-stimulations-sont-en-moyenne-similaires-elles-sont-variables-dessai-en-essai-et-une-analyse-statistique-a-permis-de-montrer-que-dans-la-majorit√©-des-cellules-les-r√©ponses-sont-dautant-plus-pr√©cises-que-la-stimulation-est-complexe--cesar-ravello&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Pour une cellule repr√©sentative, on montre ici la r√©ponse au cours du temps sous forme d&amp;amp;rsquo;impulsions pour diff√©rentes pr√©sentations (Trial) ainsi que la moyenne de cette r√©ponse (Firing rate). Les diff√©rentes colonnes repr√©sentent diff√©rentes vitesses des stimulations sur la r√©tine. Les diff√©rentes lignes sont diff√©rentes stimulations. En bleu, une stimulation classique sous forme de r√©seaux de lignes (¬´ Grating ¬ª). En vert et Orange, la r√©ponse √† une texture progressivement plus complexe (de ¬´ Mc Narrow ¬ª √† ¬´ MC Broad ¬ª). Si les r√©ponses aux diff√©rents stimulations sont en moyenne similaires, elles sont variables d‚Äôessai en essai et une analyse statistique a permis de montrer que dans la majorit√© des cellules, les r√©ponses sont d&amp;amp;rsquo;autant plus pr√©cises que la stimulation est complexe. ¬© Cesar Ravello&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;540&#34; height=&#34;416&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Pour une cellule repr√©sentative, on montre ici la r√©ponse au cours du temps sous forme d&amp;rsquo;impulsions pour diff√©rentes pr√©sentations (Trial) ainsi que la moyenne de cette r√©ponse (Firing rate). Les diff√©rentes colonnes repr√©sentent diff√©rentes vitesses des stimulations sur la r√©tine. Les diff√©rentes lignes sont diff√©rentes stimulations. En bleu, une stimulation classique sous forme de r√©seaux de lignes (¬´ Grating ¬ª). En vert et Orange, la r√©ponse √† une texture progressivement plus complexe (de ¬´ Mc Narrow ¬ª √† ¬´ MC Broad ¬ª). Si les r√©ponses aux diff√©rents stimulations sont en moyenne similaires, elles sont variables d‚Äôessai en essai et une analyse statistique a permis de montrer que dans la majorit√© des cellules, les r√©ponses sont d&amp;rsquo;autant plus pr√©cises que la stimulation est complexe. ¬© Cesar Ravello
  &lt;/figcaption&gt;


&lt;/figure&gt;















  


&lt;video controls &gt;
  &lt;source src=&#34;video_perrinet.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
Cette vid√©o montre les trois classes de stimulations utilis√©es dans cette √©tude. En plus des r√©seaux sinuso√Ødaux (‚ÄúGrating‚Äù) qui sont classiquement utilis√©s en neurosciences, cette √©tude a utilis√© des textures al√©atoires (Motion Clouds (MC)) qui sont inspir√©es de mod√®les du traitement visuel. Ils permettent en particulier de manipuler des param√®tres visuels critiques comme la vari√©t√© de fr√©quences spatiales qui sont superpos√©es: soit unique (‚ÄúGrating‚Äù), fine (‚ÄúMC Narrow‚Äù), soit plus large (‚ÄúMC Broad‚Äù). Ces vid√©os ont √©t√© directement projet√©es sur des r√©tines pos√©es sur des grilles d‚Äô√©lectrodes qui permettent de mesurer l‚Äôactivit√© neurale (voir figure). ¬© Laurent Perrinet / Cesar Ravello&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Should I stay or should I go? Adaption of human observers to the volatility of visual inputs</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-18-laconeu/</link>
      <pubDate>Fri, 18 Jan 2019 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-18-laconeu/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Pasturel, Montagnini and Perrinet (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Role of dynamics in neural computations underlying visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-17-laconeu/</link>
      <pubDate>Thu, 17 Jan 2019 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-17-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient coding of visual information in neural computations</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-16-laconeu/</link>
      <pubDate>Wed, 16 Jan 2019 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-16-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling spiking neural networks using Brian, Nest and pyNN</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-14-laconeu/</link>
      <pubDate>Mon, 14 Jan 2019 11:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-14-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rencontre avec les coll√©giens marseillais</title>
      <link>https://laurentperrinet.github.io/talk/2019-01-10-polly-maggoo/</link>
      <pubDate>Thu, 10 Jan 2019 09:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-01-10-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;cin√©ma-et-sciences--rencontre-avec-les-coll√©giens-marseillais&#34;&gt;Cin√©ma et sciences : rencontre avec les coll√©giens marseillais&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; met en place
tout le long de l‚Äôann√©e, des actions de culture scientifique et
artistique en direction du grand public et des lyc√©es, au cours
desquelles l&amp;rsquo;association programme des films √† caract√®re scientifique.
Les projections se d√©roulent en pr√©sence de chercheurs et/ou de
cin√©astes dans la perspective d‚Äôun d√©veloppement de la culture
cin√©matographique et scientifique en direction des publics scolaires.
Le jeudi 10 janvier 2019, je suis venu √©changer au c√¥t√© de Serge Dentin
autour de films traitant du rapport fiction/r√©el, des illusion visuelles
(&amp;quot; Qu‚Äôest ce qu‚Äôune image? &amp;ldquo;), des rapports d‚Äô√©chelles, de la
perception, &amp;hellip; et qui sont projet√©s lors de la s√©ance, avec les √©l√®ves
de deux classes de 4√®me. Une occasion aussi de parler du m√©tier de
chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
10 janvier 2019&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
coll√®ge Andr√© Malraux, Marseille&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&amp;ldquo;LAZARUS MIRAGES : T√âL√âPATHIE √Ä L&amp;rsquo;UNIVERSIT√â DE SHANGAI&amp;rdquo; de Patric
JEAN et Henry BROCH (France, 2012, documentaire, 3&#39;21)
/&amp;ldquo;CARLITOPOLIS&amp;rdquo; / / &amp;ldquo;BIG DATA, BIG BUSINESS&amp;rdquo; / &amp;ldquo;
&lt;a href=&#34;https://www.youtube.com/watch?v=RVeHxUVkW4w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Centrifuge
Brain Project, A Short Film by Till
Nowak&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A hierarchical, multi-layer convolutional sparse coding algorithm based on predictive coding</title>
      <link>https://laurentperrinet.github.io/publication/franciosini-perrinet-19-neurofrance/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/franciosini-perrinet-19-neurofrance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An adaptive homeostatic algorithm for the unsupervised learning of visual features</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-19-hulk/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-19-hulk/</guid>
      <description>&lt;h1 id=&#34;an-adaptive-algorithm-for-unsupervised-learning&#34;&gt;&amp;ldquo;An adaptive algorithm for unsupervised learning&amp;rdquo;&lt;/h1&gt;














  


&lt;video controls &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2019-09-11_Perrinet19.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;ul&gt;
&lt;li&gt;supplementary info : &lt;a href=&#34;https://spikeai.github.io/HULK/&#34;&gt;https://spikeai.github.io/HULK/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mdpi.com/2411-5150/3/3/47&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mdpi.com/2411-5150/3/3/47/htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mdpi.com/2411-5150/3/3/47/pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for paper: &lt;a href=&#34;https://github.com/SpikeAI/HULK&#34;&gt;https://github.com/SpikeAI/HULK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for framework: &lt;a href=&#34;https://github.com/bicv/SparseHebbianLearning/&#34;&gt;https://github.com/bicv/SparseHebbianLearning/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code for figures &lt;a href=&#34;https://github.com/SpikeAI/HULK/blob/master/Annex.ipynb&#34;&gt;https://github.com/SpikeAI/HULK/blob/master/Annex.ipynb&lt;/a&gt; (which is rendered @ &lt;a href=&#34;https://spikeai.github.io/HULK/&#34;&gt;https://spikeai.github.io/HULK/&lt;/a&gt; )&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2019-09-11_Perrinet19.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video abstract&lt;/a&gt; (and the 
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2019-09-11_video-abstract-vision.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; for generating it)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Illusions et hallucinations visuelles : une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-19-illusions/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-19-illusions/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ce texte est disponible dans cet article de 
&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Voir la @ 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34;&gt;pr√©sentation au NeuroStories&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modelling Complex Cells of Early Visual Cortex using Predictive Coding</title>
      <link>https://laurentperrinet.github.io/publication/franciosini-perrinet-19-cns/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/franciosini-perrinet-19-cns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Orientation selectivity to synthetic natural patterns in a cortical-like model of the cat primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/ladret-19-sfn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ladret-19-sfn/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Interested in orientation selectivity in V1?  at &lt;a href=&#34;https://twitter.com/hashtag/sfn2019?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#sfn2019&lt;/a&gt; ? &lt;br&gt;We tested a model getting different precision levels and then tested these predictions in real neurons ! Check out poster 403.16 / P20 @ &lt;a href=&#34;https://t.co/iHUv0AHuzl&#34;&gt;https://t.co/iHUv0AHuzl&lt;/a&gt; &lt;br&gt;-&amp;gt; more info :&lt;a href=&#34;https://t.co/JkXXgC5IVp&#34;&gt;https://t.co/JkXXgC5IVp&lt;/a&gt;&lt;br&gt;ü§ù &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://t.co/MVBz0UGH70&#34;&gt;pic.twitter.com/MVBz0UGH70&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1186513282326257665?ref_src=twsrc%5Etfw&#34;&gt;October 22, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/ladret-20-aes/&#34;&gt;Ladret and Perrinet, 2020&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Deep Predictive Coding to model visual object recognition</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;If you‚Äôre at &lt;a href=&#34;https://twitter.com/hashtag/sfn2019?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#sfn2019&lt;/a&gt; and have an interest in &lt;a href=&#34;https://twitter.com/hashtag/sparse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#sparse&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/deep?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#deep&lt;/a&gt; Predictive Coding, checkout &lt;a href=&#34;https://twitter.com/VictorBoutin?ref_src=twsrc%5Etfw&#34;&gt;@VictorBoutin&lt;/a&gt; ‚Äòs poster 403.16 / P20:&lt;a href=&#34;https://t.co/2VLEsl98oU&#34;&gt;https://t.co/2VLEsl98oU&lt;/a&gt;&lt;br&gt;&lt;br&gt;It shows today + comes with a (timely) preprint &lt;a href=&#34;https://t.co/FfKi9tjqrN&#34;&gt;https://t.co/FfKi9tjqrN&lt;/a&gt; !&lt;br&gt; &lt;a href=&#34;https://twitter.com/hashtag/SfN19?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#SfN19&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/MachineLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#MachineLearning&lt;/a&gt; &lt;a href=&#34;https://t.co/ep0RrPjzzZ&#34;&gt;pic.twitter.com/ep0RrPjzzZ&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1186196186170044421?ref_src=twsrc%5Etfw&#34;&gt;October 21, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>Temps et cerveau : comment notre perception nous fait voyager dans le temps</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-19-temps/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-19-temps/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Un article dans 
&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; dont l&amp;rsquo;objectif est d&amp;rsquo;√™tre accessible et r√©utilisable (dans des cours d&amp;rsquo;introduction aux neurosciences, sciences cognitives, vision, r√©seaux de neurones, intelligence artificielle).&lt;/li&gt;
&lt;li&gt;Le flash-lag effect original:














  


&lt;video autoplay loop &gt;
  &lt;source src=&#34;flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/li&gt;
&lt;li&gt;la m√™me chose avec un arr√™t:














  


&lt;video autoplay loop &gt;
  &lt;source src=&#34;flash_lag_stop.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/li&gt;
&lt;li&gt;pour illustrer la fleche du temps (&amp;quot; Or dans tout syst√®me, d‚Äôapr√®s le second principe de la thermodynamique, le d√©sordre mesur√© par l‚Äôentropie se doit d‚Äôaugmenter. Voil√† pourquoi il existe une asym√©trie dans l‚Äô√©coulement du temps, c‚Äôest-√†-dire une fl√®che du temps. R√©sultat, si l‚Äôon filme une partie de billard, on trouvera incongru cette s√©quence si on la projette dans le sens inverse du temps. &amp;ldquo;), on peut aussi utiliser cette video d&amp;rsquo;un bocal qui se brise qu&amp;rsquo;il est ais√© de lire dans le sens inverse du temps:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/v30b5IAgwQw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2019-10-07-neurostories-videos-of-my-talk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurostories: d&amp;rsquo;autres videos du flash-lag effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Laurent Perrinet a re√ßu des financements de l&amp;rsquo;Agence Nationale de la Recherche (ANR HOR-V1 ANR-17-CE37-0006) et du CNRS (SpikeAI). Cet article n‚Äôaurait pas vu le jour sans la journ√©e des 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-07_neurostories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurostories&lt;/a&gt; de la NeuroSchool d‚ÄôAix-Marseille Universit√©, ceux qui l‚Äôont fait vivre et parmi eux: Fran√ßois F√©ron, Alexia Belleville, 
&lt;a href=&#34;https://fr.wikipedia.org/wiki/Jean-Marc_Michelangeli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jean-Marc Michelangeli&lt;/a&gt;, Camille Grasso, Daniele Sch√∂n, Anne-Marie Fran√ßois-Bellan, Jennifer Coull, Corine Sombrun et Francis Taulelle.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Top-down connection in Hierarchical Sparse Coding</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-19-gdr-robotics/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-19-gdr-robotics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures</title>
      <link>https://laurentperrinet.github.io/publication/vacher-16/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2018-11-09 : Retinal computations</title>
      <link>https://laurentperrinet.github.io/post/2018-11-09_seminaire-escobar/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-11-09_seminaire-escobar/</guid>
      <description>&lt;h1 id=&#34;2018-11-09--retinal-computations-by-maria-jos√©-escobar-chile&#34;&gt;2018-11-09 : &amp;ldquo;Retinal computations&amp;rdquo; by Maria Jos√© Escobar (Chile)&lt;/h1&gt;
&lt;p&gt;During a seminar at the Institute of Neurosciences Timone in Marseille, 
&lt;a href=&#34;http://profesores.elo.utfsm.cl/~mjescobar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mar√≠a Jos√© Escobar, Ph.D.&lt;/a&gt; :&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Retinal computations&amp;rdquo; : The retina is part of the nervous system and consists in well-organized layers of different cell types and functions. Those cells have been vastly studied in various animal models, and also the circuits conveying to different functional categories. All these different types of either physiological properties or computation equivalents revealed the retina as not a single light to electricity encoder but a pre-processing layer, which is in charge to extract relevant visual signals from the environment that are critical for animal survival. During this seminar, we describe some of the computations performed by the retina, and how this knowledge can be applied to solve engineering problems, such as image processing and robot controllers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;http://www.int.univ-amu.fr/IMG/200x130xsiteon0.png,q1331299836.pagespeed.ic.IKYGzK4Zu8.png&#34; alt=&#34;Sponsored by&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>La mod√©lisation biomorphique de la perception visuelle</title>
      <link>https://laurentperrinet.github.io/talk/2018-10-11-bio-morphisme/</link>
      <pubDate>Thu, 11 Oct 2018 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-10-11-bio-morphisme/</guid>
      <description>&lt;h1 id=&#34;la-mod√©lisation-biomorphique-de-la-perception-visuelle&#34;&gt;La mod√©lisation biomorphique de la perception visuelle&lt;/h1&gt;
&lt;h2 id=&#34;in-la-mod√©lisation-de-la-gen√®se-physico-math√©matique-du-vivant&#34;&gt;in &amp;ldquo;La mod√©lisation de la gen√®se physico-math√©matique du vivant&amp;rdquo;&lt;/h2&gt;
&lt;h2 id=&#34;biomorphisme-et-creation-artistique-session-3&#34;&gt;BIOMORPHISME ET CREATION ARTISTIQUE¬†‚Äì Session 3&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
11 Octobre 2018&lt;/li&gt;
&lt;li&gt;Atelier&lt;br&gt;
S√©minaire/workshop organis√© dans le cadre du projet Biomorphisme.
Approches sensibles et conceptuelles des formes du vivant
&lt;a href=&#34;http://lesa.univ-amu.fr/?q=node/391&#34;&gt;http://lesa.univ-amu.fr/?q=node/391&lt;/a&gt; &lt;a href=&#34;http://centregranger.cnrs.fr&#34;&gt;http://centregranger.cnrs.fr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
B√¢timent Egger, dans la salle E 215 (2√®me √©tage c√¥t√© voie ferr√©e) -
3 avenue R. Schuman - Aix-en-Provence&lt;/li&gt;
&lt;li&gt;Visuels&lt;br&gt;

&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2018-10-11_BioMorphisme.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Organisation&lt;br&gt;
Jean Arnaud, PR arts plastiques au LESA-AMU¬†; Julien Bernard, MCF
philosophe des sciences au Centre GG Granger-AMU¬†; Sylvie Pic,
artiste&lt;/li&gt;
&lt;li&gt;R√©sum√©&lt;br&gt;
La vision utilise un faisceau d&amp;rsquo;informations de diff√©rentes qualit√©s
pour atteindre une perception unifi√©e du monde environnant. Elle
interagit avec lui en cr√©ant son propre mod√®le g√©n√©ratif de sa
structure physico-math√©matique. Avec 
&lt;a href=&#34;https://laurentperrinet.github.io/LaurentPerrinet/EtienneRey&#34;&gt;Etienne
Rey&lt;/a&gt; de l&amp;rsquo;atelier Ondes Parall√®les,
nous avons utilis√© lors de plusieurs projets art-science (voir
&lt;a href=&#34;https://github.com/NaturalPatterns&#34;&gt;https://github.com/NaturalPatterns&lt;/a&gt;) des installations permettant
de manipuler explicitement des composantes de ce flux d&amp;rsquo;information
et de r√©v√©ler des ambiguit√©s dans notre perception. Dans
l&amp;rsquo;installation

&lt;a href=&#34;https://github.com/NaturalPatterns/Tropique&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique&lt;/a&gt;, des
faisceaux de lames lumineuses sont arrang√©s dans l&amp;rsquo;espace assombri
de l&amp;rsquo;installation. Les spectateurs les observent gr√¢ce √† leur
interaction avec une brume invisible qui est diffus√©e dans l&amp;rsquo;espace.
L&amp;rsquo;ensemble des faisceaux √©volue comme autant de lames lumineuses √†
partir de 6 video-projecteurs plac√©s dans l&amp;rsquo;espace de
l&amp;rsquo;installation, suivant une dynamique autonome. En m√™me temps, la
position des spectateurs est capt√©e et permet d&amp;rsquo;alterner entre une
vision de ces sculptures d&amp;rsquo;un point de vue introceptif √† un point de
vue exteroceptif. Dans ¬´
&lt;a href=&#34;https://github.com/NaturalPatterns/elasticite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trame
√âlasticit√©&lt;/a&gt;¬ª, 25
parall√©l√©pip√®des de miroirs (3m de haut) sont arrang√©s verticalement
sur une ligne horizontale. Ces lames sont rotatives et leurs
mouvements est synchronis√©. Suivant la dyamique qui est impos√© √† ces
lames, la perception de l‚Äôespace environnent fluctue conduisant √†
recomposer l‚Äôespace de la concentration √† l‚Äôexpansion, ou encore √†
g√©n√©rer un surface semblant transparente ou inverser la visons de
ce qui est situ√©e devant et derri√®re l‚Äôobservateur. Enfin, dans
¬´
&lt;a href=&#34;https://github.com/NaturalPatterns/TRAMES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Trames&lt;/a&gt;¬ª, nous
explorons l&amp;rsquo;interaction de s√©ries p√©riodiques de points plac√©es sur
des surfaces transparentes. √Ä partir de premi√®res exp√©rimentations
utilisant une technique novatrice de s√©rigraphie, ces trames de
points sont plac√©es afin de faire √©merger des structures selon le
point de vue du spectateur. Ce qui est en jeu ici c‚Äôest l‚Äô√©mergence
de l‚Äôapparition de motifs virtuels r√©sultat de la relation entre une
r√©alit√© physique, la grandeur et l‚Äôordonnancement de trames et notre
physiologie qui conduit √† cette √©tat de perception. Lorsqu‚Äôon est
fasse √† ces motifs ce qui saute au yeux plus que le motif r√©el c‚Äôest
sa r√©sultante, instable et √©ph√©m√®re qui fait apparaitre une richesse
de figures g√©om√©triques qui se transforment et √©voluent en fonction
du temps d‚Äôobservation et du point de vue. Sur ce principe de
dispositif optique, le travail de chacun des motifs, li√© √† un
s√©quen√ßage de trames conduit √† faire apparaitre une composition et
des √©mergences de formes sp√©cifiques. L‚Äôexp√©rience de perception de
chacun des motifs explore les notions d‚Äôinstabilit√©, de flux,
d‚Äô√©mergences ‚Ä¶ dont l‚Äôexp√©rience donne √† entrevoir des formes que
l‚Äôon retrouve dans la nature ou les ph√©nom√®nes naturels: le dessin
du pelage d‚Äôun z√®bre, une accumulation de bulles de savons, ou plus
g√©n√©ralement dans les compositions chimiques issue de la th√©orie de
la morphog√©n√®se de Turing. De mani√®re g√©n√©rale, nous montrerons ici
les diff√©rentes m√©thodes utilis√©es, comme l&amp;rsquo;utilisation des limites
perceptives, et aussi les r√©sultats apport√©s par une telle
collaboration.&lt;/li&gt;
&lt;li&gt;Mots-Cl√©s&lt;br&gt;
art cin√©tique¬†;¬†science¬†;¬†vision¬†;¬†perception¬†;¬†mod√®le interne&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Intervention f√™te de la science 2018</title>
      <link>https://laurentperrinet.github.io/talk/2018-10-10-polly-maggoo/</link>
      <pubDate>Wed, 10 Oct 2018 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-10-10-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;f√™te-de-la-science-2018--alcazar--merlan&#34;&gt;F√äTE DE LA SCIENCE 2018 : Alcazar / MERLAN&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; met en place
tout le long de l‚Äôann√©e, des actions de culture scientifique et
artistique en direction du grand public et des lyc√©es, au cours
desquelles l&amp;rsquo;association programme des films √† caract√®re scientifique.
Les projections se d√©roulent en pr√©sence de chercheurs et/ou de
cin√©astes dans la perspective d‚Äôun d√©veloppement de la culture
cin√©matographique et scientifique en direction des publics scolaires.
Le samedi 6 octobre et le mercredi 10 octobre, je suis venu √©changer au
c√¥t√© de Serge Dentin autour de films traitant du rapport fiction/r√©el,
des illusion visuelles (&amp;quot; Qu‚Äôest ce qu‚Äôune image? &amp;ldquo;), des rapports
d‚Äô√©chelles, de la perception, &amp;hellip; et qui sont projet√©s lors de la
s√©ance, avec tout public (samedi) ou des √©l√®ves de lyc√©e (mercredi).
Une occasion aussi de parler du m√©tier de chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
6 octobre 2018&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
biblioth√®que de l&amp;rsquo;Alcazar (BMVR), Marseille&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&lt;em&gt;SAMSUNG GALAXY&lt;/em&gt; de Romain CHAMPALAUNE (France, 2015),
documentaire-fiction, 7‚Ä≤ / &lt;em&gt;LA DR√îLE DE GUERRE D‚ÄôALAN TURING&lt;/em&gt; de
Denis VAN WAEREBEKE (France, 2014), documentaire, 60‚Äô&lt;/li&gt;
&lt;li&gt;URL&lt;br&gt;
&lt;a href=&#34;http://pollymaggoo.org/fete-de-la-science-2018-alcazar-bmvr/&#34;&gt;http://pollymaggoo.org/fete-de-la-science-2018-alcazar-bmvr/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Date&lt;br&gt;
10 octobre 2018&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
biblioth√®que du Merlan, Marseille&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&lt;em&gt;SAMSUNG GALAXY&lt;/em&gt; de Romain CHAMPALAUNE (France, 2015),
documentaire-fiction, 7‚Ä≤ / &amp;ldquo;JE TE SUIS (JAG F√ñLJER DIG)&amp;rdquo; / &amp;ldquo;OS
Love_EN&amp;rdquo; / &amp;ldquo;BIG DATA, BIG BUSINESS&amp;rdquo; / COPIER-CLONER / et en bonus
&amp;ldquo;
&lt;a href=&#34;https://www.youtube.com/watch?v=RVeHxUVkW4w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Centrifuge Brain Project, A Short Film by Till
Nowak&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement effects in anticipatory smooth eye movements</title>
      <link>https://laurentperrinet.github.io/publication/damasse-18/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>https://laurentperrinet.github.io/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/tutorial/example/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Principles and psychophysics of Active Inference in anticipating a dynamic, switching probabilistic bias</title>
      <link>https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/</link>
      <pubDate>Thu, 05 Apr 2018 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Pasturel, Montagnini and Perrinet (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2018-04-05 : *Probabilities and Optimal Inference to understand the Brain* Workshop</title>
      <link>https://laurentperrinet.github.io/post/2018-04-05_optimal-inference-brain-workshop/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-04-05_optimal-inference-brain-workshop/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;IMG_20180406_164630.jpg&#34; alt=&#34;participants&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;probabilities-and-optimal-inference-to-understand-the-brain&#34;&gt;Probabilities and Optimal Inference to understand the Brain&lt;/h1&gt;
&lt;h2 id=&#34;a-2-day-workshop-at-the-institute-of-neurosciences-timone-in-marseille&#34;&gt;a 2-day workshop at the Institute of Neurosciences Timone in Marseille&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;affiche&#34;&gt;&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Date&lt;/dt&gt;
&lt;dd&gt;April 5-6th 2018&lt;/dd&gt;
&lt;dt&gt;Location&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute of Neurosciences Timone in Marseille in the south of
France&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Main site&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;https://opt-infer-brain.sciencesconf.org/&#34;&gt;https://opt-infer-brain.sciencesconf.org/&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Full program&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;https://opt-infer-brain.sciencesconf.org/program/details&#34;&gt;https://opt-infer-brain.sciencesconf.org/program/details&lt;/a&gt;.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Organizing committee&lt;/dt&gt;
&lt;dd&gt;Paul Apicella, Frederic Danion, Nicole Malfait, Anna Montagnini and
Laurent Perrinet&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;&lt;img src=&#34;http://www.int.univ-amu.fr/IMG/200x130xsiteon0.png,q1331299836.pagespeed.ic.IKYGzK4Zu8.png&#34; alt=&#34;Sponsored by&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilities, Bayes and the Free-energy principle</title>
      <link>https://laurentperrinet.github.io/talk/2018-03-26-cours-neuro-comp-fep/</link>
      <pubDate>Mon, 26 Mar 2018 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-03-26-cours-neuro-comp-fep/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2018-03-26 : PhD Program: course in Computational Neuroscience</title>
      <link>https://laurentperrinet.github.io/post/2018-03-26-cours-neuro-comp-fep/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-03-26-cours-neuro-comp-fep/</guid>
      <description>&lt;h1 id=&#34;phd-program-course-in-computational-neuroscience&#34;&gt;PhD Program: course in Computational Neuroscience&lt;/h1&gt;
&lt;p&gt;Context&lt;/p&gt;
&lt;p&gt;Computational neuroscience is an expending field that is proving to be essential in neurosciences. The aim of this course will be to provide a common solid background in computational neurosciences. The course will comprise historical recall of the field and a description of the different modelling approaches that are currently developed, including details about their specificities, limits and advantages.&lt;/p&gt;
&lt;p&gt;Objective&lt;/p&gt;
&lt;p&gt;The course aims at introducing students with the major tools that will be necessary during their thesis to model or analyze their neuroscientific results. While it will start by a short, generic introduction, we will then explore different systems at different scales. On the first day, we will study the different possible regimes in which a single neuron can behave, while progressively introducing the theory of dynamical systems to understand these more globally. Then, during the second day, we will introduce methods to analyze neuroscientific data in general, such as Bayesian methods and information theory. This will be implemented by simple practical examples.&lt;/p&gt;
&lt;p&gt;Language of intervention&lt;/p&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;p&gt;Number of hours&lt;/p&gt;
&lt;p&gt;~20 hours (session 1=7 + session 2=7 + session 3=4)&lt;/p&gt;
&lt;p&gt;Max participants&lt;/p&gt;
&lt;p&gt;15 for the practical sessions (afternoon Day 2 and Day 3), unlimited for theoretical courses&lt;/p&gt;
&lt;p&gt;Public priority&lt;/p&gt;
&lt;p&gt;PhD students&lt;/p&gt;
&lt;p&gt;Public concerned&lt;/p&gt;
&lt;p&gt;PhD students, interested M2 students and postdocs&lt;/p&gt;
&lt;p&gt;Location&lt;/p&gt;
&lt;p&gt;Institut des Neurosciences de la Timone (INT)&lt;/p&gt;
&lt;p&gt;Keywords&lt;/p&gt;
&lt;p&gt;neuronal modelling, neural circuit modelling, information theory, decoding and encoding&lt;/p&gt;
&lt;p&gt;Targets&lt;/p&gt;
&lt;p&gt;Understanding how computational modelling can be used to formulate and solve neuroscience problems at different spatial and temporal scales; learning the formal notions of information, encoding and decoding and experimenting their use on toy datasets&lt;/p&gt;
&lt;p&gt;Program&lt;/p&gt;
&lt;p&gt;&lt;em&gt;First session:&lt;/em&gt; Introduction to modeling single neurons (morning); An introduction to neural masses: modeling assemblies of neurons up to capturing collective oscillations and resting state dynamics in a mean-field model - presentation of the Virtual Brain software (afternoon) - &lt;em&gt;Second session:&lt;/em&gt; An overview on &amp;ldquo;What is encoding?&amp;rdquo; &amp;ldquo;What is decoding?&amp;quot;: formalization of the notion of information in neural activity; shared and transferred information; integration, segregation and complexity (morning). Bayesian probabilities, the Free-energy principle and Active Inference, with practical demonstrations in python (afternoon). &lt;em&gt;Third session:&lt;/em&gt; the problem of information estimation in practice. Practical exercices in Matlab: estimating entropy and stimulus decodability from spike trains; comparing coding hypotheses (morning).&lt;/p&gt;
&lt;p&gt;Pre-required&lt;/p&gt;
&lt;p&gt;Basic knowledge of statistics and probability and calculus (differential equations,&amp;hellip;) is useful, but steps will be explained and complex math avoided as much as possible. Practical exercises are in python and/or MATLAB, so basic knowledge of these environments is a plus.&lt;/p&gt;
&lt;h2 id=&#34;program&#34;&gt;program&lt;/h2&gt;
&lt;h3 id=&#34;day-1--2018-03-26--an-introduction-to-computational-neuroscience&#34;&gt;day 1 : 2018-03-26 : an introduction to Computational Neuroscience&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;09:30-12:30 = 
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2015-12-08_cours_neurocomp/2017-03-06_LaurentPezard.pdf&#34; title=&#34;Introduction to modeling single neurons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to modeling single neurons&lt;/a&gt; (LaP)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14:00-17:00 = An introduction to neural masses: modeling assemblies of neurons up to capturing resting state dynamics in a mean-field model - presentation of the Virtual Brain software (DaB)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-2--2018-03-27--information-theory--bayesian-models&#34;&gt;day 2 : 2018-03-27 : Information theory / bayesian models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;09:15-10:30 = An overview on &amp;ldquo;What is encoding?&amp;rdquo; &amp;ldquo;What is decoding?&amp;quot;: formalization of the notion of information in neural activity (DaB)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;11:00-12:15 = (&amp;hellip;continued after the coffee break: ) Live information! From sharing information to transferring information (and a glimpse into the zoo of higher-order friends) (DaB)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14:00-17:10 = 
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2018-03-26_cours-NeuroComp_FEP.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probabilities, the Free-energy principle and Active Inference&lt;/a&gt; (LuP).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-3--2018-03-28--practical-course-on-information-theory&#34;&gt;day 3 : 2018-03-28 : Practical course on Information theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;09:30-12:30 = Practical course on Information theory (DaB)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-material-related-to-the-course&#34;&gt;More material related to the course&lt;/h2&gt;
&lt;h3 id=&#34;day-1---morning--the-single-neuron&#34;&gt;day 1 - morning : the single neuron&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;site du livre de Gerstner et al &amp;ldquo;Neuronal Dynamics&amp;rdquo;: &lt;a href=&#34;http://neuronaldynamics.epfl.ch/&#34;&gt;http://neuronaldynamics.epfl.ch/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A (longer) introduction to the Hodgkin-Huxley model in three steps by Dr Stefano Luccioli&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez1.pdf&#34;&gt;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez1.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez2.pdf&#34;&gt;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez3.pdf&#34;&gt;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez3.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An interactive course with Wulfram Gerstner &lt;a href=&#34;https://www.edx.org/course/neuronal-dynamics-computational-epflx-bio465-1x&#34;&gt;https://www.edx.org/course/neuronal-dynamics-computational-epflx-bio465-1x&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;His book ONLINE &lt;a href=&#34;http://cn.epfl.ch/~gerstner/NeuronalDynamics-MOOC1.html&#34;&gt;http://cn.epfl.ch/~gerstner/NeuronalDynamics-MOOC1.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-1---afternoon--neural-mass-models&#34;&gt;day 1 - afternoon : neural mass models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Another interactive course @ Washington University &lt;a href=&#34;https://www.coursera.org/course/compneuro&#34;&gt;https://www.coursera.org/course/compneuro&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Collection of didactic material for the EU FP7 ITN Neural Engineering Transformative Technology &lt;a href=&#34;http://www.neural-engineering.eu/training/index.html&#34;&gt;http://www.neural-engineering.eu/training/index.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Didactic material from Lab in Computational Neuroscience &lt;a href=&#34;http://neuro.fi.isc.cnr.it/index.php?page=didactic-material&#34;&gt;http://neuro.fi.isc.cnr.it/index.php?page=didactic-material&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A open source simulator of a whole brain which runs on your laptop, &amp;ldquo;The Virtual Brain&amp;rdquo;: &lt;a href=&#34;http://thevirtualbrain.org&#34;&gt;http://thevirtualbrain.org&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-2---morning--information-theory&#34;&gt;day 2 - morning : information theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The best book on information theory and decoding, freely available directly from the author: &lt;a href=&#34;http://www.inference.phy.cam.ac.uk/itprnn/book.html&#34;&gt;http://www.inference.phy.cam.ac.uk/itprnn/book.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;a gentle introduction to bayesian methods : &lt;a href=&#34;https://homepages.inf.ed.ac.uk/pseries/Peg_files/Chapter9_SotiropoulosSeries.pdf&#34;&gt;https://homepages.inf.ed.ac.uk/pseries/Peg_files/Chapter9_SotiropoulosSeries.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-2---afternoon--bayesian-models&#34;&gt;day 2 - afternoon : bayesian models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;an interesting read : &lt;a href=&#34;http://cognitrn.psych.indiana.edu/busey/q551/PDFs/PredictivCodingRaoBallard.pdf&#34;&gt;http://cognitrn.psych.indiana.edu/busey/q551/PDFs/PredictivCodingRaoBallard.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;a tutorial on free-energy : some exercises : &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0022249615000759&#34;&gt;http://www.sciencedirect.com/science/article/pii/S0022249615000759&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;solutions to the tutorial : &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2017-01-15-bogacz-2017-a-tutorial-on-free-energy.html&#34;&gt;https://laurentperrinet.github.io/sciblog/posts/2017-01-15-bogacz-2017-a-tutorial-on-free-energy.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contacts&#34;&gt;contacts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LaP: Laurent Pezard &amp;laquo;&lt;a href=&#34;mailto:Laurent.Pezard@univ-amu.fr&#34;&gt;Laurent.Pezard@univ-amu.fr&lt;/a&gt;&amp;raquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DaB: Demian Battaglia &amp;laquo;&lt;a href=&#34;mailto:demian.battaglia@univ-amu.fr&#34;&gt;demian.battaglia@univ-amu.fr&lt;/a&gt;&amp;raquo;, INS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LuP: Laurent Udo Perrinet &amp;laquo;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&amp;raquo;, INT&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PhD program: Nicole Malfait &amp;laquo;&lt;a href=&#34;mailto:Nicole.Malfait@univ-amu.fr&#34;&gt;Nicole.Malfait@univ-amu.fr&lt;/a&gt;&amp;raquo;, Anna Montagnini &amp;laquo;&lt;a href=&#34;mailto:anna.montagnini@univ-amu.fr&#34;&gt;anna.montagnini@univ-amu.fr&lt;/a&gt;&amp;raquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://www.int.univ-amu.fr/IMG/200x130xsiteon0.png,q1331299836.pagespeed.ic.IKYGzK4Zu8.png&#34; alt=&#34;Sponsored by&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Pasturel, Montagnini and Perrinet (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;previous talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-13-law/&#34;&gt;LAW, Lyon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Exp√©riences autour de la perception de la forme en art et science</title>
      <link>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</link>
      <pubDate>Thu, 25 Jan 2018 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/</guid>
      <description>&lt;h1 id=&#34;meetup-art-et-neurosciences&#34;&gt;Meetup Art et Neurosciences&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Quoi&lt;br&gt;
Meetup Art et Neurosciences&lt;/li&gt;
&lt;li&gt;Qui&lt;br&gt;

&lt;a href=&#34;https://www.facebook.com/events/211121069456116/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Association
NeuroNautes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Quand&lt;br&gt;
25 Janvier 2018&lt;/li&gt;
&lt;li&gt;O√π&lt;br&gt;
Salle des voutes campus Saint Charles&lt;/li&gt;
&lt;li&gt;Support visuel&lt;br&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html&#34;&gt;https://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html&lt;/a&gt;
(notes: la pr√©sentation peut mettre un certain temps
√† charger. Une fois que le titre apparait, appuyer sur la touche &amp;ldquo;F&amp;rdquo;
pour mettre en plein √©cran)
&lt;img src=&#34;http://www.lafriche.org/public_data/diapo/resident/1454686884/desk/2._elasticite_dynamique-etienne_rey-photoquentin_chevrier_pour_art2m_et_arcadi_ile_de_france.jpg&#34; alt=&#34;Elasticit√©&#34; title=&#34;Elasticit√© dynamique est compos√©e des pi√®ces Expansion, Trame et Lignes sonores. Volume hexagonal en miroir de 7 m√®tres de diam√®tre, Expansion fonctionne comme une chambre d&#39;√©cho. A l&#39;int√©rieur de ce volume se situe Trame. Constitu√©e de 25 lames de miroir en rotation, cette pi√®ce r√©oriente continuellement le regard. Quant √† Lignes sonores, elle est form√©e de quatre monolithes orient√©s vers Expansion et √©met des sons qui se r√©orientent en fonction du mouvement des lames. (¬© Etienne Rey, Adagp Paris&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A low-cost, accessible eye tracking framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-18-gdr/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-18-gdr/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;poster presented @ [[https://gdrvision2018.sciencesconf.org/|GDR vision, Paris]].&lt;/li&gt;
&lt;li&gt;program : &lt;a href=&#34;https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf&#34;&gt;https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;poster : &lt;a href=&#34;https://github.com/laurentperrinet/Perrinet18gdr/raw/master/Perrinet18gdr.pdf&#34;&gt;https://github.com/laurentperrinet/Perrinet18gdr/raw/master/Perrinet18gdr.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;poster (code) :  &lt;a href=&#34;https://github.com/laurentperrinet/Perrinet18gdr/&#34;&gt;https://github.com/laurentperrinet/Perrinet18gdr/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;source code for this framework: &lt;a href=&#34;https://github.com/laurentperrinet/CatchTheEye&#34;&gt;https://github.com/laurentperrinet/CatchTheEye&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ANEMO: Quantitative tools for the ANalysis of Eye MOvements</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-18-anemo/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-18-anemo/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;as presented at &lt;a href=&#34;https://eyemovements.sciencesconf.org/&#34;&gt;https://eyemovements.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;get the 
&lt;a href=&#34;https://github.com/invibe/ANEMO/raw/master/2018-05-04_Poster_Grenoble/Pasturel_etal2018_grenoble.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code : &lt;a href=&#34;https://github.com/invibe/ANEMO/&#34;&gt;https://github.com/invibe/ANEMO/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-18-grenoble/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-18-grenoble/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;as presented at &lt;a href=&#34;https://eyemovements.sciencesconf.org/&#34;&gt;https://eyemovements.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;get the 
&lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/raw/master/Poster/2018-06-05_Poster_Workshop_Grenoble/Pasturel_etal2018grenoble.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code : &lt;a href=&#34;https://github.com/chloepasturel/AnticipatorySPEM/&#34;&gt;https://github.com/chloepasturel/AnticipatorySPEM/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-18/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>From biological vision to unsupervised hierarchical sparse coding</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-itwist/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-itwist/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;accepted submission @ 
&lt;a href=&#34;https://sites.google.com/view/itwist18&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;iTWIST: international Traveling Workshop on Interactions between low-complexity data models and Sensing Techniques&lt;/a&gt;, 21 - 23 November‚Äã, 2018&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://sites.google.com/view/itwist18/program#h.p_9OOcrreKb--s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poster session&lt;/a&gt; scheduled on Thursday, November 22th, from 10h30 till 12h00.&lt;/li&gt;
&lt;li&gt;CIRM, Marseille, France. &lt;span id=&#34;line-10&#34; class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;get the 
&lt;a href=&#34;https://arxiv.org/html/1812.00648&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;full proceedings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Poster as 
&lt;a href=&#34;boutin-franciosini-ruffier-perrinet-18-itwist.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;check-out our preprint on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34;&gt;SDPC&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation</title>
      <link>https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Origins of Hierarchy in Visual Processing</title>
      <link>https://laurentperrinet.github.io/publication/franciosini-perrinet-18-cs/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/franciosini-perrinet-18-cs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Selectivity to oriented patterns of different precisions</title>
      <link>https://laurentperrinet.github.io/publication/ladret-18-gdr/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ladret-18-gdr/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;poster pr√©sent√© au 
&lt;a href=&#34;https://gdrvision2018.sciencesconf.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GDR vision, Paris&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;program : &lt;a href=&#34;https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf&#34;&gt;https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hugoladret/InternshipM1/raw/master/2018-06_POSTER_final.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Poster (pdf)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code : &lt;a href=&#34;https://github.com/hugoladret/InternshipM1&#34;&gt;https://github.com/hugoladret/InternshipM1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Speed uncertainty and motion perception with naturalistic random textures</title>
      <link>https://laurentperrinet.github.io/publication/mansour-18-vss/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-18-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unsupervised Hierarchical Sparse Coding algorithm inspired by Biological Vision</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-doctoral-day/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-doctoral-day/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unsupervised learning applied to robotic vision</title>
      <link>https://laurentperrinet.github.io/talk/2017-11-24-neurosciences-robotique/</link>
      <pubDate>Fri, 24 Nov 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-11-24-neurosciences-robotique/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Participation au jury</title>
      <link>https://laurentperrinet.github.io/talk/2017-11-17-festival-interferences/</link>
      <pubDate>Fri, 17 Nov 2017 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-11-17-festival-interferences/</guid>
      <description>&lt;h1 id=&#34;festival-interf√©rences&#34;&gt;FESTIVAL INTERF√âRENCES‚Äã&lt;/h1&gt;
&lt;h2 id=&#34;cin√©ma-documentaire-et-d√©bat-public&#34;&gt;Cin√©ma Documentaire et D√©bat Public&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://static.wixstatic.com/media/e37617_35d8c5b48dd340a481db5f711aeaa35a~mv2_d_1772_2480_s_2.jpg/v1/fill/w_600,h_797,al_c,q_85,usm_0.66_1.00_0.01/e37617_35d8c5b48dd340a481db5f711aeaa35a~mv2_d_1772_2480_s_2.jpg&#34; alt=&#34;FESTIVAL INTERF√âRENCES‚Äã&#34; title=&#34;FESTIVAL INTERF√âRENCES‚Äã&#34;&gt;
Le collectif Sc√®nes Publiques compos√© de citoyens, chercheurs et
cin√©astes, organise la deuxi√®me √©dition du Festival Interf√©rences du 8
au 18 novembre 2017 √† Lyon. J&amp;rsquo;ai eu la chance de pouvoir participer au
jury autour de documentaires avec un regard scientifiques. Une occasion
aussi de parler du m√©tier de chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
17 et 18 Novembre 2017&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
Lyon&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&lt;a href=&#34;http://www.lacitedoc.com/interferences-programmation&#34;&gt;http://www.lacitedoc.com/interferences-programmation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What dynamic neural codes for efficient visual processing</title>
      <link>https://laurentperrinet.github.io/talk/2017-11-15-colloque-master/</link>
      <pubDate>Wed, 15 Nov 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-11-15-colloque-master/</guid>
      <description>&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;unsupervised learning : 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-10-shl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet (2010)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biologically inspired computer vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;supervised learning : &lt;a href=&#34;https://www.nature.com/articles/srep11400&#34;&gt;https://www.nature.com/articles/srep11400&lt;/a&gt; (
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more info&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;dynamics: Khoei et al (2017) - &lt;a href=&#34;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068&#34;&gt;http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068&lt;/a&gt; ( 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more info&lt;/a&gt; )&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Back to the present: dealing with delays in biological and neuromorphic systems</title>
      <link>https://laurentperrinet.github.io/talk/2017-06-28-telluride/</link>
      <pubDate>Wed, 28 Jun 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-06-28-telluride/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tutorial on predictive coding</title>
      <link>https://laurentperrinet.github.io/talk/2017-06-30-telluride/</link>
      <pubDate>Wed, 28 Jun 2017 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-06-30-telluride/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The flash-lag effect as a motion-based predictive shift</title>
      <link>https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/</link>
      <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.cnrs.fr/insb/recherche/parutions/articles2017/l-perrinet.html%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;visual-illusions-their-origin-lies-in-prediction&#34;&gt;Visual illusions: their origin lies in prediction&lt;/h1&gt;
&lt;p&gt;





  



  
  











&lt;figure id=&#34;figure-flash-lag-effect-when-a-visual-stimulus-moves-along-a-continuous-trajectory-it-may-be-seen-ahead-of-its-veridical-position-with-respect-to-an-unpredictable-event-such-as-a-punctuate-flash-this-illusion-tells-us-something-important-about-the-visual-system-contrary-to-classical-computers-neural-activity-travels-at-a-relatively-slow-speed-it-is-largely-accepted-that-the-resulting-delays-cause-this-perceived-spatial-lag-of-the-flash-still-after-several-decades-of-debates-there-is-no-consensus-regarding-the-underlying-mechanisms&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/flash_lag_hu277740dce32a2754cb1ad98f4208cb67_31157_2000x2000_fit_lanczos.gif&#34; data-caption=&#34;&amp;lt;em&amp;gt;Flash-Lag Effect.&amp;lt;/em&amp;gt; When a visual stimulus moves along a continuous trajectory, it may be seen ahead of its veridical position with respect to an unpredictable event such as a punctuate flash. This illusion tells us something important about the visual system: contrary to classical computers, neural activity travels at a relatively slow speed. It is largely accepted that the resulting delays cause this perceived spatial lag of the flash. Still, after several decades of debates, there is no consensus regarding the underlying mechanisms.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/flash_lag_hu277740dce32a2754cb1ad98f4208cb67_31157_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1000&#34; height=&#34;600&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Flash-Lag Effect.&lt;/em&gt; When a visual stimulus moves along a continuous trajectory, it may be seen ahead of its veridical position with respect to an unpredictable event such as a punctuate flash. This illusion tells us something important about the visual system: contrary to classical computers, neural activity travels at a relatively slow speed. It is largely accepted that the resulting delays cause this perceived spatial lag of the flash. Still, after several decades of debates, there is no consensus regarding the underlying mechanisms.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;strong&gt;Researchers from the Timone Institute of Neurosciences bring a new theoretical hypothesis on a visual illusion discovered at the beginning of the 20th century. This illusion remained misunderstood while it poses fundamental questions about how our brains represent events in space and time. This study published on January 26, 2017 in the journal PLOS Computational Biology, shows that the solution lies in the predictive mechanisms intrinsic to the neural processing of information.&lt;/strong&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New Research: The Flash-Lag Effect as a Motion-Based Predictive Shift &lt;a href=&#34;https://t.co/K3KWPO8l4a&#34;&gt;https://t.co/K3KWPO8l4a&lt;/a&gt; Khoei et al. &lt;a href=&#34;https://twitter.com/hashtag/vision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#vision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/motion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#motion&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuralnetworks?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuralnetworks&lt;/a&gt; &lt;a href=&#34;https://t.co/RElm4Qqo58&#34;&gt;pic.twitter.com/RElm4Qqo58&lt;/a&gt;&lt;/p&gt;&amp;mdash; PLOS Comp Biol (@PLOSCompBiol) &lt;a href=&#34;https://twitter.com/PLOSCompBiol/status/829354100273745920?ref_src=twsrc%5Etfw&#34;&gt;February 8, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Visual illusions are still popular: in a quasi-magical way, they can make objects appear where they are not expected&amp;hellip; They are also excellent opportunities to question the constraints of our perceptual system. Many illusions are based on motion, such as the flash-lag effect. Observe a luminous dot that moves along a rectilinear trajectory. If a second light dot is flashed very briefly just above the first, the moving point will always be perceived in front of the flash while they are vertically aligned.


















&lt;figure id=&#34;figure-fig-2-diagonal-markov-chain-in-the-current-study-the-estimated-state-vector-z--x-y-u-v-is-composed-of-the-2d-position-x-and-y-and-velocity-u-and-v-of-a-moving-stimulus-a-first-we-extend-a-classical-markov-chain-using-nijhawans-diagonal-model-in-order-to-take-into-account-the-known-neural-delay-œÑ-at-time-t-information-is-integrated-until-time-t--œÑ-using-a-markov-chain-and-a-model-of-state-transitions-pztztŒ¥t-such-that-one-can-infer-the-state-until-the-last-accessible-information-pztœÑi0tœÑ-this-information-can-then-be-pushed-forward-in-time-by-predicting-its-trajectory-from-t--œÑ-to-t-in-particular-pzti0tœÑ-can-be-predicted-by-the-same-internal-model-by-using-the-state-transition-at-the-time-scale-of-the-delay-that-is-pztztœÑ-this-is-virtually-equivalent-to-a-motion-extrapolation-model-but-without-sensory-measurements-during-the-time-window-between-t--œÑ-and-t-note-that-both-predictions-in-this-model-are-based-on-the-same-model-of-state-transitions-b-one-can-write-a-second-equivalent-pull-mode-for-the-diagonal-model-now-the-current-state-is-directly-estimated-based-on-a-markov-chain-on-the-sequence-of-delayed-estimations-while-being-equivalent-to-the-push-mode-described-above-such-a-direct-computation-allows-to-more-easily-combine-information-from-areas-with-different-delays-such-a-model-implements-nijhawans-diagonal-model-but-now-motion-information-is-probabilistic-and-therefore-inferred-motion-may-be-modulated-by-the-respective-precisions-of-the-sensory-and-internal-representations-c-such-a-diagonal-delay-compensation-can-be-demonstrated-in-a-two-layered-neural-network-including-a-source-input-and-a-target-predictive-layer-44-the-source-layer-receives-the-delayed-sensory-information-and-encodes-both-position-and-velocity-topographically-within-the-different-retinotopic-maps-of-each-layer-for-the-sake-of-simplicity-we-illustrate-only-one-2d-map-of-the-motions-x-v-the-integration-of-coherent-information-can-either-be-done-in-the-source-layer-push-mode-or-in-the-target-layer-pull-mode-crucially-to-implement-a-delay-compensation-in-this-motion-based-prediction-model-one-may-simply-connect-each-source-neuron-to-a-predictive-neuron-corresponding-to-the-corrected-position-of-stimulus-x--v--œÑ-v-in-the-target-layer-the-precision-of-this-anisotropic-connectivity-map-can-be-tuned-by-the-width-of-convergence-from-the-source-to-the-target-populations-using-such-a-simple-mapping-we-have-previously-shown-that-the-neuronal-population-activity-can-infer-the-current-position-along-the-trajectory-despite-the-existence-of-neural-delays&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=info:doi/10.1371/journal.pcbi.1005068.g002&#34; data-caption=&#34;Fig 2. &amp;lt;em&amp;gt;Diagonal Markov chain.&amp;lt;/em&amp;gt; In the current study, the estimated state vector z = {x, y, u, v} is composed of the 2D position (x and y) and velocity (u and v) of a (moving) stimulus. (A) First, we extend a classical Markov chain using Nijhawan‚Äôs diagonal model in order to take into account the known neural delay œÑ: At time t, information is integrated until time t ‚àí œÑ, using a Markov chain and a model of state transitions p(zt|zt‚àíŒ¥t) such that one can infer the state until the last accessible information p(zt‚àíœÑ|I0:t‚àíœÑ). This information can then be ‚Äúpushed‚Äù forward in time by predicting its trajectory from t ‚àí œÑ to t. In particular p(zt|I0:t‚àíœÑ) can be predicted by the same internal model by using the state transition at the time scale of the delay, that is, p(zt|zt‚àíœÑ). This is virtually equivalent to a motion extrapolation model but without sensory measurements during the time window between t ‚àí œÑ and t. Note that both predictions in this model are based on the same model of state transitions. (B) One can write a second, equivalent ‚Äúpull‚Äù mode for the diagonal model. Now, the current state is directly estimated based on a Markov chain on the sequence of delayed estimations. While being equivalent to the push-mode described above, such a direct computation allows to more easily combine information from areas with different delays. Such a model implements Nijhawan‚Äôs ‚Äúdiagonal model‚Äù, but now motion information is probabilistic and therefore, inferred motion may be modulated by the respective precisions of the sensory and internal representations. (C) Such a diagonal delay compensation can be demonstrated in a two-layered neural network including a source (input) and a target (predictive) layer [44]. The source layer receives the delayed sensory information and encodes both position and velocity topographically within the different retinotopic maps of each layer. For the sake of simplicity, we illustrate only one 2D map of the motions (x, v). The integration of coherent information can either be done in the source layer (push mode) or in the target layer (pull mode). Crucially, to implement a delay compensation in this motion-based prediction model, one may simply connect each source neuron to a predictive neuron corresponding to the corrected position of stimulus (x &amp;#43; v ‚ãÖ œÑ, v) in the target layer. The precision of this anisotropic connectivity map can be tuned by the width of convergence from the source to the target populations. Using such a simple mapping, we have previously shown that the neuronal population activity can infer the current position along the trajectory despite the existence of neural delays.&#34;&gt;


  &lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=info:doi/10.1371/journal.pcbi.1005068.g002&#34; alt=&#34;&#34; width=&#34;80%&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 2. &lt;em&gt;Diagonal Markov chain.&lt;/em&gt; In the current study, the estimated state vector z = {x, y, u, v} is composed of the 2D position (x and y) and velocity (u and v) of a (moving) stimulus. (A) First, we extend a classical Markov chain using Nijhawan‚Äôs diagonal model in order to take into account the known neural delay œÑ: At time t, information is integrated until time t ‚àí œÑ, using a Markov chain and a model of state transitions p(zt|zt‚àíŒ¥t) such that one can infer the state until the last accessible information p(zt‚àíœÑ|I0:t‚àíœÑ). This information can then be ‚Äúpushed‚Äù forward in time by predicting its trajectory from t ‚àí œÑ to t. In particular p(zt|I0:t‚àíœÑ) can be predicted by the same internal model by using the state transition at the time scale of the delay, that is, p(zt|zt‚àíœÑ). This is virtually equivalent to a motion extrapolation model but without sensory measurements during the time window between t ‚àí œÑ and t. Note that both predictions in this model are based on the same model of state transitions. (B) One can write a second, equivalent ‚Äúpull‚Äù mode for the diagonal model. Now, the current state is directly estimated based on a Markov chain on the sequence of delayed estimations. While being equivalent to the push-mode described above, such a direct computation allows to more easily combine information from areas with different delays. Such a model implements Nijhawan‚Äôs ‚Äúdiagonal model‚Äù, but now motion information is probabilistic and therefore, inferred motion may be modulated by the respective precisions of the sensory and internal representations. (C) Such a diagonal delay compensation can be demonstrated in a two-layered neural network including a source (input) and a target (predictive) layer [44]. The source layer receives the delayed sensory information and encodes both position and velocity topographically within the different retinotopic maps of each layer. For the sake of simplicity, we illustrate only one 2D map of the motions (x, v). The integration of coherent information can either be done in the source layer (push mode) or in the target layer (pull mode). Crucially, to implement a delay compensation in this motion-based prediction model, one may simply connect each source neuron to a predictive neuron corresponding to the corrected position of stimulus (x + v ‚ãÖ œÑ, v) in the target layer. The precision of this anisotropic connectivity map can be tuned by the width of convergence from the source to the target populations. Using such a simple mapping, we have previously shown that the neuronal population activity can infer the current position along the trajectory despite the existence of neural delays.
  &lt;/figcaption&gt;


&lt;/figure&gt;

Processing visual information takes time and even if these delays are remarkably short, they are not negligible and the nervous system must compensate them. For an object that moves predictably, the neural network can infer its most probable position taking into account this processing time. For the flash, however, this prediction can not be established because its appearance is unpredictable. Thus, while the two targets are aligned on the retina at the time of the flash, the position of the moving object is anticipated by the brain to compensate for the processing time: it is this differentiated treatment that causes the flash-lag effect.
The researchers show that this hypothesis also makes it possible to explain the cases where this illusion does not work: for example if the flash appears at the end of the moving dot&amp;rsquo;s trajectory or if the target reverses its path in an unexpected way. In this work, the major innovation is to use the accuracy of information in the dynamics of the model. Thus, the corrected position of the moving target is calculated by combining the sensory flux with the internal representation of the trajectory, both of which exist in the form of probability distributions. To manipulate the trajectory is to change the precision and therefore the relative weight of these two information when they are optimally combined in order to know where an object is at the present time. The researchers propose to call parodiction (from the ancient Greek paron, the present) this new theory that joins Bayesian inference with taking into account neuronal delays.


















&lt;figure id=&#34;figure-fig-5-histogram-of-the-estimated-positions-as-a-function-of-time-for-the-dmbp-model-histograms-of-the-inferred-horizontal-positions-blueish-bottom-panel-and-horizontal-velocity-reddish-top-panel-as-a-function-of-time-frame-from-the-dmbp-model-darker-levels-correspond-to-higher-probabilities-while-a-light-color-corresponds-to-an-unlikely-estimation-we-highlight-three-successive-epochs-along-the-trajectory-corresponding-to-the-flash-initiated-standard-mid-point-and-flash-terminated-cycles-the-timing-of-the-flashes-are-respectively-indicated-by-the-dashed-vertical-lines-in-dark-the-physical-time-and-in-green-the-delayed-input-knowing-œÑ--100-ms-histograms-are-plotted-at-two-different-levels-of-our-model-in-the-push-mode-the-left-hand-column-illustrates-the-source-layer-that-corresponds-to-the-integration-of-delayed-sensory-information-including-the-prior-on-motion-the-right-hand-illustrates-the-target-layer-corresponding-to-the-same-information-but-after-the-occurrence-of-some-motion-extrapolation-compensating-for-the-known-neural-delay-œÑ&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=10.1371/journal.pcbi.1005068.g005&#34; data-caption=&#34;Fig 5. &amp;lt;em&amp;gt;Histogram of the estimated positions as a function of time for the dMBP model.&amp;lt;/em&amp;gt; Histograms of the inferred horizontal positions (blueish bottom panel) and horizontal velocity (reddish top panel), as a function of time frame, from the dMBP model. Darker levels correspond to higher probabilities, while a light color corresponds to an unlikely estimation. We highlight three successive epochs along the trajectory, corresponding to the flash initiated, standard (mid-point) and flash terminated cycles. The timing of the flashes are respectively indicated by the dashed vertical lines. In dark, the physical time and in green the delayed input knowing œÑ = 100 ms. Histograms are plotted at two different levels of our model in the push mode. The left-hand column illustrates the source layer that corresponds to the integration of delayed sensory information, including the prior on motion. The right-hand illustrates the target layer corresponding to the same information but after the occurrence of some motion extrapolation compensating for the known neural delay œÑ.&#34;&gt;


  &lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=10.1371/journal.pcbi.1005068.g005&#34; alt=&#34;&#34; width=&#34;80%&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Fig 5. &lt;em&gt;Histogram of the estimated positions as a function of time for the dMBP model.&lt;/em&gt; Histograms of the inferred horizontal positions (blueish bottom panel) and horizontal velocity (reddish top panel), as a function of time frame, from the dMBP model. Darker levels correspond to higher probabilities, while a light color corresponds to an unlikely estimation. We highlight three successive epochs along the trajectory, corresponding to the flash initiated, standard (mid-point) and flash terminated cycles. The timing of the flashes are respectively indicated by the dashed vertical lines. In dark, the physical time and in green the delayed input knowing œÑ = 100 ms. Histograms are plotted at two different levels of our model in the push mode. The left-hand column illustrates the source layer that corresponds to the integration of delayed sensory information, including the prior on motion. The right-hand illustrates the target layer corresponding to the same information but after the occurrence of some motion extrapolation compensating for the known neural delay œÑ.
  &lt;/figcaption&gt;


&lt;/figure&gt;

Despite the simplicity of this solution, parodiction has elements that may seem counter-intuitive. Indeed, in this model, the physical world is considered &amp;ldquo;hidden&amp;rdquo;, that is to say, it can only be guessed by our sensations and our experience. The role of visual perception is then to deliver to our central nervous system the most likely information despite the different sources of noise, ambiguity and time delays. According to the authors of this publication, the visual treatment would consist in a &amp;ldquo;simulation&amp;rdquo; of the visual world projected at the present time, even before the visual information can actually modulate, confirm or cancel this simulation. This hypothesis, which seems to belong to &amp;ldquo;science fiction&amp;rdquo;, is being tested with more detailed and biologically plausible hierarchical neural network models that should allow us to better understand the mysteries underlying our perception. Visual illusions have still the power to amaze us!
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New from Khoei et al. The Flash-Lag Effect as a &lt;a href=&#34;https://twitter.com/hashtag/Motion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Motion&lt;/a&gt;-Based Predictive Shift &lt;a href=&#34;https://t.co/K3KWPO8l4a&#34;&gt;https://t.co/K3KWPO8l4a&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuralnetworks?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuralnetworks&lt;/a&gt; &lt;a href=&#34;https://t.co/iWsd9nK5qp&#34;&gt;pic.twitter.com/iWsd9nK5qp&lt;/a&gt;&lt;/p&gt;&amp;mdash; PLOS Comp Biol (@PLOSCompBiol) &lt;a href=&#34;https://twitter.com/PLOSCompBiol/status/829474896023474176?ref_src=twsrc%5Etfw&#34;&gt;February 8, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial: Active inference for eye movements: Bayesian methods, neural inference, dynamics</title>
      <link>https://laurentperrinet.github.io/talk/2017-01-20-laconeu/</link>
      <pubDate>Fri, 20 Jan 2017 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-01-20-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tutorial: Sparse optimization in neural computations</title>
      <link>https://laurentperrinet.github.io/talk/2017-01-19-laconeu/</link>
      <pubDate>Thu, 19 Jan 2017 10:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-01-19-laconeu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Back to the present: how neurons deal with delays</title>
      <link>https://laurentperrinet.github.io/talk/2017-01-18-laconeu/</link>
      <pubDate>Wed, 18 Jan 2017 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2017-01-18-laconeu/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://laconeu.cl/wp-content/uploads/2018/04/Valparaiso-3.jpg&#34; alt=&#34;Chile&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Controlling an aerial robot with human gestures using bio-inspired algorithm</title>
      <link>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-17-doctoral-day/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-17-doctoral-day/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic modulation of volatility by reward contingencies: effects on anticipatory smooth eye movement</title>
      <link>https://laurentperrinet.github.io/publication/damasse-17-vss/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-17-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient learning of sparse image representations using homeostatic regulation</title>
      <link>https://laurentperrinet.github.io/publication/boutin-ruffier-perrinet-17-neurofrance/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-ruffier-perrinet-17-neurofrance/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;This work is a followup of 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-10-shl/&#34;&gt;Perrinet, 2010, Neural Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the 
&lt;a href=&#34;https://github.com/laurentperrinet/BoutinRuffierPerrinet17spars/raw/master/docs/BoutinRuffierPerrinet17neurofrance.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poster (PDF)&lt;/a&gt; will be presented Thursday, May 18 @ 
&lt;a href=&#34;http://www.professionalabstracts.com/sn2017/programme-sn2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroFrance, Bordeaux&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;see a follow-up publication on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-ruffier-perrinet-17-spars/&#34;&gt;BoutinRuffierPerrinet17spars&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Efficient learning of sparse image representations using homeostatic regulation</title>
      <link>https://laurentperrinet.github.io/publication/boutin-ruffier-perrinet-17-spars/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/boutin-ruffier-perrinet-17-spars/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;This work is a followup of 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-10-shl/&#34;&gt;Perrinet, 2010, Neural Computation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code is available @ &lt;a href=&#34;https://github.com/laurentperrinet/BoutinRuffierPerrinet17spars&#34;&gt;https://github.com/laurentperrinet/BoutinRuffierPerrinet17spars&lt;/a&gt; and heavily uses &lt;a href=&#34;https://github.com/bicv/SparseHebbianLearning&#34;&gt;https://github.com/bicv/SparseHebbianLearning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the 
&lt;a href=&#34;https://github.com/laurentperrinet/BoutinRuffierPerrinet17spars/raw/master/docs/BoutinRuffierPerrinet17spars.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poster (PDF)&lt;/a&gt;  will be presented Thursday, June 8 @ 
&lt;a href=&#34;http://spars2017.lx.it.pt/index_files/SPARS2017_program.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPARS, Lisbon&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-17-gdr/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-17-gdr/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Exp√©riences autour de la perception de la forme en art et science</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-17-gdr/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-17-gdr/</guid>
      <description>&lt;h1 id=&#34;exp√©riences-autour-de-la-perception-de-la-forme-en-art-et-science&#34;&gt;Exp√©riences autour de la perception de la forme en art et science&lt;/h1&gt;
&lt;p&gt;La vision utilise un faisceau d&amp;rsquo;informations de diff√©rentes qualit√©s pour atteindre une perception unifi√©e du monde environnant. Nous avons utilis√© lors de plusieurs projets art-science (voir &lt;a href=&#34;https://github.com/NaturalPatterns&#34;&gt;https://github.com/NaturalPatterns&lt;/a&gt;) des installations permettant de manipuler explicitement des composantes de ce flux d&amp;rsquo;information et de r√©v√©ler des ambiguit√©s dans notre perception.
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_b.jpg&#34; alt=&#34;Tropique&#34;&gt;
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_a.jpg&#34; alt=&#34;Tropique&#34;&gt;
Dans l&amp;rsquo;installation ¬´Tropique¬ª, des faisceaux de lames lumineuses sont arrang√©s dans l&amp;rsquo;espace assombri de l&amp;rsquo;installation. Les spectateurs les observent gr√¢ce √† leur interaction avec une brume invisible qui est diffus√©e dans l&amp;rsquo;espace. Dans ¬´Trame √âlasticit√©¬ª, 25 parall√©l√©pip√®des de miroirs (3m de haut) sont arrang√©s verticalement sur une ligne horizontale. Ces lames sont rotatives et leurs mouvements est synchronis√©. Suivant la dyamique qui est impos√© √† ces lames, la perception de l‚Äôespace environnent fluctue conduisant √† recomposer l‚Äôespace de la concentration √† l‚Äôexpansion, ou encore √† g√©n√©rer un surface semblant transparente ou inverser la visons de ce qui est situ√©e devant et derri√®re l‚Äôobservateur. Enfin, dans ¬´Trame instabilit√©¬ª, nous explorons l&amp;rsquo;interaction de s√©ries p√©riodiques de points plac√©es sur des surfaces transparentes. √Ä partir de premi√®res exp√©rimentations utilisant une technique novatrice de s√©rigraphie, ces trames de points sont plac√©es afin de faire √©merger des structures selon le point de vue du spectateur. De mani√®re g√©n√©rale, nous montrerons ici les diff√©rentes m√©thodes utilis√©es, comme l&amp;rsquo;utilisation des limites perceptives, et aussi les r√©sultats apport√©s par une telle collaboration.
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2017/01/EtienneRey-TRAME-Vasarely-B.jpg&#34; alt=&#34;Elasticit√©&#34;&gt;
&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2017/01/EtienneRey-TRAME-Vasarely-D.jpg&#34; alt=&#34;Elasticit√©&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;poster pr√©sent√© au 
&lt;a href=&#34;https://gdrvision2017.sciencesconf.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GDR vision 2017, Lille&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;abstract: &lt;a href=&#34;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017abstract_168363.pdf&#34;&gt;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017abstract_168363.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;poster : &lt;a href=&#34;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017poster.pdf&#34;&gt;https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017poster.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;poster (code) : &lt;a href=&#34;https://github.com/NaturalPatterns/2017-10-12_GDR/blob/master/2017-10-12_PerrinetRey2017poster.ipynb&#34;&gt;https://github.com/NaturalPatterns/2017-10-12_GDR/blob/master/2017-10-12_PerrinetRey2017poster.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;more code : &lt;a href=&#34;https://github.com/NaturalPatterns&#34;&gt;https://github.com/NaturalPatterns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How the dynamics of human smooth pursuit is influenced by speed uncertainty</title>
      <link>https://laurentperrinet.github.io/publication/mansour-17-ecvp/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-17-ecvp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-17-gdr/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-17-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Participation au jury et entretien avec Clara Delmon</title>
      <link>https://laurentperrinet.github.io/talk/2016-11-20-polly-maggoo/</link>
      <pubDate>Sun, 20 Nov 2016 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-11-20-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;rencontres-internationales-sciences--cin√©mas&#34;&gt;RENCONTRES INTERNATIONALES SCIENCES &amp;amp; CIN√âMAS&lt;/h1&gt;
&lt;h2 id=&#34;cin√©ma-les-vari√©t√©s&#34;&gt;cin√©ma les Vari√©t√©s&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg&#34; alt=&#34;http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016\_A3-724x1024.jpg&#34; title=&#34;http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg&#34;&gt;
L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; programme la
10e √©dition des RENCONTRES INTERNATIONALES SCIENCES &amp;amp; CIN√âMAS (RISC) √†
Marseille, au cours desquelles l&amp;rsquo;association programme des films √†
caract√®re scientifique. Les projections se d√©roulent en pr√©sence de
chercheurs et/ou de cin√©astes dans la perspective d‚Äôun d√©veloppement de
la culture cin√©matographique et scientifique en direction des publics
scolaires.
Ce dimanche 20 novembre, je suis venu √©changer au c√¥t√© de Serge Dentin
et Caroline Renard (Ma√Ætre de conf√©rences en √©tudes cin√©matographiques √†
Aix-Marseille Universit√©), autour de films traitant du rapport
fiction/r√©el, de la m√©moire, et du temps. Une occasion aussi de parler
du m√©tier de chercheur.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
25 Avril 2016&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
cin√©ma les Vari√©t√©s&lt;/li&gt;
&lt;li&gt;Programmation&lt;br&gt;
&amp;ldquo;addendum&amp;rdquo; court m√©trage de J√©r√¥me Lefdup et &amp;ldquo;Po√©tique du cerveau&amp;rdquo;
long m√©trage de Nurith Aviv&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;entretien-avec-clara-delmon&#34;&gt;entretien avec Clara Delmon&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;occasion aussi d&amp;rsquo;un entretien avec Clara Delmon dans le cadre de son
m√©moire de DSAA (Dipl√¥me Sup√©rieur d‚ÄôArts Appliqu√©s) mention Design
Graphique √† Marseille, disponible sur
&lt;a href=&#34;http://www.tonerkebab.fr/wiki/doku.php/wiki:proto-memoires:clara-delmon:clara-delmon&#34;&gt;http://www.tonerkebab.fr/wiki/doku.php/wiki:proto-memoires:clara-delmon:clara-delmon&lt;/a&gt;
et &lt;a href=&#34;https://www.behance.net/claradelmon&#34;&gt;https://www.behance.net/claradelmon&lt;/a&gt; 
&lt;a href=&#34;http://www.tonerkebab.fr/wiki/lib/exe/fetch.php/wiki:proto-memoires:clara-delmon:clara_synthe_se.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;L‚Äô√©chec de la
perception&amp;rdquo;&lt;/a&gt;.
Entretien avec Laurent PERRINET, rencontr√© √† la 10e √©dition des RISC
(Rencontres Internationales de la Science et du Cin√©ma) chercheur au
CNRS (Centre National de la Recherche Scientifque) √† l‚ÄôInstitut de
Neurosciences de la Timone √† Marseille, sp√©cialis√© en perception
visuelle.&lt;/p&gt;
&lt;h2 id=&#34;entretien&#34;&gt;Entretien&lt;/h2&gt;
&lt;p&gt;Entretien avec Laurent PERRINET, rencontr√© √† la 10e √©dition des RISC
(Rencontres Internationales de la Science et du Cin√©ma)chercheur au CNRS
(Centre National de la Recherche Scientifque) √† l‚ÄôInstitut de
Neurosciences de la Timone √† Marseille, sp√©cialis√© en perception
visuelle. Ôªø
&lt;strong&gt;1 / Vous faites les Rencontres Internationales de la Science et du
CineÃÅma depuis quelques anneÃÅes deÃÅjaÃÄ, la science est de plus en plus
preÃÅsente dans les arts, comme avec certains courants artistiques comme
l‚ÄôArt CineÃÅtique ou l‚ÄôArt Optique, pourquoi pensez-vous qu‚Äôune telle
interaction est preÃÅsente aÃÄ notre eÃÅpoque ? J‚Äôai la sensation qu‚Äôil y a
un inteÃÅreÃÇt grandissant pour l‚ÄôeÃÅtude du cerveau dans le domaine des
arts et de la communication. AÃÄ votre avis, pourquoi un tel besoin de
donner de la poeÃÅsie au cerveau, (ou du cerveau aÃÄ la poeÃÅsie) ?&lt;/strong&gt;
En effet, je participe aux Rencontres Internationales de la Science et
du CineÃÅma depuis deÃÅjaÃÄ deux ans deÃÅjaÃÄ. Le but est simplement de
rentrer en contact avec le grand public et partager ma passion pour
l‚ÄôeÃÅtude de la perception visuelle et du cerveau plus geÃÅneÃÅralement.
J‚Äôattache beaucoup d‚Äôimportance aÃÄ ces rencontres car elle nous
permettent aussi de mieux comprendre l‚ÄôinteÃÅreÃÇt public pour le cerveau
dans son fonctionnement normal mais aussi dans ses dysfonctions. C‚Äôest
aussi une source d‚Äôinspiration pour savoir dans quelle direction il est
important de plus creuser nos recherches.
&lt;strong&gt;2 / Vous travaillez notamment avec Etienne Rey sur des installations
interactives, ouÃÄ la place et le ressenti du spectateur font l‚Äô≈ìuvre. La
vue est alors votre outil de travail essentiel, pourquoi ce sens est-il
plus sensiblement exposeÃÅ aÃÄ l‚ÄôexpeÃÅrience de l‚Äôillusion ? Qu‚Äôapporte
l‚ÄôexpeÃÅrience perceptive au spectateur ?&lt;/strong&gt;
En effet, en paralleÃÄle de ces actions de partage avec le public, je
travaille aussi avec &lt;em&gt;EÃÅtienne Rey&lt;/em&gt;, un artiste plasticien reÃÅsidant aÃÄ
la Friche Belle de mai aÃÄ Marseille. Notre travail s‚Äôarticule autour de
l‚ÄôambiguiÃàteÃÅ de l‚ÄôexpeÃÅrience perceptive du spectateur.
Est-il en train de se regarder lui-meÃÇme dans un miroir ou le miroir
est-il lui-meÃÇme une ≈ìuvre d‚Äôart ?
&lt;strong&gt;3 / Les graphistes d‚Äôaujourd‚Äôhui ont tendance aÃÄ brouiller les codes,
deÃÅformer, rendre illisible, en bref utiliser la complexiteÃÅ de l‚Äôimage
pour en complexifier la lecture. Pensez-vous qu‚Äôune image ouÃÄ on ne voit
rien puisse en dire plus ? C‚Äôest-aÃÄ-dire, pensez-vous qu‚Äôen accentuant
l‚Äôacte de lecture, le designer graphique ameÃÄne aÃÄ son lecteur une
activiteÃÅ qui consisterait non plus seulement aÃÄ deÃÅchiffrer un message
(preÃÅsentation d‚Äôun eÃÅveÃÄnement, publiciteÃÅ&amp;hellip;) mais aÃÄ s‚Äôobserver
lui-meÃÇme en tant que lecteur ?&lt;/strong&gt;
Le travail du systeÃÄme visuel est de deÃÅcoder les messages ambigus qui
lui sont deÃÅlivreÃÅs par la reÃÅtine. En creÃÅant des oeuvres graphiques
qui brouillent les codes et en les deÃÅformants, on oblige le cerveau aÃÄ
avoir une deÃÅmarche plus active par rapport au deÃÅcodage du message
fourni.
Tout le travail du graphiste consiste donc aÃÄ indiquer ce processus
actif tout en conservant l‚ÄôinteÃÅgriteÃÅ du message.
&lt;strong&gt;4 / Ces images utilisent le plus souvent des trames, des rayures, des
distorsions qui captent notre attention. Pourquoi notre ≈ìil est plus
attireÃÅ par ce qui est en mouvement ?&lt;/strong&gt;
Notre oeil est attireÃÅ par tout ce qui est surprenant. Cela inclut donc
tout ce qui ne peut pas arriver par hasard comme des bouts de lignes
aligneÃÅs. Mais notre oeil est aussi attireÃÅ par ce qu‚Äôil trouve
surprenant de ne pas pouvoir preÃÅdire, comme par exemple des lignes qui
sont leÃÅgeÃÄrement deÃÅcaleÃÅes ou un objet qui est en mouvement. Un
processus actif s‚ÄôeÃÅtablit alors pour comprendre cette stimulation avec
de nouvelles hypotheÃÄses.
&lt;strong&gt;5 / Il semblerait que notre ≈ìil soit attireÃÅ par des formes, des
couleurs, des objets particuliers qui diffeÃÄrent pour chacun d‚Äôentre
nous. Il y a dans la perception visuelle des notions de pulsions, de
deÃÅsirs, un besoin de voir, comment expliquez-vous que le cerveau soit
sans cesse en queÃÇte et en attente d‚Äôimages ?&lt;/strong&gt;
Pour moi la perception visuelle n‚Äôest pas juste un cineÃÅma aÃÄ
l‚ÄôinteÃÅrieur du cerveau !
C‚Äôest un processus vital qui sert aÃÄ mieux interagir avec
l‚Äôenvironnement. AÃÄ ce titre il est toujours en queÃÇte de nouvelles
images pour ameÃÅliorer ce rapport au monde que l‚Äôon construit sans
cesse. Il faut voir par exemple comment un enfant manipule des objets.
Il le fait pour mieux comprendre les images de ces objets et la facÃßon
dont il peut interagir avec le monde.
&lt;strong&gt;6 / On l‚Äôa vu notamment dans le film PoeÃÅtique du Cerveau de Nurith
Aviv diffuseÃÅ aÃÄ cette 10e eÃÅdition du RISC, la meÃÅmoire et
l‚ÄôexpeÃÅrience visuelle de chacun influent sur notre perception. Vous
avez parleÃÅ d‚Äô ¬´ autopoiÃàeÃÄse ¬ª, cela signifie-t-il que nous voyons tous
les choses diffeÃÅremment ? Est-ce qu‚Äôun systeÃÄme de donneÃÅes
preÃÅ-eÃÅtablies est formeÃÅ par notre cerveau au cours de nos anneÃÅes de
vie et sert de ¬´ lunettes ¬ª pour voir le monde ?&lt;/strong&gt;
La perception visuelle est un processus actif de compreÃÅhension d‚Äôune
repreÃÅsentation du monde. Elle est donc propre aÃÄ chacun car elle se
construit avec notre expeÃÅrience et la facÃßon dont nous interagissons
avec le monde visuel. Mais ce monde est le meÃÇme pour chaque individu et
nous partageons les meÃÇmes codes et les meÃÇmes systeÃÄmes pour apprendre
aÃÄ nous repreÃÅsenter ce monde.
Nos ¬´ lunettes ¬ª sont donc propres aÃÄ notre expeÃÅrience mais elles ont
suÃÇrement beaucoup en commun entre individus.
&lt;strong&gt;7 / Peut-on enlever ces lunettes? Des expeÃÅrimentations optiques comme
celles d‚ÄôEtienne Rey ou celles de designers graphiques conduisants une
reÃÅflexion sur notre vision peuvent-elles amener une nouvelle
expeÃÅrience visuelle remettant en question notre activiteÃÅ
perceptive?&lt;/strong&gt;
On ne pourra jamais enlever ses lunettes ! Pour voir, on est obligeÃÅ
d‚Äôinteragir avec le monde. Toute perception est une interpreÃÅtation et
ne pourra jamais eÃÇtre absolue : le monde physique nous est ¬´ cacheÃÅ ¬ª
par la meÃÅdiation avec nos sens, qui par essence sont toujours ambigus.
Par contre, ces expeÃÅrimentations optiques permettent de mieux
comprendre les limites de cet aspect de notre perception visuelle et
ainsi de donner un acceÃÄs plus direct avec cette conscience du monde
visuel.
&lt;strong&gt;8 / Le meÃÅcanisme d‚Äôanticipation mis aÃÄ l‚Äô≈ìuvre dans notre cerveau
faisant intervenir notre meÃÅmoire et notre imagination dans la
constitution d‚Äôune image stable ne nous eÃÅloigne-t-il pas trop de la
reÃÅaliteÃÅ ? Il y a une ¬´ imagination anticipative ¬ª et une confirmation
de ce reÃÅel par la mise en tension de nos projections avec la situation
preÃÅsente, ce systeÃÄme n‚Äôest-il pas proche de celui de l‚Äôillusion
d‚Äôoptique ?&lt;/strong&gt;
Au contraire je pense que ces meÃÅcanismes d‚Äôanticipation sont plus
proches de la reÃÅaliteÃÅ que celle qu‚Äôon imagine eÃÇtre la ¬´ vraie ¬ª
reÃÅaliteÃÅ. Par exemple on ne voit que dans un spectre de lumieÃÄre treÃÄs
deÃÅfini alors que les objets visuels existent potentiellement par
exemple dans la lumieÃÄre ultraviolette. Cette reÃÅaliteÃÅ laÃÄ n‚Äôest
visible qu‚Äôavec des appareils speÃÅcialiseÃÅs.
Pour moi la seule reÃÅaliteÃÅ qui vaille, c‚Äôest la reÃÅaliteÃÅ de la
construction qui est opeÃÅreÃÅe dans la perception visuelle et non la
reÃÅaliteÃÅ geÃÅneÃÅralement eÃÅtablie du monde physique externe aÃÄ nos
sens.
En comprenant mieux les meÃÅcanismes qui nous permettent de simuler cette
reÃÅaliteÃÅ physique externe, nous sommes plus objectifs par rapport aux
limites de notre connaissance du monde.
AÃÄ ce titre je pense que ces meÃÅcanismes d‚Äôanticipation sont donc plus
proches de la reÃÅaliteÃÅ par rapport aÃÄ une reÃÅaliteÃÅ objective telle
qu‚Äôon se la repreÃÅsente traditionnellement.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The flash-lag effect as a motion-based predictive shift</title>
      <link>https://laurentperrinet.github.io/talk/2016-11-03-sigma/</link>
      <pubDate>Thu, 03 Nov 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-11-03-sigma/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt; and 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;Khoei et al, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Differential response of the retinal neural code with respect to the sparseness of natural images</title>
      <link>https://laurentperrinet.github.io/publication/ravello-16-droplets/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ravello-16-droplets/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reinforcement contingencies modulate anticipatory smooth eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2016-11-03-gdr/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-11-03-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biologically-inspired characterization of sparseness in natural images</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/</link>
      <pubDate>Wed, 26 Oct 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/</link>
      <pubDate>Wed, 26 Oct 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2016-10-26 : EUVIP BICV</title>
      <link>https://laurentperrinet.github.io/post/2016-10-26_euvip-bicv/</link>
      <pubDate>Wed, 26 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2016-10-26_euvip-bicv/</guid>
      <description>&lt;h1 id=&#34;2016-10-26--euvip-special-session-on-biologically-inspired-computer-vision&#34;&gt;2016-10-26 : EUVIP Special Session on &lt;em&gt;Biologically Inspired Computer Vision&lt;/em&gt;&lt;/h1&gt;
&lt;h2 id=&#34;description-of-the-session&#34;&gt;description of the session&lt;/h2&gt;
&lt;p&gt;Recent advances in imaging technologies have yielded scientific data at
unprecedented detail and volume, leading to the need of a shift of
paradigm in image processing and computer vision. Beyond the usual
classical von Neumann architecture, one strategy that is emerging in
order to process and interpret this amount of data follows from the
architecture of biological organisms and shows for instance
computational paradigms implementing asynchronous communication with a
high degree of local connectivity in sensors or brain tissues. This
session aims at bringing together researchers from different fields of
Biologically Inspired Computer Vision to present latest results in the
field, from fundamental to more specialized topics, including visual
analysis based on a computational level, hardware implementation, and
the design of new more advanced vision sensors. It is expected to
provide a comprehensive overview in the computer area of biologically
motivated vision. On the one hand, biological organisms can provide a
source of inspiration for new computationally efficient and robust
vision models and on the other hand machine vision approaches can
provide new insights for understanding biological visual systems. This
session covers a wide range of topics from fundamental to more
specialized topics, including visual analysis based on a computational
level, hardware implementation, and the design of new more advanced
vision sensors. In particular, we expect to provide an overview of a few
representative applications and current state of the art of the research
in this area.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;URL
&lt;a href=&#34;http://www-l2ti.univ-paris13.fr/euvip2016/index.php/86-euvip2016/129-tentative-technical-program-in-detail&#34;&gt;http://www-l2ti.univ-paris13.fr/euvip2016/index.php/86-euvip2016/129-tentative-technical-program-in-detail&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date
October 26th, 2016&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Location
Ecole Centrale Marseille&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Address

&lt;a href=&#34;https://www.centrale-marseille.fr/fr/acces-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;38 rue Fr√©d√©ric Joliot-Curie 13013 Marseille,
France&lt;/a&gt; Phone : +33
(0)4 91 05 45 45&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Programme&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;13.50  
&lt;a href=&#34;http://ieeexplore.ieee.org/document/7764586/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual System Inspired Algorithm For Contours, Corner And T Junction Detection&lt;/a&gt;, Antoni Buades, &lt;em&gt;Rafael Grompone Von Gioi&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;13.50  
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biologically-inspired characterization of sparseness in natural images&lt;/a&gt;, &lt;em&gt;Laurent Perrinet&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.10 
&lt;a href=&#34;http://david.alleysson.free.fr/Publications/JIST0224reprint.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Color filter array imitating the random nature of color arrangement in the human cone mosaic&lt;/a&gt;, Prakhar Amba, &lt;em&gt;David Alleysson&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.30 
&lt;a href=&#34;http://ieeexplore.ieee.org/document/7764601/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Illuminant-Independent Analysis Of Reflectance As Sensed By Humans, And Its Applicability To Computer Vision&lt;/a&gt;, Alban Flachot, Phelma, J.Kevin O&amp;rsquo;Regan, &lt;em&gt;Edoardo Provenzi&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.50 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor&lt;/a&gt;, Lionel Fillatre, Michel Barlaud, &lt;em&gt;Laurent Perrinet&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Eye movements as a model for active inference</title>
      <link>https://laurentperrinet.github.io/talk/2016-10-13-law/</link>
      <pubDate>Thu, 13 Oct 2016 10:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-10-13-law/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See the final publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Pasturel, Montagnini and Perrinet (2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/&#34;&gt;INVIBE FEST, Paris&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/&#34;&gt;Brain workshop, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-01-18-laconeu/&#34;&gt;LACONEU, Chile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/&#34;&gt;CAUSAL Kick-off, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;next talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;NeuroFrance, Marseille&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Biologically-inspired characterization of sparseness in natural images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-16-euvip/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-16-euvip/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Operant reinforcement versus reward expectancy: effects on anticipatory eye movements</title>
      <link>https://laurentperrinet.github.io/publication/damasse-16-vss/</link>
      <pubDate>Sat, 03 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-16-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modelling the dynamics of cognitive processes: from the Bayesian brain to particles</title>
      <link>https://laurentperrinet.github.io/talk/2016-07-07-edp-proba/</link>
      <pubDate>Thu, 07 Jul 2016 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-07-07-edp-proba/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau</title>
      <link>https://laurentperrinet.github.io/talk/2016-04-28-mejanes/</link>
      <pubDate>Thu, 28 Apr 2016 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-04-28-mejanes/</guid>
      <description>&lt;h1 id=&#34;les-illusions-visuelles-un-r√©v√©lateur-du-fonctionnement-de-notre-cerveau&#34;&gt;Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau&lt;/h1&gt;
&lt;h2 id=&#34;cycle-de-conf√©rences-tous-connect√©s-biblioth√®que-de-m√©janes&#34;&gt;Cycle de conf√©rences &amp;ldquo;Tous connect√©s&amp;rdquo;, Biblioth√®que de M√©janes&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;featured.jpg&#34; alt=&#34;conf√©rence tout public √† la Biblioth√®que de M√©janes (Aix-en-Provence, Avril 2016)&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
28 Avril 2016&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
Biblioth√®que de M√©janes&lt;/li&gt;
&lt;li&gt;Visuels&lt;br&gt;

&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2016-04-28_mejanes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ANR BalaV1 (2013/2016)</title>
      <link>https://laurentperrinet.github.io/grant/anr-bala-v1/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-bala-v1/</guid>
      <description>&lt;h1 id=&#34;anr-balav1-balanced-states-in-area-v1-20132016&#34;&gt;ANR BalaV1: Balanced states in area V1 (2013/2016)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.agence-nationale-recherche.fr/Project-ANR-13-BSV4-0014&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Official website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In carnivores and primates the orientation selectivity (OS) of the cells in the primary visual cortex (V1) is organized in maps in which preferred orientations (POs) of the cells change gradually except near ‚Äúpin- wheels‚Äù, around which all orientations are present. Over the last half-century the mechanism for OS has been hotly debated. However the theories that purport to explain OS have almost all considered cortical networks in which the neurons receive input preferentially from cells with similar PO. Such theories certainly capture the connectivity for neurons in orientation domains where neurons are surrounded by other cells with similar PO. However this does not necessarily hold near pinwheels: because of the discontinuous change in orientation preference at the pinwheel, neurons in this area are surrounded by cells of all preferred orientations. Thus if the probability of connection is solely dependent on anatomical distance, the inputs that these neurons receive should represent all orientations by roughly the same amount. Thus one may expect that the response of the cells near pinwheels should hardly vary with orientation, in contrast to experimental data. As a result, the common belief is that, at least near pinwheels, the connectivity depends also on the differences between preferred orientation. The situation near pinwheels in V1 of carnivores and primates is similar to that in the whole of V1 of rodents. In these species, neurons in V1 are OS but the network does not exhibit an orientation map and the surround of the cells represents all orientations roughly equally. In a recent theoretical paper (Hansel and van Vreeswijk 2012) we have demonstrated that in this situation, the response of the cells can still be orientation selective provided that the network operates in the balanced regime. Here we hypothesize that V1 with an orientation map operates in the balanced regime and therefore neurons can exhibit OS near pinwheels even in the absence of functional specific connectivity. The goal of this interdisciplinary project is to investigate whether the ‚Äúbalance hypothesis‚Äù holds for layer 2/3 in V1 of primate and carnivore and whether the functional organization observed in that layer can be accounted for without feature specific connectivity. We will combine modeling and experiments to investigate how the response of the neurons ‚Äì the mean firing, the mean voltage, the inhibitory and excitatory conductances and importantly, the power spectrum of their fluctuations ‚Äì vary with the location in the map, and also how a population of neurons ‚Äì LFP, voltage-sensitive dye imaging or 2 photons ‚Äì is affected by the various para- meters used to test the system. Whether V1 indeed operates in the balanced regime in more realistic conditions will be further investigated by determining how the local network responds to visual stimuli beyond the classical receptive field. We will investigate this issue in models of layer 2/3 representing multiple hyper- columns to characterize center-surround interactions and their dependence on the long-range connectivity. This will provide us with predictions for center-surround interactions for cells near pinwheels and in orientation domains. These predictions will be tested experimentally.&lt;/p&gt;
&lt;p&gt;The proposed project is new and ambitious. It aims at building a comprehensive and coherent understand- ing of the physiology of V1 layer 2/3 on several spatial scales from single cells to several hypercolumns and to account for this in mechanistic models. To accomplish these ambitious aims, we propose a combination of experimental and computational studies that take advantage of the unique strengths and the complementarity of expertise of 3 research teams. The Paris team has extensive experience in large-scale modeling of V1. The Toulouse and Marseille teams master both intra- and extracellular electrophysiology. In addition, the Marseille team is expert in microscopic and mesoscopic imaging techniques in V1.&lt;/p&gt;
&lt;p&gt;Acknowledgement&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This work was supported by ANR project &amp;quot;BalaV1&amp;quot; N¬∞ ANR-13-BSV4-0014-02.  
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ANR CausaL (2018/2020)</title>
      <link>https://laurentperrinet.github.io/grant/anr-causal/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-causal/</guid>
      <description>&lt;p&gt;With Andrea Brovelli (INT), Mateus Joffily (GATE)&amp;hellip;&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://anr.fr/Project-ANR-18-CE28-0016&#34;&gt;https://anr.fr/Project-ANR-18-CE28-0016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Humans have an extraordinary capacity to infer cause-effect relations. In particular, we excel in forming ‚Äãbeliefs ‚Äãabout the ‚Äãcausal effect of actions‚Äã. Causal learning provides the basis for rational decision-making and allows people to engage in meaningful life and social interactions. Causal learning is a form of goal-directed learning, defined as the capacity to rapidly learn the consequence of actions and to select behaviours according to goals and motivational state. This ability is based on internal models of the consequence of our behaviors‚Äã and relies on learning rules determined by the‚Äã contingency between actions and outcomes‚Äã. At a first approximation, contingency Œî‚ÄãP ‚Äãis operationalized as the difference between two conditional probabilities: i) P(O|A), the probability of outcome O given action A; ii) P(O|¬¨A), the probability of the outcome when the action is withheld. In everyday life, people perceive their actions as causing a given outcome if the contingency is positive, whereas they perceive them as preventing‚Äã ‚Äãit‚Äã ‚Äãif‚Äã ‚Äãnegative;‚Äã ‚Äãwhen‚Äã ‚ÄãP(O|A)‚Äã ‚Äãand‚Äã ‚ÄãP(O|¬¨A)‚Äã ‚Äãare‚Äã ‚Äãequal,‚Äã ‚Äãpeople‚Äã ‚Äãreport‚Äã ‚Äãno‚Äã ‚Äãcausal‚Äã ‚Äãeffect‚Äã‚Äã ‚Äã. Despite the centrality of causal learning, a clear understanding of both the internal computations and neural substrates (the so-called ‚Äãcognitive architectures‚Äã) is currently missing. ‚ÄãOur project will therefore address‚Äã ‚Äãtwo‚Äã ‚Äãkey‚Äã ‚Äãquestions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What are the key ‚Äãinternal representations of causal beliefs and what are the ‚Äãcomputational processes‚Äã‚Äã ‚Äãthat‚Äã ‚Äãenable‚Äã ‚Äãtheir‚Äã ‚Äãformation‚Äã ‚Äãduring‚Äã ‚Äãlearning?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How ‚Äã‚Äãare ‚Äã‚Äãinternal‚Äã ‚Äãrepresentations‚Äã ‚Äãand ‚Äã‚Äãcomputational‚Äã‚Äã processes‚Äã ‚Äã‚Äãimplemented‚Äã ‚Äã‚Äãin ‚Äã‚Äãthe ‚Äã‚Äãbrain? CausaL‚Äã ‚Äã‚Äãwill‚Äã ‚Äãaddress‚Äã ‚Äãthese‚Äã ‚Äãtwo‚Äã ‚Äãobjectives‚Äã ‚Äãthrough‚Äã ‚Äãtwo‚Äã ‚Äãdedicated‚Äã ‚Äãresearch‚Äã ‚Äãwork‚Äã ‚Äãpackages‚Äã ‚Äã(WPs).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Acknowledgement : This work was supported by ANR project ANR-18-AAPG‚Äì‚ÄúCAUSAL, Cognitive Architectures of  Causal  Learning‚Äù.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ANR Horizontal-V1 (2017/2021)</title>
      <link>https://laurentperrinet.github.io/grant/anr-horizontal-v1/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-horizontal-v1/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Description on the official website of the 
&lt;a href=&#34;http://www.agence-nationale-recherche.fr/Project-ANR-17-CE37-0006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Horizontal-V1 project aims at understanding the emergence of sensory predictions linking local shape attributes (orientation, contour) to global indices of movement (direction, speed, trajectory) at the earliest stage of cortical processing (primary visual cortex, i.e. V1). We will study how the long-distance &amp;ldquo;horizontal&amp;rdquo; connectivity, intrinsic to V1 and the feedback from higher cortical areas contribute to a dynamic processing of local-to-global features as a function of the context (eg displacement along a trajectory; during reafference change induced by eye-movements&amp;hellip;). We will search to characterize the dynamic processes based on lateral propagation intra-V1, through which spatio-temporal inferences (continuous movement or apparent motion sequences) facilitating spatial (&amp;ldquo;filling-in&amp;rdquo;) or positional (&amp;ldquo;flash-lag&amp;rdquo;) future expected responses may be generated. The project will use a variety of animations of local oriented stimuli forming, according to their spatial and temporal coherence, predictable global patterns, apparent motion sequences and/or continuous trajectories. We will measure the cortical dynamics at two scales of neuronal integration, from micro- (intracellular, SUA) to meso-scopic levels (multi-electrode arrays (MEA) and voltage sensitive dye imaging (VSDI)) in the anesthetized (cat, marmoset) and awake fixating animal (macaca mulata). In a second step, we will combine these multiscale observations to constrain a structuro-functional model of low-level perception, integrating the micro-meso constraints. Two laboratories will participate in synergy to the project: UNIC-Gif (Dir. Yves Fr√©gnac, DRCE2 CNRS, coordinator) and INT-Marseille (NeOpTo Team Dir. Fr√©d√©ric Chavane, DR2 CNRS).&lt;/p&gt;
&lt;h1 id=&#34;wp3---design-of-novel-visual-paradigms-probabilistic-model-of-v1-and-data-driven-simulations---co-lead-unic-int&#34;&gt;WP3 - Design of novel visual paradigms, probabilistic model of V1 and data-driven simulations - co lead UNIC-INT.&lt;/h1&gt;
&lt;p&gt;Objectives : This WP will have two primary goals. The first one is theoretically driven, and for sake of simplicity will ignore the dynamic features of neural integration (as expected from a statistical model of image analysis). Binding the different features of visual objects at the local scale (contours) as well as a more global level involves understanding the statistical regularities of the sensory inflow. In particular, titrating the predictions that can be done at the statistical level can be seen as a first pass to better search for critical parameters constraining the network behaviour. From these, we will build probabilistic predictive models optimized for edge co-occurrence classification and generate novel visual statistics 1) which obey rules imposed by the functional horizontal connectivity anisotropies, such as co- circularity, and 2) which facilitate binding in the orientation domain, such as log-polar planforms. These statistics generated in the first half of the grant will be implemented and tested experimentally in the second half of the grant. The second one is more data-driven (as well as phenomenological for feedback from higher cortical areas, since it will not be explored in the grant). Since model fitting will depend on close interactions with WP1 and WP2 measurements, it will be done in the second half of the grant.&lt;/p&gt;
&lt;h2 id=&#34;wp3-task-1-theoretically-oriented-workplan--lead-int-laurent-perrinet&#34;&gt;WP3-Task 1: Theoretically oriented workplan ‚Äì Lead INT (Laurent Perrinet)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;WP3-Task 1.1 - theory : we will exploit our current expertise in integrating these statistics in the form of probabilistic models to make predictions both at the physiological and modelling levels. First, we will take advantage of our previous work on the quantification of the association field in different classes of natural images (Perrinet &amp;amp; Bednar, 2015). Using an existing library (&lt;a href=&#34;https://github.com/bicv/SparseEdges),&#34;&gt;https://github.com/bicv/SparseEdges),&lt;/a&gt; we will use the sparse representation of static natural images to compute histograms of edge co-occurrences. Using an existing algorithm for unsupervised learning (&lt;a href=&#34;https://github.com/bicv/SparseHebbianLearning),&#34;&gt;https://github.com/bicv/SparseHebbianLearning),&lt;/a&gt; we will learn the different independent components of edge co-occurrences. Such an algorithm fits well a traditional deep-learning convolutional neural network, but, in addition, will include constraints imposed by intra-layer horizontal connectivity. We expect that relevant features will be co-linear or co-circular pairs of edges, but also T-junctions or end-stopping features.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;WP3-Task 1.2 - image/film synthesis : We have previously found that random synthetic textures, coined &amp;ldquo;Motion Clouds&amp;rdquo;, can be used to quantify V1 implication in visual motion perception (Leon et al, 2012; Simoncini et al, 2012). Recently, the INT and UNIC, partners proved mathematically that these stimuli were optimal with respect to some common geometrical transformations, such as translation, zoom or rotations (Vacher et al, 2015). A main characteristic of these textures is to be generated with a maximally entropic arrangement of elementary textures (so-called textons).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;**        Informed by the generative model of edge co-occurrences studied in subtask 1, we will be able to extend the family of motion cloud stimuli (Leon et al, 2012; Simoncini et al, 2012) to include joint dependencies between different elements in position or orientation. An exact solution to this problem is hard to achieve as it involves a combinatorial search of all possible combinations of pairs of edges. However, numerous variational approaches are possible and fit well with our probabilistic framework. We will use the convolutional neural network described above but using a back-propagating stream to generate different images. Such a representation will then be optimized using an unsupervised learning method. This is similar to the process used in Generative Adversarial Networks in deep-learning architectures (Radford et al, Archives).
**        Finally, the regularities observed in static images will be extended to dynamical scenes by observing that a co-occurrence can be implemented by simple geometrical operations as they are operated in time. For instance a co-circularity is easily described as the set of smooth roto-translational transformations of an edge in time using the group of Galilean transformations (Sarti and Citti, 2006). This theory calls for a first prediction to understand the set of whole possible spatio-temporal co-occurrences of edges as geodesics in the lifted space of all possible trajectories. We predict that such decomposition should allow us to better understand the different classes of features that emerged in the first task.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WP3-Task 1.3 - Feedback of theory on experimentation : An essential aspect of this work would be to apply these stimuli in neurophysiological experiments and in the modelling. In particular, the ability to select different types of dependencies from the different classes learned above (for instance, co-circularities of a certain curvature range) will make it possible to evaluate the relative contribution of different components of the contextual information. This justifies the fact that the WP3 post-doctoral fellow should have the mobility (between INT and UNIC) and multi-disciplinar profile (theoretical and experimental) to perform this task.&lt;/li&gt;
&lt;li&gt;WP3-Task 1.4 - Generic modelling : These various subtasks will allow us to determine the hierarchy of critical features relevant to describe the full statistics of the space of spatio-temporal edge co-occurrences. Indeed, in static images, we will be able to find independent component in the histograms of edge co-occurrences between metric aspect (distance or scale between edge) from configurational aspects (difference of angle or co-circularity angle).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similarly, we expect to see that the different independent features should decompose at various scales both in space and in time. For instance, we expect configurational aspects to be more local while aspects related to a motion (Perrinet and Masson, 2012; Khoei et al, 2016) or global shape (form) should be more global. This translates into a probabilistic hierarchical model that would combine dependencies from different cues. In particular, through the emergence of differential pathways for form and motion. These quantitative predictions should finally be confronted at the modelling and neurophysiological levels.&lt;/p&gt;
&lt;h2 id=&#34;wp3-task-2--data-driven-comprehensive-model-of-v1--co-lead-unic-and-int&#34;&gt;WP3-Task 2 : Data-driven comprehensive model of V1 ‚Äì Co-lead UNIC and INT&lt;/h2&gt;
&lt;p&gt;The second task is more data-driven (as well as phenomenological for the feedback circuit part, since largely unknown). Since simulations will depend on close interactions with WP1 and WP2 measurements, it will be developed by the WP3-Post-Doc in the second half of the grant. It will benefit from existing structuro-functional models addressing separately two distinct levels of neural integration, microscopic (conductance-based in Kremkow et al, 2016; Antolik et al, submitted, Chariker et al, 2016) and mesoscopic (VSD-like mean field in Rankin and Chavane, 2017). Efforts will be made to merge these models to fit - in a unified multiscale biologically realistic model - the cellular and VSD data targeting critically horizontal propagation. The parameterization should be flexible enough to produce a generic cortical architecture accounting possibly for species-specificity (Antolik for cat; Chaliker for monkey)&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work was supported by ANR project &amp;ldquo;Horizontal-V1&amp;rdquo; N¬∞ ANR-17-CE37-0006.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ANR PredictEye (2018/2020)</title>
      <link>https://laurentperrinet.github.io/grant/anr-predicteye/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-predicteye/</guid>
      <description>&lt;p&gt;The objectives of PREDICTEYE is to rigorously test and define the functional and neurophysiological grounds of probabilistic oculomotor internal models by investigating the multiple timescales at which the trajectory of a moving target is learned and represented in a probabilistic framework (Aim #1). Second, we will investigate the role of (pre)frontal oculomotor networks in building such probabilistic representations and their impact upon two of their downstream neural targets of the brainstem premotor centers (superior colliculus for saccades; NRTP for pursuit) (Aim #2). Our third objective is to model and simulate the dynamics of target motion prediction and eye movement performance. A key question is to unveil how probabilistic information about target timing and motion (i.e. direction and speed) is sampled over trial history by neuronal populations and integrated with Prior knowledge (i.e. sequence properties and rules of conditional probabilities) in order to coordinate saccades and pursuit and optimize their precisions (Aim #3).&lt;/p&gt;
&lt;p&gt;Acknowledgement&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This work was supported by ANR project &amp;quot;PredictEye&amp;quot; ANR-XXXX.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ANR REM (2013/2016)</title>
      <link>https://laurentperrinet.github.io/grant/anr-rem/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-rem/</guid>
      <description>

















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;http://static.tvtropes.org/pmwiki/pub/images/R.E.M..jpg&#34; &gt;


  &lt;img src=&#34;http://static.tvtropes.org/pmwiki/pub/images/R.E.M..jpg&#34; alt=&#34;We were open :-)&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Reinforcement learning theory provides a general conceptual framework to account for behavioral changes. Recently the idea that reinforcement may be used to explain learning in motor responses has emerged. In particular, there is a growing interest in studying the effects of reinforcement learning in arm movements trajectories (Dam, Kording, &amp;amp; Wei, 2013), pointing movements (Trommershauser, Landy, &amp;amp; Maloney, 2006), or eye movements (Madelain, Champrenaut, &amp;amp; Chauvin, 2007; Madelain &amp;amp; Krauzlis, 2003b; Madelain, Paeye, &amp;amp; Wallman, 2011; Sugrue, Corrado, &amp;amp; Newsome, 2004; Takikawa, Kawagoe, Itoh, Nakahara, &amp;amp; Hikosaka, 2002; Xu-Wilson, Zee, &amp;amp; Shadmehr, 2009). However, and despite these few seminal studies, much is still unknown about both the details of the effects of reward on motor control and the underlying mechanisms. &lt;strong&gt;This proposal aims at a better understanding of how skilled motor responses are learned focusing on voluntary eye movements.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Although learning is often regarded as a restricted period of time during which a behavior undergo some changes we view learning as a continuously ongoing process. In the case of motor control every instance of a behavior is followed by some consequences that will affect some dimensions of the future response. These changes will in return affect the functional relations with the environment and this feedback process continues through lifetime. Therefore we do not regard motor learning as a special phase that allows the emergence of a particular motor response but as a continuous adaptation to the changes within the organism that affect the functional relations with her environment. This distinction is important because the learning situations that are experimentally tested over a short period of time may then be viewed as a condensed version of motor learning in the real life: the same adaptive processes are responsible for the changes in the response in both situations.&lt;/p&gt;
&lt;p&gt;An important aspect of this fundamental research project is that the theoretical propositions addressed provide a new view on motor learning that departs from conventional wisdom. We expect to gain considerable knowledge on learning by constructing new experimental paradigms to collect behavioural data, implementing new learning models based on Bayesian theories and testing dynamical mathematical models of behavioural changes. &lt;strong&gt;Whichever way the results turn out, we anticipate that these studies will provide a better understanding of motor learning and provide a well-defined and solid framework for studying other forms of motor plasticity.&lt;/strong&gt; If eye movement learning follows the rules of other operant responses (i.e. responses reinforced by their consequences), this will constitute a minor revolution in the study of motor control, both at the behavioral and neural levels, with important implications for the understanding of plasticity in other motor systems.&lt;/p&gt;
&lt;p&gt;Acknowledgement&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This work was supported by ANR project ANR-13-APPR-0008 &amp;quot;ANR R.E.M.&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ANR SPEED (2013/2016)</title>
      <link>https://laurentperrinet.github.io/grant/anr-speed/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-speed/</guid>
      <description>&lt;p&gt;Measuring speed and direction of moving objects is an essential computational step in order to move our eyes, hands or other body parts with respect to the environment. Whereas encoding and decoding of direction information is now largely understood in various neuronal systems, how the human brain accurately represents speed information remains largely unknown. Speed tuned neurons have been identified in several early cortical visual areas in monkeys. However, how such speed tuning emerges is not yet understood. A working hypothesis is that speed tuned neurons nonlinearly combine motion information extracted at different spatial and temporal scales, taking advantage of the statistical spatiotemporal properties of natural scenes. However, such pooling of information must be context dependent, varying with the spatial perceptual organization of the visual scenes. Furthermore, the population code underlying perceived speed is not elucidated either and therefore we are still far from understanding how speed information is decoded to drive and control motor responses or perceptual judgments.&lt;/p&gt;
&lt;p&gt;Recently, we have proposed that speed estimation is intrinsically a multi-scale, task-dependent problem (Simoncini et al., Nature Neuroscience 2012) and we have defined a new set of motion stimuli, constructed as random phase dynamical textures that mimic the statistics of natural scenes (Sanz-Leon et al., Journal of Neurophysiology 2012). This approach has proved to be fruitful to investigate nonlinear properties of motion integration.&lt;/p&gt;
&lt;p&gt;The current proposal brings together psychophysicists, oculomotor scientists and modelers to investigate speed processing in human. We aim at expanding this framework in order to understand how tracking eye movements and motion perception can take advantage of multiple scale processing for estimating target speed. We will design sets of high dimensional stimuli by extending our generative model. Using these natural-statistics stimuli, we will investigate how speed information is encoded by computing motion energy across different spatial and temporal filters. By analysing both perceptual and oculomotor responses we will probe the nonlinear mechanisms underlying the integration of the outputs of multiple spatiotemporal filters and implement these processes in a refined version of our model. Furthermore, we will test our working hypothesis that in natural scenes such nonlinear integration provides precise and reliable motion estimates, which leads to efficient motion-based behaviors. By comparing tracking responses with perception, we will also test a second critical hypothesis, that nonlinear speed computations are task-dependent. In particular, we will explore the extent to which the geometrical structures of visual scenes are decisive for perception beyond the motion energy computation used for early sensorimotor transformation. Finally we will investigate the role of contextual and extra-retinal, predictive information in building an efficient dynamic estimate of objects&#39; speed for perception and action.&lt;/p&gt;
&lt;p&gt;Acknowledgement&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This work was supported by ANR project &amp;quot;ANR Speed&amp;quot; ANR-13-BSHS2-0006.    
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ANR TRAJECTORY (2016/2019)</title>
      <link>https://laurentperrinet.github.io/grant/anr-trajectory/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-trajectory/</guid>
      <description>&lt;p&gt;Global motion processing is a major computational task of biological visual systems. When an object moves across the visual field, the sequence of visited positions is strongly correlated in space and time, forming a trajectory. These correlated images generate a sequence of local activation of the feed-forward stream. Local properties such as position, direction and orientation can be extracted at each time step by a feed-forward cascade of linear filters and static non-linearities. However such local, piecewise, analysis ignores the recent history of motion and faces several difficulties, such as systematic delays, ambiguous information processing (e.g., aperture and correspondence problems61) high sensitivity to noise and segmentation problems when several objects are present. Indeed, two main aspects of visual processing have been largely ignored by the dominant, classical feed-forward scheme. First, natural inputs are often ambiguous, dynamic and non-stationary as, e.g., objects moving along complex trajectories. To process them, the visual system must segment them from the scene, estimate their position and direction over time and predict their future location and velocity. Second, each of these processing steps, from the retina to the highest cortical areas, is implemented by an intricate interplay of feed-forward, feedback and horizontal interactions1. Thus, at each stage, a moving object will not only be processed locally, but also generate a lateral propagation of information. Despite decades of motion processing research, it is still unclear how the early visual system processes motion trajectories. We, among others, have proposed that anisotropic diffusion of motion information in retinotopic maps can contribute resolving many of these difficulties25 13. Under this perspective, motion integration, anticipation and prediction would be jointly achieved through the interactions between feed-forward, lateral and feedback propagations within a common spatial reference frame, the retinotopic maps.&lt;/p&gt;
&lt;p&gt;Addressing this question is particularly challenging, as it requires to probe these sequences of events at multiple scales (from individual cells to large networks) and multiple stages (retina, primary visual cortex (V1)). ‚ÄúTRAJECTORY‚Äù proposes such an integrated approach. Using state-of-the-art micro- and mesoscopic recording techniques combined with modeling approaches, we aim at dissecting, for the first time, the population responses at two key stages of visual motion encoding: the retina and V1. Preliminary experiments and previous computational studies demonstrate the feasibility of our work. We plan three coordinated physiology and modeling work-packages aimed to explore two crucial early visual stages in order to answer the following questions: How is a translating bar represented and encoded within a hierarchy of visual networks and for which condition does it elicit anticipatory responses? How is visual processing shaped by the recent history of motion along a more or less predictable trajectory? How much processing happens in V1 as opposed to simply reflecting transformations occurring already in the retina?&lt;/p&gt;
&lt;p&gt;The project is timely because partners master new tools such as multi-electrode arrays and voltage-sensitive dye imaging for investigating the dynamics of neuronal populations covering a large segment of the motion trajectory, both in retina and V1. Second, it is strategic: motion trajectories are a fundamental aspect of visual processing that is also a technological obstacle in computer vision and neuroprostheses design. Third, this project is unique by proposing to jointly investigate retinal and V1 levels within a single experimental and theoretical framework. Lastly, it is mature being grounded on (i) preliminary data paving the way of the three different aims and (ii) a history of strong interactions between the different groups that have decided to join their efforts.&lt;/p&gt;
&lt;h2 id=&#34;the-marseille-team&#34;&gt;The Marseille team&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Fr√©d√©ric Chavane (DR, CNRS, NEOPTO team) is working in the field of vision research for about 20 years with a special interest in the role of lateral interactions in the integration of sensory input in the primary visual cortex. His recent work suggest that lateral interactions mediated by horizontal intracortical connectivity participates actively in the input normalization that controls a wide range of function, from the contrast-response gain to the representation of illusory or real motion. His expertise range from microscopic (intracellular recordings) to mesoscopic (optical imaging, multi-electrode array) recording scales in the primary visual cortex of anesthetized and awake behaving animals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Laurent Perrinet (CR, CNRS, NEOPTO team). His scientific interests focus on bridging computational understanding of neural dynamics and low-level sensory processing by focusing on motion perception. He is the author of papers in machine learning, computational neuroscience and behavioral psychology. One key concept is the use of statistical regularities from natural scenes as a main drive to integrate local neural information into a global understanding of the scene. In a recent paper that he coauthored (in Nature Neuroscience), he developed a method to use synthesized stimuli targeted to analyze physiological data in a system-identification approach.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ivo Vanzetta (CR, CNRS, NEOPTO team). His scientific interests focus on how to optimally use photonics-based imaging methods to investigate visual information processing in low-level visual areas, in the anesthetized and awake animal (rodent &amp;amp; primate). As can be seen from his bibliographic record, these methods include optical imaging of intrinsic signals and voltage sensitive dyes and, recently, 2 photon microscopy. Finally I. Vanzetta has an ongoing collaboration with L. Perrinet on the utilization of well-controlled, synthesized nature-like visual stimuli to probe the response characteristics of the primate&amp;rsquo;s visual system (Sanz-Leon &amp;amp; al. 2012).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;progress-meeting-anr-trajectory&#34;&gt;Progress meeting ANR TRAJECTORY&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Time    January 15th, 2018&lt;/li&gt;
&lt;li&gt;Location     INT&lt;/li&gt;
&lt;li&gt;General presentation of the grant, see 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-trajectory/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anr TRAJECTORY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Overview of my current projects    &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2017-11-15_ColloqueMaster.html&#34;&gt;https://laurentperrinet.github.io/sciblog/files/2017-11-15_ColloqueMaster.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MotionClouds with trajectories    &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2018-01-16-testing-more-complex-trajectories.html&#34;&gt;https://laurentperrinet.github.io/sciblog/posts/2018-01-16-testing-more-complex-trajectories.html&lt;/a&gt; or &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2018-11-13-testing-more-complex-trajectories.html&#34;&gt;https://laurentperrinet.github.io/sciblog/posts/2018-11-13-testing-more-complex-trajectories.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;






  



  
  











&lt;figure id=&#34;figure-a-predictive-sequence-is-essential-in-resolving-the-coherence-problem--the-sequence-in-which-a-set-of-local-motion-is-shown-is-essential-for-the-detection-of-global-motion-we-replicate-here-the-experiments-by-scott-watamaniuk-and-colleagues-they-have-shown-behaviourally-that-a-dot-in-noise-is-much-more-detectable-when-it-follows-a-coherent-trajectory-up-to-an-order-of-magnitude-of-10-times-what-would-be-predicted-by-the-local-components-of-the-trajectory-in-this--movie-we-observe-white-noise-and-at-first-sight-no-information-is-detectable-in-fact-there-is-a-dot-moving-along-some-smooth-linear-trajectory-since-this-is-compatible-with-a-predictive-sequence-it-is-much-easier-to-see-the-dot-from-left-to-right-in-the-top-of-the-image-a-smooth-pursuit-helps-to-catch-it-this-simple-experiment-shows-that-even-if-local-motion-is-similar-in-both-movies-a-coherent-trajectory-is-more-easy-to-track-obviously-we-may-thus-conclude-that-the-whole-trajectory-is-more-that-its-individual-parts-and-that-the-independence-hypothesis-does-not-hold-if-we-want-to-account-for-the-predictive-information-in-input-sequences-such-as-seems-to-be-crucial-for-the-ap&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/grant/anr-trajectory/sequence_ABCD_huc740b9c3e27c30fe69cba33adb5a642d_17610017_2000x2000_fit_lanczos.gif&#34; data-caption=&#34;&amp;lt;em&amp;gt;A predictive sequence is essential in resolving the coherence problem.&amp;lt;/em&amp;gt;  The sequence in which a set of local motion is shown is essential for the detection of global motion. we replicate here the experiments by Scott Watamaniuk and colleagues. They have shown behaviourally that a dot in noise is much more detectable when it follows a coherent trajectory, up to an order of magnitude of 10 times what would be predicted by the local components of the trajectory. In this  movie we observe white noise and at first sight, no information is detectable. In fact, there is a dot moving along some smooth linear trajectory. Since this is compatible with a predictive sequence, it is much easier to see the dot (from left to right in the top of the image, a smooth pursuit helps to catch it). This simple experiment shows that, even if local motion is similar in both movies, a coherent trajectory is more easy to track. Obviously, we may thus conclude that the whole trajectory is more that its individual parts, and that the independence hypothesis does not hold if we want to account for the predictive information in input sequences such as seems to be crucial for the AP.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/grant/anr-trajectory/sequence_ABCD_huc740b9c3e27c30fe69cba33adb5a642d_17610017_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;300&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;A predictive sequence is essential in resolving the coherence problem.&lt;/em&gt;  The sequence in which a set of local motion is shown is essential for the detection of global motion. we replicate here the experiments by Scott Watamaniuk and colleagues. They have shown behaviourally that a dot in noise is much more detectable when it follows a coherent trajectory, up to an order of magnitude of 10 times what would be predicted by the local components of the trajectory. In this  movie we observe white noise and at first sight, no information is detectable. In fact, there is a dot moving along some smooth linear trajectory. Since this is compatible with a predictive sequence, it is much easier to see the dot (from left to right in the top of the image, a smooth pursuit helps to catch it). This simple experiment shows that, even if local motion is similar in both movies, a coherent trajectory is more easy to track. Obviously, we may thus conclude that the whole trajectory is more that its individual parts, and that the independence hypothesis does not hold if we want to account for the predictive information in input sequences such as seems to be crucial for the AP.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work was supported by ANR project &amp;ldquo;TRAJECTORY&amp;rdquo; N¬∞ ANR-15-CE37-0011.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Art &lt;&gt; Sciences</title>
      <link>https://laurentperrinet.github.io/project/art-science/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/project/art-science/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DOC2AMU (2016/2019)</title>
      <link>https://laurentperrinet.github.io/grant/doc-2-amu/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/doc-2-amu/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://doc2amu.univ-amu.fr/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOC2AMU&lt;/a&gt; is co-funded by the prestigious Marie Sk≈Çodowska-Curie COFUND action within the H2020 Research and Innovation programme of the European Union and by the Regional Council of Provence-Alpes-C√¥te d‚ÄôAzur, with a contribution from A*MIDEX Foundation.&lt;/p&gt;
&lt;p&gt;Within this programme, the PhD fellows will sign a three-year work contract with one of the 12 Doctoral Schools of AMU. Numerous advantages&lt;/p&gt;
&lt;p&gt;These PhD fellowships are remunerated above that of a standard French PhD contract with a gross monthly salary of 2600 ‚Ç¨ and a gross monthly mobility allowance of 300 ‚Ç¨, which after standard deductions will amount to a net salary of approximately 1625‚Ç¨/month (net amount may vary slightly). A 500‚Ç¨ travel allowance per year and per fellow is also provided for the fellows to travel between Marseille and their place of origin. Tailored training and personalised mentoring: Fellows will define and follow a Personal Career Development Plan at the beginning of their Doctoral thesis and will have access to a variety of training options and workshops. Financial support for international research training and conferences participations. A contribution to the research costs will be provided for the benefit of the fellow.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This work was supported by the Doc2Amu project which received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 713750. Projet cofinanc√© par le Conseil R√©gional Provence-Alpes-C√¥te d‚ÄôAzur. Projet cofinanc√© par le Conseil R√©gional Provence-Alpes-C√¥te d‚ÄôAzur, la commission europ√©enne et les Investissements d&amp;rsquo;Avenir.&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Science</title>
      <link>https://laurentperrinet.github.io/project/open-science/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/project/open-science/</guid>
      <description>&lt;p&gt;To enable the dissemination of the knowledge that is produced in our lab, we share all source code with open source licences. This includes code to reproduce results obtained in papers (e.g. 
&lt;a href=&#34;https://github.com/laurentperrinet/PerrinetAdamsFriston14&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Perrinet, Adams and Friston, 2015)&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/laurentperrinet/PerrinetBednar15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Perrinet and Bednar, 2015)&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Khoei et, 2017)&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/laurentperrinet/2019-05_illusions-visuelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Perrinet, 2019)&lt;/a&gt;, 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;(Pasturel et al, 2020)&lt;/a&gt;, 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34;&gt;(Dauc√© et al, 2020)&lt;/a&gt;) or courses and slides (e.g. 
&lt;a href=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-04-03: vision and modelization&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/laurentperrinet/2019-04-18_JNLF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-04-18_JNLF&lt;/a&gt;, &amp;hellip;) and also the development of the following libraries on GitHub.&lt;/p&gt;
&lt;!-- Place this tag where you want the button to render. --&gt;
&lt;p&gt;&lt;a class=&#34;github-button&#34; href=&#34;https://github.com/laurentperrinet&#34; data-size=&#34;large&#34; data-show-count=&#34;true&#34; aria-label=&#34;Follow @laurentperrinet on GitHub&#34;&gt;Follow @laurentperrinet&lt;/a&gt;&lt;/p&gt;
&lt;!-- Place this tag in your head or just before your close body tag. --&gt;
&lt;script async defer src=&#34;https://buttons.github.io/buttons.js&#34;&gt;&lt;/script&gt;
&lt;h1 id=&#34;bayesianchangepoint&#34;&gt;bayesianchangepoint&lt;/h1&gt;
&lt;p&gt;An implementation of 
&lt;a href=&#34;http://arxiv.org/abs/0710.3742&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adams &amp;amp; MacKay 2007 &amp;ldquo;Bayesian Online Changepoint Detection&amp;rdquo;&lt;/a&gt; for binary inputs in Python.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/laurentperrinet/bayesianchangepoint&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See the final publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34;&gt;Pasturel, Montagnini and Perrinet (2020)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;anemo-quantitative-tools-for-the-analysis-of-eye-movements&#34;&gt;ANEMO: Quantitative tools for the ANalysis of Eye MOvements&lt;/h1&gt;
&lt;p&gt;This implementation proposes a set of robust fitting methods for the extraction of eye movements  parameters.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/invibe/ANEMO/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a poster @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-18-anemo/&#34;&gt;Pasturel, Montagnini and Perrinet (2018)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;lecheapeyetracker&#34;&gt;LeCheapEyeTracker&lt;/h1&gt;
&lt;p&gt;Work-in-progress : an eye tracker based on webcams.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/laurentperrinet/LeCheapEyeTracker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;biologically-inspired-computer-vision&#34;&gt;Biologically inspired computer vision&lt;/h1&gt;
&lt;h2 id=&#34;slip-a-simple-library-for-image-processing&#34;&gt;SLIP: a Simple Library for Image Processing&lt;/h2&gt;
&lt;p&gt;This library collects different Image Processing tools for use with the  
&lt;a href=&#34;https://pythonhosted.org/LogGabor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LogGabor&lt;/a&gt; and  
&lt;a href=&#34;https://pythonhosted.org/SparseEdges/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SparseEdges&lt;/a&gt; libraries.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://pythonhosted.org/SLIP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Web-site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/bicv/SLIP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;loggabor-a-simple-library-for-image-processing&#34;&gt;LogGabor: a Simple Library for Image Processing&lt;/h2&gt;
&lt;p&gt;This library collects different Image Processing tools for use with the  
&lt;a href=&#34;https://pythonhosted.org/LogGabor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LogGabor&lt;/a&gt; and  
&lt;a href=&#34;https://pythonhosted.org/SparseEdges/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SparseEdges&lt;/a&gt; libraries.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://pythonhosted.org/LogGabor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Web-site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/bicv/LogGabor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sparseedges-sparse-coding-of-natural-images&#34;&gt;SparseEdges: sparse coding of natural images&lt;/h2&gt;
&lt;p&gt;Our goal here is to build practical algorithms of sparse coding for computer vision.&lt;/p&gt;
&lt;p&gt;This class exploits the 
&lt;a href=&#34;https://pythonhosted.org/SLIP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SLIP&lt;/a&gt; and 
&lt;a href=&#34;https://pythonhosted.org/LogGabor/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LogGabor&lt;/a&gt; libraries to provide with a sparse representation of edges in images.&lt;/p&gt;
&lt;p&gt;This algorithm was presented in the following paper, which is available as a reprint @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-15-bicv/&#34;&gt;https://laurentperrinet.github.io/publication/perrinet-15-bicv/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://pythonhosted.org/SparseEdges&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Web-site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/bicv/SparseEdges/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sparsehebbianlearning--unsupervised-learning-of-natural-images&#34;&gt;SparseHebbianLearning : unsupervised learning of natural images&lt;/h2&gt;
&lt;p&gt;This is a collection of python scripts to test learning strategies to efficiently code natural image patches. This is here restricted to the framework of the SparseNet algorithm from Bruno Olshausen (&lt;a href=&#34;http://redwood.berkeley.edu/bruno/sparsenet/)&#34;&gt;http://redwood.berkeley.edu/bruno/sparsenet/)&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/bicv/SparseHebbianLearning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;motionclouds&#34;&gt;MotionClouds&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;MotionClouds&lt;/strong&gt; are random dynamic stimuli optimized to study motion perception.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://neuralensemble.github.io/MotionClouds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Web-site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/NeuralEnsemble/MotionClouds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;pynn&#34;&gt;PyNN&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;PyNN&lt;/strong&gt; is a simulator-independent language for building neuronal network models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://neuralensemble.github.io/PyNN/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Web-site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/NeuralEnsemble/PyNN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PhD ICN (2017 / 2021)</title>
      <link>https://laurentperrinet.github.io/grant/phd-icn/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/phd-icn/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;The 
&lt;a href=&#34;http://neuro-marseille.org/en/phd-program-en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ph.D. program in Integrative and Clinical Neuroscience&lt;/a&gt; (Aix-Marseille University) is offering in 2017 three Ph.D. scholarships to Master students graduated from highly ranked international universities (outside France). We were awarded with one PhD position for Angelo Franciosini at the &amp;ldquo;Institut de Neurosciences de la Timone&amp;rdquo; (team &amp;ldquo;Inference and Visual Behavior&amp;rdquo;), CNRS, Marseille (France) to study trajectories in natural images and the sensory processing of contours.&lt;/p&gt;
&lt;p&gt;##Funding&lt;/p&gt;
&lt;p&gt;This project is funded by the Aix-Marseille Universit√©, which was awarded the prestigious status of &amp;ldquo;Excellence Initiative&amp;rdquo; (A*MIDEX) by the French Government and considering interdisciplinary studies as one of its main axes of growth. Within this program, the PhD fellow will sign a three-year work contract. They will enroll the ICN PhD program offering personalized follow-up to the students, a wide spectrum of scientific and professional training activities including specialized courses and career development activities and interactions with multi-disciplinary researchers at Aix-Marseille University and top world-wide visiting speakers, in a vibrant international community of students.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work was supported by the Ph.D. program in Integrative and Clinical Neuroscience (formerly &amp;ldquo;Ph.D. program in Integrative and Clinical Neuroscience&amp;rdquo;). It received funding by the Aix-Marseille Universit√© through the prestigious status of &amp;ldquo;Excellence Initiative&amp;rdquo; (A*MIDEX) awarded by the French Government.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tout public!</title>
      <link>https://laurentperrinet.github.io/project/tout-public/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/project/tout-public/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau</title>
      <link>https://laurentperrinet.github.io/talk/2016-04-25-polly-maggoo/</link>
      <pubDate>Mon, 25 Apr 2016 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2016-04-25-polly-maggoo/</guid>
      <description>&lt;h1 id=&#34;les-illusions-visuelles-un-r√©v√©lateur-du-fonctionnement-de-notre-cerveau&#34;&gt;Les illusions visuelles, un r√©v√©lateur du fonctionnement de notre cerveau&lt;/h1&gt;
&lt;h2 id=&#34;cin√©sciences-coll√®ge-clair-soleil&#34;&gt;Cin√©sciences, coll√®ge Clair Soleil&lt;/h2&gt;
&lt;p&gt;L&amp;rsquo;Association Polly Maggoo &lt;a href=&#34;http://www.pollymaggoo.org/&#34;&gt;http://www.pollymaggoo.org/&lt;/a&gt; met en place
tout le long de l‚Äôann√©e, des actions de culture scientifique et
artistique en direction des coll√®ges et des lyc√©es, les &lt;em&gt;Cin√©sciences&lt;/em&gt;,
au cours desquelles l&amp;rsquo;association programme des films √† caract√®re
scientifique, au sein d‚Äô√©tablissements scolaires. Les projections se
d√©roulent en pr√©sence de chercheurs et/ou de cin√©astes dans la
perspective d‚Äôun d√©veloppement de la culture cin√©matographique et
scientifique en direction des publics scolaires.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date&lt;br&gt;
25 Avril 2016&lt;/li&gt;
&lt;li&gt;Location&lt;br&gt;
coll√®ge Clair Soleil, Marseille&lt;/li&gt;
&lt;li&gt;Visuels&lt;br&gt;

&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2016-04-25_pollymagoo/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Elasticit√© dynamique</title>
      <link>https://laurentperrinet.github.io/post/2016-06-02_elasticite/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2016-06-02_elasticite/</guid>
      <description>&lt;h1 id=&#34;elasticit√©-dynamique---fondation-vasarely-√†-aix-en-provence&#34;&gt;Elasticit√© dynamique @  Fondation Vasarely √† Aix-en-Provence&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/198189587&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;2016, &lt;a href=&#34;https://github.com/NaturalPatterns/elasticite&#34;&gt;https://github.com/NaturalPatterns/elasticite&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;L&amp;rsquo;installation &amp;lsquo;&amp;lsquo;Elasticit√© dynamique&amp;rsquo;&amp;rsquo; agit comme un filtre et g√©n√®re de nouveaux espaces d√©multipli√©s, comme un empilement quasi infini d&amp;rsquo;horizons.
Par principe de r√©flexion, la pi√®ce absorbe l&amp;rsquo;image de l&amp;rsquo;environnement et accumule les points de vue ; le mouvement permanent requalifie continuellement ce qui est regard√© et entendu.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DIMENSIONS : 3 m de haut 5 m de large, INOX POLI MIROIR / ALUMINIUM / ACIER / MOTEURS / PROGRAMME TEMPS R√âEL&lt;/li&gt;
&lt;li&gt;LIEU : Fondation Vasarely&lt;/li&gt;
&lt;li&gt;EXPOSITION : 
&lt;a href=&#34;http://ondesparalleles.org/projets/trame-elasticite-vasarely/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multiplicit√©, Fondation Vasarely dans le cadre de l‚ÄôHommage √† Victor Vasarely&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EXPOSITION : DU 2 juin au 2 octobre 2016&lt;/li&gt;
&lt;li&gt;VIDEOS : &lt;a href=&#34;http://vimeo.com/198189587&#34;&gt;http://vimeo.com/198189587&lt;/a&gt; Cr√©dits :  ¬© Etienne Rey&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Compos√© d‚Äôune succession de lames de miroirs, verticales et rotatives, l‚Äôinstallation Trame se joue des reflets et de la d√©multiplication de l‚Äôespace, offrant au spectateur une multiplicit√© de points de vue dans lesquels il peut se perdre √† loisir. Par un effet de ¬´ porosit√© ¬ª recherch√© par l‚Äôartiste, le dispositif dialogue intens√©ment avec les Int√©grations.&lt;/p&gt;
&lt;p&gt;Devant l‚Äô≈ìuvre en constante m√©tamorphose, l‚Äôalphabet plastique de Vasarely se recompose ainsi √† l‚Äôinfini comme un jeu de construction renouvelable. Dans cette ≈ìuvre, Etienne Rey explore en profondeur les possibilit√©s offertes par le mouvement, la lumi√®re, et surtout l‚Äôinteraction entre l‚Äô≈ìuvre, le public et l‚Äôespace, ouvrant sur de nouveaux rapports sensibles et sensoriels au monde.&lt;/p&gt;
&lt;p&gt;(Texte : V√©ronique Baton)&lt;/p&gt;
&lt;h1 id=&#34;elasticit√©-dynamique---104-paris&#34;&gt;Elasticit√© dynamique @  104 (Paris)&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/150813922&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;LIEU:: NEMO, BIENNALE INTERNATIONALE DES ARTS NUMERIQUES - CENTQUATRE - 104&lt;/li&gt;
&lt;li&gt;EXPOSITION : 
&lt;a href=&#34;http://www.104.fr/programmation/evenement.html?evenement=518&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prosopop√©es : Quand les objets prennent vie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VERNISSAGE : SAMEDI 5 D√âCEMBRE 14h &amp;gt; 23h30&lt;/li&gt;
&lt;li&gt;EXPOSITION : DU 6 D√âCEMBRE 2015 AU 18 JANVIER 2016&lt;/li&gt;
&lt;li&gt;VIDEOS : &lt;a href=&#34;https://vimeo.com/150654250&#34;&gt;https://vimeo.com/150654250&lt;/a&gt; : installation;  &lt;a href=&#34;https://vimeo.com/146242233&#34;&gt;https://vimeo.com/146242233&lt;/a&gt; : simulation, Cr√©dits : Elasticit√© dynamique ¬© Etienne Rey, Adagp Paris 2015&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UvE3ysXieSk&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&#34;http://www.lafriche.org/public_data/diapo/resident/1454686884/desk/2._elasticite_dynamique-etienne_rey-photoquentin_chevrier_pour_art2m_et_arcadi_ile_de_france.jpg&#34; alt=&#34;arcadi&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Elasticit√© dynamique est compos√©e des pi√®ces Expansion, Trame et Lignes sonores. Volume hexagonal en miroir de 7 m√®tres de diam√®tre, Expansion fonctionne comme une chambre d&#39;√©cho. A l&#39;int√©rieur de ce volume se situe Trame. Constitu√©e de 25 lames de miroir en rotation, cette pi√®ce r√©oriente continuellement le regard. Quant √† Lignes sonores, elle est form√©e de quatre monolithes orient√©s vers Expansion et √©met des sons qui se r√©orientent en fonction du mouvement des lames. (¬© Etienne Rey, Adagp Paris 2015)&#39;&#39;|width=&amp;quot;100%&amp;quot;}}Elasticit√© dynamique est compos√©e des pi√®ces Expansion, Trame et Lignes sonores. &amp;lt;&amp;lt;BR&amp;gt;&amp;gt; Volume hexagonal en miroir de 7 m√®tres de diam√®tre, Expansion fonctionne comme une chambre d&#39;√©cho. A l&#39;int√©rieur de ce volume se situe Trame. Constitu√©e de 25 lames de miroir en rotation, cette pi√®ce r√©oriente continuellement le regard. Quant √† Lignes sonores, elle est form√©e de quatre monolithes orient√©s vers Expansion et √©met des sons qui se r√©orientent en fonction du mouvement des lames. &amp;lt;&amp;lt;BR&amp;gt;&amp;gt;(¬© Etienne Rey, Adagp Paris 2015)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;equipe&#34;&gt;EQUIPE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Etienne Rey : Artiste plasticien&lt;/li&gt;
&lt;li&gt;Wilfried Wendling : Compositeur&lt;/li&gt;
&lt;li&gt;Laurent Perrinet : Chercheur en Neurosciences √† l‚ÄôINT / CNRS-AMU&lt;/li&gt;
&lt;li&gt;Atelier Ni : Accompagnement conception&lt;/li&gt;
&lt;li&gt;Gauthier Le Rouzic : √âlectronique&lt;/li&gt;
&lt;li&gt;Lucie Evans : Assistante&lt;/li&gt;
&lt;li&gt;Remerciements : Guillaume Stagnaro&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;production-d√©l√©gu√©e&#34;&gt;PRODUCTION D√âL√âGU√âE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Seconde Nature&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;soutiens&#34;&gt;SOUTIENS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DRAC PACA _aide individuelle √† la cr√©ation&lt;/li&gt;
&lt;li&gt;REGION PACA _CAC art visuel&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;co-production&#34;&gt;CO-PRODUCTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ARCADI&lt;/li&gt;
&lt;li&gt;La Muse en Circuit. Centre National de Cr√©ation Musicale&lt;/li&gt;
&lt;li&gt;CNRS-AMU / INT, Institut de Neurosciences de la Timone&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Instabilit√© (series) @ Art-O-Rama</title>
      <link>https://laurentperrinet.github.io/post/2018-09-09_artorama/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-09-09_artorama/</guid>
      <description>&lt;h1 id=&#34;instabilit√©-series&#34;&gt;Instabilit√© (series)&lt;/h1&gt;
&lt;h2 id=&#34;installation-s√©rigraphie-dessin-mural-lumi√®re-2018--guest-artist-m√©c√®nes-du-sud--art-o-rama-fair-marseille-i-2018&#34;&gt;Installation (s√©rigraphie, dessin mural, lumi√®re), 2018;  Guest artist, M√©c√®nes du Sud / Art-O-Rama (Fair), Marseille I 2018&lt;/h2&gt;
&lt;p&gt;M√©c√®nes du Sud invite chaque ann√©e un artiste laur√©at pour concevoir un stand-projet au sein du salon d‚Äôart contemporain ART-O-RAMA  : &amp;quot; Le travail d&amp;rsquo;Etienne Rey, laur√©at 2011, explore la notion m√™me d&amp;rsquo;espace. Il d√©tourne des ph√©nom√®nes physiques pour faire surgir par des biais perceptifs une conscience &amp;ldquo;d&amp;rsquo;√™tre l√†&amp;rdquo;. Ses installations sont exp√©rientielles par nature. Elles ne proposent pas une exp√©rience elles la contiennent. Ce sont des oeuvres actives qui impliquent pr√©sence et espace. Instabilit√©s, l&amp;rsquo;ensemble pr√©sent√© ici, issu de recherches en cours de d√©veloppement, met en tension espace et temps dans une exp√©rience vibratoire. Chaos et cristal attisent des forces instables dans un rapport de contraintes qui d√©termine la complexit√© des oeuvres pr√©sent√©es.&amp;quot; B√©n√©dicte Chevallier&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En collaboration avec le chercheur Laurent Perrinet, CNRS-AMU / Institut de Neurosciences de la Timone&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;links--liens&#34;&gt;Links / Liens:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ART-O-RAMA - &lt;a href=&#34;http://art-o-rama.fr/en&#34;&gt;http://art-o-rama.fr/en&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lemonde.fr/argent/article/2018/08/19/avec-art-o-rama-marseille-se-demarque-sur-le-marche-de-l-art_5343904_1657007.html&#34;&gt;https://www.lemonde.fr/argent/article/2018/08/19/avec-art-o-rama-marseille-se-demarque-sur-le-marche-de-l-art_5343904_1657007.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.apollo-magazine.com/why-manifesta-makes-sense-in-marseille/&#34;&gt;https://www.apollo-magazine.com/why-manifesta-makes-sense-in-marseille/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Retour par 
&lt;a href=&#34;https://www.enrevenantdelexpo.com/2018/09/07/retour-sur-art-o-rama-2018-j1-marseille-1er-partie/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;En revenant de l&amp;rsquo;expo !&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://i0.wp.com/www.enrevenantdelexpo.com/wp-content/uploads/2018/09/Art-O-Rama-2018-M%C3%A9c%C3%A8nes-du-Sud-%C3%89ienne-Rey-01.jpg&#34; alt=&#34;Sortie mod√®le OptimalPacking&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SALON INTERNATIONAL D&amp;rsquo;ART CONTEMPORAIN ART-O-RAMA&lt;/li&gt;
&lt;li&gt;SALON : 31 AO√õT &amp;gt; 2 SEPTEMBRE 2018&lt;/li&gt;
&lt;li&gt;EXPOSITION &amp;gt; 9 SEPTEMBRE 2018&lt;/li&gt;
&lt;li&gt;ESPACE PARTENAIRES - J1, QUAI DE LA JOLIETTE, MARSEILLE 2e&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Proverbes Et Citations</title>
      <link>https://laurentperrinet.github.io/post/proverbes-et-citations/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/proverbes-et-citations/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Science is like sex: sometimes something useful comes out, but that is not the reason we are doing it&amp;rdquo; &amp;ndash; (Richard Feynman)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Mathematics is no more computation than typing is literature.&amp;rdquo; (John Allen Paulos)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;C&amp;rsquo;est ce que je fais qui m&amp;rsquo;apprend ce que je cherche.&amp;rdquo; Pierre Soulages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚Äú&lt;strong&gt;Harry Potter:&lt;/strong&gt; Is this real? Or has this been happening inside my head?  &lt;strong&gt;Professor Albus Dumbledore:&lt;/strong&gt; Of course it is happening inside your head, Harry, but why on earth should that mean that it is not real?‚Äù ‚Äï (J.K. Rowling, 
&lt;a href=&#34;https://www.goodreads.com/work/quotes/2963218&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Harry Potter and the Deathly Hallows&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;I do know what time is,&amp;rdquo; Tubby declared. He paused. &amp;ldquo;Time,&amp;rdquo; he added slowly &amp;ndash; &amp;ldquo;time is what keeps everything from happening at once. I know that&amp;ndash;I seen it in print too.&amp;rdquo; (Ray Cummings)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;You don‚Äôt see it because it‚Äôs there, it‚Äôs there because you see it.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Ni rire, ni pleurer, ni haiÃàr, mais comprendre&amp;rdquo; (Baruch Spinoza)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;For years there has been a theory that millions of monkeys typing at random on millions of typewriters would reproduce the entire works of Shakespeare. The Internet has proven this theory to be untrue.&amp;rdquo; (???, ???)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Je me suis endormie, ce matin/en pensant/sur tes l√®vres.&amp;rdquo;  (Anonyme, sur les murs, Marseille, 2017)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;If you can look into the seeds of time, And say which grain will grow and which will not; Speak&amp;hellip;&amp;rdquo; (Shakespeare, Macbeth, Act I, Scene 3)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Here, too, the honorable finds its due, and there are tears for passing things; here, too, things mortal touch the mind.&amp;rdquo; (Virgil, Aeneid, 29-19 BC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Man&amp;rsquo;s maturity is to have regained the seriousness that he had as a child at play.&amp;rdquo; (Friedrich Nietzsche)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;There are 10 types of people in the world. Those who understand binary, those who don&amp;rsquo;t, those who weren&amp;rsquo;t expecting a base 8 joke, and 5 other types of people.&amp;rdquo; 
&lt;a href=&#34;http://unix.stackexchange.com/questions/178162/why-does-bash-think-016-1-15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Story&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Ce qui fut se refait; tout coule comme une eau / Et rien dessous le Ciel ne se voit de nouveau/ Mais la forme se change en une autre nouvelle/ Et ce changement-l√†. Vivre au monde s&amp;rsquo;appelle&amp;rdquo; - Ronsard, Hymnes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Hanlon&amp;rsquo;s Razor: Never attribute to malice what is adequately explained by stupidity.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Essentially, all models are wrong, but some are useful.&amp;rdquo;  Box, George E. P.; Norman R. Draper (1987). Empirical Model-Building and Response Surfaces, p. 424, Wiley. ISBN 0471810339&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Computers were around for 50 years before we figured out how to create the internet for example.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In sum, the physicist can never subject an isolated hypothesis to experimental test, but only a whole group of hypotheses; when the experiment is in disagreement with his predictions, what he learns is that at least one of the hypotheses constituting this group is unacceptable and ought to be modified; but the experiment does not designate which one should be changed.&amp;rdquo; (P. Duhem, The Aim and Structure of Physical Theory, 1914)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Un bon ma√Ætre a ce souci constant : enseigner √† se passer de lui.&amp;rdquo; - Andr√© Gide&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚ÄúTruth in science can be defined as the working hypothesis best suited to open the way to the next better one.‚Äù(Konrad Lorenz)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚Äúall motion is illusion‚Äù (Zeno of Elea, 490 BC )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Be humble for you are made of dung. Be noble for you are made of stars.&amp;rdquo; (Serbian proverb)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;voir des objets ne consiste pas √† en extraire des traits visuels, mais √† guider visuellement l&amp;rsquo;action dirig√©e vers eux.&amp;rdquo;  (Francisco Varela in &amp;lsquo;&amp;lsquo;L&amp;rsquo;inscription corporelle de l&amp;rsquo;esprit&amp;rsquo;&#39;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;la m√©moire n‚Äôest pas faite pour se rappeler du pass√© mais pour pr√©dire le futur.&amp;rdquo; (
&lt;a href=&#34;http://dpea-archi.philo.over-blog.com/article-interview-de-alain-berthoz-par-thierry-paquot-61601814.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alain Berthoz&lt;/a&gt; , 2010)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the days when the Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. &amp;ldquo;What are you doing?&amp;rdquo;, asked Minsky. &amp;ldquo;I am training a randomly wired neural net to play Tic-tac-toe&amp;rdquo;, Sussman replied. &amp;ldquo;Why is the net wired randomly?&amp;rdquo;, asked Minsky. &amp;ldquo;I do not want it to have any preconceptions of how to play&amp;rdquo;, Sussman said. Minsky then shut his eyes. &amp;ldquo;Why do you close your eyes?&amp;rdquo; Sussman asked his teacher. &amp;ldquo;So that the room will be empty.&amp;rdquo; At that moment, Sussman was enlightened.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;It doesn&amp;rsquo;t matter how beautiful your theory is, it doesn&amp;rsquo;t matter how smart you are. If it doesn&amp;rsquo;t agree with experiment, it&amp;rsquo;s wrong.&amp;rdquo; Richard P. Feynman&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Think of the image of the world in a convex mirror. &amp;hellip; A well-made convex mirror of moderate aperture represents the objects in front of it as apparently solid and in fixed positions behind its surface. But the images of the distant horizon and of the sun in the sky lie behind the mirror at a limited distance, equal to its focal length. Between these and the surface of the mirror are found the images of all the other objects before it, but the images are diminished and flattened in proportion to the distance of their objects from the mirror. &amp;hellip; Yet every straight line or plane in the outer world is represented by a straight line or plane in the image. The image of a man measuring with a rule a straight line from the mirror, would contract more and more the farther he went, but with his shrunken rule the man in the image would count out exactly the same results as in the outer world, all lines of sight in the mirror would be represented by straight lines of sight in the mirror. In short, I do not see how men in the mirror are to discover that their bodies are not rigid solids and their experiences good examples of the correctness of Euclidean axioms. But if they could look out upon our world as we look into theirs without overstepping the boundary, they must declare it to be a picture in a spherical mirror, and would speak of us just as we speak of them; and if two inhabitants of the different worlds could communicate with one another, neither, as far as I can see, would be able to convince the other that he had the true, the other the distorted, relation. Indeed I cannot see that such a question would have any meaning at all, so long as mechanical considerations are not mixed up with it.&amp;rdquo; ‚Äî Hermann von Helmholtz In &amp;lsquo;On the Origin and Significance of Geometrical Axioms,&amp;quot; Popular Scientific Lectures&amp;lt; Second Series (1881), 57-59. In Robert  Moritz, Memorabilia Mathematica (1914), 357-358.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Whoever, in the pursuit of science, seeks after immediate practical utility, may generally rest assured that he will seek in vain.&amp;rdquo; ‚Äî Hermann von Helmholtz, Edmund Atkinson (trans.), Popular Lectures on Scientific Subjects: First Series (1883), 29&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;¬´ Au d√©part, l‚Äôart du puzzle semble un art bref, un art mince, tout entier contenu dans un maigre enseignement de la Gestalttheorie : l‚Äôobjet vis√© ‚Äî qu‚Äôil s‚Äôagisse d‚Äôun acte perceptif, d‚Äôun apprentissage, d‚Äôun syst√®me physiologique ou, dans le cas qui nous occupe, d‚Äôun puzzle en bois ‚Äî n‚Äôest pas une somme d‚Äô√©l√©ments qu‚Äôil faudrait d‚Äôabord isoler et analyser, mais un ensemble, c‚Äôest √† dire une forme, une structure : l‚Äô√©l√©ment ne pr√©existe pas √† l‚Äôensemble, il n‚Äôest ni plus imm√©diat ni plus ancien, ce ne sont pas les √©l√©ments qui d√©terminent l‚Äôensemble, mais l‚Äôensemble qui d√©termine les √©l√©ments : la connaissance du tout et de ses lois, de l‚Äôensemble et de sa structure, ne saurait √™tre d√©duite de la connaissance s√©par√©e des parties qui le composent : cela veut dire qu‚Äôon peut regarder une pi√®ce d‚Äôun puzzle pendant trois jours et croire tout savoir de sa configuration et de sa couleur sans avoir le moins du monde avanc√© : seule compte la possibilit√© de relier cette pi√®ce √† d‚Äôautres pi√®ces et, en ce sens, il y a quelque chose de commun entre l‚Äôart du puzzle et l‚Äôart du go ; seules les pi√®ces rassembl√©es prendront un caract√®re lisible, prendront un sens : consid√©r√©e isol√©ment, une pi√®ce d‚Äôun puzzle ne veut rien dire ; elle est seulement question impossible, d√©fi opaque ; mais √† peine a-t-on r√©ussi, au terme de plusieurs minutes d‚Äôessais et d‚Äôerreurs, ou en une demi-seconde prodigieusement inspir√©e, √† la connecter √† l‚Äôune de ses voisines, que la pi√®ce dispara√Æt, cesse d‚Äôexister en tant que pi√®ce : l‚Äôintense difficult√© qui a pr√©c√©d√© ce rapprochement, et que le mot puzzle ‚Äî √©nigme ‚Äî d√©signe si bien en anglais, non seulement n‚Äôa plus de raison d‚Äô√™tre, mais semble n‚Äôen avoir jamais eu, tant elle est devenue √©vidence : les deux pi√®ces miraculeusement r√©unies n‚Äôen font plus qu‚Äôune, √† son tour source d‚Äôerreur, d‚Äôh√©sitation, de d√©sarroi et d‚Äôattente. ¬ª (PEREC Georges, La vie mode d‚Äôemploi, √âditions Hachette, 1978, pp. 235-236.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;there&amp;rsquo;s really a trend toward ATLs ( Acronyms of Three Letters).&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In theory there is no difference between theory and practice. In practice there is.&amp;rdquo; Yogi Berra&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Les d√©corations, c&amp;rsquo;est comme les bombes, √ßa tombe toujours sur ceux qui ne les m√©ritent pas&amp;rdquo; 
&lt;a href=&#34;http://www.lemonde.fr/societe/article/2010/03/22/proces-viguier-deux-hommes-en-colere_1322481_3224_1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Me Dupond-Moretti&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Science is what we understand well enough to explain to a computer. Art is everything else we do.&amp;rdquo; D. Knuth, foreword to &amp;ldquo;A=B&amp;rdquo; (1995)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;les cons √ßa ose tout et c&amp;rsquo;est m√™me √† √ßa qu&amp;rsquo;on les reconna√Æt&amp;rdquo; (Audiard)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;All generalisations are dangerous, including this one.&amp;rdquo; (Alexandre Dumas)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Il vaut mieux mobiliser son intelligence sur des conneries que mobiliser sa connerie sur des choses intelligentes.&amp;rdquo; Jacques Rouxel&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Nothing in biology makes sense except in the light of evolution&amp;rdquo;. (Theodosius Dobzhansky, 1900‚Äì1975)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Copy from one, it‚Äôs plagiarism; copy from two, it‚Äôs research.&amp;rdquo; (Wilson Mizner)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In the past the man has been first, in the future the system must be first.&amp;rdquo; (F. Taylor)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;L&amp;rsquo;autorit√© n&amp;rsquo;admet que deux r√¥les : le bourreau et la victime, transforme les gens en poup√©es qui ne connaissent plus que peur et haine, tandis que la culture plonge dans les abysses. L&amp;rsquo;autorit√© d√©forme ses enfants et change leur amour en un combat de coq&amp;hellip; L&amp;rsquo;effondrement de l&amp;rsquo;autorit√© aura des r√©percussions sur le bureau, l&amp;rsquo;√©glise et l&amp;rsquo;√©cole. Tout est li√©. L&amp;rsquo;√©galit√© et la libert√© ne sont pas des luxes que l&amp;rsquo;on √©carte impun√©ment. Sans ceux-ci, l&amp;rsquo;ordre ne peut survivre longtemps sans se rapprocher de profondeurs inimaginables.&amp;rdquo; Alan Moore, 
&lt;a href=&#34;http://fr.wikipedia.org/wiki/V_pour_Vendetta&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;V pour Vendetta&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In a widely circulated joke [from the days of the first computer], a group of engineers assemble the most powerful computer that had ever been conceived and ask it the ultimate question: Is there a God? After several tense minutes of clicking and clacking and flashing of lights, a card pops out which reads: There is &amp;lsquo;&amp;lsquo;now&amp;rsquo;&amp;rsquo;.&amp;rdquo; (Alwyn Scott in &amp;lsquo;&amp;lsquo;How Smart is a Neuron?&amp;rsquo;&amp;rsquo; in A Review of Christof Kochs&amp;rsquo; &amp;lsquo;&amp;lsquo;Biophysics of Computation&amp;rsquo;&amp;rsquo; )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Que les cons le restent!&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Those who can &amp;ndash; do. Those who can&amp;rsquo;t &amp;ndash; teach. (H.L. Mencken). Those who cannot teach &amp;ndash; administrate. (Martin)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under capitalism, man exploits man. Under communism, it&amp;rsquo;s just the opposite. (John Kenneth Galbraith)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Love is the triumph of imagination over intelligence. (H. L. Mencken)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The power of accurate observation is commonly called cynicism by those who have not got it. (George Bernard Shaw)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remember, beneath every cynic there lies a romantic, and probably an injured one. (Glenn Beck)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;If we see the light at the end of the tunnel, it&amp;rsquo;s the light of an oncoming train.&amp;rdquo; &amp;ndash; Robert Lowell&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius &amp;ndash; and a lot of courage &amp;ndash; to move in the opposite direction&amp;rdquo;  &amp;ndash; Albert Einstein&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Je sais ce que je crois. Je continuerais √† exprimer ce que je crois, et ce que je crois&amp;hellip; je crois que ce que je crois est bien.&amp;quot;- GWB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Soudain, je ne sais comment, le cas fut subit, je n&amp;rsquo;eus loisir de le consid√©rer, Panurge, sans autre chose dire, jette en pleine mer son mouton criant et b√™lant. Tous les autres moutons, criant et b√™lant en pareille intonation, commenc√®rent √† se jeter et √† sauter en mer apr√®s, √† la file. La foule √©tait √† qui le premier y sauterait apr√®s leur compagnon. Il n&amp;rsquo;√©tait pas possible de les en emp√™cher, comme vous savez du mouton le naturel, toujours suivre le premier, quelque part qu&amp;rsquo;il aille.&amp;rdquo; Rabelais&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Personne ne peut dire dans quel but l&amp;rsquo;homme a √©t√© amen√© en ce monde, ni quelle sera la destin√©e de l&amp;rsquo;esp√®ce. Cependant, il y a de grands esprits qui ne vivent pas pour leur bien-√™tre pr√©sent mais en vue d&amp;rsquo;une fin impersonnelle, comme un coureur qui s&amp;rsquo;√©puiserait dans une course de relais, pour un troph√©e qu&amp;rsquo;il ignore et qu&amp;rsquo;un autre remportera.&amp;rdquo; Charles Morgan, Essai sur l&amp;rsquo;unit√© de l&amp;rsquo;esprit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Science sans conscience n&amp;rsquo;est que ruine de l&amp;rsquo;√¢me.&amp;rdquo;  (Rabelais, 1532)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚ÄúLinux is only free if your time has no value.‚Äù (Jamie Zawinski)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Napol√©on : Monsieur de Laplace, je ne trouve pas dans votre syst√®me mention de Dieu  ?    Laplace : Sire, je n&amp;rsquo;ai pas eu besoin de cette hypoth√®se. (d&amp;rsquo;autres savants ayant d√©plor√© que Laplace fasse l&amp;rsquo;√©conomie d&amp;rsquo;une hypoth√®se qui avait justement &amp;ldquo;le m√©rite d&amp;rsquo;expliquer tout&amp;rdquo;, Laplace r√©pondit cette fois-ci √† l&amp;rsquo;Empereur :    Laplace : Cette hypoth√®se, Sire, explique en effet tout, mais ne permet de pr√©dire rien. En tant que savant, je me dois de vous fournir des travaux permettant des pr√©dictions &amp;quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;The supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience&amp;rdquo; often paraphrased as &amp;ldquo;Theories should be as simple as possible, but no simpler.&amp;rdquo; Albert Einstein (1933)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;The surest sign of the existence of extra- terrestrial intelligence is that they never bothered to come down here and visit us!&amp;rdquo; Calvin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;¬´Si tu manges le fruit d&amp;rsquo;un grand arbre, n&amp;rsquo;oublie jamais de remercier le vent!¬ª &amp;lsquo;&amp;lsquo;tradition orale bambara au Mali&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Lorsque vous avez √©limin√© l‚Äôimpossible, ce qui reste, si improbable soit-il, est n√©cessairement la v√©rit√©.&amp;rdquo; - Arthur Conan Doyle - &amp;lsquo;&amp;lsquo;Le signe des Quatre&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;I have come to believe that the whole world is an enigma, a harmless enigma that is made terrible by our own mad attempt to interpret it as though it had an underlying truth.&amp;rdquo;  &amp;ndash; Umberto Eco&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nos id√©es doivent √™tre aussi vastes que la nature pour pouvoir en rendre compte.&amp;rdquo; 
&lt;a href=&#34;http://www.evene.fr/citations/auteur.php?ida=813&amp;amp;celebrite=arthur-conan-doyle%7c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arthur Conan Doyle&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Tout le monde savait que ce truc l√† √©tait impossible a faire. Jusqu&amp;rsquo;au jour ou est arriv√© quelqu&amp;rsquo;un qui ne le savait pas, et qui l&amp;rsquo;a fait.&amp;rdquo; Winston Churchill&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Ce proverbe, je l&amp;rsquo;ai sur le bout de la&amp;hellip; &amp;quot; &amp;ndash; Manu (2006-01-23T19:34:16Z)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;When you can measure what you are speaking about and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of the meager and unsatisfactory kind.&amp;rdquo; &amp;lsquo;&amp;lsquo;Lord Kelvin&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I know you believe you understand what you think I said, but I am not sure you realize that what you heard is not what I meant. &amp;lsquo;&amp;lsquo;Richard Nixon&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try to understand everything, but believe nothing! &amp;lsquo;&amp;lsquo;Unknown&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;George Walker Bush a propos de ses d√©mentis sur la coca√Øne, r√©pond ¬´ I haven&amp;rsquo;t denied anything. ¬ª&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bayes maxim : &amp;ldquo;condition the joint probability on what we know and marginalize on what we don&amp;rsquo;t care&amp;rdquo; John Coughlan (in prob. models / kersten)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Monty Python&amp;rsquo;s &amp;ldquo;Life of Brian&amp;rdquo;: {{{(Brian)-&amp;ldquo;You are all individuals!&amp;rdquo; (crowd)-&amp;ldquo;We are all individuals!&amp;quot;(Brian)-&amp;ldquo;You have to be different!&amp;rdquo; (crowd)-&amp;ldquo;Yes, we are all different!&amp;rdquo; (loner)-&amp;ldquo;I&amp;rsquo;m not.&amp;quot;}}}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Monty Python&amp;rsquo;s &amp;ldquo;Life of Brian&amp;rdquo;: (Brian:) &amp;ldquo;You have to work it out for yourselves!&amp;rdquo; (Crowd:) &amp;ldquo;Yes, we have to work it out for ourselves&amp;hellip; (silence) Tell us more!&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Combien d&amp;rsquo;autres corps c√©lestes, outre ces com√®tes, se meuvent en secret sans jamais se montrer aux yeux des hommes. Dieu n&amp;rsquo;a pas fait toutes les choses pour l&amp;rsquo;homme.&amp;rdquo; S√©n√©que&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hawking&amp;rsquo;s principle for popularizations : &amp;ldquo;each math symbol reduces the potential readership by a factor r = 1/2&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Les ordinateurs sont trop fiables pour remplacer rellement les humains.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction is very difficult, especially about the future. Niels Bohr&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On peut vous le faire :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bien&lt;/li&gt;
&lt;li&gt;vite&lt;/li&gt;
&lt;li&gt;pour peu cher
Ne choisissez pas plus de deux options.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;s. v..s p..v.z c.mpr.ndr. c.l. v..s p..v.z .tr. d.v.l.pp√©&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Intelligence artificielle : art de programmer les ordinateurs de sorte qu&amp;rsquo;ils se comportent comme ils le font dans les films.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hypoaristerolactoth√©rapie : m√©thode de d√©pannage des machines par le coup de pied en bas √† gauche.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Le peu que je sais , c&amp;rsquo;est √† mon ignorance que je le dois .GUITRY , Sacha&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TOULET , Paul-Jean &amp;ldquo;Apprends √†te conna√Ætre : tu t&amp;rsquo;aimeras moins . Et √† conna√Ætre les autres , tu ne les aimeras plus.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Occam&amp;rsquo;s razor : &amp;ldquo;Accept the simplest explanation that fits the data.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;On two occasions, I have been asked [by members of Parliament], &amp;lsquo;Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?&amp;rsquo; I am not able to rightly apprehend the kind of confusion of ideas that could provoke such a question.&amp;quot;&amp;ndash; Charles Babbage (1791-1871)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Reality is that which, when you stop believing in it, doesn&amp;rsquo;t go away&amp;rdquo;. Philip K. Dick&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Puisque ce d√©sordre nous √©chappe, feignons d&amp;rsquo;en √™tre l&amp;rsquo;organisateur. &amp;quot; Cocteau&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Non seulement la solution n&amp;rsquo;existe pas, mais en plus elle n&amp;rsquo;est pas unique.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Detection is, or ought to be, an exact science, and should be treated in the same cold and unemotional manner. You have attempted to tinge it with romanticism, which produces much the same effect as if you worked a love-story or an elopement into the fifth proposition of Euclid. &amp;ndash; SHERLOCK HOLMES &amp;lsquo;&amp;lsquo;Sign of The Four&amp;rsquo;&amp;rsquo;, Chapter 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot; Act without doing;
work without effort.
Think of the small as large
and the few as many.
Confront the difficult
while it is still easy;
accomplish the great task
by a series of small acts. &amp;quot; Lao-Tze&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Les math√©matiques ne sont pas une moindre immensit√© que la mer&amp;rdquo; V. Hugo&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Le hasard n&amp;rsquo;est que la mesure de notre ignorance&amp;rdquo; Henri Poincar√©, La science et l&amp;rsquo;hypoth√®se&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Je r√©pugne toute religion qui ne se voit philosophe&amp;rdquo; Lolo Tseu&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sans gravit√© ‚Äì une po√©tique de l‚Äôair</title>
      <link>https://laurentperrinet.github.io/post/2019-06-22_ardemone/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-06-22_ardemone/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/317504725&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;h1 id=&#34;-densit√©-flou--2019&#34;&gt;¬´ Densit√© flou ¬ª (2019)&lt;/h1&gt;






  



  
  











&lt;figure id=&#34;figure-√©tienne-rey---horizon-faille---densit√©-flou-2019---sans-gravit√©---une-po√©tique-de-lair-√†---ardenome---avignon--httpswwwenrevenantdelexpocom&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/post/2019-06-22_ardemone/Avignon-02_hu07228ddf5546db0de787e00a1ab491cd_249119_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;√âtienne Rey - Horizon faille - Densit√© flou, 2019 - Sans gravit√© - une po√©tique de l‚Äôair √† - Ardenome - Avignon ¬© &amp;lt;a href=&amp;#34;https://www.enrevenantdelexpo.com&amp;#34;&amp;gt;https://www.enrevenantdelexpo.com&amp;lt;/a&amp;gt;&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/post/2019-06-22_ardemone/Avignon-02_hu07228ddf5546db0de787e00a1ab491cd_249119_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1200&#34; height=&#34;675&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    √âtienne Rey - Horizon faille - Densit√© flou, 2019 - Sans gravit√© - une po√©tique de l‚Äôair √† - Ardenome - Avignon ¬© &lt;a href=&#34;https://www.enrevenantdelexpo.com&#34;&gt;https://www.enrevenantdelexpo.com&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;!-- https://www.enrevenantdelexpo.com/wp-content/uploads/2019/05/%C3%89tienne-Rey-Horizon-faille-Densit%C3%A9-flou-2019-Sans-gravit%C3%A9-une-po%C3%A9tique-de-l%E2%80%99air-%C3%A0-Ardenome-Avignon-02.jpg --&gt;
&lt;h1 id=&#34;-tension-superficielle--2019&#34;&gt;¬´ Tension superficielle ¬ª (2019)&lt;/h1&gt;


















&lt;figure id=&#34;figure-√©tienne-rey--horizon-faille--tension-superficielle-2019--sans-gravit√©--une-po√©tique-de-lair-√†--ardenome--avignon--httpswwwenrevenantdelexpocom&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://i1.wp.com/www.enrevenantdelexpo.com/wp-content/uploads/2019/05/%C3%89tienne-Rey-Horizon-faille-Tension-superficielle-2019-Sans-gravit%C3%A9-une-po%C3%A9tique-de-l%E2%80%99air-%C3%A0-Ardenome-Avignon-00_1.jpg&#34; data-caption=&#34;√âtienne Rey ‚Äì Horizon faille ‚Äì Tension superficielle, 2019 ‚Äì Sans gravit√© ‚Äì une po√©tique de l‚Äôair √† ‚Äì Ardenome ‚Äì Avignon ¬© &amp;lt;a href=&amp;#34;https://www.enrevenantdelexpo.com&amp;#34;&amp;gt;https://www.enrevenantdelexpo.com&amp;lt;/a&amp;gt;&#34;&gt;


  &lt;img src=&#34;https://i1.wp.com/www.enrevenantdelexpo.com/wp-content/uploads/2019/05/%C3%89tienne-Rey-Horizon-faille-Tension-superficielle-2019-Sans-gravit%C3%A9-une-po%C3%A9tique-de-l%E2%80%99air-%C3%A0-Ardenome-Avignon-00_1.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    √âtienne Rey ‚Äì Horizon faille ‚Äì Tension superficielle, 2019 ‚Äì Sans gravit√© ‚Äì une po√©tique de l‚Äôair √† ‚Äì Ardenome ‚Äì Avignon ¬© &lt;a href=&#34;https://www.enrevenantdelexpo.com&#34;&gt;https://www.enrevenantdelexpo.com&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h1 id=&#34;dans-le-cadre-de-sans-gravit√©-une-po√©tique-de-lair---etienne-rey--mathilde-lavenne--hugo-deverch√®re--edith-dekyndt---23-mars--22-juin-2019&#34;&gt;dans le cadre de &amp;ldquo;SANS GRAVIT√â, UNE PO√âTIQUE DE L&amp;rsquo;AIR - ETIENNE REY / MATHILDE LAVENNE / HUGO DEVERCH√àRE / EDITH DEKYNDT - 23 MARS &amp;gt; 22 JUIN 2019&amp;rdquo;&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Sans Gravit√© a √©t√© con√ßue par EDIS dans le cadre de ‚ÄúChroniques, Biennale des Imaginaires Num√©riques‚Äù, qui r√©unit un ensemble d‚Äôinstitutions culturelles de la R√©gion PACA.
Apr√®s Aix-Marseille, l‚ÄôArdenome √† Avignon est la seconde √©tape
de ce parcours r√©gional.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Horizon faille est une installation globale qui cherche √† d√©fier la gravit√© de la nature. Prenant appui sur deux notions dont l‚Äôartiste en a fait ses motifs principaux - les failles du paysage et l‚Äôimmat√©rielle ligne d‚Äôhorizon. La notion de faille exprime la fracture, au sens g√©ologique. Elle est √† consid√©rer comme un interstice, une zone de transformation, un passage d‚Äôun √©tat √† un autre. De m√™me, l‚Äôhorizon scinde la terre du ciel dans une tentative de g√©om√©trisation de l‚Äôunivers, de mise en espace des √©l√©ments naturels. Intouchable ligne de partage, ce filin tendu d√©signe aussi le seuil de vision du paysage. C‚Äôest la ligne imaginaire qui se forme √† partir de notre position dans l‚Äôespace. Elle est ce qui √©chappe √† la vue ou √† la repr√©sentation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;L‚Äôensemble de ces ≈ìuvres r√©sonnent en √©chos visuels les unes avec les autres. Elles d√©crivent des mouvements, des points de rupture, forment des zones de passage, explorent des √©tats de m√©tamorphose issus d‚Äôun paysage initial dont les perspectives ont √©t√© d√©pli√©s. En parcourant du regard ces diff√©rentes propositions plastiques, le visiteur prend conscience de l‚Äôespace qui l‚Äôentoure et le trouble √† la fois. Un espace √©lastique, d√©multiplicateur qui lui offre une diversit√© d‚Äôangles dans lesquels il peut se perdre √† loisir, comme en √©tat d‚Äôapesanteur. Le monde n‚Äôest pas fig√©, il est une d√©rive constante, une recomposition permanente.
V√©ronique Baton&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Accompagnement R√©alisation : Atelier Ni et Guillaume Stagnaro&lt;/li&gt;
&lt;li&gt;Variable Density et Tension Superficielle en collaboration avec Laurent Perrinet, chercheur Institut de neurosciences de la Timone, INT.&lt;/li&gt;
&lt;li&gt;Tirage s√©rigraphique Atelier Tchikebe.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;links--liens&#34;&gt;Links / Liens:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ardemone &lt;a href=&#34;https://www.ardenome.fr/sans-gravite-ardenome-chroniques&#34;&gt;https://www.ardenome.fr/sans-gravite-ardenome-chroniques&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Retour par 
&lt;a href=&#34;https://www.enrevenantdelexpo.com/2019/05/17/sans-gravite-une-poetique-de-air-ardenome-avignon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;En revenant de l&amp;rsquo;expo !&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SALON INTERNATIONAL D&amp;rsquo;ART CONTEMPORAIN ART-O-RAMA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SALON : 31 AO√õT &amp;gt; 2 SEPTEMBRE 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;EXPOSITION &amp;gt; 9 SEPTEMBRE 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ESPACE PARTENAIRES - J1, QUAI DE LA JOLIETTE, MARSEILLE 2e&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TRAMES</title>
      <link>https://laurentperrinet.github.io/post/2018-04-10_trames/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-04-10_trames/</guid>
      <description>&lt;h1 id=&#34;trames&#34;&gt;TRAMES&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/191830797&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;p&gt;√Ä la Fondation Vasarely √† Aix-en-Provence, Etienne Rey a choisi d‚Äôinstaller dans la salle des Int√©grations architectoniques un ballet visuel hypnotique.&lt;/p&gt;
&lt;p&gt;¬´Trame instabilit√©¬ª est un travail en cours de recherche. Le projet est bas√© sur des principes d‚Äôoccultations partielles en couches associ√©es √† des trames qui font √©merger une dimension immat√©rielle. L‚Äôexp√©rience de perception de ces motifs produit un sentiment de basculement de la perception dans le sens o√π le motifs r√©el passe au second plan pour laisser place √† l‚Äô√©mergence d‚Äôune figure du vide, c‚Äôest dans les blancs immat√©riel que des formes apparaissent et vacillent occupant tout notre champ visuel. Ces apparitions virtuelles, purs ph√©nom√®nes optiques n‚Äôexistent pas dans notre monde ¬´physique¬ª, r√©el. &amp;laquo;BR&amp;raquo; Ce qui est en jeu ici c‚Äôest l‚Äô√©mergence de l‚Äôapparition de motifs virtuels r√©sultat de la relation entre une r√©alit√© physique, la grandeur et l‚Äôordonnancement de trames et notre physiologie qui conduit √† cette √©tat de perception. Lorsqu‚Äôon est fasse √† ces motifs ce qui saute au yeux plus que le motif r√©el c‚Äôest sa r√©sultante, instable et √©ph√©m√®re qui fait apparaitre une richesse de figures g√©om√©triques qui se transforment et √©voluent en fonction du temps d‚Äôobservation et du point de vue. Sur ce principe de dispositif optique, le travail de chacun des motifs, li√© √† un s√©quen√ßage de trames conduit √† faire apparaitre une composition et des √©mergences de formes sp√©cifiques. L‚Äôexp√©rience de perception de chacun des motifs explore les notions d‚Äôinstabilit√©, de flux, d‚Äô√©mergences ‚Ä¶ dont l‚Äôexp√©rience donne √† entrevoir des formes que l‚Äôon retrouve dans la nature ou les ph√©nom√®nes naturels: le dessin du pelage d‚Äôun z√®bre, une accumulation de bulles de savons, ou plus g√©n√©ralement dans les compositions chimiques issue de la th√©orie de la morphog√©n√®se de Turing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;Sortie mod√®le&#34;&gt;&lt;/p&gt;
&lt;p&gt;En collaboration avec le chercheur Laurent Perrinet, CNRS-AMU / Institut de Neurosciences de la Timone.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trame, √âlasticit√© &amp;amp; √âcran n¬∞3  √©taient aussi pr√©sent√©s au Festival Ososph√®re, Strasbourg en Avril 2017.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TROPIQUE</title>
      <link>https://laurentperrinet.github.io/post/2013-10-10_tropique/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2013-10-10_tropique/</guid>
      <description>&lt;h1 id=&#34;tropique&#34;&gt;TROPIQUE&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/66161665&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/NaturalPatterns/Tropique&#34;&gt;https://github.com/NaturalPatterns/Tropique&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;http://www.ran-dan.net/eng/wp-content/uploads/2012/01/E-REY-TropiqueS2-G-1024x576.jpg&#34; alt=&#34;Sortie mod√®le&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.ososphere.org/catalogue/wp-content/uploads/2016/05/tropique_rey_oso2012_groslier8_Web_nb.jpg&#34; alt=&#34;Tropique d&amp;rsquo;Etienne Rey La Coop 2012 / Cr√©dits photo Philippe Groslier&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;lsquo;&amp;lsquo;Etienne Rey investigates the invisible and mutual relationships which take place between human and his environment. 
&lt;a href=&#34;http://digitalperformanceculture.blog.fr/2012/02/18/tropique-d-etienne-rey-12817804/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prize-winner of the 1st call for projects of the RAN&lt;/a&gt;, its project of immersive installation Tropique puts in link the perception of the space connected to the movement, to the light and to the sound. Within the framework of a residence of creation in the Centre des arts, the object of which is ‚Äù to sculpt the light ‚Äú, he presents a work in progress of this installation.&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_b.jpg&#34; alt=&#34;Tropique&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_a.jpg&#34; alt=&#34;Tropique&#34;&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://ondesparalleles.org/projets/tropique-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique&lt;/a&gt; est une installation environnementale, un espace vide de mati√®re, qui se densiÔ¨Åe en ondes sonores et lumineuses, activ√©es et modul√©es par la pr√©sence et l‚Äôactivit√© humaines. Ce projet met en lien la perception de l‚Äôespace articul√©e au mouvement, √† la lumi√®re et au son. Les personnes qui se situent dans l‚Äôespace sont entour√©es d‚Äôune aura lumineuse et sonore qui Ô¨Çuctue en fonction des mouvements et de la proximit√© des corps.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_c.jpg&#34; alt=&#34;Tropique&#34;&gt;&lt;/p&gt;
&lt;p&gt;Etienne Rey investigates the invisible and mutual relationships which take place between human and his environment. Prize-winner of the 1st call for projects of the RAN, its project of immersive installation Tropique puts in link the perception of the space connected to the movement, to the light and to the sound. Within the framework of a residence of creation in the Centre des arts, the object of which is ‚Äù to sculpt the light ‚Äú, he presents a work in progress of this installation.&#39;&#39;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/33718945&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;p&gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_d.jpg&#34; alt=&#34;Tropique&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_l.jpg&#34; alt=&#34;Tropique&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Tropique plonge le visiteur au coeur d‚Äôun espace embrum√©, sculpt√© par la lumi√®re et le son. Exp√©rience sensorielle, le monde de Tropique √©volue dans un entre-deux, rendant palpables des mat√©rialit√©s d‚Äôordinaires invisibles. Exp√©rience perceptive, le dispositif irradie l‚Äôespace, l‚Äôincorpore, l‚Äôamalgame, le dilate. La lumi√®re se diffuse jusqu‚Äô√† modifier notre rapport √† l‚Äôespace, elle le redessine √† travers notre propre vision et provoque une exp√©rience personnelle, une √©motion visuelle. L‚Äôenvironnement r√©agit aux variations de l‚Äôactivit√© dans l‚Äôinstallation, √† la fa√ßon dont nous l‚Äôhabitons et le transformons. Tropique contruit un espace dynamique une architecture mobile √† l‚Äô√©tat de l‚Äôair, en miroir √† notre pr√©sence. Ce qui est r√©v√©l√© est un ensemble vivant, la plupart du temps imperceptible, comme une mise une lumi√®re de notre √©cosyst√®me et de ses interrelations. Ce projet est √©labor√© avec le concours d‚Äôune √©quipe pluridisciplinaire compos√©e d‚Äôun chercheur en Neuroscience : Laurent Perrinet, d‚Äôun compositeur : Wilfried Wendling d‚Äôun ing√©nieur : Julien Marro Dauzat.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;affiche_TROPIQUE-INT.jpg&#34; alt=&#34;Tropique √ü @ INT&#34;&gt;&lt;/p&gt;
&lt;p&gt;Accueilli en r√©sidence dans le cadre des r√©sidences de recherche de l‚ÄôIM√©RA pendant 6 mois (3 p√©riodes de 2 mois),et soutenu dans le cadre d&amp;rsquo;un Atelier de l&#39;!EuroM√©diterran√©e, ce projet est √©labor√© en collaboration avec des chercheurs. Nous abordons ainsi les questions de la cognition et de la perception de l‚Äôespace li√©es √† la vue, √† l‚Äôaudition, et au d√©placement, auxquelles nous lierons les questions relatives √† la diffusion de ph√©nom√®nes ondulatoires.&lt;/p&gt;
&lt;h2 id=&#34;news&#34;&gt;news&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/56198653&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;space odyss√©e&lt;/a&gt; √† 
&lt;a href=&#34;http://www.institutfrancais-seoul.com/portfolio-item/exposition-home-cinema/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;l&amp;rsquo;institut francais en Coree du Sud de juin a octobre 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Un entretien d&amp;rsquo;ER durant le 
&lt;a href=&#34;https://www.youtube.com/watch?v=lA2bovigzLg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mois multi 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Un entretien avec 
&lt;a href=&#34;https://youtu.be/z8328z2WO-w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Roger Malina&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/z8328z2WO-w&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Installation 
&lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : Tourn√©e en Cor√©e avec Mac de Cr√©teil en 2016 / dates et lieux √† venir.&lt;/li&gt;
&lt;li&gt;Installation 
&lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : FESTIVAL VIA / MAUBEUGE / 12 AU 22 MARS 2015 - FESTIVAL EXIT / CR√âTEIL / 26 MARS AU 05 AVRIL 2015 - LE PRINTEMPS √Ä SAINT SAUVEUR / LILLE / 27 AVRIL 2016 AU 28 AO√õT 2016 : 
&lt;a href=&#34;http://www.maccreteil.com/fr/mac/event/338/Home-cinema-Festival-Exit#sthash.bEyRuaDX.dpuf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Home Cinema&lt;/a&gt;. √Ä lire, sur 
&lt;a href=&#34;http://www.digitalarti.com/fr/blog/digitalarti_mag/home_cinema_matiere_audiovisuelle_modulable_pour_hyper_spectateur&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;digitatarti&lt;/a&gt;.
Installation 
&lt;a href=&#34;http://ondesparalleles.org/projets/tropique-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique&lt;/a&gt; : Festival international d‚Äôarts multidisciplinaires et √©lectroniques 
&lt;a href=&#34;http://mmrectoverso.org/fr/mois-multi/spectacles/installations-2/tropique/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Le Mois Multi 16 / Qu√©bec du 4 f√©vrier au 1 mars 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/lA2bovigzLg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Installation 
&lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : Lille 3000, du 26 SEPT 2015 &amp;gt; 17 JAN 2016&lt;/li&gt;
&lt;li&gt;Installation 
&lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : FESTIVAL INTERNATIONAL EXIT 2015 Cr√©teil du 26 MARS -&amp;gt; 05 AVRIL 2015&lt;/li&gt;
&lt;li&gt;Installation 
&lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : Festival Via 2015, Maubeuge du jeudi 12 mars 2015 au dimanche 22 mars 2015&lt;/li&gt;
&lt;li&gt;Du 13 au 23 Mars 2014: 
&lt;a href=&#34;http://www.lemanege.com/cgi?lg=fr&amp;amp;pag=1310&amp;amp;tab=108&amp;amp;rec=1188&amp;amp;frm=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TROPIQUE @ FESTIVAL VIA&lt;/a&gt; : Depuis presque 30 ans, le Festival VIA flirte avec les fronti√®res des territoires artistiques, √† la crois√©e des arts de la sc√®ne, de la cr√©ation technologique et num√©rique. Toujours plus international et interdisciplinaire, VIA traduit, √† Maubeuge et √† Mons, la vitalit√© de la sc√®ne contemporaine. Il est √©galement √† noter que cette √©dition pr√©figure la Capitale europ√©enne de la culture 2015 qui se d√©roulera dans ces deux villes l&amp;rsquo;ann√©e prochaine.  Pour cette nouvelle √©dition de VIA sera pr√©sent√©e l&amp;rsquo;installation Tropique d&amp;rsquo;√âtienne Rey, produite par Seconde Nature et r√©cemment d√©voil√©e lors du festival ¬´ Chroniques des Mondes Possibles ¬ª dans le cadre d&amp;rsquo;E-topie, Marseille-Provence 2013.&lt;/li&gt;
&lt;li&gt;16 novembre au 15 d√©cembre 2013: pr√©sentation √† Paris dans le cadre du festival 
&lt;a href=&#34;http://www.digitalarti.com/fr/blog/digitalarti_mag/festival_nemo_tropiques_etienne_rey&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nemo&lt;/a&gt; ; √©couter 
&lt;a href=&#34;http://www.franceinter.fr/emission-latac-faux-sourires-vision-cosmique-installation-tropique&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;la t√™te au carr√© sur France Inter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;10 Octobre au 10 novembre 2013: pr√©sentation finale √† la fondation Vasarely √† Aix-en-Provence dans le cadre du festival 
&lt;a href=&#34;http://www.fondationvasarely.org/uk/e_topie.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;e-Topie&lt;/a&gt;, cf 
&lt;a href=&#34;http://www.liberation.fr/culture/2013/10/15/riche-e-topie-a-aix_939736&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cahier beaux-arts de lib√©&lt;/a&gt;, 
&lt;a href=&#34;http://www.laprovence.com/article/economie/2628580/aix-la-ville-candidate-pour-devenir-quartier-numerique.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;la provence&lt;/a&gt;  : 10450 spectateurs sur 32 jours&lt;/li&gt;
&lt;li&gt;Juin 2013: 
&lt;a href=&#34;https://laurentperrinet.github.io/post/2013-10-10_tropique/featured_hu4307c572b52602b5d74387cbed9b8bcf_354150_720x0_resize_q90_lanczos.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r√©sidence √† l&amp;rsquo;INT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D√©cembre 2012: dans le 
&lt;a href=&#34;http://www.mp2013.fr/evenements/2013/10/atelier-de-leuromediterranee-tropique-etienne-rey-a-limera/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;programme officiel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D√©cembre 2012: signature de la convention avec le CNRS&lt;/li&gt;
&lt;li&gt;Tropique est pr√©sent√© 
&lt;a href=&#34;http://www.ososphere.org/2012/evenement/tropique/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;07 au 14 d√©cembre 2012, dans le cadre du festival &amp;ldquo;Les nuits de l‚ÄôOsosph√®re&amp;rdquo; de Strasbourg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Juin 2012: Tropique en r√©sidence √† la fondation Vasarely&lt;/li&gt;
&lt;li&gt;Mars 2012: 
&lt;a href=&#34;http://www.mecenesdusud.fr/blog/index.php?post/2012/03/Tropique-%C3%A0-Nantes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique √† Nantes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://creative.arte.tv/en/space/Tropique/messages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arte creative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tropique @ 
&lt;a href=&#34;http://www.ran-dan.net/eng/?p=22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAN (Enghien-les-Bains)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://vimeo.com/33718945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Etienne Rey Tropique Experimentation Film2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Turbulences</title>
      <link>https://laurentperrinet.github.io/post/2018-01-20_turbulences/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-01-20_turbulences/</guid>
      <description>&lt;h1 id=&#34;turbulences&#34;&gt;Turbulences&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/303255760&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;h2 id=&#34;installation-in-situ-2018-collection-of-the-fran√ßois-schneider-foundation-wattwiller-i-2018&#34;&gt;Installation in situ, 2018; Collection of the Fran√ßois Schneider Foundation, Wattwiller I 2018&lt;/h2&gt;
&lt;p&gt;&amp;ldquo;En agissant sur les variations et les rayonnements d‚Äôordre physique, Etienne Rey fait appara√Ætre l‚Äô√©paisseur existentielle du vide.&amp;rdquo; B√©n√©dicte Chevallier, M√©c√®nes du Sud&lt;/p&gt;
&lt;p&gt;L‚Äôinstallation Turbulences explore l‚Äô√©mergence de caustiques, ph√©nom√®nes caract√©ristiques de la relation entre l‚Äôeau, la lumi√®re et l‚Äôair.
Le mouvement y perturbe un √©tat optique stable. La turbulence des plis lumineux donne l‚Äôillusion d‚Äôun corps flottant.&lt;/p&gt;
&lt;p&gt;Installation immersive plastique et sonore, l‚Äôexp√©rience de l‚Äôoeuvre se joue dans l‚Äôexploration de dimensions immat√©rielles par l‚Äôobservateur.&lt;/p&gt;
&lt;p&gt;Cr√©ation originale pour la Fondation Fran√ßois Schneider
Production : Quatre 4.0 / L‚ÄôOsospheÃÄre, en partenariat avec la Fondation Fran√ßois Schneider et le soutien de la R√©gion Grand Est
Accompagnement, production d√©l√©gu√©e : bOssa&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;INAUGURATION : SAMEDI 27 OCTOBRE 2018&lt;/li&gt;
&lt;li&gt;EXPOSITION &amp;gt; 20 JANVIER 2019&lt;/li&gt;
&lt;li&gt;FONDATION FRAN√áOIS SCHNEIDER - 27 RUE DE LA PREMI√àRE ARM√âE, 68700 WATTWILLER&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Testing the odds of inherent vs. observed overdispersion in neural spike counts</title>
      <link>https://laurentperrinet.github.io/publication/taouali-16/</link>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/taouali-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A dynamic model for decoding direction and orientation in macaque primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/taouali-15-vss/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/taouali-15-vss/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;see a follow-up in this 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/taouali-16-areadne/&#34;&gt;poster&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This is a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A dynamic model for decoding direction and orientation in macaque primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/taouali-16-areadne/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/taouali-16-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Compensation of oculomotor delays in the visual system&#39;s network</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-16-networks/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-16-networks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of motion predictability on anticipatory and visually-guided eye movements: a common prior for sensory processing and motor control?</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-16-ecvp/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-16-ecvp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling the effect of dynamic contingencies on anticipatory eye movements</title>
      <link>https://laurentperrinet.github.io/publication/damasse-16-ecvp/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-16-ecvp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Push-Pull Receptive Field Organization and Synaptic Depression: Mechanisms for Reliably Encoding Naturalistic Stimuli in V1</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-16/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-16-ecvp/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-16-ecvp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-16-gdr/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-16-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/mansour-16-sfn/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/mansour-16-sfn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motion-based prediction with neuromorphic hardware</title>
      <link>https://laurentperrinet.github.io/talk/2015-11-05-chile/</link>
      <pubDate>Thu, 05 Nov 2015 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2015-11-05-chile/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv-chap-1/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv-chap-1/</guid>
      <description>





  



  
  











&lt;figure id=&#34;figure-mindmap-of-the-book-contents-cross-links-between-chapters-have-been-indicated-as-thin-lines&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv-chap-1/mindmap_hu43e8df124a30549f11198e3614cbf641_509301_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Mindmap of the book contents. Cross-links between chapters have been indicated as thin lines.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv-chap-1/mindmap_hu43e8df124a30549f11198e3614cbf641_509301_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1280&#34; height=&#34;1276&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Mindmap of the book contents. Cross-links between chapters have been indicated as thin lines.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Sparse Models for Computer Vision</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-15-bicv/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-15-bicv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visual motion processing and human tracking behavior</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-15-bicv/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-15-bicv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motion-based prediction with neuromorphic hardware</title>
      <link>https://laurentperrinet.github.io/talk/2015-10-07-gdr-bio-comp/</link>
      <pubDate>Wed, 07 Oct 2015 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2015-10-07-gdr-bio-comp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biologically Inspired Computer Vision</title>
      <link>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/</link>
      <pubDate>Wed, 07 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/</guid>
      <description>





  



  
  











&lt;figure id=&#34;figure-biologically-inspired-computer-vision&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/header_hu7689ba15ddb3b57887b7eec74b7a9cd1_831168_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Biologically Inspired Computer vision&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/header_hu7689ba15ddb3b57887b7eec74b7a9cd1_831168_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;960&#34; height=&#34;287&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Biologically Inspired Computer vision
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h1 id=&#34;biologically-inspired-computer-vision&#34;&gt;Biologically Inspired Computer Vision&lt;/h1&gt;
&lt;p&gt;As state-of-the-art imaging technologies becomes more and more advanced, yielding scientific data at unprecedented detail and volume, the need to process and interpret all the data has made image processing and computer vision also increasingly important. Sources of data that have to be routinely dealt with today applications include video transmission, wireless communication, automatic fingerprint processing, massive databanks, non-weary and accurate automatic airport screening, robust night vision to name a few. Multidisciplinary inputs from other disciplines such as computational neuroscience, cognitive science, mathematics, physics and biology will have a fundamental impact in the progress of imaging and vision sciences. One of the advantages of the study of biological organisms is to devise very diÔ¨Äerent type of computational paradigms beyond the usual von Neumann e.g. by implementing a neural network with a high degree of local connectivity.






  



  
  











&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/header_hu7689ba15ddb3b57887b7eec74b7a9cd1_831168_2000x2000_fit_q90_lanczos.jpg&#34; &gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/header_hu7689ba15ddb3b57887b7eec74b7a9cd1_831168_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;960&#34; height=&#34;287&#34;&gt;
&lt;/a&gt;



&lt;/figure&gt;

This is a comprehensive and rigorous reference in the area of biologically motivated vision sensors. The study of biologically visual systems can be considered as a two way avenue. On the one hand, biological organisms can provide a source of inspiration for new computational efficient and robust vision models and on the other hand machine vision approaches can provide new insights for understanding biological visual systems. Along the different chapters, this book covers a wide range of topics from fundamental to more specialized topics, including visual analysis based on a computational level, hardware implementation, and the design of new more advanced vision sensors. The last two sections of the book provide an overview of a few representative applications and current state of the art of the research in this area. This makes it a valuable book for graduate, Master, PhD students and also researchers in the field.
This book contains 17 chapters that have been organized in four different parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fundamentals&lt;/li&gt;
&lt;li&gt;Sensing&lt;/li&gt;
&lt;li&gt;Modeling&lt;/li&gt;
&lt;li&gt;Applications
See the 
&lt;a href=&#34;http://bicv.github.io/toc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Table of contents&lt;/a&gt;.


















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;mindmap.png&#34; &gt;


  &lt;img src=&#34;mindmap.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Coding Of Natural Images Using A Prior On Edge Co-Occurences</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-15-eusipco/</link>
      <pubDate>Mon, 03 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-15-eusipco/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BrainScaleS (2011/2014) </title>
      <link>https://laurentperrinet.github.io/grant/brain-scales/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/brain-scales/</guid>
      <description>&lt;p&gt;List of publications that were funded by European Union&amp;rsquo;s project Number FP7-269921, &amp;ldquo;
&lt;a href=&#34;http://brainscales.kip.uni-heidelberg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BrainScales&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;http://facets.kip.uni-heidelberg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FACETS research project&lt;/a&gt; which
ended on 31 August 2010.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/facets-itn/&#34;&gt;FACETS-ITN Marie-Curie&lt;/a&gt; initital
training network for graduate training continues until August 2013&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/brain-scales/&#34;&gt;BrainScaleS project&lt;/a&gt; builds on
and extends the research done in FACETS. This 4 year project started
on January 1st, 2011.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>CODDE (2008/2012)</title>
      <link>https://laurentperrinet.github.io/grant/codde/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/codde/</guid>
      <description>&lt;p&gt;The 
&lt;a href=&#34;http://www.optimaldecisions.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CODDE&lt;/a&gt; network studies the links between sensory input, brain activity and motor output. It does this by combining behavioural techniques, brain imaging, movement recording and computational modelling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FACETS (2006/2010)</title>
      <link>https://laurentperrinet.github.io/grant/facets/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/facets/</guid>
      <description>&lt;p&gt;List of publications that were funded by the
&lt;a href=&#34;http://facets.kip.uni-heidelberg.de/&#34; class=&#34;http&#34;&gt;FACETS&lt;/a&gt;
project (more
&lt;a href=&#34;http://en.wikipedia.org/wiki/Facets_%28Science%29&#34; class=&#34;http&#34;&gt;info&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;also available on the FACET&amp;rsquo;s
&lt;a href=&#34;http://facets.kip.uni-heidelberg.de/jss/Publications/author_Perrinet&#34; class=&#34;http&#34;&gt;website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;http://facets.kip.uni-heidelberg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FACETS research project&lt;/a&gt; which
ended on 31 August 2010.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/facets-itn/&#34;&gt;FACETS-ITN Marie-Curie&lt;/a&gt; initital
training network for graduate training continues until August 2013&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/brain-scales/&#34;&gt;BrainScaleS project&lt;/a&gt; builds on
and extends the research done in FACETS. This 4 year project started
on January 1st, 2011.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FACETS-ITN (2010/2013)</title>
      <link>https://laurentperrinet.github.io/grant/facets-itn/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/facets-itn/</guid>
      <description>&lt;h1 id=&#34;facets-itn-from-neuroscience-to-neuro-inspired-computing-20102013&#34;&gt;FACETS-ITN: From Neuroscience to neuro-inspired computing (2010/2013)&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://facets.kip.uni-heidelberg.de/ITN/index.html&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://facets.kip.uni-heidelberg.de/images/e/e3/Public--ITN_PositionsPoster2.png&#34; title=&#34;http://facets.kip.uni-heidelberg.de/ITN/index.html&#34; alt=&#34;http://facets.kip.uni-heidelberg.de/ITN/index.html&#34; class=&#34;external_image&#34; style=&#34;width:25.0%&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://facets.kip.uni-heidelberg.de/ITN/index.html&#34; class=&#34;http&#34;&gt;FACETS ITN&lt;/a&gt;
project (EU funding, grant number 237955) is a &amp;lsquo;Marie-Curie Initial
Training Network&amp;rsquo; involves 15 groups at European Research Universities,
Research Centers and Industrial Partners in 6 countries. 22 Ph.D.
Positions are funded in the FACETS-ITN project in the following
scientific work areas: Neurobiology of Cells and Networks, Modelling of
Neural Systems, Neuromorphic Hardware, Neuro-Electronic Interfaces,
Computational Principles in Neural Architectures, Mechanisms of Learning
and Plasticity. &lt;span id=&#34;line-8&#34; class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;span
id=&#34;line-9&#34; class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;http://facets.kip.uni-heidelberg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FACETS research project&lt;/a&gt; which
ended on 31 August 2010.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/facets-itn/&#34;&gt;FACETS-ITN Marie-Curie&lt;/a&gt; initital
training network for graduate training continues until August 2013&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://laurentperrinet.github.io/grant/brain-scales/&#34;&gt;BrainScaleS project&lt;/a&gt; builds on
and extends the research done in FACETS. This 4 year project started
on January 1st, 2011.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PACE-ITN (2015/2019)</title>
      <link>https://laurentperrinet.github.io/grant/pace-itn/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/pace-itn/</guid>
      <description>&lt;p&gt;The PACE ITN project involved over 50 researchers spread across 10 full and 5 associated partners, from academia and the private sector, established in 7 different European and Associated countries, the PACE network gathers a broad range of expertise from experimental psychology, cognitive neurosciences, brain imaging, technology and clinical sciences.&lt;/p&gt;
&lt;p&gt;The PACE Project has received funding from the European Union‚Äôs Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 642961&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Mathematical Account of Dynamic Texture Synthesis for Probing Visual Perception</title>
      <link>https://laurentperrinet.github.io/publication/vacher-15-icms/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-15-icms/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Anticipating a moving target: role of vision and reinforcement</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-15-sfn/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-15-sfn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Anticipatory smooth eye movements and reinforcement</title>
      <link>https://laurentperrinet.github.io/publication/damasse-15-vss/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-15-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Anticipatory smooth eye movements as operant behavior</title>
      <link>https://laurentperrinet.github.io/publication/damasse-15-gdr/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-15-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Biologically Inspired Dynamic Textures for Probing Motion Perception</title>
      <link>https://laurentperrinet.github.io/publication/vacher-15-nips/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-15-nips/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge co-occurrences can account for rapid categorization of natural versus animal images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-bednar-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-bednar-15/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.cnrs.fr/insb/6.recherche/parutions2/articles2015/l-perrinet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;communiqu√© de presse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.nature.com/article-assets/npg/srep/2015/150622/srep11400/extref/srep11400-s1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;supplementary information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;PerrinetBednar15supplementary.pdf&#34;&gt;supplementary material&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-study-of-how-people-can-quickly-spot-animals-by-sight-is-helping-uncover-the-workings-of-the-human-brain&#34;&gt;A study of how people can quickly spot animals by sight is helping uncover the workings of the human brain.&lt;/h1&gt;
&lt;p&gt;Scientists examined why volunteers who were shown hundreds of pictures - some with animals and some without - were able to detect animals in as little as one-tenth of a second.
They found that one of the first parts of the brain to process visual information - the primary visual cortex - can control this fast response.
More complex parts of the brain are not required at this stage, contrary to what was previously thought.
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New info published on how the human brain processes visual information from &lt;a href=&#34;https://twitter.com/EdinburghUni?ref_src=twsrc%5Etfw&#34;&gt;@EdinburghUni&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/uniamu?ref_src=twsrc%5Etfw&#34;&gt;@uniamu&lt;/a&gt; stuidy &lt;a href=&#34;http://t.co/KUicugL8P7&#34;&gt;http://t.co/KUicugL8P7&lt;/a&gt;&lt;/p&gt;&amp;mdash; EdinUniNeuro (@EdinUniNeuro) &lt;a href=&#34;https://twitter.com/EdinUniNeuro/status/613011086829162497?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;







  



  
  











&lt;figure id=&#34;figure-edge-co-occurrences-a-an-example-image-with-the-list-of-extracted-edges-overlaid-each-edge-is-represented-by-a-red-line-segment-which-represents-its-position-center-of-segment-orientation-and-scale-length-of-segment-we-controlled-the-quality-of-the-reconstruction-from-the-edge-information-such-that-the-residual-energy-was-less-than-5-b-the-relationship-between-a-reference-edge-a-and-another-edge-b-can-be-quantified-in-terms-of-the-difference-between-their-orientations-theta-ratio-of-scale-sigma-distance-d-between-their-centers-and-difference-of-azimuth-angular-location-phi-additionally-we-define-psiphi---theta2-which-is-symmetric-with-respect-to-the-choice-of-the-reference-edge-in-particular-psi0-for-co-circular-edges--see-text-as-incitetgeisler01-edges-outside-a-central-circular-mask-are-discarded-in-the-computation-of-the-statistics-to-avoid-artifacts-image-credit-andrew-shiva-creative-commons-attribution-share-alike-30-unported-licensehttpscommonswikimediaorgwikifileelephant_28loxodonta_africana29_05jpg-this-is-used-to-compute-the-chevron-map-in-figure2&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Edge co-occurrences &amp;lt;strong&amp;gt;(A)&amp;lt;/strong&amp;gt; An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. &amp;lt;strong&amp;gt;(B)&amp;lt;/strong&amp;gt; The relationship between a reference edge &amp;lt;em&amp;gt;A&amp;lt;/em&amp;gt; and another edge &amp;lt;em&amp;gt;B&amp;lt;/em&amp;gt; can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: &amp;lt;a href=&amp;#34;https://commons.wikimedia.org/wiki/File:Elephant_/%28Loxodonta_Africana/%29_05.jpg&amp;#34;&amp;gt;Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license&amp;lt;/a&amp;gt;). This is used to compute the chevron map in Figure~2.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_model_hu48cd3bfc5a03e03c8521b92631e4b49d_29970_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;310&#34; height=&#34;393&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Edge co-occurrences &lt;strong&gt;(A)&lt;/strong&gt; An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. &lt;strong&gt;(B)&lt;/strong&gt; The relationship between a reference edge &lt;em&gt;A&lt;/em&gt; and another edge &lt;em&gt;B&lt;/em&gt; can be quantified in terms of the difference between their orientations $\theta$, ratio of scale $\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\phi$. Additionally, we define $\psi=\phi - \theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\psi=0$ for co-circular edges. % (see text). As in~\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Elephant_/%28Loxodonta_Africana/%29_05.jpg&#34;&gt;Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license&lt;/a&gt;). This is used to compute the chevron map in Figure~2.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;ÂãïÁâ©„ÅãÂê¶„Åã„ÅÆË¶ãÂàÜ„ÅëÊñπ„ÄÇ&lt;a href=&#34;http://t.co/TTY8MwZGoO&#34;&gt;http://t.co/TTY8MwZGoO&lt;/a&gt;„ÄÄÂºïÁî®„Åï„Çå„Å¶„Çã„Åë„Å©„ÄÅThorpe (1996)„ÅÆ150ms„ÅßÂå∫Âà•„Åï„Çå„Å¶„Çã„Å£„Å¶Ë©±(„Å™„Å§„Åã„Åó„ÅÑ)„Å®Èñ¢‰øÇ„ÅÇ„Çä„Åù„ÅÜ„ÄÇ&lt;/p&gt;&amp;mdash; Makito Oku (@okumakito) &lt;a href=&#34;https://twitter.com/okumakito/status/613128456637841408?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;







  



  
  











&lt;figure id=&#34;figure-the-probability-distribution-function-ppsi-theta-represents-the-distribution-of-the-different-geometrical-arrangements-of-edges-angles-which-we-call-a-chevron-map-we-show-here-the-histogram-for-non-animal-natural-images-illustrating-the-preference-for-co-linear-edge-configurations-for-each-chevron-configuration-deeper-and-deeper-red-circles-indicate-configurations-that-are-more-and-more-likely-with-respect-to-a-uniform-prior-with-an-average-maximum-of-about-3-times-more-likely-and-deeper-and-deeper-blue-circles-indicate-configurations-less-likely-than-a-flat-prior-with-a-minimum-of-about-08-times-as-likely-conveniently-this-chevron-map-shows-in-one-graph-that-non-animal-natural-images-have-on-average-a-preference-for-co-linear-and-parallel-edges-the-horizontal-middle-axis-and-orthogonal-angles-the-top-and-bottom-rowsalong-with-a-slight-preference-for-co-circular-configurations-for-psi0-and-psipm-frac-pi-2-just-above-and-below-the-central-row-we-compare-chevron-maps-in-different-image-categories-in-figure3&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&amp;#39; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_chevrons_hucb38a7eddc0dd6e870eb20519377c33e_143053_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;550&#34; height=&#34;495&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    The probability distribution function $p(\psi, \theta)$ represents the distribution of the different geometrical arrangements of edges&#39; angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\psi=0$ and $\psi=\pm \frac \pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Edge co-occurrences can account for rapid categorization of natural versus animal images&lt;a href=&#34;http://t.co/NY9HapBx2S&#34;&gt;http://t.co/NY9HapBx2S&lt;/a&gt; &lt;a href=&#34;http://t.co/rKQ8I5i6Ty&#34;&gt;pic.twitter.com/rKQ8I5i6Ty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Francis Villatoro (@emulenews) &lt;a href=&#34;https://twitter.com/emulenews/status/612988348400070656?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;







  



  
  











&lt;figure id=&#34;figure-as-for-figure-2-we-show-the-probability-of-edge-configurations-as-chevron-maps-for-two-databases-man-made-animal-here-we-show-the-ratio-of-histogram-counts-relative-to-that-of-the-non-animal-natural-image-dataset-deeper-and-deeper-red-circles-indicate-configurations-that-are-more-and-more-likely-and-blue-respectively-less-likely-with-respect-to-the-histogram-computed-for-non-animal-images-in-the-left-plot-the-animal-images-exhibit-relatively-more-circular-continuations-and-converging-angles-red-chevrons-in-the-central-vertical-axis-relative-to-non-animal-natural-images-at-the-expense-of-co-linear-parallel-and-orthogonal-configurations-blue-circles-along-the-middle-horizontal-axis-the-man-made-images-have-strikingly-more-co-linear-features-central-circle-which-reflects-the-prevalence-of-long-straight-lines-in-the-cage-images-in-that-dataset-we-use-this-representation-to-categorize-images-from-these-different-categories-in-figure4&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_chevrons2_hu12028ad970ef123637fe88634f4dfbdd_315348_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1100&#34; height=&#34;679&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.
  &lt;/figcaption&gt;


&lt;/figure&gt;







  



  
  











&lt;figure id=&#34;figure-classification-results-to-quantify-the-difference-in-low-level-feature-statistics-across-categories-see-figure3-we-used-a-standard-support-vector-machine-svm-classifier-to-measure-how-each-representation-affected-the-classifiers-reliability-for-identifying-the-image-category-for-each-individual-image-we-constructed-a-vector-of-features-as-either-fo-the-histogram-of-first-order-statistics-as-the-histogram-of-edges-orientations-cm-the-chevron-map-subset-of-the-second-order-statistics-ie-the-two-dimensional-histogram-of-relative-orientation-and-azimuth-see-figure-2--or-so-the-full-four-dimensional-histogram-of-second-order-statistics-ie-all-parameters-of-the-edge-co-occurrences-we-gathered-these-vectors-for-each-different-class-of-images-and-report-here-the-results-of-the-svm-classifier-using-an-f1-score-50-represents-chance-level-while-it-was-expected-that-differences-would-be-clear-between-non-animal-natural-images-versus-laboratory-man-made-images-results-are-still-quite-high-for-classifying-animal-images-versus-non-animal-natural-images-and-are-in-the-range-reported-bycitetserre07-f1-score-of-80-for-human-observers-and-82-for-their-model-even-using-the-cm-features-alone-we-further-extend-this-results-to-the-psychophysical-results-of-serre-et-al-2007-in-figure-5&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&amp;amp;rsquo;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&amp;#39; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80% for human observers and 82% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_results_hu1e96d4f01800cef44dc0298ead0241fe_16945_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;476&#34; height=&#34;294&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier&amp;rsquo;s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges&#39; orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of images and report here the results of the SVM classifier using an F1 score (50% represents chance level). While it was expected that differences would be clear between non-animal natural images versus laboratory (man-made) images, results are still quite high for classifying animal images versus non-animal natural images, and are in the range reported by~\citet{Serre07} (F1 score of 80% for human observers and 82% for their model), even using the CM features alone. We further extend this results to the psychophysical results of Serre et al. (2007) in Figure 5.
  &lt;/figcaption&gt;


&lt;/figure&gt;







  



  
  











&lt;figure id=&#34;figure-to-see-whether-the-patterns-of-errors-made-by-humans-are-consistent-with-our-model-we-studied-the-second-order-statistics-of-the-50-non-animal-images-that-human-subjects-in-serre-et-al-2007-most-commonly-falsely-reported-as-having-an-animal-we-call-this-set-of-images-the-false-alarm-image-dataset-left-this-chevron-map-plot-shows-the-ratio-between-the-second-order-statistics-of-the-false-alarm-images-and-the-full-non-animal-natural-image-dataset-computed-as-in-figure-3-left-just-as-for-the-images-that-actually-do-contain-animals-figure3-left-the-images-falsely-reported-as-having-animals-have-more-co-circular-and-converging-red-chevrons-and-fewer-collinear-and-orthogonal-configurations-blue-chevrons-right-to-quantify-this-similarity-we-computed-the-kullback-leibler-distance-between-the-histogram-of-each-of-these-images-from-the-false-alarm-image-dataset-and-the-average-histogram-of-each-class-the-difference-between-these-two-distances-gives-a-quantitative-measure-of-how-close-each-image-is-to-the-average-histograms-for-each-class-consistent-with-the-idea-that-humans-are-using-edge-co-occurences-to-do-rapid-image-categorization-the-50-non-animal-images-that-were-worst-classified-are-biased-toward-the-animal-histogram-d--104-while-the-550-best-classified-non-animal-images-are-closer-to-the-non-animal-histogram&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_FA_humans_hu9158f735a2d21afb5c10a78c8717ec4e_575661_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&amp;#39; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/figure_FA_humans_hu9158f735a2d21afb5c10a78c8717ec4e_575661_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2887&#34; height=&#34;1784&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    To see whether the patterns of errors made by humans are consistent with our model, we studied the second-order statistics of the 50 non-animal images that human subjects in Serre et al. (2007) most commonly falsely reported as having an animal. We call this set of images the false-alarm image dataset. (Left) This chevron map plot shows the ratio between the second-order statistics of the false-alarm images and the full non-animal natural image dataset, computed as in Figure 3 (left). Just as for the images that actually do contain animals (Figure~3, left), the images falsely reported as having animals have more co-circular and converging (red chevrons) and fewer collinear and orthogonal configurations (blue chevrons). (Right) To quantify this similarity, we computed the Kullback-Leibler distance between the histogram of each of these images from the false-alarm image dataset, and the average histogram of each class. The difference between these two distances gives a quantitative measure of how close each image is to the average histograms for each class. Consistent with the idea that humans are using edge co-occurences to do rapid image categorization, the 50 non-animal images that were worst classified are biased toward the animal histogram ($d&#39; = 1.04$), while the 550 best classified non-animal images are closer to the non-animal histogram.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Eye tracking a self-moved target with complex hand-target dynamics</title>
      <link>https://laurentperrinet.github.io/publication/danion-15-sfn/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/danion-15-sfn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On overdispersion in neuronal evoked activity</title>
      <link>https://laurentperrinet.github.io/publication/taouali-15-icmns/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/taouali-15-icmns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a follow-up in this 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/taouali-16/&#34;&gt;publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Spatiotemporal tuning of retinal ganglion cells dependent on the context of signal presentation</title>
      <link>https://laurentperrinet.github.io/publication/ravello-15/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/ravello-15/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active inference, eye movements and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</link>
      <pubDate>Tue, 16 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</guid>
      <description>&lt;h1 id=&#34;active-inference-tracking-eye-movements-and-oculomotor-delays&#34;&gt;Active Inference, tracking eye movements and oculomotor delays&lt;/h1&gt;
&lt;p&gt;Tracking eye movements face a difficult task: they have to be fast while they suffer inevitable delays. If we focus on area MT of humans for instance as it is crucial for detecting the motion of visual objects, sensory information coming to this area is already lagging some 35 milliseconds behind operational time ‚Äì that is, it reflects some past information. Still the fastest action that may be done there is only able to reach the effector muscles of the eyes some 40 milliseconds later ‚Äì that is, in the future. The tracking eye movement system is however able to respond swiftly and even to anticipate repetitive movements (e.g. Barnes et al, 2000 ‚Äì refs in manuscript). In that case, it means that information in a cortical area is both predicted from the past sensory information but also anticipated to give an optimal response in the future. Even if numerous models have been described to model different mechanisms to account for delays, no theoretical approach has tackled the whole problem explicitly. In several areas of vision research, authors have proposed models at different levels of abstractions from biomechanical models, to neurobiological implementations (e.g. Robinson, 1986) or Bayesian models. This study is both novel and important because ‚Äì using a neurobiologically plausible hierarchical Bayesian model ‚Äì it demonstrates that using generalized coordinates to finesse the prediction of a target&amp;rsquo;s motion, the model can reproduce characteristic properties of tracking eye movements in the presence of delays. Crucially, the different refinements to the model that we propose ‚Äì pursuit initiation, smooth pursuit eye movements, and anticipatory response ‚Äì are consistent with the different types of tracking eye movements that may be observed experimentally.&lt;/p&gt;






  



  
  











&lt;figure id=&#34;figure-a-this-figure-reports-the-response-of-predictive-processing-during-the-simulation-of-pursuit-initiation-using-a-single-sweep-of-a-visual-target-while-compensating-for-sensory-motor-delays-here-we-see-horizontal-excursions-of-oculomotor-angle-red-line-one-can-see-clearly-the-initial-displacement-of-the-target-that-is-suppressed-by-action-after-a-few-hundred-milliseconds-additionally-we-illustrate-the-effects-of-assuming-wrong-sensorimotor-delays-on-pursuit-initiation-under-pure-sensory-delays-blue-dotted-line-one-can-see-clearly-the-delay-in-sensory-predictions-in-relation-to-the-true-inputs-with-pure-motor-delays-blue-dashed-line-and-with-combined-sensorimotor-delays-blue-line-there-is-a-failure-of-optimal-control-with-oscillatory-fluctuations-in-oculomotor-trajectories-which-may-become-unstable-b-this-figure-reports-the-simulation-of-smooth-pursuit-when-the-target-motion-is-hemi-sinusoidal-as-would-happen-for-a-pendulum-that-would-be-stopped-at-each-half-cycle-left-of-the-vertical-broken-black-lines-in-the-lower-right-panel-we-report-the-horizontal-excursions-of-oculomotor-angle-the-generative-model-used-here-has-been-equipped-with-a-second-hierarchical-level-that-contains-hidden-states-modeling-latent-periodic-behavior-of-the-hidden-causes-of-target-motion-with-this-addition-the-improvement-in-pursuit-accuracy-apparent-at-the-onset-of-the-second-cycle-of-motion-is-observed-pink-shaded-area-similar-to-psychophysical-experimentss&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;&amp;lt;strong&amp;gt;(A)&amp;lt;/strong&amp;gt; This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. &amp;lt;strong&amp;gt;(B)&amp;lt;/strong&amp;gt; This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;2657&#34; height=&#34;1417&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;(A)&lt;/strong&gt; This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. &lt;strong&gt;(B)&lt;/strong&gt; This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Beyond simply faster and slower: exploring paradoxes in speed perception</title>
      <link>https://laurentperrinet.github.io/publication/meso-14-vss/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/meso-14-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge co-occurrences are sufficient to categorize natural versus animal images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-bednar-14-vss/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-bednar-14-vss/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction model for flash lag effect</title>
      <link>https://laurentperrinet.github.io/publication/khoei-14-vss/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-14-vss/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt; and 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;Khoei et al, 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The characteristics of microsaccadic eye movements varied with the change of strategy in a match-to-sample task</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-14-vss/</link>
      <pubDate>Fri, 22 Aug 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-14-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</link>
      <pubDate>Fri, 25 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/kaplan-khoei-14/&#34;&gt;Kaplan and al, 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>WP5 - Demo 1.3 : Spiking model of motion-based prediction</title>
      <link>https://laurentperrinet.github.io/talk/2014-03-20-manchester/</link>
      <pubDate>Thu, 20 Mar 2014 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-03-20-manchester/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Axonal delays and on-time control of eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Simple Model of Orientation Encoding Accounting For Multivariate Neural Noise</title>
      <link>https://laurentperrinet.github.io/publication/taouali-14-areadne/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/taouali-14-areadne/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a follow-up in this 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/taouali-16/&#34;&gt;publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A Simple Model of Orientation Encoding Accounting For Multivariate Neural Noise</title>
      <link>https://laurentperrinet.github.io/publication/taouali-14-neurocomp/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/taouali-14-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a follow-up in this 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/taouali-16/&#34;&gt;publication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic Textures For Probing Motion Perception</title>
      <link>https://laurentperrinet.github.io/publication/vacher-14-ihp/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-14-ihp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the nature of anticipatory eye movements and the factors affecting them</title>
      <link>https://laurentperrinet.github.io/publication/damasse-14-gdr/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/damasse-14-gdr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Relationship between natural image statistics and lateral connectivity in the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/rudiger-14-cosyne/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/rudiger-14-cosyne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/publication/kaplan-khoei-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kaplan-khoei-14/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;


















&lt;figure id=&#34;figure-figure-4-rasterplot-of-input-and-output-spikes-the-raster-plot-from-excitatory-neurons-is-ordered-according-to-their-position-each-input-spike-is-a-blue-dot-and-each-output-spike-is-a-black-dot-while-input-is-scattered-during-blanking-periods-figure-1-the-network-output-shows-shows-some-tuned-activity-during-the-blank-compare-with-the-activity-before-visual-stimulation-to-decode-such-patterns-of-activity-we-used-a-maximum-likelihood-estimation-technique-based-on-the-tuning-curve-of-the-neurons&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.frontiersin.org/files/Articles/53894/fncom-07-00112-r2/image_m/fncom-07-00112-g003.jpg&#34; data-caption=&#34;Figure 4: &amp;lt;em&amp;gt;Rasterplot of input and output spikes.&amp;lt;/em&amp;gt; The raster plot from excitatory neurons is ordered according to their position. Each input spike is a blue dot and each output spike is a black dot. While input is scattered during blanking periods (Figure 1), the network output shows shows some tuned activity during the blank (compare with the activity before visual stimulation). To decode such patterns of activity we used a maximum-likelihood estimation technique based on the tuning curve of the neurons.&#34;&gt;


  &lt;img src=&#34;https://www.frontiersin.org/files/Articles/53894/fncom-07-00112-r2/image_m/fncom-07-00112-g003.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 4: &lt;em&gt;Rasterplot of input and output spikes.&lt;/em&gt; The raster plot from excitatory neurons is ordered according to their position. Each input spike is a blue dot and each output spike is a black dot. While input is scattered during blanking periods (Figure 1), the network output shows shows some tuned activity during the blank (compare with the activity before visual stimulation). To decode such patterns of activity we used a maximum-likelihood estimation technique based on the tuning curve of the neurons.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Demo 1, Task4: Implementation of models showing emergence of cortical fields and maps</title>
      <link>https://laurentperrinet.github.io/talk/2013-11-26-brain-scales-demos/</link>
      <pubDate>Tue, 26 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2013-11-26-brain-scales-demos/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Together with Bernhard Kaplan, we talked about how we aim at &amp;ldquo;compiling&amp;rdquo; a predictive motion-based approach as a spiking neural networks and then as a parallel wafer systems in the BrainscaleS project (Demo 1, Task4).&lt;/li&gt;
&lt;li&gt;(private to the consortium: &lt;a href=&#34;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showMeetingInfoPage&amp;amp;meetingID=52&#34;&gt;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showMeetingInfoPage&amp;amp;meetingID=52&lt;/a&gt;  &lt;a href=&#34;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showAgenda&amp;amp;meetingID=52&#34;&gt;https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showAgenda&amp;amp;meetingID=52&lt;/a&gt;  including copies of the slides)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction explains the role of tracking in motion extrapolation</title>
      <link>https://laurentperrinet.github.io/publication/khoei-13-jpp/</link>
      <pubDate>Sun, 03 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-13-jpp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;Khoei et al, 2017&lt;/a&gt;






  



  
  











&lt;figure id=&#34;figure-figure-1-the-problem-of-fragmented-trajectories-and-motion-extrapolation-as-an-object-moves-in-visual-space-as-represented-here-for-commodity-by-the-red-trajectory-of-a-tennis-ball-in-a-spacetime-diagram-with-a-one-dimensional-space-on-the-vertical-axis-the-sensory-flux-may-be-interrupted-by-a-sudden-and-transient-blank-as-denoted-by-the-vertical-gray-area-and-the-dashed-trajectory-how-can-the-instantaneous-position-of-the-dot-be-estimated-at-the-time-of-reappearance-this-mechanism-is-the-basis-of-motion-extrapolation-and-is-rooted-on-the-prior-knowledge-on-the-coherency-of-trajectories-in-natural-images-we-show-below-the-typical-eye-velocity-profile-that-is-observed-during-smooth-pursuit-eye-movements-spem-as-a-prototypical-sensory-response-it-consists-of-three-phases-first-a-convergence-of-the-eye-velocity-toward-the-physical-speed-second-a-drop-of-velocity-during-the-blank-and-finally-a-sudden-catch-up-of-speed-at-reappearance-becker-and-fuchs-1985&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Figure 1: The problem of fragmented trajectories and motion extrapolation. As an object moves in visual space (as represented here for commodity by the red trajectory of a tennis ball in a space‚Äìtime diagram with a one-dimensional space on the vertical axis), the sensory flux may be interrupted by a sudden and transient blank (as denoted by the vertical, gray area and the dashed trajectory). How can the instantaneous position of the dot be estimated at the time of reappearance? This mechanism is the basis of motion extrapolation and is rooted on the prior knowledge on the coherency of trajectories in natural images. We show below the typical eye velocity profile that is observed during Smooth Pursuit Eye Movements (SPEM) as a prototypical sensory response. It consists of three phases: first, a convergence of the eye velocity toward the physical speed, second, a drop of velocity during the blank and finally, a sudden catch-up of speed at reappearance (Becker and Fuchs, 1985).&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/figure1_hu90c98597bfecb4ee68b0b9d1509b5c1c_14061_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;293&#34; height=&#34;223&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 1: The problem of fragmented trajectories and motion extrapolation. As an object moves in visual space (as represented here for commodity by the red trajectory of a tennis ball in a space‚Äìtime diagram with a one-dimensional space on the vertical axis), the sensory flux may be interrupted by a sudden and transient blank (as denoted by the vertical, gray area and the dashed trajectory). How can the instantaneous position of the dot be estimated at the time of reappearance? This mechanism is the basis of motion extrapolation and is rooted on the prior knowledge on the coherency of trajectories in natural images. We show below the typical eye velocity profile that is observed during Smooth Pursuit Eye Movements (SPEM) as a prototypical sensory response. It consists of three phases: first, a convergence of the eye velocity toward the physical speed, second, a drop of velocity during the blank and finally, a sudden catch-up of speed at reappearance (Becker and Fuchs, 1985).
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Anisotropic connectivity implements motion-based prediction in a spiking neural network</title>
      <link>https://laurentperrinet.github.io/publication/kaplan-13/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kaplan-13/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge co-occurrences and categorizing natural images</title>
      <link>https://laurentperrinet.github.io/talk/2013-07-05-cerco/</link>
      <pubDate>Fri, 05 Jul 2013 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2013-07-05-cerco/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why methods and tools are the key to artificial brain-like systems</title>
      <link>https://laurentperrinet.github.io/talk/2013-03-21-marseille/</link>
      <pubDate>Thu, 21 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2013-03-21-marseille/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active inference, eye movements and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-13-cns/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-13-cns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active inference, eye movements and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-13-jffos/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-13-jffos/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Advances in Texture Analysis for Emphysema Classification</title>
      <link>https://laurentperrinet.github.io/publication/nava-13/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/nava-13/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How and why do image frequency properties influence perceived speed?</title>
      <link>https://laurentperrinet.github.io/publication/meso-13-vss/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/meso-13-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-13-vss/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-13-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motion-based prediction and development of the response to an &#39;on the way&#39; stimulus</title>
      <link>https://laurentperrinet.github.io/publication/khoei-13-cns/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-13-cns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Smooth Pursuit and Visual Occlusion: Active Inference and Oculomotor Control in Schizophrenia</title>
      <link>https://laurentperrinet.github.io/publication/adams-12/</link>
      <pubDate>Fri, 26 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/adams-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;adams-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2012-05-10-itwist/</link>
      <pubDate>Thu, 10 May 2012 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-05-10-itwist/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Apparent motion in V1 - Probabilistic approaches</title>
      <link>https://laurentperrinet.github.io/talk/2012-03-23-juelich/</link>
      <pubDate>Fri, 23 Mar 2012 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-03-23-juelich/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MotionClouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception</title>
      <link>https://laurentperrinet.github.io/talk/2012-03-22-juelich/</link>
      <pubDate>Thu, 22 Mar 2012 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-03-22-juelich/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The behavioral receptive field underlying motion integration for primate tracking eye movements</title>
      <link>https://laurentperrinet.github.io/publication/masson-12/</link>
      <pubDate>Wed, 21 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/masson-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;masson-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motion Clouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception</title>
      <link>https://laurentperrinet.github.io/publication/sanz-12/</link>
      <pubDate>Wed, 14 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/sanz-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;sanz-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grabbing, tracking and sniffing as models for motion detection and eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-27-fil/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-27-fil/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-24-edinburgh/</link>
      <pubDate>Tue, 24 Jan 2012 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-24-edinburgh/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</link>
      <pubDate>Thu, 12 Jan 2012 17:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Active inference, smooth pursuit and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-12-areadne/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-12-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Complex dynamics in recurrent cortical networks based on spatially realistic connectivities</title>
      <link>https://laurentperrinet.github.io/publication/voges-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;voges-12.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Effect of image statistics on fixational eye movements</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-12-vss/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-12-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception.</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-12-coding/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-12-coding/</guid>
      <description></description>
    </item>
    
    <item>
      <title>More is not always better: dissociation between perception and action explained by adaptive gain control</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;simoncini-12.png&#34; alt=&#34;header&#34;&gt;






  



  
  











&lt;figure id=&#34;figure-band-pass-motion-stimuli-for-perception-and-action-tasks-a-in-the-space-representing-temporal-against-spatial-frequency-each-line-going-through-the-origin-corresponds-to-stimuli-moving-at-the-same-speed-a-simple-drifting-grating-is-a-single-point-in-this-space-our-moving-texture-stimuli-had-their-energy-distributed-within-an-ellipse-elongated-along-a-given-speed-line-keeping-constant-the-mean-spatial-and-temporal-frequencies-the-spatio-temporal-bandwidth-was-manipulated-by-co-varying-bsf-and-btf-as-illustrated-by-the-xyt-examples-human-performance-was-measured-for-two-different-tasks-run-in-parallel-blocks-b-for-ocular-tracking-motion-stimuli-were-presented-for-a-short-duration-200ms-in-the-wake-of-a-centering-saccade-to-control-both-attention-and-fixation-states-c-for-speed-discrimination-test-and-reference-stimuli-were-presented-successively-for-the-same-duration-and-subjects-were-instructed-to-indicate-whether-the-test-stimulus-was-perceived-as-slower-or-faster-than-reference&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_2000x2000_fit_lanczos.gif&#34; data-caption=&#34;&amp;lt;em&amp;gt;Band-pass motion stimuli for perception and action tasks.&amp;lt;/em&amp;gt; (a) In the space representing temporal against spatial frequency, each line going through the origin corresponds to stimuli moving at the same speed. A simple drifting grating is a single point in this space. Our moving texture stimuli had their energy distributed within an ellipse elongated along a given speed line, keeping constant the mean spatial and temporal frequencies. The spatio-temporal bandwidth was manipulated by co-varying Bsf and Btf as illustrated by the (x,y,t) examples. Human performance was measured for two different tasks, run in parallel blocks. (b) For ocular tracking, motion stimuli were presented for a short duration (200ms) in the wake of a centering saccade to control both attention and fixation states. (c) For speed discrimination, test and reference stimuli were presented successively for the same duration and subjects were instructed to indicate whether the test stimulus was perceived as slower or faster than reference.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/simoncini-12/grating_hu30edaebf880fa16017958e882ef41604_19283739_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;256&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Band-pass motion stimuli for perception and action tasks.&lt;/em&gt; (a) In the space representing temporal against spatial frequency, each line going through the origin corresponds to stimuli moving at the same speed. A simple drifting grating is a single point in this space. Our moving texture stimuli had their energy distributed within an ellipse elongated along a given speed line, keeping constant the mean spatial and temporal frequencies. The spatio-temporal bandwidth was manipulated by co-varying Bsf and Btf as illustrated by the (x,y,t) examples. Human performance was measured for two different tasks, run in parallel blocks. (b) For ocular tracking, motion stimuli were presented for a short duration (200ms) in the wake of a centering saccade to control both attention and fixation states. (c) For speed discrimination, test and reference stimuli were presented successively for the same duration and subjects were instructed to indicate whether the test stimulus was perceived as slower or faster than reference.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/publication/masson-12-areadne/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/masson-12-areadne/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-12-pred/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-12-pred/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-12-pred.png&#34; alt=&#34;header&#34;&gt;






  



  
  











&lt;figure id=&#34;figure-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-receptive-field-of-a-neuron-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-introducing-predictive-coding-resolves-the-aperture-problem&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_2000x2000_fit_lanczos.gif&#34; data-caption=&#34;The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/line_particles_hu7e6cd28581c7ffe8e1cbeb15ea016578_13208353_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;300&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.
  &lt;/figcaption&gt;


&lt;/figure&gt;







  



  
  











&lt;figure id=&#34;figure-figure-1-a-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-dotted-circle-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-due-to-the-limited-size-of-receptive-fields-in-sensory-cortical-areas-such-as-shown-by-the-dotted-white-circle-such-problem-is-faced-by-local-populations-of-neurons-that-visually-estimate-the-motion-of-objects-a-inset-on-a-polar-representation-of-possible-velocity-vectors-the-cross-in-the-center-corresponds-to-the-null-velocity-the-outer-circle-corresponding-to-twice-the-amplitude-of-physical-speed-we-plot-the-empirical-histogram-of-detected-velocity-vectors-this-representation-gives-a-quantification-of-the-aperture-problem-in-the-velocity-domain-at-the-onset-of-motion-detection-information-is-concentrated-along-an-elongated-constraint-line-whitehigh-probability-blackzero-probability-b-we-use-the-prior-knowledge-that-in-natural-scenes-motion-as-defined-by-its-position-and-velocity-is-following-smooth-trajectories-quantitatively-it-means-that-velocity-is-approximately-conserved-and-that-position-is-transported-according-to-the-known-velocity-we-show-here-such-a-transition-on-position-and-velocity-respectively-x_t-and-v_t-from-time-t-to-t--dt-with-the-perturbation-modeling-the-smoothness-of-prediction-in-position-and-velocity-respectively-n_x-and-n_v-c-applying-such-a-prior-on-a-dynamical-system-detecting-motion-we-show-that-motion-converges-to-the-physical-motion-after-approximately-one-spatial-period-the-line-moved-by-twice-its-height-c-inset-the-read-out-of-the-system-converged-to-the-physical-motion-motion-based-prediction-is-sufficient-to-resolve-the-aperture-problem-d-as-observed-at-the-perceptual-level-castet-et-al-1993-pei-et-al-2010-size-and-duration-of-the-tracking-angle-bias-decreased-with-respect-to-the-height-of-the-line-height-was-measured-relative-to-a-spatial-period-respectively-60-40-and-20-here-we-show-the-average-tracking-angle-red-out-from-the-probabilistic-representation-as-a-function-of-time-averaged-over-20-trials-error-bars-show-one-standard-deviation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Figure 1: &amp;lt;em&amp;gt;(A)&amp;lt;/em&amp;gt; The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. &amp;lt;em&amp;gt;(A-inset)&amp;lt;/em&amp;gt; On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). &amp;lt;em&amp;gt;(B)&amp;lt;/em&amp;gt; We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t &amp;#43; dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). &amp;lt;em&amp;gt;(C)&amp;lt;/em&amp;gt; Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). &amp;lt;em&amp;gt;(C-Inset)&amp;lt;/em&amp;gt; The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. &amp;lt;em&amp;gt;(D)&amp;lt;/em&amp;gt; As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;717&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 1: &lt;em&gt;(A)&lt;/em&gt; The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: it‚Äôs the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. &lt;em&gt;(A-inset)&lt;/em&gt; On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). &lt;em&gt;(B)&lt;/em&gt; We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t + dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). &lt;em&gt;(C)&lt;/em&gt; Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). &lt;em&gt;(C-Inset)&lt;/em&gt; The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. &lt;em&gt;(D)&lt;/em&gt; As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).
  &lt;/figcaption&gt;


&lt;/figure&gt;







  



  
  











&lt;figure id=&#34;figure-figure-2-architecture-of-the-model-the-model-is-constituted-by-a-classical-measurement-stage-and-of-a-predictive-coding-layer-the-measurement-stage-consists-of-a-inferring-from-two-consecutive-frames-of-the-input-flow-b-a-likelihood-distribution-of-motion-this-layer-interacts-with-the-predictive-layer-which-consists-of-c-a-prediction-stage-that-infers-from-the-current-estimate-and-the-transition-prior-the-upcoming-state-estimate-and-d-an-estimation-stage-that-merges-the-current-prediction-of-motion-with-the-likelihood-measured-at-the-same-instant-in-the-previous-layer-b&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;695&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).
  &lt;/figcaption&gt;


&lt;/figure&gt;







  



  
  











&lt;figure id=&#34;figure-figure-3-to-explore-the-state-space-of-the-dynamical-system-we-simulated-motion-based-prediction-for-a-simple-small-dot-size-25-of-a-spatial-period-moving-horizontally-from-the-left-to-the-right-of-the-screen-we-tested-different-levels-of-sensory-noise-with-respect-to-different-levels-of-internal-noise-that-is-to-different-values-of-the-strength-of-prediction-right-results-show-the-emergence-of-different-states-for-different-prediction-precisions-a-regime-when-prediction-is-weak-and-which-shows-high-tracking-error-and-variability-no-tracking---nt-a-phase-for-intermediate-values-of-prediction-strength-as-in-figure-1-exhibiting-a-low-tracking-error-and-low-variability-in-the-tracking-phase-true-tracking---tt-and-finally-a-phase-corresponding-to-higher-precisions-with-relatively-efficient-mean-detection-but-high-variability-false-tracking---ft-we-give-3-representative-examples-of-the-emerging-states-at-one-contrast-level-c--01-with-starting-red-and-ending-blue-points-and-respectively-nt-tt-and-ft-by-showing-inferred-trajectories-for-each-trial-left-we-define-tracking-error-as-the-ratio-between-detected-speed-and-target-speed-and-we-plot-it-with-respect-to-the-stimulus-contrast-as-given-by-the-inverse-of-sensory-noise-error-bars-give-the-variability-in-tracking-error-as-averaged-over-20-trials-as-prediction-strength-increases-there-is-a-transition-from-smooth-contrast-response-function-nt-to-more-binary-responses-tt-and-ft&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. &amp;lt;em&amp;gt;(Right)&amp;lt;/em&amp;gt; Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. &amp;lt;em&amp;gt;(Left)&amp;lt;/em&amp;gt; We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;483&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. &lt;em&gt;(Right)&lt;/em&gt; Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. &lt;em&gt;(Left)&lt;/em&gt; We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).
  &lt;/figcaption&gt;


&lt;/figure&gt;







  



  
  











&lt;figure id=&#34;figure-figure-4-top-prediction-implements-a-competition-between-different-trajectories-here-we-focus-on-one-step-of-the-algorithm-by-testing-different-trajectories-at-three-key-positions-of-the-segment-stimulus-the-two-edges-and-the-center-dashed-circles-compared-to-the-pure-sensory-velocity-likelihood-left-insets-in-grayscale-prediction-modulates-response-as-shown-by-the-velocity-vectors-direction-coded-as-hue-as-in-figure-1-and-by-the-ratio-of-velocity-probabilities-log-ratio-in-bits-right-insets-there-is-no-change-for-the-middle-of-the-segment-yellow-tone-but-trajectories-that-are-predicted-out-of-the-line-are-explained-away-navy-tone-while-others-may-be-amplified-orange-tone-notice-the-asymmetry-between-both-edges-the-upper-edge-carrying-a-suppressive-predictive-information-while-the-bottom-edge-diffuses-coherent-motion-bottom-finally-the-aperture-problem-is-solved-due-to-the-repeated-application-of-this-spatio-temporal-contextual-information-modulation-to-highlight-the-anisotropic-diffusion-of-information-over-the-rest-of-the-line-we-plot-as-a-function-of-time-horizontal-axis-the-histogram-of-the-detected-motion-marginalized-over-horizontal-positions-vertical-axis-while-detected-direction-of-velocity-is-given-by-the-distribution-of-hues-blueish-colors-correspond-to-the-direction-perpendicular-to-the-diagonal-while-a-green-color-represents-a-disambiguated-motion-to-the-right-as-in-figure-1-the-plot-shows-that-motion-is-disambiguated-by-progressively-explaining-away-incoherent-motion-note-the-asymmetry-in-the-propagation-of-coherent-information&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;Figure 4: &amp;lt;em&amp;gt;(Top)&amp;lt;/em&amp;gt; Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are ‚Äúexplained away‚Äù (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. &amp;lt;em&amp;gt;(Bottom)&amp;lt;/em&amp;gt; Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;968&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Figure 4: &lt;em&gt;(Top)&lt;/em&gt; Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are ‚Äúexplained away‚Äù (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. &lt;em&gt;(Bottom)&lt;/em&gt; Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-11-vss/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-11-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Perceptions as Hypotheses: Saccades as Experiments</title>
      <link>https://laurentperrinet.github.io/publication/friston-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/friston-12/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;friston-12.png&#34; alt=&#34;header&#34;&gt;


















&lt;figure id=&#34;figure-this-schematic-shows-the-dependencies-among-various-quantities-that-are-assumed-when-modeling-the-exchanges-of-a-self-organizing-system-like-the-brain-with-the-environment-the-top-panel-describes-the-states-of-the-environment-and-the-system-or-agent-in-terms-of-a-probabilistic-dependency-graph-where-connections-denote-directed-dependencies-the-quantities-are-described-within-the-nodes-of-this-graph-with-exemplar-forms-for-their-dependencies-on-other-variables-see-main-text-here-hidden-and-internal-states-are-separated-by-action-and-sensory-states-both-action-and-internal-states-encoding-a-conditional-density-minimize-free-energy-while-internal-states-encoding-prior-beliefs-maximize-salience-both-free-energy-and-salience-are-defined-in-terms-of-a-generative-model-that-is-shown-as-fictive-dependency-graph-in-the-lower-panel-note-that-the-variables-in-the-real-world-and-the-form-of-their-dynamics-are-different-from-that-assumed-by-the-generative-model-this-is-why-external-states-are-in-bold-furthermore-note-that-action-is-a-state-in-the-model-of-the-brain-but-is-replaced-by-hidden-controls-in-the-brains-model-of-its-world-this-means-that-the-agent-is-not-aware-of-action-but-has-beliefs-about-hidden-causes-in-the-world-that-action-can-fulfill-through-minimizing-free-energy-these-beliefs-correspond-to-prior-expectations-that-sensory-states-will-be-sampled-in-a-way-that-optimizes-conditional-confidence-or-salience&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://www.frontiersin.org/files/Articles/21922/fpsyg-03-00151-r4/image_m/fpsyg-03-00151-g001.jpg&#34; data-caption=&#34;&amp;lt;strong&amp;gt;This schematic shows the dependencies among various quantities that are assumed when modeling the exchanges of a self organizing system like the brain with the environment.&amp;lt;/strong&amp;gt; The top panel describes the states of the environment and the system or agent in terms of a probabilistic dependency graph, where connections denote directed dependencies. The quantities are described within the nodes of this graph with exemplar forms for their dependencies on other variables (see main text). Here, hidden and internal states are separated by action and sensory states. Both action and internal states encoding a conditional density minimize free energy, while internal states encoding prior beliefs maximize salience. Both free energy and salience are defined in terms of a generative model that is shown as fictive dependency graph in the lower panel. Note that the variables in the real world and the form of their dynamics are different from that assumed by the generative model; this is why external states are in bold. Furthermore, note that action is a state in the model of the brain but is replaced by hidden controls in the brain‚Äôs model of its world. This means that the agent is not aware of action but has beliefs about hidden causes in the world that action can fulfill through minimizing free energy. These beliefs correspond to prior expectations that sensory states will be sampled in a way that optimizes conditional confidence or salience.&#34;&gt;


  &lt;img src=&#34;https://www.frontiersin.org/files/Articles/21922/fpsyg-03-00151-r4/image_m/fpsyg-03-00151-g001.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;This schematic shows the dependencies among various quantities that are assumed when modeling the exchanges of a self organizing system like the brain with the environment.&lt;/strong&gt; The top panel describes the states of the environment and the system or agent in terms of a probabilistic dependency graph, where connections denote directed dependencies. The quantities are described within the nodes of this graph with exemplar forms for their dependencies on other variables (see main text). Here, hidden and internal states are separated by action and sensory states. Both action and internal states encoding a conditional density minimize free energy, while internal states encoding prior beliefs maximize salience. Both free energy and salience are defined in terms of a generative model that is shown as fictive dependency graph in the lower panel. Note that the variables in the real world and the form of their dynamics are different from that assumed by the generative model; this is why external states are in bold. Furthermore, note that action is a state in the model of the brain but is replaced by hidden controls in the brain‚Äôs model of its world. This means that the agent is not aware of action but has beliefs about hidden causes in the world that action can fulfill through minimizing free energy. These beliefs correspond to prior expectations that sensory states will be sampled in a way that optimizes conditional confidence or salience.
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Role of motion-based prediction in motion extrapolation</title>
      <link>https://laurentperrinet.github.io/publication/khoei-12-sfn/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-12-sfn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2011-11-15-sfn/</link>
      <pubDate>Tue, 15 Nov 2011 08:45:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-11-15-sfn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Abstract Control Number: 17671&lt;/li&gt;
&lt;li&gt;Presentation Number: 530.04&lt;/li&gt;
&lt;li&gt;Presentation Time: 8:45am - 9:00am&lt;/li&gt;
&lt;li&gt;session:&lt;/li&gt;
&lt;li&gt;Session Type: Nanosymposium&lt;/li&gt;
&lt;li&gt;Session Number: 530&lt;/li&gt;
&lt;li&gt;Session Title: Development of Motor and Sensory Systems&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Demo 1, Task4: Implementation of models showing emergence of cortical fields and maps</title>
      <link>https://laurentperrinet.github.io/talk/2011-10-05-brain-scales-ess/</link>
      <pubDate>Wed, 05 Oct 2011 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-10-05-brain-scales-ess/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/talk/2011-09-28-ermites/</link>
      <pubDate>Wed, 28 Sep 2011 13:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-09-28-ermites/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-11-pattern/</link>
      <pubDate>Fri, 23 Sep 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-11-pattern/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Propri√©t√©s √©mergentes d&#39;un mod√®le de pr√©diction probabiliste utilisant un champ neural</title>
      <link>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</link>
      <pubDate>Sat, 02 Jul 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</guid>
      <description>&lt;p&gt;La finalit√© de cette manifestation est de permettre √† nos chercheurs de se r√©unir en groupes de travail et en ateliers afin de d√©couvrir la th√©matique des neurosciences et son interdisciplinarit√©. La manifestation se tient dans le cadre des activit√©s du laboratoire LAMS, de ABC MATHINFO, du GDRI NeurO et du r√©seau m√©diterran√©en 
&lt;a href=&#34;http://www.neuromedproject.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroMed&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;SPIE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Qui cr√©era le premier ordinateur intelligent?</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-doc-sciences/</link>
      <pubDate>Mon, 20 Jun 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-doc-sciences/</guid>
      <description>&lt;h1 id=&#34;qui-cr√©era-le-premier-ordinateur-intelligent&#34;&gt;Qui cr√©era le premier ordinateur intelligent?&lt;/h1&gt;
&lt;p&gt;Les ordinateurs classiques sont de plus en plus puissants, mais restent toujours aussi ¬´ stupides ¬ª. Impossible d‚Äôen trouver un avec lequel on puisse dialoguer de fa√ßon naturelle. Aucun syst√®me visuel artificiel ne voit aussi bien que nous, ou qu‚Äôune mouche ! Alors qui inventera le premier calculateur intelligent ?
&lt;img src=&#34;featured.jpg&#34; alt=&#34;Code neural&#34;&gt;
Le code neural (En haut : ¬© F. Chavane, en bas : ¬© T. Bal).
Le code neural est mieux compris gr√¢ce aux techniques d‚Äôimagerie r√©centes. Les neurosciences computationnelles permettent d‚Äô√©tudier les propri√©t√©s des r√©seaux de neurones.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pursuing motion illusions: a realistic oculomotor framework for Bayesian inference</title>
      <link>https://laurentperrinet.github.io/publication/bogadhi-11/</link>
      <pubDate>Fri, 22 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/bogadhi-11/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;bogadhi-11.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Saccadic foveation of a moving visual target in the rhesus monkey</title>
      <link>https://laurentperrinet.github.io/publication/fleuriet-11/</link>
      <pubDate>Tue, 01 Feb 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fleuriet-11/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-11-sfn/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-11-sfn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-bednar-15/&#34;&gt;Perrinet and Bednar, 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Role of motion inertia in dynamic motion integration for smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/khoei-11-ecvp/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-11-ecvp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic models of the low-level visual system: the role of prediction in detecting motion</title>
      <link>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</link>
      <pubDate>Fri, 17 Dec 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</guid>
      <description>&lt;p&gt;An event ranging &amp;ldquo;From Mathematical Image Analysis to Neurogeometry of the Brain&amp;rdquo; 
&lt;a href=&#34;http://www.conftauc.cnrs-gif.fr/programme.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LADISLAV TAUC &amp;amp; GDR MSPC NEUROSCIENCES CONFERENCE&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication from Mina Khoei @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;TAUC 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Phase space analysis of networks based on biologically realistic parameters</title>
      <link>https://laurentperrinet.github.io/publication/voges-10-jpp/</link>
      <pubDate>Wed, 10 Nov 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-10-jpp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;voges-10-jpp.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Role of homeostasis in learning sparse representations</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-shl/</link>
      <pubDate>Sat, 17 Jul 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-shl/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-10-shl.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2010-06-22 : CodeJamNr4</title>
      <link>https://laurentperrinet.github.io/post/2010-06-22_codejam-nr4/</link>
      <pubDate>Tue, 22 Jun 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2010-06-22_codejam-nr4/</guid>
      <description>&lt;h1 id=&#34;facets-code-jam-workshop-4&#34;&gt;FACETS Code Jam Workshop #4&lt;/h1&gt;
&lt;p&gt;We held a CodeJam 22nd-24th June 2010, in Marseille.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://neuralensemble.org/media/images/codejam4_group_photo.jpg&#34; alt=&#34;Participants&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the

&lt;a href=&#34;http://neuralensemble.org/meetings/CodeJam4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The goal of the FACETS CodeJam workshops is to catalyze open-source, collaborative software development in computational and systems neuroscience and neuroinformatics, by bringing together researchers, students and engineers to share ideas, present their work, and write code together. The general format of the workshops is to dedicate the mornings to invited and contributed talks, leaving the afternoons free for discussions and code sprints. &lt;BR&gt;
For the 4th FACETS CodeJam, the main theme of the meeting will be workflows: what are the best practices for combining different tools (simulators, analysis tools, visualization tools, databases etc.) to ensure the efficient and reproducible flow of data and information from experiment conception to publication and archiving? &lt;BR&gt;
(&amp;hellip;) &lt;BR&gt;
The meeting organizers gratefully acknowledge the support of the European Union through the FACETS Project (grant no. IST-2005-15879), and the International Neuroinformatics Co-ordinating Facility (INCF). We also wish to express our great appreciation to the DyVA team at the Institut de Neurosciences Cognitives de la M√©diterran√©e for providing us with a great location and much assistance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuralensemble.org/meetings/CodeJam4.html&#34;&gt;http://neuralensemble.org/meetings/CodeJam4.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuralensemble.org/meetings/CJ4_Program_v2.pdf&#34;&gt;http://neuralensemble.org/meetings/CJ4_Program_v2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://facets.kip.uni-heidelberg.de/internal/jss/AttendMeeting?mI=73&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FACETS code jam #4&lt;/a&gt;{.https}&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;Affiche&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functional consequences of correlated excitatory and inhibitory conductances in cortical networks</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-10-jcns/</link>
      <pubDate>Thu, 03 Jun 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-10-jcns/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;kremkow-10-jcns.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2010-05-27 : Neurocomp08</title>
      <link>https://laurentperrinet.github.io/post/2008-10-08_neurocomp/</link>
      <pubDate>Thu, 27 May 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2008-10-08_neurocomp/</guid>
      <description>&lt;h1 id=&#34;2008-10-08--deuxi√®me-conf√©rence-fran√ßaise-de-neurosciences-computationnelles-neurocomp08&#34;&gt;2008-10-08 : Deuxi√®me conf√©rence fran√ßaise de Neurosciences Computationnelles, &amp;ldquo;Neurocomp08&amp;rdquo;&lt;/h1&gt;
&lt;p&gt;La deuxi√®me conf√©rence fran√ßaise de Neurosciences Computationnelles, &amp;ldquo;Neurocomp08&amp;rdquo;, s&amp;rsquo;est d√©roul√©e √† la Facult√© de M√©decine de Marseille du 8 au 11 octobre 2008. Cette conf√©rence, organis√©e par le groupe de travail Neurocomp, a permis de r√©unir les principaux acteurs fran√ßais du domaine (francophones ou non). Le champ des Neurosciences Computationnelles porte sur l&amp;rsquo;√©tude des m√©canismes de calcul qui sont √† l&amp;rsquo;origine de nos capacit√©s cognitives. Cette approche n√©cessite l&amp;rsquo;int√©gration constructive de nombreux domaines disciplinaires, du neurone au comportement, des sciences du vivant √† la mod√©lisation num√©rique. Avec ce colloque, nous avons offert un lieu d&amp;rsquo;√©changes afin de favoriser des collaborations interdisciplinaires entre des √©quipes relevant des neurosciences, des sciences de l&amp;rsquo;information, de la physique statistique, de la robotique. Cette √©dition a √©galement √©t√© l&amp;rsquo;occasion d&amp;rsquo;ouvrir le cadre √† de nouveaux domaines (mod√®les pour l&amp;rsquo;imagerie, interfaces cerveau-machine,&amp;hellip;) notamment gr√¢ce √† des ateliers th√©matiques (une nouveaut√© dans cette √©dition). Certains des principaux enjeux du domaine ont √©t√© pr√©sent√©s par quatre conf√©renciers invit√©s : Ad Aertsen (Freiburg, Allemagne), Gustavo Deco (Barcelone, Espagne), Gregor Sch√∂ner (Bochum, Allemagne), Andrew B. Schwartz (Pittsburgh, USA). Des interventions orale plus courtes et plus sp√©cifiques √©taient √©galement au programme, sur la base d&amp;rsquo;une s√©lection du comit√© de lecture. Une cinquantaine de posters ont √©galement √©t√© pr√©sent√©s au cours de ces journ√©es. Le premier jour √©tait consacr√© aux mod√®les de la cellule neurale, aux mod√®les des traitements visuels et corticaux, ainsi qu&amp;rsquo;aux mod√®les de r√©seaux de neurones bio-mim√©tiques. La seconde journ√©e √©tait consacr√©e aux interfaces cerveau-machine, √† la dynamique des grands ensembles de neurones, √† la plasticit√© fonctionnelle et aux interfaces neurales. Enfin, la journ√©e de samedi √©tait consacr√©e √† des ateliers th√©matiques, l&amp;rsquo;un sur les interfaces cerveau-machine, l&amp;rsquo;autre sur la vision computationnnelle. Cette conf√©rence a connu un beau succ√®s de par l&amp;rsquo;affluence (200 personnes environ) et la qualit√© des interventions. Ce succ√®s tient √©galement au fort soutien financier et organisationnel qu&amp;rsquo;elle a obtenu de ses partenaires. Les organisateurs remercient le CNRS, la Soci√©t√© des neurosciences, le conseil r√©gional de la r√©gion Provence Alpes C√¥te d&amp;rsquo;Azur, le conseil g√©n√©ral des Bouches de Rh√¥ne, la mairie de Marseille, l&amp;rsquo;universit√© de Provence, l&amp;rsquo;IFR &amp;ldquo;Sciences du cerveau et de la cognition&amp;rdquo;, l&amp;rsquo;INRIA, ainsi que la facult√© de m√©decine de Marseille et l&amp;rsquo;universit√© de la M√©diterran√©e qui ont h√©berg√© la conf√©rence.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les actes de la conf√©rence regroupant les 68 contributions sont disponibles sur le 
&lt;a href=&#34;http://hal.archives-ouvertes.fr/NEUROCOMP08&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serveur HAL d√©di√©&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;featured.jpg&#34; alt=&#34;Affiche&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diffraction monochromatique, spectre audiographique</title>
      <link>https://laurentperrinet.github.io/talk/2010-04-14-ondes-paralleles/</link>
      <pubDate>Wed, 14 Apr 2010 19:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-04-14-ondes-paralleles/</guid>
      <description>&lt;h1 id=&#34;diffraction-monochromatique-spectre-audiographique&#34;&gt;Diffraction monochromatique, spectre audiographique&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;ttp://ondesparalleles.org/wp-content/uploads/2014/02/cloche_fiche_a.jpg&#34; alt=&#34;Diffraction&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diffraction est une sculpture en suspension compos√©e d‚Äôune multitude de plaques de mati√®re transparente et r√©fl√©chissante. L‚Äôinstallation met en jeu notre perception de l‚Äôespace par des ph√©nom√®nes de r√©sonance et de r√©flection de la lumi√®re. Chaque lieu d‚Äôexposition donne √† exp√©rimenter et √† √©laborer, in situ, de nouvelles formes. A Seconde Nature, Etienne Rey abordera la relation entre le volume et le son en prenant comme base de construction un spectre audio, en collaboration avec l‚Äôartiste sonore Mathias Delplanque.&lt;/li&gt;
&lt;li&gt;Live de Mathias Delplanque et rencontre autour de Diffraction, le Mercredi 14 avril 2010: A l‚Äôoccasion de cette rencontre publique, quatre chercheurs sp√©cialistes de l‚Äôarchitecture, de la perception, du son, et de la lumi√®re exposeront depuis leurs domaines de recherches les processus engag√©s autour de Diffraction.`&lt;/li&gt;
&lt;li&gt;Farid Ameziane, Ecole Nationale Sup√©rieure d‚ÄôArchitecture de Marseille Luminy (EAML), Directeur de l‚ÄôInsARTis, Marseille&lt;/li&gt;
&lt;li&gt;Guillaume Bonello, Charg√© de mission, POPsud, co/OAMP, Marseille&lt;/li&gt;
&lt;li&gt;Fabrice Mortessagne, Directeur du laboratoire de Physique de la Mati√®re Condens√©e (LPMC), Nice-Sophia Antipolis&lt;/li&gt;
&lt;li&gt;Laurent Perrinet, Chercheur √† l‚ÄôInstitut de Neurosciences Cognitives de M√©diterran√©e, Equipe DyVA, Marseille&lt;/li&gt;
&lt;li&gt;Mod√©ratrice : Colette Tron, Fondatrice d‚ÄôAlphabetville, Marseille&lt;/li&gt;
&lt;li&gt;Entr√©e libre &amp;amp; gratuite - 19h, dur√©e 2h.&lt;/li&gt;
&lt;li&gt;Renseignements pratiques :&lt;/li&gt;
&lt;li&gt;Espace Sextius investi par Seconde Nature  :&lt;/li&gt;
&lt;li&gt;27bis rue du 11 novembre,&lt;/li&gt;
&lt;li&gt;13100 Aix-en-Provence&lt;/li&gt;
&lt;li&gt;(!) visitez le site de Seconde Nature&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;notes-de-lintervention-de-laurent-perrinet&#34;&gt;notes de l&amp;rsquo;intervention de Laurent Perrinet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Qu&amp;rsquo;est-ce que voir?&lt;/strong&gt; En perception, les neurones ¬´ parlent ¬ª tous
en m√™me temps par de br√®ves impulsions √©lectrochimiques, g√©n√©rant un
m√©lange de signaux, un bruit. Pourtant c&amp;rsquo;est par eux que nous
pensons, voyons, sentons. Les ordinateurs sont diff√©rents, plus
rapides. Ils sont construits avec pour mod√®le la grammaire humaine
autour d‚Äôune unit√© centrale, car on imaginait la cognition sous cet
angle √† leur invention. Le bit est le quantum d‚Äôun &lt;strong&gt;algorithme
m√©canique&lt;/strong&gt; (th√®se de Church-Turing). Une th√©orie tranche par
rapport √† la pr√©c√©dente, propos√©e par ¬´von Neumann¬ª : beaucoup
d‚Äôunit√©s sont pr√©sentes dans le cerveau. Compar√©e √† la cha√Æne
logique du langage, dans cet algorithme, beaucoup d‚Äôautres cha√Ænes
et logiques se m√™lent. Comment vont-elles ¬´ parler ¬ª entre elles ?
Existe-t-il des &lt;strong&gt;algorithmes biologiques&lt;/strong&gt; ?
&lt;img src=&#34;ouchi.jpg&#34; alt=&#34;OUCHI&#34;&gt;
D√©finir ce ¬´ langage ¬ª, c&amp;rsquo;est comprendre comment une &lt;strong&gt;somme
d‚Äôinformations locales&lt;/strong&gt; peut produire une &lt;strong&gt;perception globale&lt;/strong&gt;.
Comment en jouant avec les atomes du code, en les superposant, les ¬´
cassant ¬ª pour les mettre en r√©sonance, les neurosciences et l&amp;rsquo;artiste
questionnent le langage de notre pens√©e ? Quel est le code utilis√© par
les neurones pour communiquer (code neuronal ? existe-t-il un m√™me
&lt;strong&gt;vocabulaire&lt;/strong&gt; au sens homomorphique ?). En pratique, on apprend par
exemple la s√©lectivit√© √† l&amp;rsquo;orientation. Les ph√©nom√®nes d‚Äôorientation
sont radicaux √† la fin de l‚Äôexp√©rience, ¬´ gelant ¬ª son √©volution. Un
lien √©vident avec l‚Äôinstallation &lt;em&gt;Phytosph√®re&lt;/em&gt; d‚ÄôEtienne Rey.
L‚Äôinformation dans le cerveau se propage &lt;strong&gt;par diffusion, par
diffraction&lt;/strong&gt; (contamination des informations entre neurones pour
occuper l‚Äôespace), en &lt;strong&gt;lien avec le travail sur la lumi√®re d‚ÄôEtienne
Rey.&lt;/strong&gt; L&amp;rsquo;image a besoin de 30 millisecondes pour se diffuser de l‚Äô≈ìil
vers l‚Äôarri√®re du cr√¢ne et 85 millisecondes pour produire un r√©flexe
oculaire. Les neurosciences cherchent √† savoir comment comprendre la
&lt;strong&gt;globalit√© par l&amp;rsquo;√©mergence&lt;/strong&gt;.
Il y a donc une &lt;strong&gt;superposition d‚Äô√©tats&lt;/strong&gt;, comme dans la &lt;em&gt;diffraction&lt;/em&gt;
d‚ÄôEtienne Rey.
En perception, le m√©canisme
neuronal cherche √† &lt;strong&gt;sortir de l‚Äôambigu√Øt√©&lt;/strong&gt; premi√®re quand il conna√Æt
une image : il &lt;strong&gt;superpose&lt;/strong&gt; des particules √©l√©mentaires d&amp;rsquo;information,
les diffuse pour les prendre toutes. Ce qui √©merge est non lin√©aire. Le
cerveau interf√®re ces particules, donc les met en comp√©tition, en
coop√©ration (voir exp√©rience plus haut avec les neurones rouges et
bleus), dans une dynamique o√π ces particules se r√©orientent elles-m√™mes.
Elles cr√©ent des ph√©nom√®nes d‚Äôorganisation, se collent, deviennent plus
lumineuses. &lt;strong&gt;La perception n‚Äôest donc pas s√©quentielle mais fluide&lt;/strong&gt; et
la sortie de l&amp;rsquo;ambiguit√© depuis l&amp;rsquo;image pixel vient de l&amp;rsquo;introduction de
ces contraintes. Ainsi quand nous voyons un objet, nous le ¬´ capturons
¬ª. Quand nous sommes vus, nous cherchons √† nous s√©parer de cette
capture.
Un probl√®me classique est l&amp;rsquo;ambiguit√© du monde sensible. Une couleur que
l‚Äôon ne voit pas va appara√Ætre visuellement. &lt;strong&gt;L‚Äôinpainting&lt;/strong&gt; cr√©√© une
≈ìuvre qui correspond √† un m√©canisme neuronal, cherchant √† reproduire
toujours une m√™me structure. La m√©moire iconique du monde ext√©rieur va
impr√©gner le cerveau, s‚Äôy figer. Tout le probl√®me de la perception pour
les neurosciences repose sur deux dialectiques. La premi√®re pr√©sente une
analogie avec les images informatiques par pixels : ce serait en
neurosciences une m√©taphore de la sensation pure. La seconde rappelle
l‚Äôimage vectoris√©e : pour s‚Äôextraire de la sensation pure, le cerveau
retiendra des r√®gles proches des algorithmes. En cognition, il permet de
mettre en lumi√®re le symptome d**&amp;lsquo;autisme**. Dans un sch√©ma montrant un
bloc derri√®re un arbre, d√©passant des deux c√¥t√©s, sera d√©coup√©
visuellement par l‚Äôautiste en plusieurs morceaux distincts. Il ne
g√©n√©ralise pas l‚Äôinformation.
&lt;img src=&#34;featured.jpg&#34; alt=&#34;diffractionFriche_0134.jpg&#34;&gt;
Comment √™tre s√ªr d‚Äôune perception globale
en d√©signant les modules de l‚Äôinstallation d‚ÄôEtienne Rey, ou signifiants
des atomes, dans ce passage du local au global ? Les modules ne se
voient pas forc√©ment dans l‚Äôinstallation, mais d‚Äôautres aspects sont
per√ßus. La relation √† l‚Äôatome, m√™me si elle n‚Äôest pas signifiante pour
le public, n‚Äôest pas primordiale. Le public voit une accumulation de ¬´
choses ¬ª, car par principe quand un ph√©nom√®ne est concentr√© ¬´ il se
passe des choses ¬ª par jeu de contraste. Le fait de bouger face √†
l‚Äôinstallation rend unique √† l&amp;rsquo;individu la perception et r√©alise la
globalit√© de l‚Äô≈ìuvre: on a alors passage de l‚Äôatome √† la forme globale.
Cette r√©solution rejoint Giotto et les d√©buts de la perspective en art
pictural. Il a r√©v√©l√© la question du point de vue, par positionnement et
d√©placement. En effet, les personnes penchent la t√™te dans
l‚Äôinstallation s*pirale* en container, d‚ÄôEtienne Rey, pour le festival
Ozosph√®re √† Strasbourg. Ce ph√©nom√®ne est √† rattach√© aux th√©ories sur la
perception.
**Biographie** Laurent Perrinet, chercheur √† l‚ÄôInstitut de Neurosciences
Cognitives de la M√©diterran√©e √† Marseille, unit√© mixte du CNRS, aime
citer ¬´ La vie de Brian ¬ª des Monty Python : (Brian:) &amp;ldquo;You have to work
it out for yourselves!&amp;rdquo; (Crowd:) &amp;ldquo;Yes, we have to work it out for
ourselves&amp;hellip; (silence) Tell us more!&amp;rdquo;. L‚Äôindividualit√© et la perception
du monde‚Ä¶ Dans l‚Äô√©quipe DyVA (pour Dynamique de la perception visuelle
et de l&amp;rsquo;action), Laurent Perrinet s&amp;rsquo;int√©resse aux neurones impulsionnels
et au codage neuronal, ainsi qu‚Äô√† la perception des mouvements
spatio-temporels. Ces processus d√©finis comme des algorithmes, la
repr√©sentation du flux vid√©o mod√©lise via l‚Äôinformatique ces
interactions au niveau cellulaire (colonnes corticales) et au niveau
cognitif (aires corticales). Il cherche √† comprendre le fonctionnement
des calculs corticaux dans le syst√®me visuel. Cette recherche fournit
des r√©ponses aux probl√®mes cognitifs. Apr√®s un dipl√¥me d&amp;rsquo;ing√©nieur de
traitement du signal et de mod√©lisation stochastique de l&amp;rsquo;√©cole
d‚Äôa√©ronautique Supa√©ro √† Toulouse et des √©tudes √† San Diego et √†
Pasadena (Californie) pour la Nasa, Laurent Perrinet obtient un doctorat
de Sciences Cognitives. R√©pondant aux questions ¬´ Peut-on parler
d‚Äôintelligence m√©canique ? ¬ª, ¬´ Pourquoi une grenouille gobe mieux une
mouche qu‚Äôun robot ? ¬ª ou ¬´ Quelle est la diff√©rence entre intelligence
et algorithme ? ¬ª, il intervient en 2009 au colloque marseillais ¬´ Les
chemins de l‚Äôintelligence ¬ª. Parmi ses publications : *Role of
homeostasis in learning sparse representations*, et sa th√®se *Comment
d√©chiffrer le code impulsionnel de la vision ? √âtude du flux parall√®le,
asynchrone et √©pars dans le traitement visuel ultra-rapide*&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Models of low-level vision: linking probabilistic models and neural masses</title>
      <link>https://laurentperrinet.github.io/talk/2010-01-08-facets/</link>
      <pubDate>Fri, 08 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-01-08-facets/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A recurrent Bayesian model of dynamic motion integration for smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/bogadhi-10-vss/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/bogadhi-10-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Computational Neuroscience, from Multiple Levels to Multi-level</title>
      <link>https://laurentperrinet.github.io/publication/dauce-10/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dauce-10/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Different pooling of motion information for perceptual speed discrimination and behavioral speed estimation</title>
      <link>https://laurentperrinet.github.io/publication/simoncini-10-vss/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/simoncini-10-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical emergence of a neural solution for motion integration</title>
      <link>https://laurentperrinet.github.io/publication/khoei-10-tauc/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-10-tauc/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Khoei et al, 2013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dynamical emergence of a neural solution for motion integration</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-areadne/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Phase space analysis of networks based on biologically realistic parameters</title>
      <link>https://laurentperrinet.github.io/publication/voges-10-neurocomp/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-10-neurocomp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic models of the low-level visual system: the role of prediction in detecting motion</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-tauc/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-tauc/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Reading out the dynamics of lateral interactions in the primary visual cortex from VSD data</title>
      <link>https://laurentperrinet.github.io/talk/2009-11-30-vss/</link>
      <pubDate>Mon, 30 Nov 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-11-30-vss/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent poster @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-09-vss/&#34;&gt;VSS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Peut-on parler d&#39;intelligence m√©canique?</title>
      <link>https://laurentperrinet.github.io/talk/2009-11-24-intelligence-mecanique/</link>
      <pubDate>Tue, 24 Nov 2009 18:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-11-24-intelligence-mecanique/</guid>
      <description>&lt;p&gt;Nous parlerons de cette partie &amp;ldquo;m√©canique&amp;rdquo; du cerveau animal ou humain qui permet de percevoir les mouvements et de &amp;hellip; survivre au sein de l&amp;rsquo;environnement. On verra, par exemple, que notre cerveau peut-√™tre 
&lt;a href=&#34;http://interstices.info/classificateur&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;plus rapide que nous&lt;/a&gt;, qu&amp;rsquo;il y a des solutions &amp;ldquo;stupides&amp;rdquo; qui marchent remarquablement bien pour 
&lt;a href=&#34;http://interstices.info/generation-trajectoires&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sortir d&amp;rsquo;un labyrinthe&lt;/a&gt;, et qui si la grenouille sait 
&lt;a href=&#34;http://interstices.info/grenouille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gober une mouche bien mieux qu&amp;rsquo;un robot&lt;/a&gt; &amp;hellip; elle n&amp;rsquo;est pas plus maligne ! Parce que ce qu&amp;rsquo;il ne faut pas confondre ici c&amp;rsquo;est 
&lt;a href=&#34;https://interstices.info/calculer-penser/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;la diff√©rence entre calculer et penser&lt;/a&gt;, entre 
&lt;a href=&#34;http://interstices.info/algo-mode-emploi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intelligence et algorithmes&lt;/a&gt;. En comprenant cela, avec 
&lt;a href=&#34;http://fr.wikipedia.org/wiki/Alan_Turing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alan Mathison Turing&lt;/a&gt;, le Gutenberg du XX√®me si√®cle, l&amp;rsquo;humanit√© a bascul√© des temps modernes √† l&amp;rsquo;√®re du num√©rique.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(!) visitez le 
&lt;a href=&#34;https://interstices.info/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;site d&amp;rsquo;interstices&lt;/a&gt;!&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Control of the temporal interplay between excitation and inhibition by the statistics of visual input</title>
      <link>https://laurentperrinet.github.io/talk/2009-07-18-kremkow-09-cnstalk/</link>
      <pubDate>Sat, 18 Jul 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-07-18-kremkow-09-cnstalk/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this subsequent paper in the 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/kremkow-10-jcns/&#34;&gt;Journal of Computational Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Decoding low-level neural information to track visual motion</title>
      <link>https://laurentperrinet.github.io/talk/2009-04-01-int/</link>
      <pubDate>Wed, 01 Apr 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2009-04-01-int/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Decoding center-surround interactions in population of neurons for the ocular following response</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-09-cosyne/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-09-cosyne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical state spaces of cortical networks representing various horizontal connectivities</title>
      <link>https://laurentperrinet.github.io/publication/voges-09-cosyne/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-09-cosyne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamics of cortical networks including long-range patchy connections</title>
      <link>https://laurentperrinet.github.io/publication/voges-09-gns/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-09-gns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Functional consequences of correlated excitation and inhibition on single neuron integration and signal propagation through synfire chains</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-09-gns/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-09-gns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this subsequent paper in the 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/kremkow-10-jcns/&#34;&gt;Journal of Computational Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Inferring monkey ocular following responses from V1 population dynamics using a probabilistic model of motion integration</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-09-vss/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-09-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NeuralEnsemble: Towards a meta-environment for network modeling and data analysis</title>
      <link>https://laurentperrinet.github.io/publication/yger-09-gns/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/yger-09-gns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Computational Neuroscience: From Representations to Behavior</title>
      <link>https://laurentperrinet.github.io/post/2010-05-27_neurocomp-marseille-workshop/</link>
      <pubDate>Wed, 08 Oct 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2010-05-27_neurocomp-marseille-workshop/</guid>
      <description>&lt;h1 id=&#34;computational-neuroscience-from-representations-to-behavior&#34;&gt;Computational Neuroscience: From Representations to Behavior&lt;/h1&gt;
&lt;h2 id=&#34;second-neurocomp-marseille-workshop&#34;&gt;Second NeuroComp Marseille Workshop&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Date: 27-28 May 2010&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Location: Amphith√©√¢tre Charve at the Saint-Charles&#39; University campus&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M√©tro :
Line 1 et 2 (St Charles), a 5 minute walk from the railway station.
&lt;a href=&#34;http://maps.google.com/maps/ms?ie=UTF8&amp;amp;hl=fr&amp;amp;t=h&amp;amp;msa=0&amp;amp;msid=104552809318940980121.0004855ba608957ac9d29&amp;amp;ll=43.297245,5.369546&amp;amp;spn=0.011978,0.027874&amp;amp;z=16&#34; class=&#34;http&#34;&gt;&lt;/li&gt;
&lt;li&gt;Map (Amphith√©√¢tre Charve, University Main Entrance, etc.)&lt;/a&gt;
&lt;a href=&#34;http://85.31.207.119/SITERTM_WEB/PagesFlash/pdf/PlanReseau.pdf&#34; class=&#34;http&#34;&gt;&lt;/li&gt;
&lt;li&gt;Metro, Bus and Tramway&lt;/a&gt;
&lt;a href=&#34;http://www.navettemarseilleaeroport.com/indexA.php&#34; class=&#34;http&#34;&gt;&lt;/li&gt;
&lt;li&gt;Getting to Marseille from Airport&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Computational Neuroscience emerges now as a major breakthrough in
exploring cognitive functions. It brings together theoretical tools that
elucidate fundamental mechanisms responsible for experimentally observed
behaviour in the applied neurosciences. This is the second Computational
Neuroscience Workshop organized by the &amp;ldquo;NeuroComp Marseille&amp;rdquo; network.&lt;/p&gt;
&lt;p&gt;It will focus on latest advances on the understanding of how information
may be represented in neural activity (1st day) and on computational
models of learning, decision-making and motor control (2nd day). The
workshop will bring together leading researchers in these areas of
theoretical neuroscience. The meeting will consist of invited speakers
with sufficient time to discuss and share ideas and data. All
conferences were in English.&lt;/p&gt;
&lt;h2 id=&#34;program&#34;&gt;Program&lt;/h2&gt;
&lt;p&gt;27 May 2010 &lt;strong&gt;Neural representations for sensory information &amp;amp; the
structure-function relation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;9h00-9h30&lt;/p&gt;
&lt;p&gt;Reception and coffee&lt;/p&gt;
&lt;p&gt;9h30-10h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io/&#34; class=&#34;http&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;
Institut de Neurosciences Cognitives de la M√©diterran√©e, CNRS and
Universit√© de la M√©diterran√©e - Marseille
&lt;strong&gt;¬´Presentation of the Workshop and Topic¬ª&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;10h00-11h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.ceremade.dauphine.fr/~peyre/&#34; class=&#34;http&#34;&gt;Gabriel Peyr√©&lt;/a&gt;&lt;/em&gt;
CNRS and Universit√© Paris-Dauphine
&lt;a href=&#34;http://www.ceremade.dauphine.fr/~peyre/talks/2010-05-20-neurosciences-marseilles.pdf&#34; class=&#34;http&#34;&gt;&lt;strong&gt;¬´Sparse Geometric Processing of Natural Images¬ª&lt;/strong&gt;&lt;/a&gt;
In this talk, I will review recent works on the sparse representations
of natural images. I will in particular focus on both the application of
these emerging models to image processing problems, and their potential
implication for the modeling of visual processing.
Natural images exhibit a wide range of geometric regularities, such as
curvilinear edges and oscillating textures. Adaptive image
representations select bases from a dictionary of orthogonal or
redundant frames that are parameterized by the geometry of the image. If
the geometry is well estimated, the image is sparsely represented by
only a few atoms in this dictionary.
On an ingeniering level, these methods can be used to enhance the
resolution of super-resolution inverse problems, and can also be used to
perform texture synthesis. On a biological level, these mathematical
representations share similarities with low level grouping processes
that operate in areas V1 and V2 of the visual brain. We believe both
processing and biological application of geometrical methods work hand
in hand to design and analyze new cortical imaging methods.&lt;/p&gt;
&lt;p&gt;11h00-12h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.crea.polytechnique.fr/JeanPetitot/home.html&#34; class=&#34;http&#34;&gt;Jean Petitot&lt;/a&gt;&lt;/em&gt;
Centre d&amp;rsquo;Analyse et de Math√©matique Sociales, Ecole des Hautes Etudes en
Sciences Sociales - Paris &lt;strong&gt;¬´Neurogeometry of visual perception¬ª&lt;/strong&gt;
In relation with experimental data, we propose a geometric model of the
functional architecture of the primary visual cortex (V1) explaining
contour integration. The aim is to better understand the type of
geometry algorithms implemented by this functional architecture. The
contact structure of the 1-jet space of the curves in the plane, with
its generalization to the roto-translation group, symplectifications,
and sub-Riemannian geometry, are all neurophysiologically realized by
long-range horizontal connections. Virtual structures, such as illusory
contours of the Kanizsa type, can then be explained by this model.&lt;/p&gt;
&lt;p&gt;12h00&lt;/p&gt;
&lt;p&gt;Lunch&lt;/p&gt;
&lt;p&gt;14h00-14h45&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/pseries/&#34; class=&#34;http&#34;&gt;Peggy Series&lt;/a&gt;&lt;/em&gt;
Institute for Adaptive and Neural Computation, Edinburgh
&lt;strong&gt;¬´Bayesian Priors in Perception and Decision Making¬ª&lt;/strong&gt;
We&amp;rsquo;ll present two recent projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first project (with M. Chalk and A. R. Seitz) is an experimental
investigation of the influence of expectations on the perception of
simple stimuli. Using a simple task involving estimation and detection
of motion random dots displays, we examined whether expectations can be
developed quickly and implicitly and how they affect perception. We find
that expectations lead to attractive biases such that stimuli appear as
being more similar to the expected one than they really are, as well as
visual hallucinations in the absence of a stimulus. We discuss our
findings in terms of Bayesian Inference.&lt;/li&gt;
&lt;li&gt;In the second project (with A. Kalra and Q. Huys), we explore the
concepts of optimism and pessimism in decision making. Optimism is
usually assessed using questionnaires, such as the LOT-R. Here, using a
very simple behavioral task, we show that optimism can be described in
terms of a prior on expected future rewards. We examine the correlation
between the shape of this prior for individual subjects and their scores
on questionnaires, as well as with other measures of personality traits.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;14h45-15h45&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.informatik.uni-ulm.de/ni/staff/HNeumann/&#34; class=&#34;http&#34;&gt;Heiko Neumann&lt;/a&gt;&lt;/em&gt; (in
collaboration with Florian Raudies)
Inst. of Neural Information Processing, Ulm University Germany
&lt;strong&gt;¬´Cortical mechanisms of transparent motion perception ‚Äì a neural
model¬ª&lt;/strong&gt;
Transparent motion is perceived when multiple motions different in
directions and/or speeds are presented in the same part of visual space.
In perceptual experiments the conditions have been studied under which
motion transparency occurs. An upper limit in the number of perceived
transparent layers has been investigated psychophysically. Attentional
signals can improve the perception of a single motion amongst several
motions. While criteria for the occurrence of transparent motion have
been identified only few potential neural mechanisms have been discussed
so far to explain the conditions and mechanisms for segregating multiple
motions.
A neurodynamical model is presented which builds upon a previously
developed neural architecture emphasizing the role of feedforward
cascade processing and feedback from higher to earlier stages for
selective feature enhancement and tuning. Results of computational
experiments are consistent with findings from physiology and
psychophysics. Finally, the model is demonstrated to cope with realistic
data from computer vision benchmark databases.
Work supported by European Union (project SEARISE), BMBF, and CELEST&lt;/p&gt;
&lt;p&gt;15h45-15h00&lt;/p&gt;
&lt;p&gt;Coffee break&lt;/p&gt;
&lt;p&gt;16h00-17h00&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CANCELED&lt;/strong&gt;
&lt;em&gt;&lt;a href=&#34;http://pauli.uni-muenster.de/tp/index.php?id=9&amp;amp;L=1&#34; class=&#34;http&#34;&gt;Rudolf Friedrich&lt;/a&gt;&lt;/em&gt;
Institute f√ºr Theoretische Physik Westf√§lische Wilhelms Universit√§t
M√ºnster
&lt;strong&gt;¬´Windows to Complexity: Disentangling Trends and Fluctuations in
Complex Systems¬ª&lt;/strong&gt;
In the present talk, we discuss how to perform an analysis of
experimental data of complex systems by disentangling the effects of
dynamical noise (fluctuations) and deterministic dynamics (trends). We
report on results obtained for various complex systems like turbulent
fields, the motion of dissipative solitons in nonequilibrium systems,
traffic flows, and biological data like human tremor data and brain
signals. Special emphasis is put on methods to predict the occurrence of
qualitative changes in systems far from equilibrium.
[1] R. Friedrich, J. Peinke, M. Reza Rahimi Tabar: Importance of
Fluctuations: Complexity in the View of stochastic Processes (in:
Springer Encyclopedia on Complexity and System Science, (2009))&lt;/p&gt;
&lt;p&gt;17h00-17h45&lt;/p&gt;
&lt;p&gt;General Discussion&lt;/p&gt;
&lt;p&gt;&lt;span id=&#34;line-39&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;28 May 2010 &lt;strong&gt;Computational models of learning and decision making&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;9h30-10h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://brovelli.free.fr/&#34; class=&#34;http&#34;&gt;Andrea Brovelli&lt;/a&gt;&lt;/em&gt;
Institut de Neurosciences Cognitives de la M√©diterran√©e, CNRS and
Universit√© de la M√©diterran√©e - Marseille
&lt;strong&gt;¬´An introduction to Motor Learning, Decision-Making and Motor
Control¬ª&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;10h00-11h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://emmanuel.dauce.free.fr&#34; class=&#34;http&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;&lt;/em&gt;
Mouvement &amp;amp; Perception, UMR 6152, Facult√© des sciences du sport
&lt;strong&gt;¬´Adapting the noise to the problem : a Policy-gradient approach of
receptive fields formation¬ª&lt;/strong&gt;
In machine learning, Kernel methods are give a consistent framework for
applying the perceptron algorithm to non-linear problems. In
reinforcement learning, the analog of the perceptron delta-rule is
called the &amp;ldquo;policy-gradient&amp;rdquo; approch proposed by Williams in 1992 in the
framework of stochastic neural networks. Despite its generality and
straighforward applicability to continuous command problems, quite few
developments of the method have been proposed since. Here we present an
account of the use of a kernel transformation of the perception space
for learning a motor command, in the case of eye orientation and
multi-joint arm control. We show that such transformation allows the
system to learn non-linear transformation, like the log-like resolution
of a foveated retina, or the transformation from a cartesian perception
space to a log-polar command, by shaping appropriate receptive fields
from the perception to the command space. We also present a method for
using multivariate correlated noise for learning high-DOF control
problems, and propose some interpretations on the putative role of
correlated noise for learning in biological systems.&lt;/p&gt;
&lt;p&gt;11h00-12h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.eng.cam.ac.uk/~ml468/&#34; class=&#34;http&#34;&gt;M√°t√© Lengyel&lt;/a&gt;&lt;/em&gt;
Computational &amp;amp; Biological Learning Lab, Department of Engineering,
University of Cambridge
&lt;strong&gt;¬´Why remember? Episodic versus semantic memories for optimal decision
making¬ª&lt;/strong&gt;
Memories are only useful inasmuch as they allow us to act adaptively in
the world. Previous studies on the use of memories for decision making
have almost exclusively focussed on implicit rather than declarative
memories, and even when they did address declarative memories they dealt
only with semantic but not episodic memories. In fact, from a purely
computational point of view, it seems wasteful to have memories that are
episodic in nature: why should it be better to act on the basis of the
recollection of single happenings (episodic memory), rather than the
seemingly normative use of accumulated statistics from multiple events
(semantic memory)? Using the framework of reinforcement learning, and
Markov decision processes in particular, we analyze in depth the
performance of episodic versus semantic memory-based control in a
sequential decision task under risk and uncertainty in a class of simple
environments. We show that episodic control should be useful in a range
of cases characterized by complexity and inferential noise, and most
particularly at the very early stages of learning, long before
habitization (the use of implicit memories) has set in. We interpret
data on the transfer of control from the hippocampus to the striatum in
the light of this hypothesis.&lt;/p&gt;
&lt;p&gt;12h00-14h00&lt;/p&gt;
&lt;p&gt;Lunch&lt;/p&gt;
&lt;p&gt;14h00-15h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.cs.bris.ac.uk/~rafal/&#34; class=&#34;http&#34;&gt;Rafal Bogacz&lt;/a&gt;&lt;/em&gt;
Department of Computer Science, University of Bristol
&lt;strong&gt;¬´Optimal decision making and reinforcement learning in the
cortico-basal-ganglia circuit¬ª&lt;/strong&gt;
During this talk I will present a computational model describing
decision making process in the cortico-basal ganglia circuit. The model
assumes that this circuit performs statistically optimal test that
maximizes speed of decisions for any required accuracy. In the model,
this circuit computes probabilities that considered alternatives are
correct, according to Bayes‚Äô theorem. This talk will show that the
equation of Bayes‚Äô theorem can be mapped onto the functional anatomy of
a circuit involving the cortex, basal ganglia and thalamus. This theory
provides many precise and counterintuitive experimental predictions,
ranging from neurophysiology to behaviour. Some of these predictions
have been already validated in existing data and others are a subject of
ongoing experiments. During the talk I will also discuss the
relationships between the above model and current theories of
reinforcement learning in the cortico-basal-ganglia circuit.&lt;/p&gt;
&lt;p&gt;15h00-15h30&lt;/p&gt;
&lt;p&gt;Coffee break&lt;/p&gt;
&lt;p&gt;15h30-16h30&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://e.guigon.free.fr/&#34; class=&#34;http&#34;&gt;Emmanuel Guigon&lt;/a&gt;&lt;/em&gt;
Institut des Syst√®mes Intelligents et de Robotique, UPMC - CNRS / UMR
7222
&lt;strong&gt;¬´Optimal feedback control as a principle for adaptive control of
posture and movement¬ª&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;16h30-17h15&lt;/p&gt;
&lt;p&gt;General Discussion&lt;/p&gt;
&lt;p&gt;&lt;span id=&#34;line-54&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&#34;line-57&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;sponsored-by&#34;&gt;Sponsored by&lt;/h2&gt;
&lt;p&gt;&lt;span id=&#34;line-59&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.incm.cnrs-mrs.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.incm.cnrs-mrs.fr/images/logo-INCM.png&#34; title=&#34;http://www.incm.cnrs-mrs.fr/&#34; class=&#34;external_image&#34; style=&#34;width:15.0%&#34; alt=&#34;http://www.incm.cnrs-mrs.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-60&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.ism.univmed.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.ism.univmed.fr/IMG/logoISM2.gif&#34; title=&#34;http://www.ism.univmed.fr/&#34; class=&#34;external_image&#34; style=&#34;width:10.0%&#34; alt=&#34;http://www.ism.univmed.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-61&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://sites.univ-provence.fr/ifrscc/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://sites.univ-provence.fr/ifrscc/plugins/kitcnrs/images/logoifr.jpg&#34; title=&#34;http://sites.univ-provence.fr/ifrscc/&#34; class=&#34;external_image&#34; style=&#34;width:5.0%&#34; alt=&#34;http://sites.univ-provence.fr/ifrscc/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-62&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.univmed.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.univmed.fr/App_Themes/Default/images/hp/logo_d.gif&#34; title=&#34;http://www.univmed.fr/&#34; class=&#34;external_image&#34; style=&#34;width:8.0%&#34; alt=&#34;http://www.univmed.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-63&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.univ-provence.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.univ-provence.fr/Local/up/fr/bandeau/logo_up.gif&#34; title=&#34;http://www.univ-provence.fr/&#34; class=&#34;external_image&#34; style=&#34;width:5.0%&#34; alt=&#34;http://www.univ-provence.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-64&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.univ-provence.fr/gsite/index.php?project=pole3c&#34; class=&#34;http&#34;&gt;Pole 3c&lt;/a&gt;
&lt;span id=&#34;line-66&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;span
id=&#34;line-68&#34; class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;span id=&#34;line-69&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;featured.jpg&#34; alt=&#34;Affiche&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing cortical network dynamics with respect to different connectivity assumptions</title>
      <link>https://laurentperrinet.github.io/publication/voges-08-neurocomp/</link>
      <pubDate>Fri, 03 Oct 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-08-neurocomp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Functional properties of feed-forward inhibition</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-08-neurocomp/</link>
      <pubDate>Fri, 03 Oct 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-08-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this subsequent paper in the 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/kremkow-10-jcns/&#34;&gt;Journal of Computational Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Proceedings of the second french conference on Computational Neuroscience, Marseille</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-neurocomp/</link>
      <pubDate>Fri, 03 Oct 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-neurocomp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decoding the population dynamics underlying ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/talk/2008-06-01-ulm/</link>
      <pubDate>Sun, 01 Jun 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2008-06-01-ulm/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;related publications  @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-06-fens/&#34;&gt;FENS 2006&lt;/a&gt;,   @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-neurocomp/&#34;&gt;NeuroComp 2008&lt;/a&gt; and   @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-areadne/&#34;&gt;AREADNE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ 
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>From neural activity to behavior: computational neuroscience as a synthetic approach for understanding the neural code.</title>
      <link>https://laurentperrinet.github.io/talk/2008-04-01-incm/</link>
      <pubDate>Tue, 01 Apr 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2008-04-01-incm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamics of distributed 1D and 2D motion representations for short-latency ocular following</title>
      <link>https://laurentperrinet.github.io/publication/barthelemy-08/</link>
      <pubDate>Sun, 03 Feb 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/barthelemy-08/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;barthelemy-08.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modeling of spikes, sparseness and adaptation in the primary visual cortex: applications to imaging</title>
      <link>https://laurentperrinet.github.io/talk/2008-02-01-toledo/</link>
      <pubDate>Fri, 01 Feb 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2008-02-01-toledo/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;related publications  @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-06-fens/&#34;&gt;FENS 2006&lt;/a&gt;,   @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-neurocomp/&#34;&gt;NeuroComp 2008&lt;/a&gt; and   @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-areadne/&#34;&gt;AREADNE 2008&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Sparse Spike Coding : applications of Neuroscience to the compression of natural images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-spie/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-spie/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Control of the temporal interplay between excitation and inhibition by the statistics of visual input: a V1 network modelling study</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-08-sfn/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-08-sfn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this subsequent paper in the 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/kremkow-10-jcns/&#34;&gt;Journal of Computational Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Decoding the population dynamics underlying ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-areadne/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamics of cortical networks based on patchy connectivity patterns</title>
      <link>https://laurentperrinet.github.io/publication/voges-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/voges-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling spatial integration in the ocular following response to center-surround stimulation using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-a/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PyNN: A Common Interface for Neuronal Network Simulators</title>
      <link>https://laurentperrinet.github.io/publication/davison-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/davison-08/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;davison-08.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What adaptive code for efficient spiking representations? A model for the formation of receptive fields of simple cells</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-08.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What efficient code for adaptive spiking representations?</title>
      <link>https://laurentperrinet.github.io/talk/2007-12-01-rankprize/</link>
      <pubDate>Sat, 01 Dec 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2007-12-01-rankprize/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Codes for Adaptive Sparse Representations of Natural Images</title>
      <link>https://laurentperrinet.github.io/talk/2007-09-01-mipm/</link>
      <pubDate>Sat, 01 Sep 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2007-09-01-mipm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Synchrony in thalamic inputs enhances propagation of activity through cortical layers</title>
      <link>https://laurentperrinet.github.io/publication/kremkow-07-cns/</link>
      <pubDate>Fri, 06 Jul 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kremkow-07-cns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this subsequent paper in the 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/kremkow-10-jcns/&#34;&gt;Journal of Computational Neuroscience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dynamical Neural Networks: modeling low-level vision at short latencies</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07/</link>
      <pubDate>Sat, 03 Mar 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07/</guid>
      <description>&lt;p&gt;Dynamical Neural Networks (DyNNs) are a class of models for networks of neurons where particular focus is put on the role of time in the emergence of functional computational properties. The definition and study of these models involves the cooperation of a large range of scientific fields from statistical physics, probabilistic modelling, neuroscience and psychology to control theory. It focuses on the mechanisms that may be relevant for studying cognition by hypothesizing that information is distributed in the activity of the neurons in the system and that the timing helps in maintaining this information to lastly form decisions or actions. The system responds at best to the constraints of the outside world and learning strategies tune this internal dynamics to achieve optimal performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision</title>
      <link>https://laurentperrinet.github.io/publication/cessac-07-a/</link>
      <pubDate>Sat, 03 Mar 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/cessac-07-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision</title>
      <link>https://laurentperrinet.github.io/publication/cessac-07/</link>
      <pubDate>Sat, 03 Mar 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/cessac-07/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-Invertible 2D Log-Gabor Wavelets</title>
      <link>https://laurentperrinet.github.io/publication/fischer-07-cv/</link>
      <pubDate>Sat, 13 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-07-cv/</guid>
      <description>





  



  
  











&lt;figure id=&#34;figure-figure-1-multiresolution-schemes-a-schematic-contours-of-the-log-gabor-filters-in-the-fourier-domain-with-5-scales-and-8-orientations-only-the-contours-at-78-of-the-filter-maximum-are-drawn-b-the-real-part-of-the-corresponding-filters-is-drawn-in-the-spatial-domain-the-two-first-scales-are-drawn-at-the-bottom-magnified-by-a-factor-of-4-for-a-better-visualization-the-different-scales-are-arranged-in-rows-and-the-orientations-in-columns-the-low-pass-filter-is-drawn-in-the-upper-left-part-c-the-corresponding-imaginary-parts-of-the-filters-are-shown-in-the-same-arrangement-note-that-the-low-pass-filter-does-not-have-imaginary-part-insets-b-and-c-show-the-final-filters-built-through-all-the-processes-described-in-section-2-d-in-the-proposed-scheme-the-elongation-of-log-gabor-wavelets-increases-with-the-number-of-orientations-nt--here-the-real-parts-left-column-and-imaginary-parts-right-column-are-drawn-for-the-3-4-6-8-10-12-and-16-orientation-schemes-e-as-a-comparison-orthogonal-wavelet-filters-db4-are-shown-horizontal-vertical-and-diagonal-wavelets-are-arranged-on-columns-low-pass-on-top-f-as-a-second-comparison-steerable-pyramid-filters-portilla-et-al-2003-are-shown-the-arrangement-over-scales-and-orientations-is-the-same-as-for-the-log-gabor-scheme&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/fischer-07-cv/figure1_hu855fde1d10292610ea30d8b3a201517d_94218_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;&amp;lt;strong&amp;gt;Figure 1&amp;lt;/strong&amp;gt; Multiresolution schemes. (a) Schematic contours of the log-Gabor filters in the Fourier domain with 5 scales and 8 orientations (only the contours at 78% of the filter maximum are drawn). (b) The real part of the corresponding filters is drawn in the spatial domain. The two first scales are drawn at the bottom magnified by a factor of 4 for a better visualization. The different scales are arranged in rows and the orientations in columns. The low-pass filter is drawn in the upper-left part. (c) The corresponding imaginary parts of the filters are shown in the same arrangement. Note that the low-pass filter does not have imaginary part. Insets (b) and (c) show the final filters built through all the processes described in Section 2. (d) In the proposed scheme the elongation of log-Gabor wavelets increases with the number of orientations nt . Here the real parts (left column) and imaginary parts (right column) are drawn for the 3, 4, 6, 8, 10, 12 and 16 orientation schemes. (e) As a comparison orthogonal wavelet filters ‚ÄòDb4‚Äô are shown. Horizontal, vertical and diagonal wavelets are arranged on columns (low-pass on top). (f) As a second comparison, steerable pyramid filters (Portilla et al., 2003) are shown. The arrangement over scales and orientations is the same as for the log-Gabor scheme.&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/fischer-07-cv/figure1_hu855fde1d10292610ea30d8b3a201517d_94218_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;80%&#34; height=&#34;392&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Figure 1&lt;/strong&gt; Multiresolution schemes. (a) Schematic contours of the log-Gabor filters in the Fourier domain with 5 scales and 8 orientations (only the contours at 78% of the filter maximum are drawn). (b) The real part of the corresponding filters is drawn in the spatial domain. The two first scales are drawn at the bottom magnified by a factor of 4 for a better visualization. The different scales are arranged in rows and the orientations in columns. The low-pass filter is drawn in the upper-left part. (c) The corresponding imaginary parts of the filters are shown in the same arrangement. Note that the low-pass filter does not have imaginary part. Insets (b) and (c) show the final filters built through all the processes described in Section 2. (d) In the proposed scheme the elongation of log-Gabor wavelets increases with the number of orientations nt . Here the real parts (left column) and imaginary parts (right column) are drawn for the 3, 4, 6, 8, 10, 12 and 16 orientation schemes. (e) As a comparison orthogonal wavelet filters ‚ÄòDb4‚Äô are shown. Horizontal, vertical and diagonal wavelets are arranged on columns (low-pass on top). (f) As a second comparison, steerable pyramid filters (Portilla et al., 2003) are shown. The arrangement over scales and orientations is the same as for the log-Gabor scheme.
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Bayesian modeling of dynamic motion integration</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-07/</link>
      <pubDate>Wed, 03 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-07/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;montagnini-07.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic inference for motion tracking</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-07-a/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-07-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling spatial integration in the ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-07-neurocomp.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Codes for Adaptive Sparse Representations of Natural Images</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07-mipm/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07-mipm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On efficient sparse spike coding schemes for learning natural scenes in the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07-cns/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07-cns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PyNN: towards a universal neural simulator API in Python</title>
      <link>https://laurentperrinet.github.io/publication/davison-07-cns/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/davison-07-cns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse Approximation of Images Inspired from the Functional Architecture of the Primary Visual Areas</title>
      <link>https://laurentperrinet.github.io/publication/fischer-07/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-07/</guid>
      <description>

















&lt;figure id=&#34;figure-schematic-structure-of-the-primary-visual-cortex-implemented-in-the-present-study-simple-cortical-cells-are-modeled-through-log-gabor-functions-they-are-organized-in-pairs-in-quadrature-of-phase-dark-gray-circles-for-each-position-the-set-of-different-orientations-compose-a-pinwheel-large-light-gray-circles-the-retinotopic-organization-induces-that-adjacent-spatial-positions-are-arranged-in-adjacent-pinwheels-inhibition-interactions-occur-towards-the-closest-adjacent-positions-which-are-in-the-direc-tions-perpendicular-to-the-cell-preferred-orientation-and-toward-adjacent-orientations-light-red-connections-facilitation-occurs-to-wards-co-aligned-cells-up-to-a-larger-distance-dark-blue-connections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; data-caption=&#34;Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).&#34;&gt;


  &lt;img src=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; alt=&#34;&#34; width=&#34;80%&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Visual tracking of ambiguous moving objects: A recursive Bayesian model</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-07-b/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-07-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An efficiency razor for model selection and adaptation in the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-cns/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-cns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian modeling of dynamic motion integration</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-06-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-06-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Contrast sensitivity adaptation in a virtual spiking retina and its adequation with mammalians retinas</title>
      <link>https://laurentperrinet.github.io/publication/wohrer-06/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/wohrer-06/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical contrast gain control mechanisms in a layer 2/3 model of the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-ciotat/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-ciotat/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical contrast gain control mechanisms in a layer 2/3 model of the primary visual cortex</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-fab/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-fab/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-fens/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-fens/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/talk/2006-01-01-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2006-01-01-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;related publication @ 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-spie/&#34;&gt;SPIE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in 
&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Modeling of simple cells through a sparse overcomplete gabor wavelet representation based on local inhibition and facilitation</title>
      <link>https://laurentperrinet.github.io/publication/redondo-05/</link>
      <pubDate>Wed, 03 Aug 2005 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/redondo-05/</guid>
      <description>

















&lt;figure id=&#34;figure-schematic-structure-of-the-primary-visual-cortex-implemented-in-the-present-study-simple-cortical-cells-are-modeled-through-log-gabor-functions-they-are-organized-in-pairs-in-quadrature-of-phase-dark-gray-circles-for-each-position-the-set-of-different-orientations-compose-a-pinwheel-large-light-gray-circles-the-retinotopic-organization-induces-that-adjacent-spatial-positions-are-arranged-in-adjacent-pinwheels-inhibition-interactions-occur-towards-the-closest-adjacent-positions-which-are-in-the-direc-tions-perpendicular-to-the-cell-preferred-orientation-and-toward-adjacent-orientations-light-red-connections-facilitation-occurs-to-wards-co-aligned-cells-up-to-a-larger-distance-dark-blue-connections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; data-caption=&#34;Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).&#34;&gt;


  &lt;img src=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; alt=&#34;&#34; width=&#34;80%&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Sparse Gabor wavelets by local operations</title>
      <link>https://laurentperrinet.github.io/publication/fischer-05-a/</link>
      <pubDate>Wed, 29 Jun 2005 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-05-a/</guid>
      <description>

















&lt;figure id=&#34;figure-schematic-structure-of-the-primary-visual-cortex-implemented-in-the-present-study-simple-cortical-cells-are-modeled-through-log-gabor-functions-they-are-organized-in-pairs-in-quadrature-of-phase-dark-gray-circles-for-each-position-the-set-of-different-orientations-compose-a-pinwheel-large-light-gray-circles-the-retinotopic-organization-induces-that-adjacent-spatial-positions-are-arranged-in-adjacent-pinwheels-inhibition-interactions-occur-towards-the-closest-adjacent-positions-which-are-in-the-direc-tions-perpendicular-to-the-cell-preferred-orientation-and-toward-adjacent-orientations-light-red-connections-facilitation-occurs-to-wards-co-aligned-cells-up-to-a-larger-distance-dark-blue-connections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; data-caption=&#34;Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).&#34;&gt;


  &lt;img src=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; alt=&#34;&#34; width=&#34;80%&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Dynamics of motion representation in short-latency ocular following: A two-pathways Bayesian model</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-05-a/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-05-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient representation of natural images using local cooperation</title>
      <link>https://laurentperrinet.github.io/publication/fischer-05/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/fischer-05/</guid>
      <description>

















&lt;figure id=&#34;figure-schematic-structure-of-the-primary-visual-cortex-implemented-in-the-present-study-simple-cortical-cells-are-modeled-through-log-gabor-functions-they-are-organized-in-pairs-in-quadrature-of-phase-dark-gray-circles-for-each-position-the-set-of-different-orientations-compose-a-pinwheel-large-light-gray-circles-the-retinotopic-organization-induces-that-adjacent-spatial-positions-are-arranged-in-adjacent-pinwheels-inhibition-interactions-occur-towards-the-closest-adjacent-positions-which-are-in-the-direc-tions-perpendicular-to-the-cell-preferred-orientation-and-toward-adjacent-orientations-light-red-connections-facilitation-occurs-to-wards-co-aligned-cells-up-to-a-larger-distance-dark-blue-connections&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; data-caption=&#34;Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).&#34;&gt;


  &lt;img src=&#34;https://laurentperrinet.github.io/publication/fischer-07/figure2.png&#34; alt=&#34;&#34; width=&#34;80%&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Efficient Source Detection Using Integrate-and-Fire Neurons</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-05/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-05/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coding static natural images using spiking event times: do neurons cooperate?</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-03-ieee/</link>
      <pubDate>Fri, 03 Sep 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-03-ieee/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-03-ieee.png&#34; alt=&#34;header&#34;&gt;






  



  
  











&lt;figure id=&#34;figure-progressive-reconstruction-of-a-static-image-using-spikes-in-a-multi-scale-oriented-representation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-03-ieee/v1_tiger_hue35305ea0fde0004c7038403208fa3b1_2047419_2000x2000_fit_lanczos.gif&#34; data-caption=&#34;&amp;lt;em&amp;gt;Progressive reconstruction of a static image using spikes in a multi-scale oriented representation.&amp;lt;/em&amp;gt;&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-03-ieee/v1_tiger_hue35305ea0fde0004c7038403208fa3b1_2047419_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;320&#34; height=&#34;240&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Progressive reconstruction of a static image using spikes in a multi-scale oriented representation.&lt;/em&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature detection using spikes : the greedy approach</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-04-tauc/</link>
      <pubDate>Sat, 03 Jul 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-04-tauc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-02-sparse/</link>
      <pubDate>Wed, 03 Mar 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-02-sparse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Finding Independent Components using spikes : a natural result of Hebbian learning in a sparse spike coding scheme</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-04/</link>
      <pubDate>Sat, 03 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-04/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comment d√©chiffrer le code impulsionnel de la vision ? √âtude du flux parall√®le, asynchrone et √©pars dans le traitement visuel ultra-rapide</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-03-these/</link>
      <pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-03-these/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emergence of filters from natural scenes in a sparse spike coding scheme</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-03/</link>
      <pubDate>Wed, 01 Jan 2003 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-03/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coherence detection in a spiking neuron via Hebbian learning</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-02-stdp/</link>
      <pubDate>Mon, 03 Jun 2002 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-02-stdp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;perrinet-02-stdp.png&#34; alt=&#34;header&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Image Coding Using an Asynchronous Spiking Neural Network</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-02-esann/</link>
      <pubDate>Tue, 01 Jan 2002 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-02-esann/</guid>
      <description>





  



  
  











&lt;figure id=&#34;figure-progressive-reconstruction-of-a-static-image-using-spikes-in-a-laplacian-pyramid&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-02-esann/lena256pyr_hu10a5fd7b14a34cb61c37032ce2cfe15e_3122787_2000x2000_fit_lanczos.gif&#34; data-caption=&#34;&amp;lt;em&amp;gt;Progressive reconstruction of a static image using spikes in a Laplacian pyramid.&amp;lt;/em&amp;gt;&#34;&gt;


  &lt;img data-src=&#34;https://laurentperrinet.github.io/publication/perrinet-02-esann/lena256pyr_hu10a5fd7b14a34cb61c37032ce2cfe15e_3122787_2000x2000_fit_lanczos.gif&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Progressive reconstruction of a static image using spikes in a Laplacian pyramid.&lt;/em&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Visual Strategies for Sparse Spike Coding</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-02-nsi/</link>
      <pubDate>Tue, 01 Jan 2002 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-02-nsi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Network of integrate-and-fire neurons using Rank Order Coding A: how to implement spike timing dependant plasticity</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-01/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-01/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Network of integrate-and-fire neurons using Rank Order Coding B: spike timing dependant plasticity and emergence of orientation selectivity</title>
      <link>https://laurentperrinet.github.io/publication/delorme-01/</link>
      <pubDate>Mon, 01 Jan 2001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/delorme-01/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A generative model for Spike Time Dependent Hebbian Plasticity</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-00/</link>
      <pubDate>Sat, 01 Jan 2000 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-00/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Apprentissage hebbien d&#39;un reseau de neurones asynchrone a codage par rang</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-99/</link>
      <pubDate>Fri, 01 Jan 1999 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-99/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://laurentperrinet.github.io/slides/example-slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/example-slides/</guid>
      <description>&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/hulk.png&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/hulk.png&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
