[{"authors":["antoine-grimaldi"],"categories":null,"content":"“Ultra-fast vision using Spiking Neural Networks” (PhD position, 2020-09 / 2023-09)   Venue: Aix-Marseille Université with the APROVIS3D grant (ANR-19-CHR3-0008-03)\n  Keywords: Vision, Spiking Neural Networks, Bio-Inspired Computer Vision\n  Thesis director: Dr. Laurent PERRINET, Research unit: Institut de Neurosciences de la Timone (INT)\n  Detailed description: “Ultra-fast vision using Spiking Neural Networks” Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. Crucially, given an equal constraint on energy consumption, these algorithms are relatively slow compared to biological vision. It is believed that one major factor of this rapidity is the fact that visual information is represented by short pulses (spikes) at analog – not discrete – times (Paugam and Bohte, 2012). However, most classical computer vision algorithms rely on such frame-based approaches. One solution to overcome their limitations is to use event-based representations, but these still lack in practice, and their high potential is largely underexploited. Inspired by biology, the project addresses the scientific question of developing a low-power sensing architecture for the processing of visual scenes, able to function on analog devices without a central clock and aimed at being validated in real-life situations. More specifically, the project will develop new paradigms for biologically inspired computer vision (Cristobal, Keil and Perrinet, 2015), from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes.\nIn this doctoral project, we propose to address major limitations of classical computer vision by implementing specific dynamical features of cortical circuits: spiking neural networks (Perrinet, Thorpe and Samuelides, 2004; Lagorce et al., 2018), lateral diffusion of neural information (Chavane et al., 2011; Muller et al., 2018) and dynamic neuronal association fields (Frégnac et al., 2012; Frégnac et al., 2016; Gerard-Mercier et al., 2016). One starting point is to use event-based cameras (Dupeyroux et al., 2018) and to extend results of self-supervised learning that we have obtained on static, natural images (Boutin et al., 2020) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the “association field” described at the psychophysical (Field et al., 1993), spiking (Li and Gilbert, 2002) and synaptic (Gerard-Mercier et al., 2016) levels. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (Voges and Perrinet, 2012). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). It is not well understood, but probably decisive for ultra-fast vision, how recurrent cortico-cortical loops add a level of distributed top-down complexity in the feed-forward stream of information which participates to the ultra-fast integration of sensory input and perceptual context (Keller et al., 2019). Coupled with the dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for defining ultra-fast vision algorithms.\nResearch context The thesis will be carried out in the team “NEuronal OPerations in visual TOpographic maps” (NeOpTo) within the Institut de Neurosciences de la Timone in Marseille, a welcoming and lively town by the Mediterranean sea in the south of France. The research team is led by F. Chavane (DR2, CNRS) and currently hosts 4 permanent staff, 3 post-docs and 4 PhD students. The research themes of the team are focused on neuronal operations within visual cortical maps. Indeed, along the cortical hierarchy, low-level features such as the position and orientation of the visual stimulus (but also auditory tone, somatosensory touch, etc…) but also higher-level features (such as faces, viewpoints of objects, etc…) are represented topographically on the cortical surface.\nThis work will be conducted in direct collaboration with Jean Martinet who will co-supervise the thesis. We will develop these algorithms in collaboration with Ryad Benosman (Université Pierre et Marie Curie) and Stéphane Viollet (équipe biorobotique, Institut des Sciences du Mouvement).\nFR: Description du sujet de thèse La vision biologique est …","date":1656460800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653046958,"objectID":"c2f0a6a942593f19f40fafaac178a1ee","permalink":"https://laurentperrinet.github.io/author/antoine-grimaldi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/antoine-grimaldi/","section":"authors","summary":"“Ultra-fast vision using Spiking Neural Networks” (PhD position, 2020-09 / 2023-09)   Venue: Aix-Marseille Université with the APROVIS3D grant (ANR-19-CHR3-0008-03)\n  Keywords: Vision, Spiking Neural Networks, Bio-Inspired Computer Vision","tags":["aprovis-3-d"],"title":"Antoine Grimaldi","type":"authors"},{"authors":["emmanuel-dauce"],"categories":null,"content":"Emmanuel Daucé is associate professor at the Ecole Centrale de Marseille, doing his research in Computational Neuroscience at the Institut de Neurosciences de la Timone (France), a joint research unit (CNRS / Aix-Marseille Université). His research lies at the crossroad of machine learning, artificial intelligence and neuroscience, seeking to develop innovative computational models and methods though remaining consistent with the principles of biological systems.\n","date":1656460800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653046958,"objectID":"fc438c70df365b40938335cb7e325000","permalink":"https://laurentperrinet.github.io/author/emmanuel-dauce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/emmanuel-dauce/","section":"authors","summary":"Emmanuel Daucé is associate professor at the Ecole Centrale de Marseille, doing his research in Computational Neuroscience at the Institut de Neurosciences de la Timone (France), a joint research unit (CNRS / Aix-Marseille Université).","tags":null,"title":"Emmanuel Daucé","type":"authors"},{"authors":["jean-nicolas-jeremie"],"categories":null,"content":"PhD Student (2021-10 / 2024-09): Bio-mimetic agile aerial robots flying in real-life conditions   this fellowship is part of the AgileNeuRobot project\n  Institut des Neurosciences de la Timone, Aix-Marseille Université / CNRS\n  Thesis direction: Laurent Perrinet and co-direction: Emmanuel Daucé\n   A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively).  Projet #1 : ‘Fast \u0026amp; Curious’: Modèles ultra-rapides de recherche visuelle Project #1 : ‘Fast \u0026amp; Curious’: Models for ultra-fast visual search Mots clés - Keywords  recherche visuelle, vision active, apprentissage profond, neurosciences computationnelles, comportement, sacades visual search, active vision, deep learning, computational neuroscience, behavior, sacades\n Description de la problématique de recherche - Project description La détection d’informations visuelles pertinentes dans une image nécessite un traitement ultra-rapide couvrant l’ensemble du champ visuel. Ces tâches comprennent, par exemple, les décisions concernant la présence ou non d’un animal dans la scène, ou la présence de proies ou de prédateurs. Il a été démontré qu’un tel traitement ultra-rapide est effectivement à l’œuvre, notamment dans le système visuel des primates (Thorpe et al., 1996) avec des réponses motrices de l’ordre de 150 ms chez l’homme (Kirchner et Thorpe, 2006). Une application de ces principes neuroscientifiques peut être établie en ce qui concerne la vision par ordinateur et appliquée en particulier aux systèmes embarqués tels que les robots aériens. Un drone, par exemple, doit être capable de distinguer une cible d’un obstacle, une tâche qui est actuellement impossible avec une latence rapide (Gallego et al., 2019). Cependant, avec le changement de paradigme provoqué par la nouvelle révolution de l’intelligence artificielle, et en particulier l’apprentissage profond, nous avons accès à de nouveaux outils et méthodes. Il est relativement facile de localiser des objets d’une certaine catégorie dans une image et donc de sélectionner un endroit par rapport à un autre. On peut citer par exemple les modèles VGG sur les bases de données Imagenet et sur les techniques de localisation telles que YOLO ou SSD. Cependant, ces techniques nécessitent l’utilisation d’infrastructures lourdes de type GPU qui sont difficilement transposables aux calculs embarqués. De plus, ces techniques sont basées sur le traitement statique des images alors que la plupart des flux naturels sont dynamiques et peuvent changer rapidement. Le but de la thèse est de développer de nouveaux algorithmes bio-mimétiques de recherche visuelle ultra-rapide.\nDetecting relevant visual information in an image requires ultra-fast processing covering the entire visual field. These tasks include, for example, decisions about the presence or absence of an animal in the scene, or the presence of prey or predators. It has been shown that such ultra-fast processing is indeed at work, particularly in the visual system of primates (Thorpe et al., 1996) with motor responses of the order of 150 ms in humans (Kirchner and Thorpe, 2006). An application of these neuroscience principles can be established with respect to computer vision and applied in particular to embedded systems such as aerial robots. A drone, for example, must be able to distinguish a target from an obstacle, a task that is currently impossible with rapid latency (Gallego et al., 2019). However, with the paradigm shift brought about by the new artificial intelligence revolution, and in particular deep learning, we have access to new tools and methods. It is relatively easy to locate objects of a certain category in an image and thus to select one location relative to another. Examples are the VGG models on Imagenet databases and localization techniques such as YOLO or SSD. However, these techniques require the use of heavy GPU-type infrastructures that are difficult to transpose to on-board calculations. Moreover, these techniques are based on static image processing whereas most natural flows are dynamic and can change rapidly. The goal of the thesis is to develop new bio-mimetic algorithms for ultra-fast visual search.\nThématique / Domaine / Contexte Pour résoudre ce problème vision par ordinateur, les neurosciences offrent de nouvelles perspectives autour d’une approche générique de vision active. En effet le fonctionnement du système visuel des mammifères repose sur une rétine non homogène (fovéale) souligne l’importance du traitement central de l’information visuelle. L’identité des objets dans l’environnement semble en effet nécessiter un centrage précis, réalisé à l’aide de saccades oculaires vers des cibles visuelles. Ce centrage nécessite un double traitement …","date":1656460800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653046958,"objectID":"e4fa79ffe4edef44a752f2d1b3d2458b","permalink":"https://laurentperrinet.github.io/author/jean-nicolas-jeremie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jean-nicolas-jeremie/","section":"authors","summary":"PhD Student (2021-10 / 2024-09): Bio-mimetic agile aerial robots flying in real-life conditions   this fellowship is part of the AgileNeuRobot project\n  Institut des Neurosciences de la Timone, Aix-Marseille Université / CNRS","tags":null,"title":"Jean-Nicolas Jérémie","type":"authors"},{"authors":["laurent-u-perrinet"],"categories":null,"content":"Laurent Perrinet is a computational neuroscientist specialized in large scale neural network models of low-level vision, perception and action, currently at the “Institut de Neurosciences de la Timone” (France), a joint research unit (CNRS / Aix-Marseille Université). He co-authored more than 40 articles in computational neuroscience and computer vision. He graduated from the aeronautics engineering school SUPAERO, in Toulouse (France) with a signal processing and applied mathematics degree. He received a PhD in Cognitive Science in 2003 on the mathematical analysis of temporal spike coding of images by using a multi-scale and adaptive representation of natural scenes. His research program is focusing in bridging the complex dynamics of realistic, large-scale models of spiking neurons with functional models of low-level vision. In particular, as part of the FACETS and BrainScaleS consortia, he has developed experimental protocols in collaboration with neurophysiologists to characterize the response of population of neurons. Recently, he extended models of visual processing in the framework of predictive processing in collaboration with the team of Karl Friston at the University College of London. This method aims at characterizing the processing of dynamical flow of information as an active inference process. His current challenge within the NeOpTo team is to translate, or compile in computer terminology, this mathematical formalism with the event-based nature of neural information with the aim of pushing forward the frontiers of Artificial Intelligence systems.\n","date":1656460800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1653046958,"objectID":"34290dc2ee08b914b9858e658a955aa2","permalink":"https://laurentperrinet.github.io/author/laurent-u-perrinet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/laurent-u-perrinet/","section":"authors","summary":"Laurent Perrinet is a computational neuroscientist specialized in large scale neural network models of low-level vision, perception and action, currently at the “Institut de Neurosciences de la Timone” (France), a joint research unit (CNRS / Aix-Marseille Université).","tags":null,"title":"Laurent U Perrinet","type":"authors"},{"authors":["hugo-ladret"],"categories":null,"content":"PhD Student (2019-09 / 2023-11): A multiscale cortical model to account for orientation selectivity in natural-like stimulations  Aix-Marseille Université, Institut des Neurosciences de la Timone Université de Montréal, Laboratoire des Neurosciences de la Vision  master 2B (undergrad, 2019-01-12 / 2019-05-24)  Université de Montréal, Laboratoire des Neurosciences de la Vision  master 2A (undergrad, 2018-09-10 / 2019-01-11) : Learning temporal integrations in a visual spiking neural network Building upon our previous work, we are investigating how recurrent neural networks learn to integrate temporal information, a dimension which is absent in most deep learning networks but provides a wealth of information in biological neural networks.\nTo be able to generalize our findings, I created a model of the early visual pathway (retina and thalamus) that generates neural activity from any natural image, based on data gathered in biological systems for the past several decades. The output from this early visual pathway is then processed by a recurrent spiking neural network whose dynamics match that of the primary visual cortex.\nWe showed that Spike Timing Dependant Plasticity (STDP) and recurrence are key components that allow spiking neural networks to extract patterns from noisy input and build strong internal representations. Such representations not only correctlt predict spatial informations (for example the organization of a visual scene) but also predict temporal structure underlying such informations.\n source code : https://github.com/hugoladret/InternshipM2  Neuroscience Specialist for Artistic Creation (2018-07 / 2018-09) I developed computational neuroscience and computational physics models, in collaboration with well-known contemporary artist Etienne Rey at Friche la Belle de Mai (Marseille) and AI researcher Laurent Perrinet. The idea behind our project was to create works of art by distributing particles in a constrained, semi-stable space, thereby creating discrete illusory perceptions.\nTo dive into more technical details, my work included the implementation of a Boltzmann lattice for computational fluid dynamics (D2Q9 structure), as well as various electro-magnetic interaction models. On the neuroscience side, I used Deep Convoluted Generative Adverserial Networks (DCGAN), Kohonen maps and Canny edge detectors to generate triangulated graphs with a hidden underlying structure. In order to facilitate collaboration between the three of us, I also developed a GUI and multi-threading support that allowed us to work efficiently and use at best each our respective skill set.\n source code : https://github.com/NaturalPatterns/  master 1 (undergrad, 2018-04 / 2018-06): Orientation selectivity in a ring model of the primary visual cortex I created a ring model that performs orientation discrimination tasks, using an hybrid model of convolutionnal and recurrent networks. This work was, to our knowledge, the first visual ring model based on deep learning techniques.\nThe recurrence in the network plays a role akin to that of lateral interactions within the primary visual cortex. We have shown in this work that these lateral interactions provide robustness to noisy inputs in the model, which we infer to also be the the case in the brain. To verify this assessment, I designed a 2-outcome discriminative psychophysics task (2AFC) and compared various metrics for human and model trials. The results showed that the lateral interactions allowed human-like performance, which is a strong qualitative argument in favor of the biological plausiblity of this model.\n all material is available @ https://github.com/hugoladret/InternshipM1  ","date":1647907200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1642377600,"objectID":"93c3936b7d939ed96ca258bc12454ed2","permalink":"https://laurentperrinet.github.io/author/hugo-ladret/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hugo-ladret/","section":"authors","summary":"PhD Student (2019-09 / 2023-11): A multiscale cortical model to account for orientation selectivity in natural-like stimulations  Aix-Marseille Université, Institut des Neurosciences de la Timone Université de Montréal, Laboratoire des Neurosciences de la Vision  master 2B (undergrad, 2019-01-12 / 2019-05-24)  Université de Montréal, Laboratoire des Neurosciences de la Vision  master 2A (undergrad, 2018-09-10 / 2019-01-11) : Learning temporal integrations in a visual spiking neural network Building upon our previous work, we are investigating how recurrent neural networks learn to integrate temporal information, a dimension which is absent in most deep learning networks but provides a wealth of information in biological neural networks.","tags":null,"title":"Hugo Ladret","type":"authors"},{"authors":["ryad-benosman"],"categories":null,"content":"","date":1642032000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1642084030,"objectID":"a01288858b16f5c5a382d30ab0c0b619","permalink":"https://laurentperrinet.github.io/author/ryad-benosman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ryad-benosman/","section":"authors","summary":"","tags":null,"title":"Ryad Benosman","type":"authors"},{"authors":["victor-boutin"],"categories":null,"content":"Controlling an aerial robot by human semaphore gestures using a bio-inspired neural network (PhD, 12/2016 - 02/2020) The brain is a complex machinery that is incredibly efficient and flexible. Thanks to efficient training processes, it tackles a high diversity of tasks with a high robustness. In contrast, states-of-the-art machine learning algorithms exhibit great performances, but are also highly task-specialized. Consequently, neuroscience is potentially a great source of inspiration to design more efficient artificial intelligence algorithms.\nIn particular, vision is predominant compared to other senses in terms of computational resources. So understanding visual processing has the potential to reveal the core computational mechanisms that give the brain such performances. In my research, I am interested in extracting the fundamental principles that are at stake in the visual system, and to apply them to develop better machine learning algorithms. As I consequence, I tend to adopt a cross-level analysis approach. I develop algorithms that simultaneously model low-level neural mechanisms and account for higher-level visual tasks such as object recognition, denoising, inpainting, image generation, etc.\nThe ultimate objective would be to develop a framework that successfully solves all these visual tasks without being extensively retrained from scratch for each of those tasks. Such an algorithm would be a first step towards the ultimate goal of every researcher in machine learning, that is, general artificial intelligence.\n  Venue: Aix-Marseille University DOC2AMU is an innovative H2020-MSCA-COFUND\n  Keywords: Aerial Robots, Vision, Neural Networks, Bio-Inspired Computer Vision, Gaze orientation, learning\n  Thesis director: Dr. Laurent PERRINET, Director’s research unit: Institut de Neurosciences de la Timone (INT)\n  Thesis co-supervisition: Dr. Franck RUFFIER Co-director’s research unit: Institut des Sciences du Mouvement (ISM)\n  Get the thesis manuscript at http://www.theses.fr/2020AIXM0028\n  a nice presentation of the work of Victor and presented at the Redwood Center For Theoretical Neuroscience (University of California, Berkeley) is available online.\n  Main publications:    Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI      Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI      Angelo Franciosini, Victor Boutin, Frédéric Chavane, Laurent U Perrinet  (2021). Pooling in a predictive model of V1 explains functional and structural diversity across species. bioRxiv.  Preprint  PDF  Cite  DOI     (Initial) Description of the PHD thesis project Robotics is a rapidly evolving technology that allows for fast, low-risk and low-cost tasks with a worldwide market of over 80 billion dollars over the next few years. In particular, aerial robots, also known as drones, provide a breakthrough to easily image and access all sorts of terrains and situations and are useful for instance in surveillance and forensics, emergency industrial inspection or a search and rescue operation. A major difficulty for their global acceptance is the difficulty for controlling their flight and interacting with them.\nIndeed, aerial robots are generally operated using a (central) ground station which is not compatible with the time pressure required by emergency conditions, for instance when rescuing a person out of reach with the ground station. This PhD project aims at concealing such obstacles and construct an aerial robot which is able to be autonomously and interactively controlled by simple human gestures, for instance that of a rescuer. The main scientific challenges are (i) to embed in the aerial robot all the electronics for the visual system from the retina to the control signals to the propellers, (ii) to very quickly recognize a variety of simple gestures on-board using a neuromimetic architecture and (iii) to make the robot react in real time to these gestures. As such, this project is inter-disciplinary by positively combining advanced algorithms from event-based bio-inspired computer vision and the latest technology in aerial robots.\n  R. Benosman , S.-H. Leng , C. Clercq , C. Bartolozzi \u0026amp; M. Srinivasan (2012) “Asynchronous frameless event-based optical flow”, Neural Networks - Elsevier\n  S.-C. Liu \u0026amp; T. Delbruck (2010) “Neuromorphic sensory systems”, Current opinion in neurobiology - Elsevier\n  J. Nagi, A. Giusti, G. A. Di Caro, L. M. Gambardella (2014) “HRI in the Sky, Controlling UAVs using Face Poses and Hand Gestures”, HRI\n  3I dimensions and other aspects of the project The present PhD proposal is at the crossroad between various disciplines. It first concerns biology and …","date":1642032000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1642084030,"objectID":"91114010830ff1a964236b7870fe74f4","permalink":"https://laurentperrinet.github.io/author/victor-boutin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/victor-boutin/","section":"authors","summary":"Controlling an aerial robot by human semaphore gestures using a bio-inspired neural network (PhD, 12/2016 - 02/2020) The brain is a complex machinery that is incredibly efficient and flexible. Thanks to efficient training processes, it tackles a high diversity of tasks with a high robustness.","tags":["doc-2-amu"],"title":"Victor Boutin","type":"authors"},{"authors":["etienne-rey"],"categories":null,"content":"collaboration avec Etienne Rey Le travail d’Etienne Rey explore la notion même d’espace. L’enjeu est de produire des déplacements de perception. La question du lieu et de l’environnement, de l’in situ et de l’architecture participent à la découverte de structures spatiales par le biais de déplacements et de la démultiplication des points de vue.\nLes diverses installations ont pour point commun d’inviter à des expériences constituées de matériel et d’immatériel, d’énergies et d’attractions qui mettent en jeu des phénomènes physiques dont le vecteur principal est la lumière. Des transformations réflexives s’opèrent entre perception, propre à chacun, et conscience de l’impact de notre présence. L’intention est de produire des expériences d’espace. Les pièces dévoilent la façon dont ce dernier se structure. Entre installations immatérielles faites de brume et de lumière et celles employant des matériaux aux propriétés optiques, toutes les oeuvres élaborent des filtres perceptifs de l’environnement nous amenant à questionner notre relation au réel.\n lire un portrait dans le journal ventilo  ","date":1633219200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1633219200,"objectID":"4f8b2943f6d01c9f962ff621d9b80f8f","permalink":"https://laurentperrinet.github.io/author/etienne-rey/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/etienne-rey/","section":"authors","summary":"collaboration avec Etienne Rey Le travail d’Etienne Rey explore la notion même d’espace. L’enjeu est de produire des déplacements de perception. La question du lieu et de l’environnement, de l’in situ et de l’architecture participent à la découverte de structures spatiales par le biais de déplacements et de la démultiplication des points de vue.","tags":null,"title":"Étienne Rey","type":"authors"},{"authors":["alberto-vergani"],"categories":null,"content":"Project description: Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. It is clear from recent advances in system and computational neuroscience that nonlinear, recurrent interactions in visual cortical networks are key to this efficiency (Tang et al., 2018; Kietzmann et al., 2019). We will use inspiration from neurophysiology and brain imaging to resolve this apparent gap between traditional CNNs and biological visual systems.\nIn this post-doctoral project, I will address these major limitations by focusing on specific dynamical features of cortical circuits: lateral diffusion of sensory-evoked traveling waves (Chavane et al., 2011; Muller et al., 2018) and dynamic neuronal association fields (Frégnac et al., 2012; Frégnac et al., 2016; Gerard-Mercier et al., 2016). Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (Voges and Perrinet, 2012). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). Less studied, but probably decisive in active vision, recurrent cortico-cortical loops add a level of distributed top-down complexity which participates to the lateral integration of sensory input and perceptual context (Keller et al., 2019). Coupled with the continuous time dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for generating information diffusion through traveling waves. Inspired by recent work in neuroscience uncovering the ubiquity of these waves during visual processing, we aim to design a self-supervised CNN that will exploit these dynamics for new applications in computer vision.\nThe proposed work will be organized as a collaboration between two labs (INT, Marseille and UNIC, Gif) along three tasks to be integrated in a unified model:\n  The starting point will be to extend results of self-supervised learning that we have obtained on static, natural images (Boutin et al., 2019) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the “association field” described at the psychophysical (Field et al., 1993), spiking (Li and Gilbert, 2002) and synaptic (Gerard-Mercier et al., 2016) levels.\n  The central aim will be to develop a dynamical version of this feedback/lateral kernel in the context of the ANR Horizontal-V1 project, linking the two labs and confronted to their recent electrophysiological data pointing to different classes of spatio-temporal diffusion and different degree of anisotropies during apparent and continuous motion.\n  The implementation of this kernel inspired by CNN theory will be compared with a biologically realistic models of the early visual system (Antolik et al., 2019), and simulations of the lateral diffusion kernel will be developed in collaboration with Jan Antolik, external collaborator to the ANR grant. In parallel, using tools linking neural activity to VSD imaging (Muller et al., 2014; Chemla et al., 2019), we will analyze at a more mesocopic level the role of observed traveling waves in forming efficient representations of the visual world.\n  Research context This project is funded by the French National Research Agency (ANR) under the ANR Horizontal V1 grant (coordinator Y. Frégnac) which aims at understanding the emergence of sensory predictions linking local shape attributes (orientation, contour) to global indices of movement (direction, speed, trajectory) at the earliest stage of cortical processing (primary visual cortex, i.e. V1). The cross-talk between physiological and theoretical approaches is fostered by the close collaboration with the teams of Frédéric Chavane at INT and Yves Frégnac at UNIC. The theoretical work is performed in close collaboration with Lyle Muller (Western U) and Jan Antolik (Prague). This project is primarily hosted at the Institut de Neurosciences de la Timone.\nReferences   Antolik, J, C Monier, Y Frégnac, AP Davison. (2019).  “A comprehensive data-driven model of cat primary visual cortex.” BioRxiv, 416156.\n  Boutin, Victor, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, and Laurent U Perrinet. (2019).  “Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.” arXiv\n  Chavane, …","date":1632268800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632268800,"objectID":"ffd0421ebbcd6a9694092807965ab7a5","permalink":"https://laurentperrinet.github.io/author/alberto-arturo-vergani/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alberto-arturo-vergani/","section":"authors","summary":"Project description: Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision.","tags":["phd-icn"],"title":"Alberto Arturo Vergani","type":"authors"},{"authors":["angelo-franciosini"],"categories":null,"content":"Trajectories in natural images and the sensory processing of contours (PhD position, 2017-09 / 2021-03)   Venue: Aix-Marseille Université’s Neuroschool PhD program in Neuroscience (formerly known as “Ph.D. program in Integrative and Clinical Neuroscience”)\n  Keywords: Vision, Neural Networks, Bio-Inspired Computer Vision, contours, learning\n  Thesis director: Dr. Laurent PERRINET, Director’s research unit: Institut de Neurosciences de la Timone (INT)\n  Main publications:    Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI      Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI      Angelo Franciosini, Victor Boutin, Frédéric Chavane, Laurent U Perrinet  (2021). Pooling in a predictive model of V1 explains functional and structural diversity across species. bioRxiv.  Preprint  PDF  Cite  DOI     (Initial) Description of the PHD thesis project Binding the different features of objects in images is at the core of visual perception. As such, the visual system needs to detect local edges and to bind them together to form contours at a higher, more global level. A state-of-the art theory is that of the “association field”: the confidence of an edge depends on the configuration of neighboring edges. For instance it is facilitated for co-linear or co-circular edges. This process takes advantage of the statistical regularities of edges that are present in natural images. In particular, we have developed a method to quantify the association field in different classes of natural images (Perrinet \u0026amp; Bednar, 2015). Using an existing library, it is possible to compute histograms of edge co-occurrences from the sparse representation of static natural images. We have already shown that these different statistics were sufficient to categorize images, for instance to know if they contain an animal or not. At the neural level, modeling the representation of the image, such as that formed in the primary visual cortex of primates (V1), this heuristics translates to a set of rules that adapts dynamically the activity of isolated neurons representing edges into the coherent population activity of contours. ‘‘‘Yet, we miss an understanding of the link between these statistics and the probabilistic rules that binds features together and how this information is dynamically encoded in V1.’’’\nObjectives In this computational neuroscience project, we will exploit our current expertise in computer vision for the statistical integration of visual of objects to translate them in the form of probabilistic predictive models for biological vision. ‘‘‘Our core hypothesis is that in natural scenes, contours follow coherent trajectories and that this knowledge is integrated (learned) by the visual system to optimally inform the representation of the image.’’’\nMethods First, we will learn the different classes of edge co-occurrences that are relevant to natural images. Using an existing unsupervised learning algorithm, we will learn these as an independent components analysis. Such an algorithm extends well to a deep-learning convolutional neural network, but importantly, it will be informed by our expertise of modeling neural networks in low-level visual areas by including horizontal connectivity. We expect that relevant features will be mainly the predictable arrangements, such as co-linear or co-circular pairs of edges, but also highly surprising ones, such as T-junctions or end-stopping features. Importantly, we will be able to compare this representation with that present in higher level areas and to refine our knowledge on the representation of natural-like images. Second, we have previously found that using synthetic textures could further advance our understanding of neural computations and perception. These random synthetic textures, coined “Motion Clouds” were initially targeted to quantify the integration properties of visual motion perception (Leon et al, 2012, Simoncini et al, 2012). Informed by the generative model of edge co-occurrences studied above, an extension to such stimuli would be to include dependencies between different elements. As such, we will be able to manipulate the level of dependency between different elements, whether in space, time or feature space (orientations). A potential outcome will be to use these in neurophysiological and psychophysical experiments within the team. In particular, the ability to select different classes of dependencies learned above will make it possible to evaluate the relative contribution of each component to the association field.\nExpected results Finally, those two tasks converge to a long-term goal of ‘‘‘understanding the impact of the …","date":1631178000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631178000,"objectID":"06c9fe668b8bf4cf0c35b58dd20bf104","permalink":"https://laurentperrinet.github.io/author/angelo-franciosini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/angelo-franciosini/","section":"authors","summary":"Trajectories in natural images and the sensory processing of contours (PhD position, 2017-09 / 2021-03)   Venue: Aix-Marseille Université’s Neuroschool PhD program in Neuroscience (formerly known as “Ph.D. program in Integrative and Clinical Neuroscience”)","tags":["phd-icn"],"title":"Angelo Franciosini","type":"authors"},{"authors":["anna-montagnini"],"categories":null,"content":"","date":1626134400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1626134400,"objectID":"45d66edaf2e796c6cc66249755f036b9","permalink":"https://laurentperrinet.github.io/author/anna-montagnini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/anna-montagnini/","section":"authors","summary":"","tags":null,"title":"Anna Montagnini","type":"authors"},{"authors":["frederic-y-chavane"],"categories":null,"content":"","date":1618963200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1636363878,"objectID":"79ef82413d16cf2e1a5766e01762cafa","permalink":"https://laurentperrinet.github.io/author/frederic-chavane/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/frederic-chavane/","section":"authors","summary":"","tags":null,"title":"Frédéric Chavane","type":"authors"},{"authors":["stephane-viollet"],"categories":null,"content":"","date":1607299200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607299200,"objectID":"adc6213381a5850e4fbf4c7e27167a5b","permalink":"https://laurentperrinet.github.io/author/stephane-viollet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/stephane-viollet/","section":"authors","summary":"","tags":null,"title":"Stéphane Viollet","type":"authors"},{"authors":["chloe-pasturel"],"categories":null,"content":"","date":1579996800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1579996800,"objectID":"4042de78386674a4f316fdbc08f66050","permalink":"https://laurentperrinet.github.io/author/chloe-pasturel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chloe-pasturel/","section":"authors","summary":"","tags":null,"title":"Chloé Pasturel","type":"authors"},{"authors":["wahiba-taouali"],"categories":null,"content":"Motion Integration By V1 Population (Post-Doc, 2013-03 / 2015-01) Description Wahiba hold the postdoctoral position at the “Institut de Neurosciences de la Timone”, CNRS, Marseille (France) to study object motion integration and representation at the level of V1 populations:\n The objective is in modeling, with Laurent Perrinet, anisotropic diffusive processes, such as observed in V1, at the functional and neural levels. The work was done in collaboration with a post-doc in physiology, with Frédéric Chavane, that focused on the role of propagation and diffusion of activity at the level of neuronal population in V1 of awake monkeys (using Voltage-sensitive dye imaging and UTAH array recording).  Wahiba is now scientific software developper at Enthought.\nMain publications:    Wahiba Taouali, Giacomo Benvenuti, Pascal Wallisch, Frédéric Chavane, Laurent U Perrinet  (2016). Testing the odds of inherent vs. observed overdispersion in neural spike counts. Journal of Neurophysiology.  Preprint  PDF  Cite  DOI     Context   This grant was funded by a large European integrated project called BrainScales whose aim is to understand brain information processing at multiple spatial and temporal scales. The successful applicants will have the opportunity to interact with a large and exciting consortium composed of 18 europeans teams working in biology, modeling and hardware.  References::\n Reynaud A., Masson G. S. and Chavane F. Dynamics of Local Input Normalization Result from Balanced Short- and Long-Range Intracortical Interactions in Area V1 Journal of Neuroscience, 2012, 32(36): 12558-12569 Reynaud A., Takerkart S, Masson G. S. and Chavane F. Linear model decomposition for voltage-sensitive dye imaging signals: Application in awake behaving monkey. Neuroimage, 2011, 54(2), 1196–1210 Perrinet, L. and Masson G. Motion-based prediction is sufficient to solve the aperture problem Neural Computation, 2012  ","date":1569196800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1569196800,"objectID":"d493b06b93829b02586500399e4fd937","permalink":"https://laurentperrinet.github.io/author/wahiba-taouali/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wahiba-taouali/","section":"authors","summary":"Motion Integration By V1 Population (Post-Doc, 2013-03 / 2015-01) Description Wahiba hold the postdoctoral position at the “Institut de Neurosciences de la Timone”, CNRS, Marseille (France) to study object motion integration and representation at the level of V1 populations:","tags":null,"title":"Wahiba Taouali","type":"authors"},{"authors":["jean-martinet"],"categories":null,"content":"","date":1568109600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1568109600,"objectID":"847ab6cf27dddd48dd26a2d065afd9cc","permalink":"https://laurentperrinet.github.io/author/jean-martinet/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jean-martinet/","section":"authors","summary":"","tags":null,"title":"Jean Martinet","type":"authors"},{"authors":["maria-jose-escobar"],"categories":null,"content":"","date":1548288000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1548288000,"objectID":"7dfa07b73d50ce46cd7ca6806804603f","permalink":"https://laurentperrinet.github.io/author/maria-jose-escobar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/maria-jose-escobar/","section":"authors","summary":"","tags":null,"title":"Maria Jose Escobar","type":"authors"},{"authors":["andrew-isaac-meso"],"categories":null,"content":"","date":1542758400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1542758400,"objectID":"bd2a55eaf2afd6d4e931c0cab19b9ca9","permalink":"https://laurentperrinet.github.io/author/andrew-isaac-meso/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/andrew-isaac-meso/","section":"authors","summary":"","tags":null,"title":"Andrew Isaac Meso","type":"authors"},{"authors":["jonathan-vacher"],"categories":null,"content":"Mathematics, Machine Learning \u0026amp; Computational Neuroscience\n","date":1542758400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1542758400,"objectID":"85d09b298a6ddb3ba3f13c92a308109c","permalink":"https://laurentperrinet.github.io/author/jonathan-vacher/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jonathan-vacher/","section":"authors","summary":"Mathematics, Machine Learning \u0026 Computational Neuroscience","tags":null,"title":"Jonathan Vacher","type":"authors"},{"authors":["jean-bernard-damasse"],"categories":null,"content":"Smooth pursuit eye movements and learning: Role of motion probability and reinforcement contingencies (PhD, 2014-2017)  Thesis director: Anna Montagnini Thesis co-supervisition: Laurent Perrinet  In the continuous flow of sensory evidence, cognitive systems must provide rapid behavioral choices across different time scales. For instance, seeing a moving object may result in various responses such as catching or avoiding collision depending on the trajectory and the nature of the object, but also depending on the recent experience and the expectations associated with that object and its motion properties. The principal goal of the larger scientific project in which this PhD thesis is inscribed (see ANR-REM project) is the analysis of reinforcement learning processes in the domain of voluntary eye movements (saccades and smooth pursuit eye movements) in humans. Within this PhD project we will use a dual approach, based on behavioural experiments on human subjects and on computational modelling of the experimental data, in order to address this important question, with a particular emphasis on the time course of learning effects and on the hypothesised role of probabilistic inference as underlying mechanism. «BR» Visually driven eye movements provide an ideal experimental preparation to probe sensorimotor behavior across different time-scales, processing levels (from sensory encoding to the final categorical choice) and movement repertoire (e.g. smooth pursuit and saccades). In addition, a remarkable flexibility of oculomotor behaviors has been highlighted by manipulating the expectancy for sensory features or the outcome associated to particular motor responses.\n thesis available @ https://www.theses.fr/s137225  ","date":1538352000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1538352000,"objectID":"e2fda6d909096801dec6b423ae8a360a","permalink":"https://laurentperrinet.github.io/author/jean-bernard-damasse/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jean-bernard-damasse/","section":"authors","summary":"Smooth pursuit eye movements and learning: Role of motion probability and reinforcement contingencies (PhD, 2014-2017)  Thesis director: Anna Montagnini Thesis co-supervisition: Laurent Perrinet  In the continuous flow of sensory evidence, cognitive systems must provide rapid behavioral choices across different time scales.","tags":null,"title":"Jean-Bernard Damasse","type":"authors"},{"authors":["laurent-madelain"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1538352000,"objectID":"e876c9bdb626727aba9712f71872b32d","permalink":"https://laurentperrinet.github.io/author/laurent-madelain/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/laurent-madelain/","section":"authors","summary":"","tags":null,"title":"Laurent Madelain","type":"authors"},{"authors":["kiana-mansour-pour"],"categories":null,"content":"Predicting and selecting sensory events: inference for smooth eye movements (PhD: 2015 - 2019)   Funding: This position is funded by the Marie Skodowska-Curie program of the H2020 European Union program, as part of the Innovative Training Network PACE (Perception and Action in Complex Environments).\n  Thesis director: Anna Montagnini\n  Thesis co-supervisition: Guillaume Masson, Laurent Perrinet\n  Description of the PHD thesis project In everyday life, we constantly need to track relevant moving targets in complex environments with our eyes such as, for instance, when we try to catch someone running in the crowd. However, this seemingly simple task demands to deal with several dynamic sources of uncertainty, related to intrinsic, target-related properties or to external, environment-related factors. In addition, one single object has to be selected at a time for accurate visual processing and ocular tracking in presence of a multitude of competing signals. «BR» The PhD project aims at understanding the dynamic inference and decision processes underlying smooth eye movements. The PhD fellow will conduct psychophysics and oculomotor recordings on healthy subjects, as well as modeling work, in order to elucidate the effects of sensory uncertainty on the accuracy and the dynamics of visuomotor decisions. Bayesian Inference will provide a general and solid framework for behavioral models. Oculomotor decision times, such as those characterizing the dynamic switch between smooth pursuit and saccades during motion tracking, or transitions between two alternative tracking solutions, will be modeled and benchmarked against the predictions of current models of choice reaction times (“accumulation-to-threshold” models).\n","date":1514764800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1514764800,"objectID":"2adb17fe516413f3646852a4c427b616","permalink":"https://laurentperrinet.github.io/author/kiana-mansour-pour/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kiana-mansour-pour/","section":"authors","summary":"Predicting and selecting sensory events: inference for smooth eye movements (PhD: 2015 - 2019)   Funding: This position is funded by the Marie Skodowska-Curie program of the H2020 European Union program, as part of the Innovative Training Network PACE (Perception and Action in Complex Environments).","tags":null,"title":"Kiana Mansour-Pour","type":"authors"},{"authors":["mina-a-khoei"],"categories":null,"content":"Emerging properties in a neural field model implementing probabilistic prediction (PhD, 2011-2014) In the early visual system, information about the visual world as represented by neural activity is dynamically building up from sensory input but also by contextual information coming from neighboring cells and re-entrant signal from other cortical areas. Low-level sensory areas are therefore an excellent model for exploring how neural computations solve the problem of selecting a single, coherent and global representation from the dispersed information collected locally and in parallel by neurons. Our goal in this program is to study the dynamics of neural fields implementing probabilistic computations for early sensory processing. Emphasis will be put onto the role of anisotropic diffusion, in particular within a cortical area through lateral interactions.\nWe have previously elaborated probabilistic (Perrinet \u0026amp; Masson, 2010) or dynamical (Tlapale et al., 2010) models of motion information diffusion along cortical retinotopic trajectories. Probabilistic models give a complete representation of the information that is represented by populations of neurons. In such a dynamical system, prediction acts as a prior, filtering possible future states knowing the current one. An approximation using particle filtering methods will be used to investigate how this propagation can solve low-level computational problems such as integration, extrapolation or prediction in visual (Mason \u0026amp; Ilg, 2010) or somatosensory (Shulz et al. 2006) cortices.\nUsing this architecture, we will explore the consequences of such context-dependent propagation in terms of coding and of learning. First at the time scale of coding, knowing the prior, we will study the emergent properties of the system like its ability to track objects independently of their shape or to segment parts of the scene that are moving coherently. We will study of this motion information may help shape the selectivity of neurons in a given area, for instance orientation selectivity on the priamry visual cortex. At the time scale of learning, we will build models exploring the emergence of maps of cortical receptive fields optimally tuned to elaborate sparse, multi-scale representations of the visual or tactile world. In fact, a simple functional model allows to understand emergence in a model of a simple macro-column of the primary visual cortex (Perrinet, 2010). One challenging question is whether these functional models of self-organization can be translated to large-scale networks of the early sensory system. Using the probabilistic model, we will investigate how spatio-temporal receptive fields can emerge through learning of statistical regularities in the images and study how hierarchic structures can arise as a self-organized property.\nMain publications:    Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI      Bernhard A Kaplan, Mina A Khoei, Anders Lansner, Laurent U Perrinet  (2014). Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network. IEEE International Joint Conference on Neural Networks (IJCNN) 2014 Beijing, China.  PDF  Cite  DOI      Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     Propriétés émergentes d’un modèle de prédiction probabiliste utilisant un champ neural Dans le système visuel de bas niveau, des informations sur le monde visuel tel que celles représentées par l’activité neuronale est dynamiquement causée par l’entrée sensorielle, mais aussi par des informations contextuelles provenant de cellules voisines et par le signal réentrant d’autres aires corticales. Les aires sensorielles primaires sont donc un excellent modèle pour étudier comment les neurones peuvent résoudre le problème de la sélection d’une seul représentation globale et cohérente depuis l’information collectée localement et en parallèle par les neurones. Notre objectif dans ce programme est d’étudier la dynamique de champs neuronaux mettant en œuvre des calculs probabilistes pour le traitement sensoriel précoce.\nL’accent sera mis sur le rôle de la diffusion anisotrope, en particulier celle implémentée par les interactions latérales dans une aire corticale. Nous avons déjà élaboré des modèles probabiliste (Perrinet \u0026amp; Masson, 2010) ou dynamique (Tlapale et al., 2010) de diffusion de l’information de mouvement le long de trajectoires. Les modèles probabilistes donnent une représentation complète de l’information qui est représentée par des populations de neurones. Dans un tel système dynamique, la prédiction agit comme un prior, qui permet un filtrage des états futurs possibles en sachant la distribution de probabilité de l’état actuel. Une approximation à …","date":1485388800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1485388800,"objectID":"602b3a46878cf54531cb59dbb12d7838","permalink":"https://laurentperrinet.github.io/author/mina-a-khoei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mina-a-khoei/","section":"authors","summary":"Emerging properties in a neural field model implementing probabilistic prediction (PhD, 2011-2014) In the early visual system, information about the visual world as represented by neural activity is dynamically building up from sensory input but also by contextual information coming from neighboring cells and re-entrant signal from other cortical areas.","tags":null,"title":"Mina A Khoei","type":"authors"},{"authors":["yves-fregnac"],"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1461715200,"objectID":"dd74cd239700d381ea29c95d9041a1a4","permalink":"https://laurentperrinet.github.io/author/yves-fregnac/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yves-fregnac/","section":"authors","summary":"","tags":null,"title":"Yves Fregnac","type":"authors"},{"authors":["jens-kremkow"],"categories":null,"content":"Correlating Excitation and Inhibition in Visual Cortical Circuits: Functional Consequences and Biological Feasibility (PhD, 2006-01 / 2009-05) The goal of the FACETS (Fast Analog Computing with Emergent Transient States) project was to create a theoretical and experimental foundation for the realisation of novel computing paradigms which exploit the concepts experimentally observed in biological nervous systems. The continuous interaction and scientific exchange between biological experiments, computer modelling and hardware emulations within the project provides a unique research infrastructure that will in turn provide an improved insight into the computing principles of the brain. This insight may potentially contribute to an improved understanding of mental disorders in the human brain and help to develop remedies.\n Venue: Thèse de Doctorat de l’Université d’Aix-Marseille II, Ecole Doctorale des Sciences de la Vie et de la Santé Marseille, France en Cotutelle avec la Fakultät für Biologie Albert-Ludwigs-Universität Freiburg im Breisgau, Allemagne Thesis director: Guillaume MASSON and Dr. Laurent PERRINET  Main publications:    Jens Kremkow, Laurent U Perrinet, Cyril Monier, Jose-Manuel Alonso, Ad M Aertsen, Yves Fregnac, Guillaume S Masson  (2016). Push-Pull Receptive Field Organization and Synaptic Depression: Mechanisms for Reliably Encoding Naturalistic Stimuli in V1. Frontiers in Neural Circuits.  Preprint  PDF  Cite  DOI      Jens Kremkow, Laurent U Perrinet, Guillaume S Masson, Ad M Aertsen  (2010). Functional consequences of correlated excitatory and inhibitory conductances in cortical networks. Journal of Computational Neuroscience.  PDF  Cite  DOI     Description of the PHD thesis project The primary visual cortex (V1) is one of the most studied cortical area in neuroscience. Together with the retina and the lateral geniculate nucleus (LGN), it forms the early visual system, which has become a common model for studying computational principles in the sensory systems. Simple artificial stimuli (such as drifting gratings (DG)) have given precious insights into the neural basis of visual processing. However, recently more researchers have used more complex natural images (NI) visual stimuli, arguing that the low dimensional artificial stimuli are not sufficient for a complete understanding of the visual system. For example, whereas the responses of V1 neurons to DG are dense but with variable spike timings, the neurons are activated with only few and precise spikes to NI. Furthermore, if linear receptive field models provide a good fit to responses during simple stimuli, they often fail during NI.\nTo investigate the mechanisms behind the stimulus dependent responses of cortical neurons we have built a biophysical, yet simple and comprehensible, model of the early visual system. We show how the spatial and temporal stimulus properties interact with the model architecture to give rise to differential response behaviour. Our results show in particular that during NI, the LGN afferents show epochs of correlated activity. These temporal correlations are necessary to induce transient excitatory synaptic inputs, and result in precise spike timings in V1. Furthermore, the sparseness of the responses to NI can be explained by a hardwired, correlated and lagging inhibitory conductance, or conductance temporal window, which is induced by the interactions of the thalamocortical circuit with the spatiotemporal correlations in the stimulus.\nWe continue by investigating the origin of nonlinear responses during NI in the temporal window, by comparing models of different complexity. Our results suggest first that adaptive processes shape the responses, depending on the temporal properties of the stimuli. The different spatial properties can result in nonlinear inputs through the recurrent cortical network. We then study the functional consequences of correlated excitatory and inhibitory condutances in more details in general models. These results show that: (1) spiking of individual neurons becomes sparse and precise, (2) the selectivity of signal propagation increases and the detailed delay allows to gate the propagation through feed-forward structures (3) and recurrent cortical networks are more stable and more likely to elicit in vivo type activity states. Lastly our work illustrates new advances in methods of constructing and exchanging models of neuronal systems by the means of a simulator independent description language (called PyNN). We use this new tool to investigate the feasibility of comparing software simulations with neuromorphic hardware emulations. The presented work give new perspectives on the way conductances can be used for computations and it opens the door for more elaborated models of visual system’s mechanisms.\n","date":1451606400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1451606400,"objectID":"2694be611ae15e17c0ebceda5740e2cd","permalink":"https://laurentperrinet.github.io/author/jens-kremkow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jens-kremkow/","section":"authors","summary":"Correlating Excitation and Inhibition in Visual Cortical Circuits: Functional Consequences and Biological Feasibility (PhD, 2006-01 / 2009-05) The goal of the FACETS (Fast Analog Computing with Emergent Transient States) project was to create a theoretical and experimental foundation for the realisation of novel computing paradigms which exploit the concepts experimentally observed in biological nervous systems.","tags":null,"title":"Jens Kremkow","type":"authors"},{"authors":["karl-friston"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1451606400,"objectID":"e8bfb2a0a02acaed154d860ce7a34ed3","permalink":"https://laurentperrinet.github.io/author/karl-friston/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/karl-friston/","section":"authors","summary":"","tags":null,"title":"Karl Friston","type":"authors"},{"authors":["rick-a-adams"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1451606400,"objectID":"0e6842b04949c5791fed872f341e6425","permalink":"https://laurentperrinet.github.io/author/rick-a-adams/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rick-a-adams/","section":"authors","summary":"","tags":null,"title":"Rick A Adams","type":"authors"},{"authors":["james-a-bednar"],"categories":null,"content":"","date":1440028800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1440028800,"objectID":"95ac6d2085e8eb21bc1b2d2ab4975b26","permalink":"https://laurentperrinet.github.io/author/james-a-bednar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/james-a-bednar/","section":"authors","summary":"","tags":null,"title":"James A Bednar","type":"authors"},{"authors":["paula-s-leon"],"categories":null,"content":"","date":1331683200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1331683200,"objectID":"1e7464dae55f75833e41b2f2dc75c992","permalink":"https://laurentperrinet.github.io/author/paula-sanz-leon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/paula-sanz-leon/","section":"authors","summary":"","tags":null,"title":"Paula Sanz Leon","type":"authors"},{"authors":["nicole-voges"],"categories":null,"content":"Complex dynamics in recurrent cortical networks based on spatially realistic connectivities (Post-Doc, 2008 / 2010) Description Most studies on the dynamics of recurrent cortical networks are either based on purely random wiring or neighborhood couplings. Neuronal cortical connectivity, however, shows a complex spatial pattern composed of local and remote patchy connections. We ask to what extent such geometric traits influence the “idle” dynamics of two-dimensional (2d) cortical network models composed of conductance-based integrate-and-fire (iaf) neurons. In contrast to the typical 1 mm2 used in most studies, we employ an enlarged spatial set-up of 25 mm2 to provide for long-range connections. Our models range from purely random to distance-dependent connectivities including patchy projections, i.e., spatially clustered synapses. Analyzing the characteristic measures for synchronicity and regularity in neuronal spiking, we explore and compare the phase spaces and activity patterns of our simulation results. Depending on the input parameters, different dynamical states appear, similar to the known synchronous regular (SR) or asynchronous irregular (AI) firing in random networks. Our structured networks, however, exhibit shifted and sharper transitions, as well as more complex activity patterns. Distance-dependent connectivity structures induce a spatio-temporal spread of activity, e.g., propagating waves, that random networks cannot account for. Spatially and temporally restricted activity injections reveal that a high amount of local coupling induces rather unstable AI dynamics. We find that the amount of local versus long-range connections is an important parameter, whereas the structurally advantageous wiring cost optimization of patchy networks has little bearing on the phase space.\nMain publications:    Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI      Nicole Voges, Laurent U Perrinet  (2012). Complex dynamics in recurrent cortical networks based on spatially realistic connectivities. Frontiers in Computational Neuroscience.  PDF  Cite  DOI     Context The goal of the FACETS (Fast Analog Computing with Emergent Transient States) project was to create a theoretical and experimental foundation for the realisation of novel computing paradigms which exploit the concepts experimentally observed in biological nervous systems. The continuous interaction and scientific exchange between biological experiments, computer modelling and hardware emulations within the project provides a unique research infrastructure that will in turn provide an improved insight into the computing principles of the brain. This insight may potentially contribute to an improved understanding of mental disorders in the human brain and help to develop remedies.\nReferences  Reynaud A., Masson G. S. and Chavane F. Dynamics of Local Input Normalization Result from Balanced Short- and Long-Range Intracortical Interactions in Area V1 Journal of Neuroscience, 2012, 32(36): 12558-12569 Reynaud A., Takerkart S, Masson G. S. and Chavane F. Linear model decomposition for voltage-sensitive dye imaging signals: Application in awake behaving monkey. Neuroimage, 2011, 54(2), 1196–1210 Perrinet, L. and Masson G. Motion-based prediction is sufficient to solve the aperture problem Neural Computation, 2012  ","date":1325376000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1325376000,"objectID":"a4fceb7769dc94859717dfbf34c830ba","permalink":"https://laurentperrinet.github.io/author/nicole-voges/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nicole-voges/","section":"authors","summary":"Complex dynamics in recurrent cortical networks based on spatially realistic connectivities (Post-Doc, 2008 / 2010) Description Most studies on the dynamics of recurrent cortical networks are either based on purely random wiring or neighborhood couplings.","tags":null,"title":"Nicole Voges","type":"authors"},{"authors":["manuel-samuelides"],"categories":null,"content":"","date":1174348800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1174348800,"objectID":"92f848359d58e0553e39a2af8124a2e2","permalink":"https://laurentperrinet.github.io/author/manuel-samuelides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/manuel-samuelides/","section":"authors","summary":"","tags":null,"title":"Manuel Samuelides","type":"authors"},{"authors":["simon-j-thorpe"],"categories":null,"content":"","date":1095638400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1095638400,"objectID":"43b342a48baff06be9410b1fb58e3e34","permalink":"https://laurentperrinet.github.io/author/simon-thorpe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/simon-thorpe/","section":"authors","summary":"","tags":null,"title":"Simon Thorpe","type":"authors"},{"authors":["Antoine Grimaldi","Laurent U Perrinet"],"categories":[],"content":"","date":1656460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653046958,"objectID":"114caed130d547ee9287480da934dc12","permalink":"https://laurentperrinet.github.io/talk/2022-06-29-areadne-heterosynaptic/","publishdate":"2022-05-20T11:42:36.747953Z","relpermalink":"/talk/2022-06-29-areadne-heterosynaptic/","section":"talk","summary":"The response of a biological neuron depends on the precise timing of afferent spikes. This temporal aspect of the neuronal code is essential in understanding information processing in neurobiology and applies particularly well to the output of neuromorphic hardware such as event-based cameras. However, most artificial neuronal models do not take advantage of this minute temporal dimension. Inspired by this neuroscientific observation, we develop a model for the efficient detection of temporal spiking motifs based on a layer of neurons with hetero-synaptic delays. Indeed, the connectivity of the dendritic tree allows to discriminate between different temporal sequences, and we show that this can be formalized as a time-invariant logistic regression which can be trained using labelled data. We apply this model to solve the specific computer vision problem of motion detection and demonstrate its application to synthetic nature videos transformed into event streams similar to the output of event-based cameras. In particular, we quantify how its accuracy can vary with the total computational load. This end-to-end event-driven computational brick could help improve the performance of future spiking neural network (SNN) solutions currently used in neuromorphic chips.","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"Learning hetero-synaptic delays of Spiking Neurons for motion detection","type":"talk"},{"authors":["Jean-Nicolas Jérémie","Emmanuel Daucé","Laurent U Perrinet"],"categories":[],"content":"","date":1656460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653046958,"objectID":"7f3731fd99f3e6008f6b0cf89fa3874b","permalink":"https://laurentperrinet.github.io/talk/2022-06-29-areadne-fast-and-curious/","publishdate":"2022-05-20T11:42:38.160617Z","relpermalink":"/talk/2022-06-29-areadne-fast-and-curious/","section":"talk","summary":"Visual search, that is, the simultaneous localization and detection of a visual target of interest, is a vital task. Applied to the case of natural scenes, searching for example to an animal (either a prey, a predator or a partner) constitutes a challenging problem due to large variability over numerous visual dimensions such as shape, pose, size, texture or position. Yet, biological visual systems are able to perform such detection efficiently in  briefly flashed scenes and in a very short amount of time.Deep convolutional neuronal networks (CNNs) were shown to be well fitted to the image classification task, providing with human (or even super-human) performance. Previous models also managed to solve the visual search task, by roughly dividing the image into sub-areas. This is at the cost, however, of computer-intensive parallel processing on relatively low-resolution image samples. Taking inspiration from natural vision systems, we develop here a model that builds over the anatomical visual processing pathways observed in mammals, namely the What and the Where pathways. It operates in two steps, one by selecting regions of interest, before knowing their actual visual content, through an ultra-fast/low resolution analysis of the full visual field, and the second providing a detailed categorization over the detailed foveal selected region attained with a saccade.","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"Ultra-rapid visual search in natural images using active deep learning","type":"talk"},{"authors":["Ilias Rentzeperis","Luca Calatroni","Laurent U Perrinet","Dario Prand"],"categories":[],"content":"","date":1656460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653046958,"objectID":"3edba7a84131451aca0c6ce6eb2b1024","permalink":"https://laurentperrinet.github.io/talk/2022-06-29-areadne-sparse/","publishdate":"2022-05-20T11:42:38.576727Z","relpermalink":"/talk/2022-06-29-areadne-sparse/","section":"talk","summary":"Experimental evidence suggests that activity in sensory cortices is sparse in that only few neurons out of a large pool that could respond to sensed stimuli, are active at a time. Generative learning models that aim to replicate sensory systems could deviate from sparse activity patterns when representing noisy signals. We ask: are there biologically plausible implementations that will maintain sparse activations for different levels of noise while representing the underlying signal? A family of generative algorithms modelling sensory systems represent a stimulus as a linear sum of an overcomplete dictionary of vectors with their corresponding coefficients taking the role of activations. Olshausen and Field [1] showed that a learning algorithm that is set to reconstruct natural images with sparse activations develops vectors with properties, found in the receptive fields of neurons in V1, i.e. they are localized, band-pass, and oriented.","tags":[],"title":"Which sparsity problem does the brain solve?","type":"talk"},{"authors":["Antoine Grimaldi","Laurent U Perrinet"],"categories":[],"content":"","date":1655596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653042858,"objectID":"0c1473a94ea78019174c50b46126f0af","permalink":"https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-heterosynaptic/","publishdate":"2022-05-20T10:34:17.824678Z","relpermalink":"/talk/2022-06-19-neuro-vision-heterosynaptic/","section":"talk","summary":"","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"Learning hetero-synaptic delays of Spiking Neurons for motion detection","type":"talk"},{"authors":["Jean-Nicolas Jérémie","Emmanuel Daucé","Laurent U Perrinet"],"categories":[],"content":" Follows a previous work  Emmanuel Daucé, Pierre Albigès, Laurent U Perrinet  (2020). A dual foveal-peripheral visual processing model implements efficient saccade selection. Journal of Vision.  Preprint  PDF  Cite  Code  DOI     ","date":1655596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653045148,"objectID":"e8e84c8175deabdf845bae15af6e8f05","permalink":"https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-retinotopic/","publishdate":"2022-05-20T11:12:27.976406Z","relpermalink":"/talk/2022-06-19-neuro-vision-retinotopic/","section":"talk","summary":" Follows a previous work  Emmanuel Daucé, Pierre Albigès, Laurent U Perrinet  (2020). A dual foveal-peripheral visual processing model implements efficient saccade selection. Journal of Vision.  Preprint  PDF  Cite  Code  DOI     ","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"Retinotopic mapping improves the reliability of image classification","type":"talk"},{"authors":["Antoine Grimaldi","Laurent U Perrinet"],"categories":null,"content":" Follow this future presentations  Antoine Grimaldi, Laurent U Perrinet  (2022). Learning hetero-synaptic delays of Spiking Neurons for motion detection. NeuroVision Workshop in conjunction with CVPR 2022.  Cite  URL    Glad to meet the @centuri_ls crowd at the #CENTURIday ! With @A_Grismaldi https://t.co/r4633Vzg4F https://t.co/GbOGKhB6zA\n— laurentperrinet (@laurentperrinet) May 20, 2022  ","date":1652918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652918400,"objectID":"c4a0c62865c1a1f9c7dd25dd57e383e1","permalink":"https://laurentperrinet.github.io/talk/2022-05-19-centuri-day/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/2022-05-19-centuri-day/","section":"talk","summary":"Follow this future presentations  Antoine Grimaldi, Laurent U Perrinet  (2022). Learning hetero-synaptic delays of Spiking Neurons for motion detection. NeuroVision Workshop in conjunction with CVPR 2022.  Cite  URL    Glad to meet the @centuri_ls crowd at the #CENTURIday !","tags":null,"title":"Polychrony detection using heterosynaptic delays","type":"talk"},{"authors":["Jean-Nicolas Jérémie","Laurent U Perrinet"],"categories":[],"content":" this is a follow-up of:   ","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652254324,"objectID":"5ba5c02bd8dba9ff02ab9f28f8ef3f91","permalink":"https://laurentperrinet.github.io/publication/jeremie-22-ultra-fast-cat/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/jeremie-22-ultra-fast-cat/","section":"publication","summary":"Humans are able to robustly categorize images and can, for instance, detect the presence of an animal in a briefly flashed image in as little as 120 ms. Initially inspired by neuroscience, deep-learning algorithms literally bloomed up in the last decade such that the accuracy of machines is at present superior to humans for visual recognition tasks. However, these artificial networks are usually trained and evaluated on very specific tasks, for instance on the 1000 separate categories of IMAGENET. In that regard, biological visual systems are more flexible and efficient compared to artificial systems on generic ecological tasks. In order to deepen this comparison, we retrained the standard VGG Convolutional Neural Network (CNN) on two independent tasks which are ecologically relevant for humans: one task defined as detecting the presence of an animal and the other as detecting the presence of an artifact. We show that retraining the network achieves human-like performance level which is reported in psychophysical tasks. We also compare the accuracy of the detection on an image-by-image basis. This showed in particular that the two models perform better when combining their outputs. Indeed, animals (e.g. lions) tend to be less present in photographs containing artifacts (e.g. buildings). These re-trained models could reproduce some unexpected behavioral observations from humans psychophysics such as the robustness to rotations (e.g. upside-down or slanted image) or to a grayscale transformation. Finally, we quantitatively tested the number of layers of the CNN which are necessary to reach such a performance, showing that a good accuracy for ultra-fast categorization could be reached with only a few layers, challenging the belief that image recognition would require a deep sequential analysis of visual objects. We expect to apply this framework to guide future model-based psychophysical experiments and biomimetic deep neuronal architectures designed for such tasks.","tags":[],"title":"Ultra-Fast Image Categorization in Vivo and in Silico","type":"publication"},{"authors":null,"categories":null,"content":"Réseaux de neurones artificiels et apprentissage machine appliqués à la compréhension de la vision Laurent Perrinet [2022-03-23] Master 1 Neurosciences et Sciences Cognitives     Principes de la Vision  Only the speaker can read these notes Press S key to view more on doc    À quoi sert la vision?   An Unexpected Visitor (Ilya Repin, 1884)   À quoi sert la vision?   An Unexpected Visitor (Yarbus, 1965)   À quoi sert la vision?   An Unexpected Visitor - Age? (Yarbus, 1965)   À quoi sert la vision?   An Unexpected Visitor - How long? (Yarbus, 1965)   Les illusions visuelles   Hering illusion   Les illusions visuelles   Hering illusion   Les illusions visuelles   Ilusions of brightness or lightness Akiyoshi KITAOKA\n Les illusions visuelles   Rotating Snakes Akiyoshi KITAOKA   Les illusions visuelles : Paréidolie   Cydonia Mensae (1976) Viking Orbiter image   Les illusions visuelles : Paréidolie   Cydonia Mensae (2007) Mars Global Surveyor   Les illusions visuelles : Paréidolie   Cydonia Mensae (2007) Mars Global Surveyor   Les neurosciences computationnelles   [Sejnowski, Koch \u0026amp; Churchland (1998)]   De V1 aux réseaux convolutionnels  Le système visuel   Système visuel humain (Wikipedia)   Le cortex visuel primaire   [Hubel \u0026amp; Wiesel, 1962]   Hubel \u0026amp; Wiesel  [Hubel \u0026amp; Wiesel, 1962]\n Réseaux convolutionnels : hiérarchie   [Boutin et al, 2021]   Réseaux convolutionnels : Math  Convolution discrète uni-dimensionnelle (eg dans le temps) avec un noyau f de rayon $K$: $$ (f \\ast g)[n]=\\sum_{m=-K}^{K} f[m] g[n-m] $$   Réseaux convolutionnels : Math  Convolution discrète d’une image (bi-dimensionnelle):  $$ (f \\ast g)[x, y] = \\sum_{i=-K}^{K} \\sum_{j=-K}^{K} f[i, j] g[i-x, j-y] $$\n Réseaux convolutionnels : l’opération de convolution   [Amidi \u0026amp; Amidi]   Réseaux convolutionnels : Math  Convolution discrète d’une image sur plusieurs canaux de sortie:  $$ (f \\ast g)[x, y, k] = \\sum_{i=-K}^{K} \\sum_{j=-K}^{K} f[k, i, j, k] g[i-x, j-y] $$\n Réseaux convolutionnels : Math  Convolution discrète d’une image multi-canaux (eg. RGB) sur plusieurs canaux de sortie (noter l’ordre des indices):  $$ (f \\ast g)[x, y, k] = \\ \\sum_{i=-K}^{K} \\sum_{j=-K}^{K} \\sum_{c=1}^{C} f[k, c, i, j] g[i-x, j-y, c] $$\n Réseaux convolutionnels : CNN   [Amidi \u0026amp; Amidi]   Mise en pratique: détecter \u0026amp; apprendre   Tutoriel Apprentissage profond\n  Notebook A_Détecter.ipynb\n  Notebook B_Apprendre.ipynb\n   Perspectives  Réseaux convolutionnels : hiérarchie   [Boutin et al, 2021]   Réseaux prédictifs   [Boutin et al, 2021]   Topographie dans V1   [Bosking et al, 1997]   Spiking Neural Networks   From frame-based to event-based cameras.   Recurrent processing   [Amidi \u0026amp; Amidi]   Dynamique de la vision   [Thorpe (2001)]   Applications robotiques   Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.   Questions? Ask info @ laurent.perrinet@univ-amu.fr\nMore info @ web-site\n","date":1648026000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648026000,"objectID":"17cb0484027595c8ce4a1245a27f4af5","permalink":"https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/","publishdate":"2012-03-21T06:00:00Z","relpermalink":"/slides/2022-03-23_ue-neurosciences-computationnelles/","section":"slides","summary":"Réseaux de neurones artificiels et apprentissage machine appliqués à la compréhension de la vision","tags":null,"title":"2022-03-23_UE-neurosciences-computationnelles","type":"slides"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"  Où: Salle PHY51 - Marseille (France)\n  Quoi: Master 1 Neurosciences et Sciences Cognitives\n   Réseaux neuronaux artificiels pour la vision   Mercredi 23/03/2022 de 9h-12h Introduction aux Neurosciences de la Vision Réseaux de neurones artificiels et apprentissage machine slides  Neurones impulsionnels et modèles des fonctions visuelles   Mercredi 23/03/2022 de 13h30-16h30 TP via notebook https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/  ","date":1647993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647993600,"objectID":"6394dd3363977f9738128121bcc2fd65","permalink":"https://laurentperrinet.github.io/talk/2022-03-23-ue-neurosciences-computationnelles/","publishdate":"2022-03-21T10:44:45.446866Z","relpermalink":"/talk/2022-03-23-ue-neurosciences-computationnelles/","section":"talk","summary":"Où: Salle PHY51 - Marseille (France)\n  Quoi: Master 1 Neurosciences et Sciences Cognitives\n   Réseaux neuronaux artificiels pour la vision   Mercredi 23/03/2022 de 9h-12h Introduction aux Neurosciences de la Vision Réseaux de neurones artificiels et apprentissage machine slides  Neurones impulsionnels et modèles des fonctions visuelles   Mercredi 23/03/2022 de 13h30-16h30 TP via notebook https://github.","tags":null,"title":"Réseaux de neurones artificiels et apprentissage machine appliqués à la compréhension de la vision","type":"talk"},{"authors":["Hugo Ladret","Laurent U Perrinet"],"categories":[],"content":" see previous work: https://laurentperrinet.github.io/sciblog/posts/2018-11-05-statistics-of-the-natural-input-to-a-ring-model.html  Mini-Symposium “Learning from vision: Efficient representation, sparse coding, and modelling” Although recent years have seen a striking improvement in imaging techniques, there are many tasks for which human interaction is still essential, as color gamut correction in the cinema industry. This suggests that a better understanding of the mechanisms underlying the visual system is instrumental to advances in imaging techniques. Along these lines, various ideas from computational neurosciences have found application in imaging, from pattern recognition to image inpainting. A promising line of investigation is built on methods based on models of the primary visual cortex and on neural coding, in particular via the efficient representation principle. These methods have recently allowed to define new artificial neural networks paradigms and to reproduce complex visual illusions. In this mini-symposium we aim to gather together experts working in the field of mathematical neuroscience and imaging, with a focus on these methods. In particular, the speakers will present recent results based on sparse coding and models of the visual system.\nOrganizer: Dario Prandi   12:40-1:05 The intrinsically nonlinear nature of receptive fields in vision: implications for imaging, vision science and artificial neural networks Marcelo Bertalmío, Spanish National Research Council, Spain\n  1:10-1:35 ChebLieNet: Invariant Spectral Graph Nns Turned Equivariant by Sub-Riemannian Geometry on Lie Groups Erik Bekkers, University of Amsterdam, Netherlands\n  1:40-2:05 Deep Predictive Coding for More Robust and Human-Like Vision Rufin VanRullen, Centre de Recherche Cerveau et Cognition (CerCo), France\n  2:10-2:35 Statistics of the Sparse Representations of Natural Images Hugo Ladret and Laurent U. Perrinet, CNRS \u0026amp; Aix-Marseille Université, Marseille, France\n  More on https://meetings.siam.org/sess/dsp_programsess.cfm?sessioncode=73028\n","date":1647907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634035485,"objectID":"fe90662bf985c8dfdfaa357e2427bee5","permalink":"https://laurentperrinet.github.io/talk/2022-03-22-siam-is-22/","publishdate":"2021-10-12T10:44:45.446866Z","relpermalink":"/talk/2022-03-22-siam-is-22/","section":"talk","summary":"In most mammals, the primary visual cortex (V1) processes complex mixtures of orientations to construct an accurate neural representation of our visual environment. Sparse coding has been used to model the emergence of orientation-selective localized receptive fields and in particular the diversity of their shapes which reflects the richness of textures found in natural images. However, it is not yet known whether the corresponding sparse coefficients follow consistent patterns in natural images. Here, we explored the statistics of the distribution of sparse coefficients extracted from a sequence of images in a database of natural scenes. We first replicated previous results showing that sparse coefficients decrease with prototypical shapes. Then, we extended these results by showing the dependence of sparse coefficients on their relative position but also on the inverse variance of their orientation selectivity, that is, its precision. This demonstrates that orientation precision is an important variable in the encoding of visual information. The analysis also showed that different precisions followed different dynamics, a prior information which could be further exploited in a dynamical model. This model-based hypothesis is finally confronted with neurophysiological recordings made in V1.","tags":[],"title":"Statistics of the sparse representations of natural images","type":"talk"},{"authors":["Frédéric Chavane","Laurent U Perrinet","James Rankin"],"categories":null,"content":"new paper with F Chavane and J Rankin on \u0026#34;Revisiting horizontal connectivity rules in V1: from like‑to‑like towards like‑to‑all\u0026#34; aiming at understanding lateral message passing in V1 :https://t.co/fjYuy0bHTz\n👉 happy to share a reprint via DM @univamu @INSB_CNRS @CNRS_dr12 pic.twitter.com/DgK4vMPDQ8\n— laurentperrinet (@laurentperrinet) February 7, 2022  ","date":1644019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644019200,"objectID":"2f102deff3140d34ed14d341f601c03b","permalink":"https://laurentperrinet.github.io/publication/chavane-22/","publishdate":"2022-02-05T00:00:00Z","relpermalink":"/publication/chavane-22/","section":"publication","summary":"Horizontal connections in the primary visual cortex of carnivores, ungulates and primates organize on a near-regular lattice. Given the similar length scale for the regularity found in cortical orientation maps, the currently accepted theoretical standpoint is that these maps are underpinned by a like-to-like connectivity rule: horizontal axons connect preferentially to neurons with similar preferred orientation. However, there is reason to doubt the rule's explanatory power, since a growing number of quantitative studies show that the like-to-like connectivity preference and bias mostly observed at short-range scale, are highly variable on a neuron-to-neuron level and depend on the origin of the presynaptic neuron. Despite the wide availability of published data, the accepted model of visual processing has never been revised. Here,~we review three lines of independent evidence supporting a much-needed revision of the like-to-like connectivity rule, ranging from anatomy to population functional measures, computational models and to theoretical approaches. We advocate an alternative, distance-dependent connectivity rule that is consistent with new structural and functional evidence: from like-to-like bias at short horizontal distance to like-to-all at long horizontal distance. This generic rule accounts for the observed high heterogeneity in interactions between the orientation and retinotopic domains, that we argue is necessary to process non-trivial stimuli in a task-dependent manner.","tags":["area-v1"],"title":"Revisiting Horizontal Connectivity Rules in V1: From like-to-like towards like-to-All","type":"publication"},{"authors":["Hugo Ladret","Nelson Cortes","Lamyae Ikan","Frédéric Chavane","Christian Casanova","Laurent U Perrinet"],"categories":null,"content":"now (2:00 PM - 3:00 PM CEST on Thursday, May 20), you can hear Hugo Ladret @univamu present his poster P4.47 \u0026#34;Processing of orientation precision in the primary visual cortex\u0026#34;https://t.co/vuT6gegwtO\nNeurofrance 2021 #NF2021 #NeuroFrance2021 @SocNeuro_Tweets pic.twitter.com/YQNF9FiB6m\n— laurentperrinet (@laurentperrinet) May 20, 2021  ","date":1642377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642377600,"objectID":"f7007dde5cf3c699911c5dcefe86c031","permalink":"https://laurentperrinet.github.io/publication/ladret-22/","publishdate":"2022-01-17T00:00:00Z","relpermalink":"/publication/ladret-22/","section":"publication","summary":"The primary visual cortex (V1) processes complex mixtures of orientations to build neural representations of our visual environment. It remains unclear how V1 adapts to the highly volatile distributions of orientations found in natural images. We used naturalistic stimuli and measured the response of V1 neurons to orientation distributions of varying bandwidth. Although broad distributions decreased single neuron tuning, a neurally plausible decoder could robustly retrieve the orientations of stimuli from the population activity at all bandwidths. This decoder demonstrates that V1 population co-encodes orientation and its precision, which enhances population decoding performances. This internal representation is mediated by temporally distinct neural dynamics and supports a precision-weighted description of neuronal message passing in the visual cortex.","tags":["decoding","orientation","precision","predictive coding","V1"],"title":"Dynamical processing of orientation precision in the primary visual cortex","type":"publication"},{"authors":["Antoine Grimaldi","Victor Boutin","Sio-Hoi Ieng","Ryad Benosman","Laurent U Perrinet"],"categories":[],"content":"","date":1642032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642084030,"objectID":"18587c96456e6bf6e26ffd40937f4e7b","permalink":"https://laurentperrinet.github.io/publication/grimaldi-22-pami/","publishdate":"2022-01-13T00:00:00Z","relpermalink":"/publication/grimaldi-22-pami/","section":"publication","summary":"We propose a neuromimetic architecture able to perform always-on pattern recognition. To achieve this, we extended an existing event-based algorithm [1], which introduced novel spatio-temporal features as a Hierarchy Of Time-Surfaces (HOTS). Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient event-based pattern recognition architecture. Inspired by neuroscience, we extended this method to increase its performance. Our first contribution was to add a homeostatic gain control on the activity of neurons to improve the learning of spatio-temporal patterns [2]. A second contribution is to draw an analogy between the HOTS algorithm and Spiking Neural Networks (SNN). Following that analogy, our last contribution is to modify the classification layer and remodel the offline pattern categorization method previously used into an online and event-driven one. This classifier uses the spiking output of the network to define novel time surfaces and we then perform online classification with a neuromimetic implementation of a multinomial logistic regression. Not only do these improvements increase consistently the performances of the network, they also make this event-driven pattern recognition algorithm online and bio-realistic. Results were validated on different datasets: DVS barrel [3], Poker-DVS [4] and N-MNIST [5]. We foresee to develop the SNN version of the method and to extend this fully event-driven approach to more naturalistic tasks, notably for always-on, ultra-fast object categorization. ","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"A Robust Event-Driven Approach to Always-on Object Recognition","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Nous aurons le plaisir d’échanger avec notre conférencier Laurent Perrinet et nous vous espérons nombreux. Pour situer le conférencier : https://laurentperrinet.github.io/2019-05_illusions-visuelles/ « C’est toujours fascinant de voir ou de revoir des illusions visuelles. C’est encore plus fascinant de plonger dans leurs explications. »\n ","date":1642010400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642010400,"objectID":"155e526512e971ac496acbed325a3872","permalink":"https://laurentperrinet.github.io/talk/2022-01-12-neuro-cercle/","publishdate":"2022-01-12T18:00:00Z","relpermalink":"/talk/2022-01-12-neuro-cercle/","section":"talk","summary":"Les illusions visuelles sont des créations d'artistes, de scientifiques et plus récemment, grâce aux réseaux sociaux, du grand public qui proposent des situations souvent incongrues, dans lesquelles l'eau remonte une cascade, les personnes volent dans les airs ou des serpents se mettent à tourner. Au-delà de leur indéniable coté ludique, ces illusions nous apprennent beaucoup sur le fonctionnement du cerveau, notamment quand celles-ci se transforment en hallucinations visuelles, dépassant ainsi les limites des capacités de notre perception. En tant que chercheur en Neurosciences à l'Institut de Neurosciences de la Timone à Marseille, je vous dévoilerai des aspects du fonctionnement du cerveau qui sont souvent méconnus. En particulier, nous verrons pourquoi un magicien peut tromper nos sens ou comment des objets peuvent voyager dans le temps. Surtout nous essaierons de comprendre le fonctionnement de notre perception visuelle sur les bases d'une théorie de la vision non pas comme une simple caméra qui enregistre des images mais comme un processus actif en relation avec le monde qui nous entoure.","tags":null,"title":"Des illusions aux hallucinations visuelles: une porte sur la perception","type":"talk"},{"authors":["Hugo Ladret","Laurent U Perrinet"],"categories":[],"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634039054,"objectID":"45d682b6a831312d54878d4d48833fd1","permalink":"https://laurentperrinet.github.io/publication/ladret-21-crs/","publishdate":"2021-10-15T00:00:00Z","relpermalink":"/publication/ladret-21-crs/","section":"publication","summary":"","tags":["decoding","orientation","precision","predictive coding","V1"],"title":"Decoding orientation distributions from noisy observations in V1","type":"publication"},{"authors":["Jean-Nicolas Jérémie","Laurent U Perrinet"],"categories":[],"content":" see a follow-up in:  Jean-Nicolas Jérémie, Laurent U Perrinet  (2022). Ultra-Fast Image Categorization in Vivo and in Silico. arXiv:2205.03635 [cs, q-bio].  Preprint  PDF  Cite     ","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634039054,"objectID":"8315e9ad7c56dbe6c96cf16bd4afb738","permalink":"https://laurentperrinet.github.io/publication/jeremie-21-crs/","publishdate":"2021-10-15T00:00:00Z","relpermalink":"/publication/jeremie-21-crs/","section":"publication","summary":" see a follow-up in:  Jean-Nicolas Jérémie, Laurent U Perrinet  (2022). Ultra-Fast Image Categorization in Vivo and in Silico. arXiv:2205.03635 [cs, q-bio].  Preprint  PDF  Cite     ","tags":["deep-learning","object categorization","psychophysics"],"title":"Ultra-fast categorization of images containing animals in vivo and in computo","type":"publication"},{"authors":["Antoine Grimaldi","Victor Boutin","Sio-Hoi Ieng","Ryad Benosman","Laurent U Perrinet"],"categories":[],"content":"    this proceedings paper follows up the poster presented at CBMI :  Antoine Grimaldi, Victor Boutin, Sio-Hoi Ieng, Laurent U Perrinet, Ryad Benosman  (2021). A homeostatic gain control mechanism to improve event-driven object recognition. Content-Based Multimedia Indexing (CBMI) 2021.  Preprint  PDF  Cite  Video  DOI    read the follow-up paper :  Antoine Grimaldi, Victor Boutin, Sio-Hoi Ieng, Ryad Benosman, Laurent U Perrinet  (2022). A Robust Event-Driven Approach to Always-on Object Recognition. TechRxiv preprint.  PDF  Cite  DOI  URL    ","date":1634169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634035474,"objectID":"c4484b43dc6f117e91ee77977927013e","permalink":"https://laurentperrinet.github.io/publication/grimaldi-21-crs/","publishdate":"2021-10-14T00:00:00Z","relpermalink":"/publication/grimaldi-21-crs/","section":"publication","summary":"We propose a neuromimetic online classifier for always-on digit recognition. To achieve this, we extend an existing event-based algorithm which introduced novel spatio-temporal features: time surfaces. Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and create an efficient hierarchical event-based pattern recognition architecture. Its formalism was previously adapted in the computational neuroscience domain by showing it may be implemented using a Spiking Neural Network (SNN) of leaky integrate-and-fire models and Hebbian learning. Here, we add an online classification layer using a multinomial logistic regression which is compatible with a neural implementation. A decision can be taken at any arbitrary time by taking the argmax of the probability values associated to each class. We extend the parallel with computational neuroscience by demonstrating that this classification layer is also equivalent to a layer of spiking neurons with a Hebbian-like learning mechanism. Our method obtains state-of-the-art performances on the N-MNIST dataset and we show that it is robust to both spatial and temporal jitter. As a summary, we were able to develop a neuromimetic SNN model for online digit classification. We aim at pursuing the study of this architecture for natural scenes and hope to offer insights on the efficiency of neural computations, and in particular how mechanisms of decision-making may be formed.","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"From event-based computations to a bio-plausible Spiking Neural Network","type":"publication"},{"authors":["Laurent U Perrinet","Étienne Rey"],"categories":null,"content":"Horizon Faille Festival interstices, du 5 au 17 octobre Du mercredi au dimanche de 14h à 18h Du 5 au 17 octobre, le festival interstices présente des expositions et performances qui proposent à travers des créations spectaculaires.\n        Horizon Faille est une installation globale qui cherche à défier la gravité de la nature. Prenant appui sur deux notions dont l’artiste en a fait ses motifs principaux – les failles du paysage et l’immatérielle ligne d’horizon – elle relève autant de la poésie que de l’expérience sensorielle. La notion de faille exprime la fracture, au sens géologique. Elle est à considérer comme un interstice, une zone de transformation, un passage d’un état à un autre. De même, l’horizon scinde la terre du ciel dans une tentative de géométrisation de l’univers, de mise en espace des éléments naturels. Intouchable ligne de partage, ce filin tendu désigne aussi le seuil de vision du paysage. C’est la ligne imaginaire qui se forme à partir de notre position dans l’espace. Elle est ce qui échappe à la vue ou à la représentation.\n","date":1633219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633219200,"objectID":"59cb62fdc1e7f25cdf21500a37d3ce66","permalink":"https://laurentperrinet.github.io/post/2021-10-04_interstices/","publishdate":"2021-10-03T00:00:00Z","relpermalink":"/post/2021-10-04_interstices/","section":"post","summary":"Horizon Faille @ interstices, Orangerie du jardin des plantes, Caen, 2021.","tags":["art-science"],"title":"Horizon Faille @ interstices","type":"post"},{"authors":["Alberto Arturo Vergani","Laurent U Perrinet"],"categories":null,"content":" poster number: 94 scheduled on Wednesday, Sep 22, 18:00 CEST. https://abstracts.g-node.org/conference/BC21/abstracts#/uuid/05f81f30-d5d5-4467-b977-f28e9bed65f0  ","date":1632268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632268800,"objectID":"c0955c967c6a3fab4c5cbfa2c9c247cb","permalink":"https://laurentperrinet.github.io/publication/vergani-21-bernstein/","publishdate":"2021-09-22T00:00:00Z","relpermalink":"/publication/vergani-21-bernstein/","section":"publication","summary":" poster number: 94 scheduled on Wednesday, Sep 22, 18:00 CEST. https://abstracts.g-node.org/conference/BC21/abstracts#/uuid/05f81f30-d5d5-4467-b977-f28e9bed65f0  ","tags":["lateral interactions"],"title":"Simulating anticipatory activity in a 1D Spiking Neural Network Model","type":"publication"},{"authors":["Laurent U Perrinet","Angelo Franciosini"],"categories":null,"content":"“SDPC : a sparse and predictive model of the early visual system” Soutenance de thèse Angelo Franciosini   Date : Mardi 28 septembre 2021 à 13h (CEST)\n  Lieu: en virtuel et salle Henri Gastaut, au rez de chaussée de l’INT (how to get there). La thèse était suivie d’un pot au R+4 de l’Institut de Neurosciences de la Timone (how to get there)\n  Quoi: le manuscrit sera disponible après la soutenance.\n  Jury  Anthony Burkitt, University of Melbourne, Rapporteur Thomas Serre, Brown University, Rapporteur Laura Dugué, Integrative Neuroscience \u0026amp; Cognition Center, Examinateur Emmanuel Daucé, CNRS, Examinateur Stéphane Viollet, CNRS, Examinateur Laurent Perrinet, CNRS, Directeur de thèse  Abstract One goal of visual neuroscience is to understand how the brain interprets sensory information and to describe cortical representations according to a specific computational model. In this thesis, we describe how a successful model for visual perception, Predictive Coding (PC), can be extended to account for highly nonlinear operations in the primary visual cortex of mammals (V1). In this thesis, we generalize PC in a convolutional network and propose an algorithm called Sparse Deep Predictive Coding (SDPC), which models the properties of the early visual cortex. We present the SDPC framework in two scientific articles: in the first, we use our network to model local interactions in the early visual system (V1/V2) and we show how feedback connectivity allows the visual system to adapt to the statistics of natural images. In a second article, we show that the SDPC can predict the emergence of nonlinear responses in V1 (complex cells) and explain the link between complex cells and higher-level structures like cortical orientation maps, across species. Finally, we will propose some extensions that will allow the SDPC to serve as a general model of the visual system.\nRésumé Un des objectifs des neurosciences visuelles est de comprendre comment le cerveau interprète les informations sensorielles et de décrire les représentations corticales grâce à un modèle computationnel. Dans cette thèse, nous décrivons comment un modèle de perception visuelle, le Codage Prédictif, peut être étendu pour rendre compte des opérations non linéaires dans le cortex visuel primaire des mammifères (V1). Dans cette thèse, nous généralisons le Codage Prédictif dans un réseau convolutif pour créer un modèle appelé Sparse Deep Predictive Coding (SDPC). Nous présentons le SDPC dans deux articles scientifiques : dans le premier, nous utilisons notre réseau pour modéliser les interactions locales dans le système visuel précoce (V1/V2) et nous montrons comment la connectivité de rétroaction permet au système visuel de s’adapter aux statistiques des images naturelles. Dans un second article, nous montrons que le SDPC peut prédire l’émergence de réponses non linéaires dans V1 (cellules complexes) et expliquer le lien entre cellules complexes et des structures de plus haut niveau comme les cartes d’orientation corticales, et ceci pour différentes espèces. Enfin, nous proposerons quelques extensions qui permettront au SDPC de servir de modèle général du système visuel.\n","date":1631178000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631178000,"objectID":"ed3440d28a66ee1005819dade202fdc1","permalink":"https://laurentperrinet.github.io/post/2021-09-28_soutenance-angelo-franciosini/","publishdate":"2021-09-09T09:00:00Z","relpermalink":"/post/2021-09-28_soutenance-angelo-franciosini/","section":"post","summary":"Angelo Franciosini (Equipe NeOpTo) a soutenu sa thèse de doctorat intitulée: *SDPC : a sparse and predictive model of the early visual system* le Mardi 28 septembre 2021 à 13h à 15h30","tags":["events"],"title":"Soutenance Angelo Franciosini","type":"post"},{"authors":["Hugo Ladret","Nelson Cortes","Lamyae Ikan","Frédéric Chavane","Christian Casanova","Laurent U Perrinet"],"categories":null,"content":" This is 40th edition of Dynamicsdays Nice, 23-27 August 2021 - https://dynamicsdays2021.univ-cotedazur.fr check out the book of abstracts In this talk, we will present the following paper :  Preliminary Program:  Bruno Cessac, The Retina as a Dynamical System Hugo Ladret \u0026amp; Laurent Perrinet, Dynamics of the processing of orientation precision in the primary visual cortex Gianluigi Mongillo, Glassy phase in dynamically balanced networks Romain Veltz, Spatial and color hallucinations in a mathematical model of primary visual cortex    ","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626097969,"objectID":"7d8de021af5b2cfcb5eec63df9b1750f","permalink":"https://laurentperrinet.github.io/talk/2021-08-27-ddxl/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/talk/2021-08-27-ddxl/","section":"talk","summary":"The primary visual cortex (V1) processes complex mixtures of orientations to build neural representations of our visual environment. It remains unclear how V1 adapts to the highly volatile distributions of orientations found in natural images. We used naturalistic stimuli and measured the response of V1 neurons to orientation distributions of varying bandwidth. Although broad distributions decreased single neuron tuning, a neurally plausible decoder could robustly retrieve the orientations of stimuli from the population activity at all bandwidths. This decoder demonstrates that V1 population co-encodes orientation and its precision, which enhances population decoding performances. This internal representation is mediated by temporally distinct neural dynamics and supports a precision-weighted description of neuronal message passing in the visual cortex.","tags":["area-v1"],"title":"Dynamical processing of orientation precision in the primary visual cortex","type":"talk"},{"authors":["Anna Montagnini","Emmanuel Daucé","Laurent U Perrinet"],"categories":null,"content":"Contextual motor adaptation is the ability to produce different motor responses depending on different contingencies signaled by specific cues or contexts. This requires to learn the relation between antecedent stimuli, that signal the future state of the environment, motor responses, and outcomes. A wealth of research have demonstrated that motor systems such as the saccadic or the pursuit eye movement system may simultaneously adapt in two opposite directions (for instance increasing and decreasing the saccade amplitudes) when a context, such as the orbital position of the eye before the movement, signals different contingencies for each response.\nHowever, it has also been repeatedly reported that some cues, such as the target color or its shape, do not come to control the adaptation of the motor response. These observations remain unexplained and we lack adequate theoretical concepts to account for them: any stimulus, or context, that is perfectly correlated with the experimental manipulation should, in theory, induce contextual adaptation as it is conventionally thought that outcome predictability is the main factor controlling contextual learning. This has been a particularly vexing problem for the past 25 years as motor adaptation has become one of the main experimental model to study learning in humans.\nTo solve this problem, the ACEs project relies on a general conceptual framework that elaborates on the active-inference view (REF) as well as recent proposals regarding the relation between value-based decision making and attention. Our conceptual model is grounded on the notion that, at each moment, several hypotheses regarding credit assignment (what causes what?) are competing to produce a behavioral policy. The inputs are categorized, somehow arbitrarily, as internal status, prior knowledge and sensory inputs. Sensory inputs might be viewed as affecting the hypothesis space while prior knowledge and internal status would provide bias in favor of various credit assignment hypothesis. Competition in the hypothesis space, relying on Bayesian inference, determines a unique motor response. Because out of all the different credit assignment hypotheses only one will prevail and determine the actual behavioral policy, the influence of the inputs on behavior are limited by their specific contribution to the dominating hypothesis, i.e. their weight.\nFiche d’identité  Acronyme : ACES (ANR-21-CE28-0013) Title : Assignment of credit and constraints on eye movement learning Coordinateur Scientifique : Laurent Madelain (ScaLab) Responsable Scientifique local : Anna Montagnini (UMR7289) Durée: 4 ans, à partir du 1er mars 2021 - 1er décembre 2024 Budget total: 435 k€ https://anr.fr/Projet-ANR-21-CE28-0013  Acknowledgement This work was supported by ANR project ANR-21-CE28-0013 \u0026#34;ANR ACES\u0026#34;.  ","date":1626134400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626134400,"objectID":"3527a637156d2149d5ab188bc3b8ccd5","permalink":"https://laurentperrinet.github.io/grant/anr-aces/","publishdate":"2021-07-13T00:00:00Z","relpermalink":"/grant/anr-aces/","section":"grant","summary":"Assignment of credit and constraints on eye movement learning (2021/2026).","tags":["grant","current-grant"],"title":"ANR ACES (2021/2026)","type":"grant"},{"authors":["Antoine Grimaldi","Victor Boutin","Sio-Hoi Ieng","Laurent U Perrinet","Ryad Benosman"],"categories":null,"content":" to be presented at the Bio-inspired circuits, systems and algorithms for multimedia special session of the Content-Based Multimedia Indexing (CBMI) 2021 conference that you can watch on Youtube. this proceedings paper follows up he poster presented in :  Antoine Grimaldi, Victor Boutin, Sio-Hoi Ieng, Laurent U Perrinet, Ryad Benosman  (2021). A robust bio-inspired approach to event-driven object recognition. Computational and Systems Neuroscience (Cosyne) 2021.  PDF  Cite    this proceedings paper was followed by the poster presented at CRS :  Antoine Grimaldi, Victor Boutin, Sio-Hoi Ieng, Ryad Benosman, Laurent U Perrinet  (2021). From event-based computations to a bio-plausible Spiking Neural Network. Champalimaud Research Symposium (CRS21).  PDF  Cite  Video    read the follow-up paper :  Antoine Grimaldi, Victor Boutin, Sio-Hoi Ieng, Ryad Benosman, Laurent U Perrinet  (2022). A Robust Event-Driven Approach to Always-on Object Recognition. TechRxiv preprint.  PDF  Cite  DOI  URL   Antoine Grimaldi and Laurent Perrinet received funding from the European Union ERA-NET CHIST-ERA 2018 research and innovation program under grant agreement No ANR-19-CHR3-0008-03.  ","date":1624492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618934488,"objectID":"dce6a0e8843fa41799d73e57f8327fcb","permalink":"https://laurentperrinet.github.io/publication/grimaldi-21-cbmi/","publishdate":"2021-06-24T00:00:00Z","relpermalink":"/publication/grimaldi-21-cbmi/","section":"publication","summary":"We propose a neuromimetic architecture able to perform pattern recognition. To achieve this, we extended the existing event-based algorithm from Lagorce et al (2017) which introduced novel spatio-temporal features: time surfaces. Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and create an efficient hierarchical event-based pattern recognition architecture. Inspired by biological findings and the efficient coding hypothesis, our main contribution is to integrate homeostatic regulation into the Hebbian learning rule. Indeed, in order to be optimally informative, average neural activity within a layer should be equally balanced across neurons. We used that principle to regularize neurons within the same layer by setting a gain depending on their past activity and such that they emit spikes with balanced firing rates. The efficiency of this technique was first demonstrated through a robust improvement in spatio-temporal patterns which were learnt during the training phase. In order to compare with state-of-the-art methods, we replicated past results on the same dataset as Lagorce et al (2017) and extended results in this study to the widely used N-MNIST dataset.","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"A homeostatic gain control mechanism to improve event-driven object recognition","type":"publication"},{"authors":["Laurent U Perrinet","Angelo Franciosini"],"categories":null,"content":"Presenting my poster tonight at 8:00p #cosyne2020, a work developed using Sparse Deep Predictive Coding (SDPC) during my PhD @laurentperrinet @NeuroSchool_mrs pic.twitter.com/LtUEBnlPNt\n— AF (@Angelo_RDN) February 28, 2020  This is from:\nKoray Kavukcuoglu, Marc\u0026#39;Aurelio Ranzato, Rob Fergus and Yann LeCun: Learning Invariant Features through Topographic Filter Maps, Proc. International Conference on Computer Vision and Pattern Recognition (CVPR\u0026#39;09), IEEE, 2009 pic.twitter.com/4gH6L3dmaJ\n— Yann LeCun (@ylecun) April 21, 2021   \n In this talk, I will present the following paper :  Angelo Franciosini, Victor Boutin, Frédéric Chavane, Laurent U Perrinet  (2021). Pooling in a predictive model of V1 explains functional and structural diversity across species. bioRxiv.  Preprint  PDF  Cite  DOI    see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1623755700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623698809,"objectID":"722399ac201158ec97eb1b1f482a4ef7","permalink":"https://laurentperrinet.github.io/talk/2021-06-15-smb/","publishdate":"2021-06-15T11:15:00Z","relpermalink":"/talk/2021-06-15-smb/","section":"talk","summary":"Presenting my poster tonight at 8:00p #cosyne2020, a work developed using Sparse Deep Predictive Coding (SDPC) during my PhD @laurentperrinet @NeuroSchool_mrs pic.twitter.com/LtUEBnlPNt\n— AF (@Angelo_RDN) February 28, 2020  This is from:","tags":[],"title":"Pooling in a predictive model of V1 explains functional and structural diversity across species","type":"talk"},{"authors":["Alberto Arturo Vergani"],"categories":null,"content":"Neural Turing Patterns Il y a des structures dans la nature qui émergent spontanément. Ils ont été étudiés pour la première fois par le mathématicien anglais Alan Turing (1912 – 1954), qui a également introduit le concept de machine informatique contemporaine. Des exemples de ces formes sont les rayures, les taches, les grilles, les pavages, les bulles, les spirales, les mousses et les vagues. Dans le cas spécifique de cette œuvre, il est montré une collection de modèles de Turing générés via des simulations neuronales. Chaque carré est un réseau neuronal de cellules qui se déclenchent avec des intensités faibles (bleues), moyennes (blanches) et élevées (rouge). Beaucoup d’entre eux se ressemblent beaucoup, tandis que d’autres sont très différents. Regroupées comme elles le sont, les images semblent avoir une continuité graphique avec leurs voisins plus proches. Mais, cette fonctionnalité visuelle est bien une illusion de la Gestalt, car ce sont des résultats de simulation complètement indépendants.\nThere are structures in nature that spontaneously emerge. They were studied for the first time by the English mathematician Alan Turing (1912 – 1954), who also introduced the concept of the contemporary computational machine. Examples of those shapes are stripes, spots, grids, tessellations, bubbles, spirals, foams and waves. In the specific case of this artwork, a collection of Turing patterns are generated via brain simulations. Each square is a network of neural cells that fire with low (blue), medium (white) and high (red) intensities. Many of them look very similar, while others are very different. Grouped as they are, pictures seem to have a graphical continuity with their closest. But, this visual feature is indeed a Gestalt illusion, because they are completely independent simulation results.\n","date":1623715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623715200,"objectID":"876a3949ece612f3d61ce997f638860e","permalink":"https://laurentperrinet.github.io/post/2021-06-15_neural-turing/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/post/2021-06-15_neural-turing/","section":"post","summary":"Neural Turing Patterns","tags":["art-science"],"title":"Neural Turing Patterns","type":"post"},{"authors":["Hugo Ladret","Nelson Cortes","Lamyae Ikan","Frédéric Chavane","Christian Casanova","Laurent U Perrinet"],"categories":null,"content":" As presented during the NeuroFrance 2021 meeting now (2:00 PM - 3:00 PM CEST on Thursday, May 20), you can hear Hugo Ladret @univamu present his poster P4.47 \u0026#34;Processing of orientation precision in the primary visual cortex\u0026#34;https://t.co/vuT6gegwtO\nNeurofrance 2021 #NF2021 #NeuroFrance2021 @SocNeuro_Tweets pic.twitter.com/YQNF9FiB6m\n— laurentperrinet (@laurentperrinet) May 20, 2021   get the abstract book In this talk, we will present the following paper :   ","date":1621519200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626099338,"objectID":"1d6c885271892386f4751586e97c71c1","permalink":"https://laurentperrinet.github.io/talk/2021-05-20-neuro-france/","publishdate":"2021-05-20T14:00:00Z","relpermalink":"/talk/2021-05-20-neuro-france/","section":"talk","summary":"The primary visual cortex (V1) processes complex mixtures of orientations to build neural representations of our visual environment. It remains unclear how V1 adapts to the highly volatile distributions of orientations found in natural images. We used naturalistic stimuli and measured the response of V1 neurons to orientation distributions of varying bandwidth. Although broad distributions decreased single neuron tuning, a neurally plausible decoder could robustly retrieve the orientations of stimuli from the population activity at all bandwidths. This decoder demonstrates that V1 population co-encodes orientation and its precision, which enhances population decoding performances. This internal representation is mediated by temporally distinct neural dynamics and supports a precision-weighted description of neuronal message passing in the visual cortex.","tags":["area-v1"],"title":"Dynamical processing of orientation precision in the primary visual cortex","type":"talk"},{"authors":["Angelo Franciosini","Victor Boutin","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":"Amazing work from @Angelo_RDN building on the unsupervised learning network architecture from @VictorBoutin 🚀 It captures the diversity observed in different species (from rabbits to primates) for different functions (complex cells, topography) into a synthetic model... https://t.co/xXksKXqQts\n— laurentperrinet (@laurentperrinet) April 21, 2021   this paper follows this COSYNE presentation :  Angelo Franciosini, Victor Boutin, Laurent U Perrinet  (2020). Modelling Complex-cells and topological structure in the visual cortex of mammals using Sparse Predictive Coding. Computational and Systems Neuroscience (Cosyne) 2020.  PDF  Cite    see a related work describing SDPC in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI      This is from:\nKoray Kavukcuoglu, Marc\u0026#39;Aurelio Ranzato, Rob Fergus and Yann LeCun: Learning Invariant Features through Topographic Filter Maps, Proc. International Conference on Computer Vision and Pattern Recognition (CVPR\u0026#39;09), IEEE, 2009 pic.twitter.com/4gH6L3dmaJ\n— Yann LeCun (@ylecun) April 21, 2021  ","date":1618963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963200,"objectID":"8d08c0ab2a4abe7b1476dbfd55f83c5b","permalink":"https://laurentperrinet.github.io/publication/franciosini-21/","publishdate":"2021-04-21T00:00:00Z","relpermalink":"/publication/franciosini-21/","section":"publication","summary":"Neurons in the primary visual cortex are selective to orientation with various degrees of selectivity to the spatial phase, from high selectivity in simple cells to low selectivity in complex cells. Various computational models have suggested a possible link between the presence of phase invariant cells and the existence of cortical orientation maps in higher mammals' V1. These models, however, do not explain the emergence of complex cells in animals that do not show orientation maps. In this study, we build a model of V1 based on a convolutional network called Sparse Deep Predictive Coding (SDPC) and show that a single computational mechanism, pooling, allows the SDPC model to account for the emergence of complex cells as well as cortical orientation maps in V1, as observed in distinct species of mammals. By using different pooling functions, our model developed complex cells in networks that exhibit orientation maps (e.g., like in carnivores and primates) or not (e.g., rodents and lagomorphs). The SDPC can therefore be viewed as a unifying framework that explains the diversity of structural and functional phenomena observed in V1. In particular, we show that orientation maps emerge naturally as the most cost-efficient structure to generate complex cells under the predictive coding principle.","tags":["deep-learning","sparse coding"],"title":"Pooling in a predictive model of V1 explains functional and structural diversity across species","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Publication d’un nouvel article généraliste autour des illusions visuelles, “Les illusions sèment le trouble dans les esprits” à découvrir dans lee dossier La Recherche n°565 (trimestriel N°565 daté avril-juin 2021):\nNouvel article \u0026#34;Les illusions sèment le trouble dans les esprits\u0026#34; avec Hervé Ratel https://t.co/4YIEGdsrby @maglarecherche @univamu @CNRS @INSB_CNRS - et ému de publier dans le mag de mes années ado qui m\u0026#39;a permis de découvrir le monde de la recherche🧑‍🔬 juste retour des choses!\n— laurentperrinet (@laurentperrinet) April 11, 2021     Les objectifs sont :\n mieux comprendre la fonction de la perception visuelle en explorant certaines limites; mieux comprendre l’importance de l’aspect dynamique de la perception; mieux comprendre le rôle de l’action dans la perception.  Une version précédente est accessible sur le repo GitHub, ainsi que les sources.\n","date":1617667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617667200,"objectID":"954fca40706c31a944a6dd7e8328c427","permalink":"https://laurentperrinet.github.io/post/2021-04-06-larecherche/","publishdate":"2021-04-06T00:00:00Z","relpermalink":"/post/2021-04-06-larecherche/","section":"post","summary":"Article de dissémination sur la perception visuelle, vue à travers illusions visuelles.","tags":["neuroscience","vision","psychiatry"],"title":"Les illusions sèment le trouble dans les esprits","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Le but de RUBIN-VASE est de concevoir et valider de nouveaux modèles variationnels pour l’évolution des activations neuronales dans les systèmes visuel et auditif, codant naturellement le principe neurobiologique de représentation efficace. En nous concentrant sur des modifications des équations de Wilson-Cowan pour la dynamique neuronale, nous visons à i) valider cette approche pour le cortex visuel primaire, à travers l’étude des patterns hallucinatoires ; ii) développer un cadre neuro-inspiré pour le traitement sonore et la reconstruction vocale, à partir des mêmes principes ; iii) comparer les modèles variationnels proposés à des modèles data-driven. Pour atteindre nos objectifs nous couplerons le développement de théories mathématiques rigoureuses avec leur validation numérique et expérimentale. Cela se fera à travers une interaction originale entre des techniques variationnelles ou issues de la théorie du contrôle et des expériences psycho-physiques.\ncarte d’identité du projet  Durée: 4 ans, à partir du 1er avril 2021 Budget total (partenaire français): 665 k€ Coordinateur Scientifique : Dario PRANDI (Laboratoire des Signaux et Systèmes) Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE, CE48 - Fondements du numérique: informatique, automatique, traitement du signal Responsable Scientifique INT : Laurent PERRINET (UMR7289) https://anr.fr/Projet-ANR-20-CE48-0003  Acknowledgement This work was supported by ANR project “RubinVase” N° ANR-20-CE48-0003.\n","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"08d692f14f4d5631aacf91d7778aeec6","permalink":"https://laurentperrinet.github.io/grant/anr-rubinvase/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/grant/anr-rubinvase/","section":"grant","summary":"RedUndancy-free neuro-BIological desigN of Visual and Auditory SEnsing","tags":["grant","current-grant"],"title":"ANR RubinVase (2021/2024)","type":"grant"},{"authors":["Antoine Grimaldi","Victor Boutin","Sio-Hoi Ieng","Laurent U Perrinet","Ryad Benosman"],"categories":null,"content":"Tomorrow Antoine Grimaldi will present our joint work on \u0026#34;A robust bio-inspired approach to event-driven object recognition\u0026#34; at #cosyne2021 check-out the poster now https://t.co/DUNQPcv1mx or meet him tomorrow during the poster session ! pic.twitter.com/wKTJPZbR6B\n— laurentperrinet (@laurentperrinet) February 25, 2021   \n see the poster online on the Hopin platform see a follow-up in:  Antoine Grimaldi, Victor Boutin, Sio-Hoi Ieng, Laurent U Perrinet, Ryad Benosman  (2021). A homeostatic gain control mechanism to improve event-driven object recognition. Content-Based Multimedia Indexing (CBMI) 2021.  Preprint  PDF  Cite  Video  DOI    read also the follow-up paper :  Antoine Grimaldi, Victor Boutin, Sio-Hoi Ieng, Ryad Benosman, Laurent U Perrinet  (2022). A Robust Event-Driven Approach to Always-on Object Recognition. TechRxiv preprint.  PDF  Cite  DOI  URL   Antoine Grimaldi and Laurent Perrinet received funding from the European Union ERA-NET CHIST-ERA 2018 research and innovation program under grant agreement No ANR-19-CHR3-0008-03.  ","date":1614297600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614268888,"objectID":"85fe9fb573bf1560df28441593963f38","permalink":"https://laurentperrinet.github.io/publication/grimaldi-21-cosyne/","publishdate":"2021-02-26T00:00:00Z","relpermalink":"/publication/grimaldi-21-cosyne/","section":"publication","summary":"We propose a neuromimetic  architecture able to perform online pattern recognition. To achieve this, we extended the existing event-based algorithm from Lagorce et al (2017) which introduced novel spatio-temporal features: time-surfaces. Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient hierarchical event-based pattern recognition architecture. Inspired by biological findings and the efficient coding hypothesis, our main contribution is to integrate homeostatic regulation to the Hebbian learning rule. Indeed, in order to be optimally informative, average neural activity within a layer should be equally balanced across neurons. We used that principle to regularize neurons within the same layer by setting a gain depending on their past activity and such that they emit spikes with balanced firing rates. The efficiency of this technique was first demonstrated through a robust improvement in spatio-temporal patterns which were learned during the training phase. We validated classification performance with the widely used N-MNIST dataset reaching 87.3 percent accuracy with homeostasis compared to 72.5 percent accuracy without homeostasis. Finally, by studying the impact of input jitter on classification highlights resilience of this method. We expect to extend this fully event-driven approach to more naturalistic tasks, notably for ultra-fast object categorization.","tags":["efficient coding","event-based vision","homeostasis","neuromorphic hardware","online classification"],"title":"A robust bio-inspired approach to event-driven object recognition","type":"publication"},{"authors":["Victor Boutin","Angelo Franciosini","Frédéric Chavane","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":"My latest work with @Angelo_RDN, Frederic Chavane, Franck Ruffier and @laurentperrinet has been released in PLOS CB (https://t.co/0uvFeiSuOR). Our model combines Sparse Coding and Predictive Coding and introduce a novel way to visualize neural representation : the interaction map pic.twitter.com/AORwdFAMw3\n— Victor Boutin (@VictorBoutin) January 31, 2021    Fig 1. Architecture of a 2-layered SDPC model.  One often compares biological vision to a camera-like system where an image would be processed according to a sequence of successive transformations. In particular, this “feedforward” view is prevalent in models of visual processing such as deep learning. However, neuroscientists have long stressed that more complex information flow is necessary to reach natural vision efficiency. In particular, recurrent and feedback connections in the visual cortex allow to integrate contextual information in our representation of visual stimuli. These modulations have been observed both at the low-level of neural activity and at the higher level of perception.   Fig 2. Results of training SDPC on the natural images (left column) and on the face database (right column) with a feedback strength kFB = 1.    Fig 14. Illustration of the hierarchical generative model learned by the SDPC model on the face database.  In this study, we present an architecture that describes biological vision at both levels of analysis. It suggests that the brain uses feedforward and feedback connections to compare the sensory stimulus with its own internal representation. In contrast to classical deep learning approaches, we show that our model learns interpretable features.   Fig 5. Example of a 9 × 9 interaction map of a V1 area centered on neurons strongly responding to a central preferred orientation of 30°.    Fig 7. Example of a 9 × 9 interaction map of a V1 area centered on neurons strongly responding to a central preferred orientation of 45°, and colored with the relative response w.r.t. no feedback.  Moreover, we demonstrate that feedback signals modulate neural activity to promote good continuity of contours. Finally, the same model can disambiguate images corrupted by noise. To the best of our knowledge, this is the first time that the same model describes the effect of recurrent and feedback modulations at both neural and representational levels.   Fig 10. Effect of the feedback strength on noisy images from natural images database. \n more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI    presented during this talk:  Victor Boutin, Angelo Franciosini, Laurent U Perrinet  (2019). From the retina to action: Predictive processing in the visual system.  PDF  Cite  Code  Slides     ","date":1611619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611619200,"objectID":"b3343e5ca0ad1a97dadc92ed53b8659d","permalink":"https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/","publishdate":"2021-01-26T00:00:00Z","relpermalink":"/publication/boutin-franciosini-chavane-ruffier-perrinet-20/","section":"publication","summary":"Both neurophysiological and psychophysical experiments have pointed out the crucial role of recurrent and feedback connections to process context-dependent information in the early visual cortex. While numerous models have accounted for feedback effects at either neural or representational level, none of them were able to bind those two levels of analysis. Is it possible to describe feedback effects at both levels using the same model? We answer this question by combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical and convolutional framework. In this Sparse Deep Predictive Coding (SDPC) model, the SC component models the internal recurrent processing within each layer, and the PC component describes the interactions between layers using feedforward and feedback connections. Here, we train a 2-layered SDPC on two different databases of images, and we interpret it as a model of the early visual system (V1~\u0026~V2). We first demonstrate that once the training has converged, SDPC exhibits oriented and localized receptive fields in V1 and more complex features in V2. Second, we analyze the effects of feedback on the neural organization beyond the classical receptive field of V1 neurons using interaction maps. These maps are similar to association fields and reflect the Gestalt principle of good continuation. We demonstrate that feedback signals reorganize interaction maps and modulate neural activity to promote contour integration. Third, we demonstrate at the representational level that the SDPC feedback connections are able to overcome noise in input images. Therefore, the SDPC captures the association field principle at the neural level which results in better disambiguation of blurred images at the representational level.","tags":["deep-learning","sparse coding"],"title":"Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Ce texte est disponible dans cet article de The Conversation. Une version longue (et son code) sont aussi disponibles.  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"74f033971fce2f6d027bba0a688d5386","permalink":"https://laurentperrinet.github.io/publication/perrinet-21-hasard/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/perrinet-21-hasard/","section":"publication","summary":"Dans la pièce de théâtre la plus célèbre de Marivaux Le jeu de l'amour et du hasard, l'auteur joue à inverser le rôle des personnages, et le hasard est invité à guider leurs destins. De la même façon, notre cerveau est ballotté au gré du hasard, aussi bien dans une loterie que dans les incertitudes et ambiguı̈tés révélées dans la vision par les illusions d'optique. Au point que l'on peut attribuer à un esprit malin le fait que la tartine tombe du côté de la confiture, ou que la fiche du câble USB soit toujours dans le mauvais sens. Le hasard s'invite comme un personnage à part entière dans la cognition, et on peut s'interroger du rôle que celui-ci peut jouer dans le fonctionnement de notre cerveau.","tags":null,"title":"Le jeu du cerveau et du hasard","type":"publication"},{"authors":["Hugo Ladret","Nelson Cortes","Lamyae Ikan","Frédéric Chavane","Christian Casanova","Laurent U Perrinet"],"categories":[],"content":"Interested in understanding how the precision of visual features may be encoded in the brain?\nCome see @hugoladret poster P538.07 at virtual #SfN21 @SfNtweets https://t.co/yxU58qEMFo It\u0026#39;s today November 8, 2021, 1:30 PM CST / 8:30 PM CET - details 👉 https://t.co/jpl3zFtoka pic.twitter.com/y9Su9yAaVS\n— laurentperrinet (@laurentperrinet) November 8, 2021    ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636363878,"objectID":"f2457062b0823b61382219d518a5a8f7","permalink":"https://laurentperrinet.github.io/publication/ladret-21-sfn/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/ladret-21-sfn/","section":"publication","summary":"The primary visual cortex (V1) processes complex mixtures of orientations to build neural representations of our everyday visual environment. It remains unclear how V1 adapts to the highly volatile distributions of orientations found in natural images. We used naturalistic stimuli and measured the response of V1 neurons to orientation distributions of varying bandwidth. Although broad distributions decreased single neuron tuning, a neurally plausible decoder could robustly retrieve the orientations of stimuli from the population activity at all bandwidths. This decoder demonstrates that V1 population co-encodes orientation and its precision, enhancing population decoding performances compared to sole orientation decoding. This internal representation is mediated by temporally distinct neural dynamics and supports a precision-weighted description of neuronal message passing in the visual cortex, in line with predictive processing theories.","tags":["area-v1"],"title":"Modulation of orientation selectivity by orientation precision","type":"publication"},{"authors":["Emmanuel Daucé","Laurent U Perrinet"],"categories":null,"content":" a follow-up of:  Emmanuel Daucé, Pierre Albigès, Laurent U Perrinet  (2020). A dual foveal-peripheral visual processing model implements efficient saccade selection. Journal of Vision.  Preprint  PDF  Cite  Code  DOI   the mathematical details are described as a talk the 1st International WS on #ActiveInference #IWAI2020 at @ECMLPKDD https://t.co/4s7gHbMxiT and paper \u0026#34;Visual search as active inference\u0026#34; https://t.co/yNCOFHf7FS\n— laurentperrinet (@laurentperrinet) September 14, 2020    What:: talk @ 1st International Workshop on Active Inference (IWAI 2020) Who:: Emmanuel Daucé and Laurent Perrinet Where: Ghent (Belgium), gone virtual, see https://laurentperrinet.github.io/talk/2020-09-14-iwai When: 14/09/2020, time: 12:20:00-12:40:00 What:  Slides @ https://laurentperrinet.github.io/2020-09-14_IWAI Code for slides @ https://github.com/laurentperrinet/2020-09-14_IWAI/    ","date":1608163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608551480,"objectID":"893fd31974e87ce4da03838bd55104e9","permalink":"https://laurentperrinet.github.io/publication/dauce-20-iwai/","publishdate":"2020-12-17T00:00:00Z","relpermalink":"/publication/dauce-20-iwai/","section":"publication","summary":"a follow-up of:  Emmanuel Daucé, Pierre Albigès, Laurent U Perrinet  (2020). A dual foveal-peripheral visual processing model implements efficient saccade selection. Journal of Vision.  Preprint  PDF  Cite  Code  DOI   the mathematical details are described as a talk the 1st International WS on #ActiveInference #IWAI2020 at @ECMLPKDD https://t.","tags":["Active Inference","Deep-Learning","Object localization","Visual search","Visuomotor control"],"title":"Visual search as active inference","type":"publication"},{"authors":["Emmanuel Daucé","Stéphane Viollet","Ryad Benosman","Laurent U Perrinet"],"categories":null,"content":" An Unmanned aerial vehicle (UAV) flying autonomously in a cluttered environment would require the agility to navigate rapidly by detecting as fast as possible potential obstacles, as represented here by the collision zone, given a cruising speed, associated to slow or fast latencies (respectively red and blue shaded areas). This project will provide with a novel neuromorphic architecture designed to meet these requirements thanks to an event-based, two-way processing.  Fiche d’identité  Acronyme : AgileNeuRobot (ANR-20-CE23-0021) Titre : Robots aériens agiles bio-mimetiques pour le vol en conditions réelles Title : Bio-mimetic agile aerial robots flying in real-life conditions CES : CE23 - Intelligence Artificielle / Instrument de financement : Projet de recherche collaborative (PRC) / Catégorie R\u0026amp;D : Recherche fondamentale Coordinateur Scientifique : PERRINET Laurent (UMR7289) Durée: 3 ans, à partir du 1er mars 2021 - 1er décembre 2024 Budget total: 435 k€ Responsables Scientifiques : Stéphane Viollet (BioRobotique, Inst Sciences Mouvement), Ryad Benosman (Inst de la Vision ) | Laurent Perrinet (NeOpTo, Inst Neurosciences de la Timone, coordinateur)   A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively).  Résumé Des robots aériens autonomes seraient des outils essentiels dans les opérations de recherche et de sauvetage. Toutefois, voler dans des environnements complexes exige un haut niveau d’agilité, ce qui implique par exemple la capacité de déclencher des manœuvres agressives pour esquiver les obstacles: Les caméras et algorithmes d’intelligence artificielle conventionnels n’ont pas ces capacités. Dans ce projet, nous proposerons une solution associant de manière bio-inspirée une dynamique rapide de détection visuelle et de stabilisation. Nous intégrerons ces différents aspects dans un système neuromorphique événementiel de bout en bout. La clé de cette approche est l’optimisation des délais du système par traitement prédictif. Ceci permettra de voler indépendamment, sans aucune intervention de l’utilisateur. Notre objectif à plus long terme est de satisfaire ces besoins avec un minimum d’énergie et de fournir des solutions novatrices aux défis des algorithmes traditionnels d’IA.\n Our system is divided into 3 units to process visual inputs (ATIS) until the rotors: the Camera, Processor and Motor units. Each represents respectively multi-channel feature maps ($C_i$), an estimate of the depth-of-field ($P$) and a navigation map, for instance time-of-contacts on a polar map ($M$). Compared to a discrete-time pipeline, we will design an integrated, back-to-back event-driven system based on a fast, two-way processing between the C, P and M units. Event-driven, feed-forward and feed-back communications are denoted respectively in yellow, black and red. Notice the attention module $A$ from $P$ to $C$ and the feed-back of navigation information from $M$ and the IMU to $P$.  Abstract Autonomous aerial robots would be essential tools in search and rescue operations. But flying in complex environments requires a high level of agility, which implies the ability to initiate aggressive maneuvers to avoid obstacles: Conventional AI cameras and algorithms do not have these capabilities. In this project, we propose a solution that will integrate bio-inspired rapid visual detection and stabilization dynamics into an end-to-end event based neuromorphic system. The key to this approach will be the optimization of delays through predictive processing. This will allow these robots to fly independently, without any user intervention. Our longer-term goal is to meet the requirements with very little power and provide innovative solutions to the challenges of traditional AI algorithms.\nAcknowledgement This work was supported by ANR project “AgileNeuRobot” N° ANR-20-CE23-0021.\n","date":1607299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607299200,"objectID":"0fedfaaffaf53c3f73de8e387b81556a","permalink":"https://laurentperrinet.github.io/grant/anr-anr/","publishdate":"2020-12-07T00:00:00Z","relpermalink":"/grant/anr-anr/","section":"grant","summary":"Robots aériens agiles bio-mimetiques pour le vol en conditions réelles","tags":["grant","current-grant"],"title":"ANR AgileNeuRobot (2021/2024)","type":"grant"},{"authors":["Angelo Franciosini","Victor Boutin","Laurent U Perrinet"],"categories":null,"content":"Presenting my poster tonight at 8:00p #cosyne2020, a work developed using Sparse Deep Predictive Coding (SDPC) during my PhD @laurentperrinet @NeuroSchool_mrs pic.twitter.com/LtUEBnlPNt\n— AF (@Angelo_RDN) February 28, 2020   \n see the follow-up paper in:  Angelo Franciosini, Victor Boutin, Frédéric Chavane, Laurent U Perrinet  (2021). Pooling in a predictive model of V1 explains functional and structural diversity across species. bioRxiv.  Preprint  PDF  Cite  DOI    see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI   This is from:\nKoray Kavukcuoglu, Marc\u0026#39;Aurelio Ranzato, Rob Fergus and Yann LeCun: Learning Invariant Features through Topographic Filter Maps, Proc. International Conference on Computer Vision and Pattern Recognition (CVPR\u0026#39;09), IEEE, 2009 pic.twitter.com/4gH6L3dmaJ\n— Yann LeCun (@ylecun) April 21, 2021    ","date":1601164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601164800,"objectID":"cff46ae201941ec5dc0e778a3cd6f471","permalink":"https://laurentperrinet.github.io/publication/franciosini-20-cosyne/","publishdate":"2020-09-27T00:00:00Z","relpermalink":"/publication/franciosini-20-cosyne/","section":"publication","summary":"Cells in the primary visual cortex of mammals (V1) have historically been divided into two classes: simple and complex. Simple cells exhibit a rectified linear response to oriented visual stimuli while complex cells show various degrees of invariance with respect to the stimulus' phase (position). The existence of these two populations can be explained by hierarchical models where simple cells feed information into complex cells through a non-linear spatial pooling [1]. Nevertheless, how the brain develops this structure remains an open question. One of the most successful theories to model hierarchical processing in the brain is Predictive Coding (PC): a framework introduced by Rao \u0026 Ballard [2] that exploits feedback and feedforward connectivity to solve a Bayesian inference problem. We extended the classical PC to account for a sparse representation of the input data (natural images) and a convolutional structure to allow translation invariance. We demonstrate that this framework, called Sparse Deep Predictive Coding (SDPC) [3], can easily replicate complex-like neurons when a non-linear pooling is included between the layers. In particular, we show that a large population of complex-like neurons, showing various degrees of phase invariance, emerges in the 2nd layer of the model when the pooling function is extended to include not only neighboring spatial locations but also neighboring neurons with different tuning properties. We trained various networks on natural images (STL-10 data-set). To quantify the complex behavior of the model neurons, we used the modulation ratio F1/F0[4]: if F1/F0 ≥ 1 the cell is identified as simple-like, if F1/F0","tags":["deep-learning","sparse coding"],"title":"Modelling Complex-cells and topological structure in the visual cortex of mammals using Sparse Predictive Coding","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" What:: talk @ Séminaire à l’Institut de Recherche sur les Phénomènes Hors Équilibre (IRPHÉ) Who:: Perrinet, Laurent U Where: Marseille (France), see https://laurentperrinet.github.io/talk/2020-09-25-irphe When: 25/09/2020, time: 15:45:00-16:30:00 What:  Slides @ https://laurentperrinet.github.io/2020-09-25_IRPHE Code for slides @ https://github.com/laurentperrinet/2020-09-25_IRPHE/ Abstract: Building models which efficiently process images is a great source of inspiration to better understand the processes which underly our visual perception. I will present some classical models stemming from the Machine Learning community and propose some extensions inspired by Nature. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computations at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision (CNNs in the terminology of deep learning) take into account neurophysiological observations and provide as of today the most successful framework for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. We will here introduce a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective and we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational levels. These results from the SDPC model additionally demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.    ","date":1601048700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601048700,"objectID":"c3528a13a118955eb71bf48abb4c4a5b","permalink":"https://laurentperrinet.github.io/talk/2020-09-25-irphe/","publishdate":"2020-09-25T15:45:00Z","relpermalink":"/talk/2020-09-25-irphe/","section":"talk","summary":"Building models which efficiently process images is a great source of inspiration to better understand the processes which underly our visual perception. I will present some classical models stemming from the Machine Learning community and propose some extensions inspired by Nature. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computations at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision (CNNs in the terminology  of deep learning) take into account neurophysiological observations and provide as of today the most successful framework for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. We will here introduce a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective and we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational levels. These results from the SDPC model additionally demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.","tags":null,"title":"Understanding natural vision using deep predictive coding","type":"talk"},{"authors":["Emmanuel Daucé","Laurent Perrinet"],"categories":null,"content":" see proceedings paper:  Emmanuel Daucé, Laurent U Perrinet  (2020). Visual search as active inference. IWAI 2020.  PDF  Cite  Code  Slides  DOI   the mathematical details are described as a talk the 1st International WS on #ActiveInference #IWAI2020 at @ECMLPKDD https://t.co/4s7gHbMxiT and paper \u0026#34;Visual search as active inference\u0026#34; https://t.co/yNCOFHf7FS\n— laurentperrinet (@laurentperrinet) September 14, 2020    What:: talk @ 1st International Workshop on Active Inference (IWAI 2020) Who:: Emmanuel Daucé and Laurent Perrinet Where: Ghent (Belgium), gone virtual, see https://laurentperrinet.github.io/talk/2020-09-14-iwai When: 14/09/2020, time: 12:20:00-12:40:00 What:  Slides @ https://laurentperrinet.github.io/2020-09-14_IWAI Code for slides @ https://github.com/laurentperrinet/2020-09-14_IWAI/ Abstract: Visual search is an essential cognitive ability, offering a prototypical control problem to be addressed with Active Inference. Under a Naive Bayes assumption, the maximisation of the information gain objective is consistent with the separation of the visual sensory flow in two independent pathways, namely the “What” and the “Where” pathways. On the “What” side, the processing of the central part of the visual field (the fovea) provides the current interpretation of the scene, here the category of the target. On the “Where” side, the processing of the full visual field (at lower resolution) is expected to provide hints about future central foveal processing given the potential realisation of saccadic movements. A map of the classification accuracies, as obtained by such counterfactual saccades, defines a utility function on the motor space, whose maximal argument prescribes the next saccade. The comparison of the foveal and the peripheral predictions finally forms an estimate of the future information gain, providing a simple and resource-efficient way to implement information gain seeking policies in active vision. This dual-pathway information processing framework is found efficient on a synthetic visual search task and we show here quantitatively the role of the precision encoded within the accuracy map. More importantly, it is expected to draw connections toward a more general actor-critic principle in action selection, with the accuracy of the central processing taking the role of a value (or intrinsic reward) of the previous saccade.    ","date":1600106400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600106400,"objectID":"6577bd5a5bb95b187787cb3e360308c5","permalink":"https://laurentperrinet.github.io/talk/2020-09-14-iwai/","publishdate":"2020-09-14T18:00:00Z","relpermalink":"/talk/2020-09-14-iwai/","section":"talk","summary":"Visual search is an essential cognitive ability, offering a prototypical control problem to be addressed with Active Inference. Under a Naive Bayes assumption, the maximization of the information gain objective is consistent with the separation of the visual sensory flow in two independent pathways, namely the \\\"What\\\" and the \\\"Where\\\" pathways. On the \\\"What\\\" side, the processing of the central part of the visual field (the fovea) provides the current interpretation of the scene, here the category of the target. On the \\\"Where\\\" side, the processing of the full visual field (at lower resolution) is expected to provide hints about future central foveal processing given the potential realization of saccadic movements. A map of the classification accuracies, as obtained by such counterfactual saccades, defines a utility function on the motor space, whose maximal argument prescribes the next saccade. The comparison of the foveal and the peripheral predictions finally forms an estimate of the future information gain, providing a simple and resource-efficient way to implement information gain seeking policies in active vision. This dual-pathway information processing framework is found efficient on a synthetic visual search task and we show here quantitatively the role of the precision encoded within the accuracy map. More importantly, it is expected to draw connections toward a more general actor-critic principle in action selection, with the accuracy of the central processing taking the role of a value (or intrinsic reward) of the previous saccade.","tags":null,"title":"Visual search as active inference","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"During a seminar at the Institute of Neurosciences Timone in Marseille, Thomas Serre will present his recent work on “Feedforward and feedback processes in visual recognition”:\n Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching – and sometimes even surpassing – human accuracy on a variety of visual recognition tasks. In this talk, however, I will show that these neural networks and their recent extensions exhibit a limited ability to solve seemingly simple visual reasoning problems involving incremental grouping, similarity, and spatial relation judgments. Our group has developed a recurrent network model of classical and extra-classical receptive fields that is constrained by the anatomy and physiology of the visual cortex. The model was shown to account for diverse visual illusions providing computational evidence for a novel canonical circuit that is shared across visual modalities. I will show that this computational neuroscience model can be turned into a modern end-to-end trainable deep recurrent network architecture that addresses some of the shortcomings exhibited by state-of-the-art feedforward networks for solving complex visual reasoning tasks. This suggests that neuroscience may contribute powerful new ideas and approaches to computer science and artificial intelligence.\n  Dr. Thomas Serre is an Associate Professor in Cognitive Linguistic \u0026amp; Psychological Sciences and an affiliate of the Carney Institute for Brain Science at Brown University. He received a Ph.D. in Neuroscience from MIT in 2006 and an MSc in EECS from Télécom Bretagne (France) in 2000. His research seeks to understand the neural computations supporting visual perception and has been featured in the BBC series “Visions from the Future” and other news articles (The Economist, New Scientist, Scientific American, IEEE Computing in Science and Technology, Technology Review and Slashdot). Dr. Serre is the Faculty Director of the Center for Computation and Visualization and the Associate Director of the Initiative for Computation in Brain and Mind at Brown University. He also holds an International Chair in AI within the Artificial and Natural Intelligence Toulouse Institute (France). Dr. Serre has served as an area chair and a senior program committee member for top-tier machine learning and computer vision conferences including AAAI, CVPR, and NeurIPS. He is currently serving as a domain expert for IARPA’s Machine Intelligence from Cortical Networks (MICrONS) program and as a scientific advisor for Vium, Inc. He was the recipient of an NSF Early Career Award as well as DARPA’s Young Faculty Award and Director’s Award.   ","date":1599832800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599199200,"objectID":"04939deaa9b1ecdc41af376401382d9e","permalink":"https://laurentperrinet.github.io/post/2020-09-11_seminaire-thomas-serre/","publishdate":"2020-09-11T14:00:00Z","relpermalink":"/post/2020-09-11_seminaire-thomas-serre/","section":"post","summary":"A seminar by Thomas Serre at the Institute of Neurosciences Timone in Marseille.","tags":["events"],"title":"2020-09-11 : CONECT seminar - \"Feedforward and feedback processes in visual recognition\" (T Serre)","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" THE POSITION HAS BEEN FILLED.   Dear colleagues,\nApplications are welcome for a fully funded doctoral position at INT in Marseille, France. Your mission will be to build ultra-fast vision algorithms using event-based cameras and spiking neural networks. The project is funded by the APROVIS3D grant (ANR-19-CHR3-0008-03) and will be coordinated by Laurent Perrinet. The work will be carried out in collaboration with a leading computer science institute at Université Côte d’Azur (Sophia Antipolis, France), the Laboratoire d’Informatique, Signaux et Systèmes de Sophia-Antipolis (I3S, UMR7271 - UNS CNRS), that will be part of the supervision team. We are seeking candidates with a strong background in machine learning, computer vision and computational neuroscience.\nTo obtain further information, please visit https://laurentperrinet.github.io/post/2020-06-30_phd-position or contact me @ Laurent.Perrinet@univ-amu.fr. To candidate, follow instructions on the dedicated server from the CNRS.\nThe starting date is set to October 1st, 2020 and the appointment is for 36 month. Applications are welcome immediately.\nThanks for distributing this announcement to potential candidates!\nCD Doctorant \u0026#34;Vision ultra-rapide utilisant des Réseaux de neurones impulsionnels\u0026#34; H/F (MARSEILLE) (MARSEILLE 05) https://t.co/I5CXWxR3zi #Emploi #OffreEmploi #Recrutement\n— EmploiCNRS (@EmploiCNRS) June 30, 2020  Detailed description: “Ultra-fast vision using Spiking Neural Networks” Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. Crucially, given an equal constraint on energy consumption, these algorithms are relatively slow compared to biological vision. It is believed that one major factor of this rapidity is the fact that visual information is represented by short pulses (spikes) at analog – not discrete – times (Paugam and Bohte, 2012). However, most classical computer vision algorithms rely on such frame-based approaches. One solution to overcome their limitations is to use event-based representations, but these still lack in practice, and their high potential is largely underexploited. Inspired by biology, the project addresses the scientific question of developing a low-power sensing architecture for the processing of visual scenes, able to function on analog devices without a central clock and aimed at being validated in real-life situations. More specifically, the project will develop new paradigms for biologically inspired computer vision (Cristobal, Keil and Perrinet, 2015), from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes.\nIn this doctoral project, we propose to address major limitations of classical computer vision by implementing specific dynamical features of cortical circuits: spiking neural networks (Perrinet, Thorpe and Samuelides, 2004; Lagorce et al., 2018), lateral diffusion of neural information (Chavane et al., 2011; Muller et al., 2018) and dynamic neuronal association fields (Frégnac et al., 2012; Frégnac et al., 2016; Gerard-Mercier et al., 2016). One starting point is to use event-based cameras (Dupeyroux et al., 2018) and to extend results of self-supervised learning that we have obtained on static, natural images (Boutin et al., 2020) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the “association field” described at the psychophysical (Field et al., 1993), spiking (Li and Gilbert, 2002) and synaptic (Gerard-Mercier et al., 2016) levels. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (Voges and Perrinet, 2012). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). It is not well understood, but probably decisive for ultra-fast vision, how recurrent cortico-cortical loops add a level of distributed top-down complexity in the feed-forward stream of information which participates to the ultra-fast integration of sensory input and perceptual context (Keller et al., 2019). Coupled with the dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for defining ultra-fast vision algorithms.\nExpected profile of the candidate Candidates should have experience in the …","date":1593507600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593507600,"objectID":"0ad2dfb60790794e7c4b5f42cdc483ed","permalink":"https://laurentperrinet.github.io/post/2020-06-30_phd-position/","publishdate":"2020-06-30T09:00:00Z","relpermalink":"/post/2020-06-30_phd-position/","section":"post","summary":"THE POSITION HAS BEEN FILLED. Offre de thèse \"Vision ultra-rapide par réseaux de neurones impulsionnels\" à Marseille","tags":["events"],"title":"PhD offer \"Ultra-fast vision using Spiking Neural Networks\"","type":"post"},{"authors":["Emmanuel Daucé","Pierre Albigès","Laurent U Perrinet"],"categories":null,"content":"    for a more mathematical treatment, see  Emmanuel Daucé, Laurent U Perrinet  (2020). Visual search as active inference. IWAI 2020.  PDF  Cite  Code  Slides  DOI   New @ARVOJOV : \u0026#34;A dual foveal-peripheral visual processing model implements efficient saccade selection\u0026#34; https://t.co/JqnpBM5bcd comes with code @ https://t.co/5MoIh00Bb8 #OpenAccess #visionscience\n— laurentperrinet (@laurentperrinet) September 14, 2020    ","date":1591315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591315200,"objectID":"090e6ee26df5708b2ea1882c377f7817","permalink":"https://laurentperrinet.github.io/publication/dauce-20/","publishdate":"2020-06-05T00:00:00Z","relpermalink":"/publication/dauce-20/","section":"publication","summary":"In computer vision, the visual search task consists in extracting a scarce and specific visual information (the target) from a large and crowded visual display. This task is usually implemented by scanning the different possible target identities at all possible spatial positions, hence with strong computational load. The human visual system employs a different strategy, combining a foveated sensor with the capacity to rapidly move the center of fixation using saccades. Saccade-based visual exploration can be idealized as an inference process, assuming that the target position and category are independently drawn from a common generative process. Knowing that process, visual processing is then separated in two specialized pathways, the where pathway mainly conveying information about target position in peripheral space, and the what pathway mainly conveying information about the category of the target. We consider here a dual neural network architecture learning independently where to look and then at what to see. This allows in particular to infer target position in retinotopic coordinates, independently to its category. This framework was tested on a simple task of finding digits in a large, cluttered image. Simulation results demonstrate the benefit of specifically learning where to look before actually knowing the target category. The approach is also energy-efficient as it includes the strong compression rate performed at the sensor level, by retina and V1 encoding, which is preserved up to the action selection level, highlighting the advantages of bio-mimetic strategies with regards to traditional computer vision when computing resources are at stake.","tags":["Active Inference","Deep-Learning","Object localization","Visual search","Visuomotor control"],"title":"A dual foveal-peripheral visual processing model implements efficient saccade selection","type":"publication"},{"authors":["Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":"A fundamental goal of systems neuroscience is to describe how sensory inputs are integrated and guide an animal’s behavior. To be able to integrate these inputs, early sensory systems have developed selectivities for specific stimulus features that allow them to analyze the inputs using these features as basis. We aim to uncover how disparate motion signals are integrated to produce a global percept of motion, and to understand the conditions in which such integration fails. Our proposal reflects the fact that adaptive behaviors in complex environments face numerous challenges, from processing noisy and uncertain visual motion information to predict future events on target trajectory contingencies and its interactions with a dynamic, cluttered environment. We propose to use dynamic inference as an efficient theoretical framework to understand how the brain integrates Prior knowledges elaborated from statistical regularities of natural environments with different sources of information across different time scales in order to extract relevant motion information from the sensory flow and predict future events or actions. The smooth pursuit system is an excellent probe of such hierarchical dynamical inferences from target motion computation to target trajectory prediction. In marmosets, we have access to populations of neurons in pivotal cortical areas along the occipito-parieto- frontal network that have been identified in non-human and human primates. We seek to uncover a unifying empirical and theoretical framework to capture inference across different time scales.\n With Guilhem Ibos, Guillaume Masson \u0026amp; Nicholas Priebe.  Aim 3, modelling behavioural and neuronal data within the active inference framework  Type de contrat : CRCNS US-French Research Proposal Durée: 4 ans, à partir du 1er novembre 2020 Budget total (partenaire français): 341 k€ to be recruited: Post-doctoral fellow: A post-post-doctoral fellow in computational neuroscience will be recruited. With a 2-5 years experience, salary cost is of 52K€/year, for 2 years (total: 104K€). Coordinateur Scientifique : MONTAGNINI, Anna \u0026amp; PERRINET Laurent (UMR7289) Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE Responsable Scientifique INT : MASSON Guillaume (UMR7289)  Acknowledgement This work was supported by ANR project “PRIOSENS” N° ANR-20-NEUC-0002.\n","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"d93c04e50939688a8d34c33669cf32f0","permalink":"https://laurentperrinet.github.io/grant/anr-priosens/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/grant/anr-priosens/","section":"grant","summary":"Integration sensory and prior information to control behavior (CRCNS US-French Research Proposal)","tags":["grant","current-grant"],"title":"ANR PRIOSENS (2021/2024)","type":"grant"},{"authors":["Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":"The natural visual environments in which we have evolved have shaped and constrained the neural mechanisms of vision. Rapid progress has been made in recent years in understanding how the retina and visual cortex are specifically adapted to processing natural scenes.1–3 However, studies in this research tradition have mainly addressed the processing of natural images in the spatial domain. Although the processing of temporal properties of visual stimuli is just as important as spatial properties, stimuli with naturalistically valid temporal dynamics have not been sufficiently investigated. Although objects and creatures we view undergo a variety of intrinsic movements, probably the most common motions on the retina are image shifts due to our own eye movements: in free viewing in humans, ocular saccades occur about three times every second, shifting the retinal image at speeds of 100-500 degrees of visual angle per second.4 How these very fast shifts are suppressed, leading to clear, accurate and stable representations of the visual scene is an fundamental unsolved problem in visual neuroscience known as saccadic suppression. One reason why this problem is difficult is technological: to make progress we need to visually simulate these fast retinal shifts, but computer displays have been too slow to produce adequate simulations.\nIn this project we propose a unique convergence between neurophysiology, modeling and psychophysics, aided by recent technological developments. Some of the partners have been at the forefront of recent developments that have led to a realization that moving stimuli lead to traveling waves of activity in primary visual cortex, propagating at speeds similar to those produced by saccades. Other partners have developed detailed models of the retina and primary visual cortex based on multielectrode recordings from the retina and optical imaging of the cortex that have been able to account for these wave phenomena. Finally, another partner recently made psychophysical observations—aided by new, ultrafast computer displays that allow us to realistically simulate saccadic dynamics on a static retina—that show how image dynamics alone can account for saccadic suppression phenomena.\nWe expect that the convergence of these three research currents and methodologies will lead to rapid progress in understanding how the visual system is adapted to naturalistic dynamics. The psychophysical observations will provide new leads and targets for the neurophysiology and modeling, which in turn may provide detailed neural explanations for the psychophysics. Our main hypothesis is that the neural architectures that have been uncovered in the retina and the primary visual cortex will be revealed as most effective when processing naturalistic, fast stimuli that arise as the consequence of eye movements.\ncarte d’identité du projet  Durée: 4 ans, à partir du 1er avril 2021 Budget total (partenaire français): 665 k€ Coordinateur Scientifique : Mark WEXLER (CNRS‐INCC) Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE Responsable Scientifique INT : Frédéric CHAVANE (UMR7289)  Acknowledgement This work was supported by ANR project “ShootingStar” N° ANR-XX-XXX-XXXX.\n","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"35a2622585f5b219cc9942bdf67378b0","permalink":"https://laurentperrinet.github.io/grant/anr-shootingstar/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/grant/anr-shootingstar/","section":"grant","summary":"Processing of naturalistic motion in early vision","tags":["grant","current-grant"],"title":"ANR ShootingStar (2021/2024)","type":"grant"},{"authors":null,"categories":null,"content":"","date":1586044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586044800,"objectID":"d65ebea8c3c30617541a610167bcfb75","permalink":"https://laurentperrinet.github.io/project/courses/","publishdate":"2020-04-05T00:00:00Z","relpermalink":"/project/courses/","section":"project","summary":"Liste de cours et tutoriels.","tags":["research-interests"],"title":"Cours et tutoriels","type":"project"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"2020-04_UE-neurosciences-computationnelles, matériel pour le cours de modélisation  Où: Marseille (France) Quoi: Master Neurosciences et Sciences Cognitives But de ce travail: lire un article scientifique, pouvoir le reproduire avec des simulations d’un neurone et afin d’améliorer sa compréhension. Modalités: les étudiants s’organisent seuls, en binome ou en trinome pour fournir un mémoire sous forme de notebook complété à partir du modèle qui est fourni. Suivez les balises TODO dans le notebook pour vous guider dans cette rédaction. Les commentaires doivent être fait en français (ou en anglais si nécessaire) dans le notebook (n’oubliez-pas de sauver vos changements) et envoyé par e-mail à mailto:laurent.perrinet@univ-amu.fr une fois votre travail fini (de préférence avant le 31 avri). Outils nécessaires: Jupyter, avec numpy et matplotlib. Ce sont des outils standard et qui sont facilement installables sur toute plateforme. Si vous avez des problèmes, me joindre par e-mail ou sur le forum 👇  ","date":1585929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585929600,"objectID":"27f396d72dc33cc1d08a146c01713680","permalink":"https://laurentperrinet.github.io/talk/2020-04-ue-neurosciences-computationnelles/","publishdate":"2020-04-03T16:00:00Z","relpermalink":"/talk/2020-04-ue-neurosciences-computationnelles/","section":"talk","summary":"Visual areas are essential in transforming the raw luminous signal into a representation which efficiently conveys information about the environment. This process is constrained by various factors such as a wide variety of changes in the characteristics of the visual image but also by the necessity to be able to respond as quickly as possible to the incoming sensory stream, for instance to drive a movement of the eyes to the location of a potential danger. To achieve this, it is believed that the visual system takes advantage of the existence of a priori knowledge in the structure of visual information, such as the regularity in the shape and motion of visual objects. We will review different models around the predictive coding coding framework to offer a unified theory to explain many of the mechanisms at the different levels of the visual system and which were unveiled by decades of study in neurophysiology and psychophysics.","tags":null,"title":"From the retina to action: Understanding visual processing","type":"talk"},{"authors":["Angelo Franciosini","Victor Boutin","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1585526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585526400,"objectID":"9ec63b346d330dc70a1bf245c0ad901e","permalink":"https://laurentperrinet.github.io/publication/franciosini-20-sigma/","publishdate":"2020-03-30T00:00:00Z","relpermalink":"/publication/franciosini-20-sigma/","section":"publication","summary":"Cells in the primary visual cortex of mammals (V1) have historically been divided into two classes: simple and complex. Simple cells exhibit a rectified linear response to oriented visual stimuli while complex cells show various degrees of invariance with respect to the stimulus' phase (position). The existence of these two populations can be explained by hierarchical models where simple cells feed information into complex cells through a non-linear spatial pooling [1]. Nevertheless, how the brain develops this structure remains an open question. One of the most successful theories to model hierarchical processing in the brain is Predictive Coding (PC): a framework introduced by Rao \u0026 Ballard [2] that exploits feedback and feedforward connectivity to solve a Bayesian inference problem. We extended the classical PC to account for a sparse representation of the input data (natural images) and a convolutional structure to allow translation invariance. We demonstrate that this framework, called Sparse Deep Predictive Coding (SDPC) [3], can easily replicate complex-like neurons when a non-linear pooling is included between the layers. In particular, we show that a large population of complex-like neurons, showing various degrees of phase invariance, emerges in the 2nd layer of the model when the pooling function is extended to include not only neighboring spatial locations but also neighboring neurons with different tuning properties. We trained various networks on natural images (STL-10 data-set). To quantify the complex behavior of the model neurons, we used the modulation ratio F1/F0[4]: if F1/F0 ≥ 1 the cell is identified as simple-like, if F1/F0","tags":["deep-learning","sparse coding"],"title":"Modelling Complex-cells and topological structure in the visual cortex of mammals using Sparse Predictive Coding","type":"publication"},{"authors":["Laurent U Perrinet","Victor Boutin"],"categories":null,"content":"Intéressés par le \u0026#34;Sparse deep predictive coding\u0026#34; / \u0026#34;codage hiérarchique, épars et prédictif\u0026#34; ? Victor Boutin (Equipe NeOpTo) soutiendra sa thèse de doctorat intitulée Vendredi 13 mars à 14h https://t.co/BIrciyiRUf\n🤝 @univamu @CNRS @regionpaca @CNRS_dr12 #INT\n@FranckRUFFIER\n— laurentperrinet (@laurentperrinet) March 4, 2020    Date : Vendredi 13 mars à 14h\n  Lieu: salle Henri Gastaut, au rez de chaussée de l’INT (how to get there). La thèse était suivie d’un pot au R+4 de l’Institut de Neurosciences de la Timone (how to get there)\n  Quoi: le manuscrit est disponible sur http://www.theses.fr/2020AIXM0028\n  Jury  Ryad Benosman, Université Pierre et Marie Curie, Rapporteur Simon Thorpe, CNRS, Rapporteur Sandrine Anthoine, CNRS, Examinateur Yves Frégnac, CNRS, Examinateur Sid Kouider, CNRS, Examinateur Laurent Perrinet, CNRS, Directeur de thèse Franck Ruffier, CNRS, Co-directeur de thèse Mossadek Talby, AMU, Jury invité  Abstract Building models to efficiently represent images is a central and difficult problem in the machine learning community. The neuroscientific study of the early visual cortical areas is a great source of inspiration to find economical and robust solutions. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computation at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision have accounted for neurophysiological evidence and provide the most successful frameworks for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. The presented thesis introduces a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective. In terms of computation, the recurrent connectivity introduced by the PC framework allows the SDPC to converge to lower prediction errors with a higher convergence rate. In addition, we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational level. At the neural organization level, the feedback signal of the model accounted for a reorganization of the V1 association fields that promotes contour integration. At the representational level, the SDPC exhibited significant denoising ability which is highly correlated with the strength of the feedback from V2 to V1. These results from the SDPC model demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.\nRésumé La représentation concise et efficace de l’information est un problème qui occupe une place centrale dans l’apprentissage machine. Le cerveau, et plus particulièrement le cortex visuel, ont depuis longtemps trouvé des solutions performantes et robustes afin de résoudre un tel problème. A l’échelle locale, le codage épars est l’un des mécanismes les plus prometteurs pour modéliser le traitement de l’information au sein des populations de neurones dans le cortex visuel. Le codage épars introduit une compétition entre les neurones afin de décrire un stimulus visuel en limitant le nombre de neurones actifs. A l’échelle structurelle, les modèles dits ascendants décrivent le cortex visuel comme une succession d’unités de traitement dans lesquelles l’information se propage de la rétine vers les couches profondes du cortex. Ces modèles ont expliqué avec succès un grand nombre de phénomènes neuro-physiologiques et ont servi d’inspiration afin de construire des algorithmes de reconnaissance d’objets extrêmement performants. Néanmoins, les modèles ascendants n’expliquent pas le grand nombre de connections récurrentes et descendantes que l’on trouve dans le cortex visuel. Ces connections sont connues pour moduler l’activité des neurones en incluant des details contextuels au flux d’information ascendant. La théorie du codage prédictif a été suggérée pour modéliser les connections ascendantes, récurrentes, et descendantes que l’on retrouve entre les différentes régions corticales. Cette thèse propose de combiner codage épars et codage prédictif au sein d’un modèle hiérarchique et convolutif. Nous avons entrainé ce modèle sur différentes bases de données afin de l’analyser avec une perspective à …","date":1583330400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583312400,"objectID":"06d55ed43377c2a1448b64d7fa3bf91d","permalink":"https://laurentperrinet.github.io/post/2020-03-13_soutenance-victor-boutin/","publishdate":"2020-03-04T14:00:00Z","relpermalink":"/post/2020-03-13_soutenance-victor-boutin/","section":"post","summary":"Victor Boutin (Equipe NeOpTo) a soutenu sa thèse de doctorat intitulée: *Sparse deep predictive coding: a bio-inspired model of visual perception* / **Etude d’un algorithme hiérarchique et codage épars et prédictif : vers un modèle bio-inspiré de la perception visuelle** le Vendredi 13 mars à 14h","tags":["events"],"title":"2020-03-13: Soutenance Victor Boutin","type":"post"},{"authors":["Giacomo Benvenuti","Sandrine Chemla","Arjan Boonman","Laurent U Perrinet","Guillaume S Masson","Frederic Chavane"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"026073a51fc01a13bfac9f36c42c385d","permalink":"https://laurentperrinet.github.io/publication/benvenuti-20/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/benvenuti-20/","section":"publication","summary":"What are the neural mechanisms underlying motion integration of translating objects? Visual motion integration is generally conceived of as a feedforward, hierarchical, information processing. However, feedforward models fail to account for many contextual effects revealed using natural moving stimuli. In particular, a translating object evokes a sequence of transient feedforward responses in the primary visual cortex but also propagations of activity through horizontal and feedback pathways. We investigated how these pathways shape the representation of a translating bar in monkey V1. We show that, for long trajectories, spiking activity builds-up hundreds of milliseconds before the bar enters the neurons receptive fields. Using VSDI and LFP recordings guided by a phenomenological model of propagation dynamics, we demonstrate that this anticipatory response arises from the interplay between horizontal and feedback networks driving V1 neurons well ahead of their feedforward inputs. This mechanism could subtend several perceptual contextual effects observed with translating objects.","tags":null,"title":"Anticipatory Responses along Motion Trajectories in Awake Monkey Area V1","type":"publication"},{"authors":["Victor Boutin","Angelo Franciosini","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":"What is the effect of top-down connections in Hierarchical Sparse Coding? This work by @VictorBoutin + Angelo Franciosini @RaguDellaNonna Franck Ruffier and myself proposes to leverage this problem using predictive coding. Read more in Neural Computation: https://t.co/g0Sig7uDMq pic.twitter.com/fiJxlJ7tNG\n— laurentperrinet (@laurentperrinet) November 3, 2020   get the code @ https://github.com/VictorBoutin/SPC_2L see a related work describing SDPC in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580774400,"objectID":"338ab318fd2c2d5c36294d396d3783f6","permalink":"https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-20-feedback/","publishdate":"2020-02-04T00:00:00Z","relpermalink":"/publication/boutin-franciosini-ruffier-perrinet-20-feedback/","section":"publication","summary":"Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the 2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks are trained on 4 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we show that the 2L-SPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the 2L-SPC features are more generic and informative.","tags":["deep-learning","sparse coding"],"title":"Effect of top-down connections in Hierarchical Sparse Coding","type":"publication"},{"authors":["Chloé Pasturel","Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":"“Humans adapt their anticipatory eye movements to the volatility of visual motion properties”   check out our new paper @PLOSCompBiol : \u0026#34;Humans adapt their anticipatory eye movements to the volatility of visual motion properties\u0026#34; with Chloé Pasturel and @MontagniniAnna - talks about how to perform optimal decisions when the environment abruptly switches its statistics... pic.twitter.com/GKg87lGqdS\n— laurentperrinet (@laurentperrinet) April 24, 2020  At what point should we become alarmed? When faced with changes in the environment, the sensory system provides an effective response. The current health situation has shown us how abruptly our environment can change from one state to another, tragically illustrating the volatility we can face. To understand this notion of volatility, let’s take the case of a doctor who, among the patients he receives, usually diagnoses one out of ten cases of flu. Suddenly, he gets 5 out of 10 patients who test positive. Is this an unfortunate coincidence or are we now sure that there is a switch to a flu episode? Recent events have shown us how difficult it is to make a rational decision in times of uncertainty, and in particular to decide when to act. However, mathematical solutions exist that adapt our behavior by optimally combining the information explored recently with that exploited in the past. In an article published in PLoS Computational Biology, Pasturel, Montagnini and Perrinet show that our brain responds to changes in the sensory environment in the same way as this mathematical model.   By manipulating the probability bias of the presentation of a visual target on a screen, this experiment manipulates the volatility of the environment in a controlled way by introducing switches in the probability bias. These switches randomly change the bias among different degrees of probability (both left and right). At each trial, the bias then generates a realization, either left (L) or right (R). The target moves in blocks of 50 trials (1 to 50) and these realizations are the only ones to be observed, the evolution of the bias and its shifts remaining hidden from the observer. Compared to the floating average that is conventionally used, a mathematical model can be deduced as a predictive average that allows to better follow the dynamics of the probability bias. Thanks to psychophysical experiments, we have shown that observers preferentially follow the predictive mean, rather than the floating mean, both in explicit judgements (predictive betting) and, more surprisingly, in the anticipatory movements of the eyes that are carried out without the observers being aware of them.  These theoretical and experimental results show that in this realistic situation in which the context changes at random moments throughout the experiment, our sensory system adapts to volatility in an adaptive manner over the course of the trials. In particular, the experiments show in two behavioural experiments that humans adapt to volatility at the early sensorimotor level, through their anticipatory eye movements, but also at a higher cognitive level, through explicit evaluations. These results thus suggest that humans (and future artificial systems) can use much richer adaptation strategies than previously assumed. They provide a better understanding of how humans adapt to changing environments in order to make judgements or plan responses based on information that varies over time.\n read the preprint (the official online publication or in PDF is wrongly typeset by interverting figures 2 \u0026amp; 3 - the poilicy of the journal is to issue a correction, but not to correct it.) get a ) Abstract HTML PDF supplementary info : https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020/blob/master/Pasturel_etal2020_PLoS-CB_SI.pdf Communiqué de presse INSB-CNRS (en français) code for paper: https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020 code for framework: https://github.com/chloepasturel/AnticipatorySPEM code for the Bayesian model: https://github.com/laurentperrinet/bayesianchangepoint code for figures Figure 1, Figure 2, Figure 3, Figure 4, Figure 5 video abstract (and the code for generating the video abstract) Notre papier avec Chloé Pasturel et @MontagniniAnna figure dans les faits marquants 2020 de la Société des Neurosciences! Voir aussi https://lejournal.cnrs.fr/nos-blogs/aux-frontieres-du-cerveau/les-faits-marquants-2020-de-la-societe-de-neurosciences : Notre papier avec Chloé Pasturel et @MontagniniAnna figure dans les faits marquants 2020 de la Société des Neurosciences! https://t.co/w825oTz1o5#neuroscience #actu #sciences @SocNeuro_Tweets@CNRS @univamu https://t.co/KeKh5MNWu2 https://t.co/eojTqsp6gD\n— laurentperrinet (@laurentperrinet) March 15, 2021    ","date":1579996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579996800,"objectID":"b4685fdaab1feb369c0aeda4e477a0a3","permalink":"https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/","publishdate":"2020-01-26T00:00:00Z","relpermalink":"/publication/pasturel-montagnini-perrinet-20/","section":"publication","summary":"Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high*resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye*to*target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate \\\"how much they are confident that the target will move to the right or left in the next trial\\\"  and to adjust the cursor's position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random*direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch*up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile).","tags":["motion anticipation"],"title":"Humans adapt their anticipatory eye movements to the volatility of visual motion properties","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Watch “ÇA TOURNE” on #Vimeo https://t.co/nzUAEwjTeD (english subs) réalisé par Camille Goujon et par les élèves du Lycée Professionnel Domaine Eguille, Vedène (France) - https://t.co/EhVz0YZdPs #StopMotion #vision #neuroscience #outreach\n— laurentperrinet (@laurentperrinet) July 19, 2020     ÇA TOURNE a été sélectionné pour participer à la compétition du « Alexandre Trauner ART/Film Festival » (Szolnok, Hongrie) : http://www.ataff.hu/ visible aux Soirée Courts Métrages Ciné Rencontre de la Ville de Berre l’Étang https://www.berreletang.fr/soiree-courts-metrages?periode=2021-04-30%2017%3A23%3A26 ÇA TOURNE de Camille Goujon, a été sélectionné au 27ème Festival national du film d’animation de Rennes Métropole, dans la catégorie Autoproductions du 7 au 11 octobre 2021 http://festival-film-animation.fr/ le court-métrage a été sélectionné pour participer au « Happy Valley Animation Festival » (Pennsylvanie, USA) : https://happyvalleyanimationfestival.org/ Le film “ÇA TOURNE” a été sélectionné pour faire partie de la compétition catégorie «FILMS SCOLAIRES\u0026#34; diffusée du 4 au 7 novembre 2020 dans le cadre du festival “7ème Art Jeunes Talent! : http://www.festivaltournezjeunesse.com Dans le cadre d’un projet Région (APERLA) les élèves de seconde Bac Pro Menuisiers agenceurs ont conçu ce film sous la direction de leur professeur Mme Bomont et sous la direction artistique de Camille Goujon, artiste et cinéaste d’animation https://www.domaine-eguilles.fr/realisation-collective-de-lyceens-sous-la-direction-artistique-de-camille-goujon-artiste-et-cineaste-d-animation Ce court métrage fait partie des 7 films réalisés dans le cadre des « Ateliers de réalisation Cinésciences » proposés par l’association Polly Maggoo http://festivalrisc.org/films-dateliers/ ref sur http://www.lussasdoc.org/film-ca_tourne-1,53288.html Le texte de cette présentation est reprise dans cet article de The Conversation (lien direct). Voir la @ présentation au NeuroStories sur un thème similaire  ","date":1579514400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579514400,"objectID":"51af06ae3b04ae3b4df4ac34e1ae9ea7","permalink":"https://laurentperrinet.github.io/talk/2020-01-20-atelier-sciences-cinema/","publishdate":"2020-01-20T10:00:00Z","relpermalink":"/talk/2020-01-20-atelier-sciences-cinema/","section":"talk","summary":"Les illusions visuelles sont des créations d'artistes, de scientifiques et plus récemment, grâce aux réseaux sociaux, du grand public qui proposent des situations souvent incongrues, dans lesquelles l'eau remonte une cascade, les personnes volent dans les airs ou des serpents se mettent à tourner. Au-delà de leur indéniable coté ludique, ces illusions nous apprennent beaucoup sur le fonctionnement du cerveau, notamment quand celles-ci se transforment en hallucinations visuelles, dépassant ainsi les limites des capacités de notre perception. En tant que chercheur en Neurosciences à l'Institut de Neurosciences de la Timone à Marseille, je vous dévoilerai des aspects du fonctionnement du cerveau qui sont souvent méconnus. En particulier, nous verrons pourquoi un magicien peut tromper nos sens ou comment des objets peuvent voyager dans le temps. Surtout nous essaierons de comprendre le fonctionnement de notre perception visuelle sur les bases d'une théorie de la vision non pas comme une simple caméra qui enregistre des images mais comme un processus actif en relation avec le monde qui nous entoure.","tags":null,"title":"Des illusions aux hallucinations visuelles: une porte sur la perception","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Find the text at https://laurentperrinet.github.io/Perrinet20PredictiveProcessing/ The source code of the text is available at https://github.com/laurentperrinet/Perrinet20PredictiveProcessing This chapter is available as part of the book “The Philosophy and Science of Predictive Processing” : List of Contributors : Preface: The Brain as a Prediction Machine, Anil Seth Introduction, Dina Mendonça, Manuel Curado \u0026amp; Steven S. Gouveia Part I: Predictive Processing: Philosophical Approaches   Predictive Processing and Representation: How Less Can Be More, Erik Myin and Thomas van Es A Humean Challenge to Predictive Coding, Colin Klein Are Markov Blankets Real and Does it Matter?, Richard Menary and Alexander J. Gillett Predictive Processing and Metaphysical Views of the Self, Robert Clowes and Klaus Gärtner   Part II: Predictive Processing: Cognitive Science and Neuroscientific Approaches  From the Retina to Action: Dynamics of Predictive Processing in the Visual System, Laurent Perrinet Predictive Processing and Consciousness: Prediction Fallacy and its Spatiotemporal Resolution, Steven S. Gouveia The Many Faces of Attention: Why Precision Optimization is not Attention, Sina Fazelpour and Madeleine Ransom Predictive Processing: Does it Compute?, Chris Thornton   Part III: Predictive Processing: Mental Health  The Predictive Brain, Conscious Experience and Brain-related Conditions, Lisa Feldman Barrett and Lorena Chanes Disconnection and Diaschisis: Active Inference in Neuropsychology, Thomas Parr and Karl Friston The Phenomenology and Predictive Processing of Time in Depression, Zachariah Neemeh and Shaun Gallagher Why Use Predictive Processing to Explain Psychopathology? The Case of Anorexia Nervosa, Jakob Hohwy and Stephen Gadsby   Afterword, Manuel Curado  ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"7a61ba997c5aff4fa948be08c818e4d2","permalink":"https://laurentperrinet.github.io/publication/perrinet-20/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/perrinet-20/","section":"publication","summary":"Within the central nervous system, visual areas are essential in transforming the raw luminous signal into a representation which efficiently conveys information about the environment. This process is constrained by the necessity of being robust and rapid. Indeed, there exists both a wide variety of potential changes in the geometrical characteristics of the visual scene and also a necessity to be able to respond as quickly as possible to the incoming sensory stream, for instance to drive a movement of the eyes to the location of a potential danger. Decades of study in neurophysiology and psychophysics at the different levels of vision have shown that this system takes advantage of a priori knowledge about the structure of visual information, such as the regularity in the shape and motion of visual objects. As such, the predictive processing framework offers a unified theory to explain a variety of visual mechanisms. However, we still lack a global normative approach unifying those mechanisms and we will review here some recent and promising approaches. First, we will describe Active Inference, a form of predictive processing equipped with the ability to actively sample the visual space. Then, we will extend this paradigm to the case where information is distributed on a topography, such as is the case for retinotopically organized visual areas. In particular, we will compare such models in light of recent neurophysiological data showing the role of traveling waves in shaping visual processing. Finally, we will propose some lines of research to understand how these functional models may be implemented at the neural level. In particular, we will review potential models of cortical processing in terms of prototypical micro-circuits. These allow to separate the different flows of information, from feed-forward prediction error to feed-back anticipation error. Still, the design of such a generic predictive processing circuit is still not fully understood and we will enumerate some possible implementations using biomimetic neural networks.","tags":["active inference","area-v1","Bayesian model","psychophysics"],"title":"From the retina to action: Dynamics of predictive processing in the visual system","type":"publication"},{"authors":["Hugo Ladret","Laurent U Perrinet"],"categories":null,"content":" See also Ladret and Perrinet, 2019  ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"6950764f0dfb43c25ee01b33327ba3e0","permalink":"https://laurentperrinet.github.io/publication/ladret-20-aes/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/ladret-20-aes/","section":"publication","summary":"*Background*: The primary visual cortex (V1) is a key component of the visual system that builds some of the first levels of coherent visual representations from sparse visual inputs. While the study of its dynamics has been the focus of many computational models for the past years, there is still relatively few research works that put an emphasis on both synaptic plasticity in V1 and biorealism in the context of learning visual inputs. Here, we present a recurrent spiking neural network that is capable of spike timing dependent plasticity (STDP) and we demonstrate its capacity to discriminate spatio-temporal orientation patterns in noisy natural images.  *Methods*: A two stage model was developed. First, natural images flux (be it videos/gratings/camera) were converted into spikes, using a difference of gaussians (DOG) approach. This transformation approximates the retina-lateral geniculate nucleus (LGN) organization. Secondly, a spiking neural network was build using PyNN simulator, mimicking cortical neurons dynamics and plasticity, as well as V1 topology. This network was then fed with spikes generated by the first model and its ability to build visual representations was assessed using control gratings inputs.  *Results*: The neural network exhibited several interesting properties. After a short period of learning, it was capable of learning multiples orientations and reducing noise in such learned feature, compared to the inputs. These learned features were stable even after increasing the noise in inputs and were found to not only encoding the spatial properties of the input, but also its temporal aspects (i.e., the time of each grating presentation  *Conclusions*: Our work shows that topological structuring of the cortical neural networks, combined with simple plasticity rules, are sufficient to drive strong learning dynamics of natural images properties. This computational model fits many properties found in the literature and provides some theoritical explanations for the shape of tuning curve of certain layers of V1. Further investigations are now conducted to validate its properties against the neuronal responses of rodents, using identical visual stimuli.","tags":["area-v1"],"title":"Learning dynamics in a neural network model of the primary visual cortex","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" THE POSITION HAS BEEN FILLED.   Applications are welcome for a post-doctoral position at INT-Marseille, France exploring novel visual computations using spatio-temporal diffusion kernels and travelling waves. More info @ https://t.co/f6tUR8XW6y pic.twitter.com/odzckjtloa\n— laurentperrinet (@laurentperrinet) October 28, 2019  Dear colleagues,\nApplications are welcome for a post-doctoral position at INT in Marseille, France. Your mission will be to explore novel visual computations using spatio-temporal diffusion kernels and traveling waves. The project is funded by the ANR Horizontal V1 grant (ANR-17-CE37-0006) from the French National Research Agency (ANR) and will be coordinated by Laurent Perrinet, in collaboration with Lyle Muller and Frédéric Chavane at INT and Yves Frégnac and Jan Antolik at UNIC-NeuroPSI, Gif. We are seeking candidates with a strong background in machine learning, computer vision and computational neuroscience.\nFor more information, visit https://laurentperrinet.github.io/post/2019-10-28_postdoc-position.\nThe starting date is set to January 6th, 2020 but can be flexibly extended. To obtain further information or send applications (including a full CV, a letter of motivation, 2 reference names), please contact: Laurent.Perrinet@univ-amu.fr. The appointment is for 18 month. Applications are welcome immediately and until the end of year 2019.\nThanks for distributing this announcement to potential candidates!\nDetailed description: Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. It is clear from recent advances in system and computational neuroscience that nonlinear, recurrent interactions in visual cortical networks are key to this efficiency (Tang et al., 2018; Kietzmann et al., 2019). We will use inspiration from neurophysiology and brain imaging to resolve this apparent gap between traditional CNNs and biological visual systems.\nIn this post-doctoral project, we propose to address these major limitations by focusing on specific dynamical features of cortical circuits: lateral diffusion of sensory-evoked traveling waves (Chavane et al., 2011; Muller et al., 2018) and dynamic neuronal association fields (Frégnac et al., 2012; Frégnac et al., 2016; Gerard-Mercier et al., 2016). Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (Voges and Perrinet, 2012). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). Less studied, but probably decisive in active vision, recurrent cortico-cortical loops add a level of distributed top-down complexity which participates to the lateral integration of sensory input and perceptual context (Keller et al., 2019). Coupled with the continuous time dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for generating information diffusion through traveling waves. Inspired by recent work in neuroscience uncovering the ubiquity of these waves during visual processing, we aim to design a self-supervised CNN that will exploit these dynamics for new applications in computer vision.\nThe proposed work will be organized as a collaboration between two labs (INT, Marseille and UNIC, Gif) along three tasks to be integrated in a unified model:\n  The starting point will be to extend results of self-supervised learning that we have obtained on static, natural images (Boutin et al., 2019) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the “association field” described at the psychophysical (Field et al., 1993), spiking (Li and Gilbert, 2002) and synaptic (Gerard-Mercier et al., 2016) levels.\n  The central aim will be to develop a dynamical version of this feedback/lateral kernel in the context of the ANR Horizontal-V1 project, linking the two labs and confronted to their recent electrophysiological data pointing to different classes of spatio-temporal diffusion and different degree of anisotropies during apparent and continuous motion.\n  The implementation of this kernel inspired by CNN theory will be compared with a biologically realistic models of the early visual system (Antolik et al., 2019), and simulations of the lateral diffusion kernel will be developed …","date":1571648400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571648400,"objectID":"bcec3e4d5dc3336840b18108295d2f83","permalink":"https://laurentperrinet.github.io/post/2019-10-28_postdoc-position/","publishdate":"2019-10-21T09:00:00Z","relpermalink":"/post/2019-10-28_postdoc-position/","section":"post","summary":"THE POSITION HAS BEEN FILLED. 18 month Post-doc position coordinated by [Laurent Perrinet](https://laurentperrinet.github.io/), supported by (INT, Marseille) and [Yves Frégnac](http://neuro-psi.cnrs.fr/spip.php?article934\u0026lang=fr) (UNIC-NeuroPSI, Gif).","tags":["events"],"title":"Postdoc position on Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Avec Anna Montagnini, Manuel Vidal et Françoise Vitu, nous organisons cette année le GDR Vision à Marseille les journées du 10 et 11 octobre.\n plus d’infos sur https://gdrvision2019.sciencesconf.org/ nous aurons un atelier méthodologique le jeudi matin sur les apports possibles du Deep Learning pour les sciences de la vision: Utiliser l’apprentissage profond en vision la session spéciale du jeudi est sponsorisée par la projet SpikeAI  Réunions passées:\n Lille: https://gdrvision2017.sciencesconf.org/ Paris: https://gdrvision2018.sciencesconf.org/  ","date":1570708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570708800,"objectID":"da2bb8483ded46c271649e8224e717a1","permalink":"https://laurentperrinet.github.io/post/2019-10-10_gdrvision/","publishdate":"2019-10-10T12:00:00Z","relpermalink":"/post/2019-10-10_gdrvision/","section":"post","summary":"Le GDR Vision réunit toute la communauté des chercheurs en France travaillant sur la perception visuelle (de la perception des attributs visuels comme la couleur et le mouvement à la reconnaissance des mots et des visages), l'intégration multi-sensorielle, la motricité (dont notamment le contrôle des mouvements des yeux et de la tête dans une variété de tâches; des plus basiques aux plus cognitives), la représentation de l'espace et les processus décisionnels. Il s'agit d'un réseau pluridisciplinaire réunissant psychophysiciens, psychologues, chercheurs en neurosciences (électrophysiologie et imagerie chez l'animal et l'Homme), et modélisateurs.","tags":["events","vision"],"title":"2019-10-10: GDR vision 2019","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Date : jeudi 10 octobre de 9h30 à 12h30\nIntervenants : Laurent Perrinet et Chloé Pasturel\n  programme: Nous proposons dans cet atelier pratique de présenter les nouveaux enjeux apportés par l’apprentissage profond et plus généralement par l’apprentissage machine. L’objectif est de montrer sous forme de simples exercises pratiques comment ces nouveaux outils permettent 1) de catégoriser des images 2) d’apprendre un tel modèles 3) de générer de nouvelles images à partir d’une base existante.\n  https://github.com/SpikeAI/2019-10-10_ML-tutorial\n  Atelier concocté en collaboration avec Chloé Pasturel.\n  cet atelier fait partie du GDR vision 2019\n  ","date":1570699800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570699800,"objectID":"d614d1f416bfe4f31978a22167606abb","permalink":"https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/","publishdate":"2019-10-10T09:30:00Z","relpermalink":"/post/2019-10-10_gdrvision-atelier/","section":"post","summary":"Le GDR Vision réunit toute la communauté française de chercheurs en vision. Nous aurons un atelier méthodologique le jeudi matin sur les apports possibles du Deep Learning pour les sciences de la vision: Utiliser l'apprentissage profond en vision.","tags":["events","vision"],"title":"2019-10-10: Atelier Utiliser l'apprentissage profond en vision","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Cette présentation lors des NeuroStories vise à aborder la notion de temps dans le cerveau.      “Chaque année, NeuroSchool nous raconte des histoires sur un thème à la fois philosophique et scientifique. L’objectif est de faire connaître, d’une manière inventive, les recherches de pointe menées à Marseille et ailleurs, dans le domaine des neurosciences. Le format inventif associe des NeuroStories et des causeries scientifiques.” http://neuroschool-stories.com/\n  Le texte de cette présentation est repris dans The Conversation (lien direct) ainsi que dans Science \u0026amp; Vie.\n  Neurostories: d’autres figures animées du flash-lag effect\n  ","date":1570471200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570471200,"objectID":"f3586d76a52a1fc235fbcefdc83a5353","permalink":"https://laurentperrinet.github.io/post/2019-10-07_neurostories/","publishdate":"2019-10-07T18:00:00Z","relpermalink":"/post/2019-10-07_neurostories/","section":"post","summary":"Dans le monde qui nous entoure, nous percevons le temps s’écouler de façon immuable et universelle. Pourtant, il existe un temps pour chaque sens. Laurent Perrinet (AMU) exposera la dynamique des réseaux de neurones et le temps particulier qui est associé à l’un de ces sens, la vision.","tags":["events","vision"],"title":"2019-10-07: Le temps des sens","type":"post"},{"authors":["Wahiba Taouali","Giacomo Benvenuti","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":"","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569196800,"objectID":"8455c83ddc3c334581db010c7849b500","permalink":"https://laurentperrinet.github.io/publication/perrinet-19-nccd/","publishdate":"2019-09-23T00:00:00Z","relpermalink":"/publication/perrinet-19-nccd/","section":"publication","summary":"When objects are in motion, the local orientation of their contours and the direction of motion are two essential components of visual information which are processed in parallel in the early visual areas. Generally, to probe a neuron's response property to moving stimuli, bars or gratings are drifted across neuron's receptive field at various angles. The resulting tuning curve will reflect the \\\"confound\\\" selectivity to both the orientation and direction of motion orthogonal to the orientation. Focusing on the primary visual cortex of the macaque monkey (V1), we challenged different models for the joint representation of orientation and direction within the neural activity. Precisely, we considered the response of V1 neurons to an oriented moving bar to investigate whether, and how, the information about the bar's orientation and direction could be encoded dynamically at the population activity level. For that purpose, we used a decoding approach based on a space-time receptive field model that encodes jointly orientation and direction. Then, using this model and a maximum likelihood paradigm, we inferred the most likely representation for a given network activity [1, 2]. We tested this model on surrogate data and on extracellular recordings in area V1 of awake macaque monkeys in response to oriented bars moving in 12 different directions. Using a cross-validation method we could robustly decode both the orientation and the direction of the bar within the classical receptive field (cRF). Furthermore, this decoding approach shows different properties: First, information about the orientation and direction of the bar is emerging before entering the cRF. Second, when testing different orientations with the same direction, our approach unravels that we can ``unconfound'' the information about direction and orientation by decoding them independently. Finally, our results demonstrate that the orientation and the direction of motion of an ambiguous moving bar can be progressively decoded in V1. This is a signature of a dynamic solution to the aperture problem in area V1, similarly to what was already found in area MT [3].  [1] M. Jazayeri and J.A. Movshon. Optimal representation of sensory information by neural populations. Nature Neuroscience, 9(5):690--696, 2006. [2] W. Taouali, G. Benvenuti, P. Wallisch, F. Chavane, L. Perrinet. Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts. Journal of Neurophysiology, 2015. [3] C. Pack, R. Born. Temporal dynamics of a neural solution to the aperture problem in visual area MT of macaque brain. Nature, 409(6823), 1040--1042. 2001.","tags":["coding decoding"],"title":"A dynamic model for decoding direction and orientation in macaque primary visual cortex","type":"publication"},{"authors":null,"categories":null,"content":"In the field of neuroscience, modeling has long made it possible to validate and predict theories of information processing in the neural networks that make up the brain. The recent emergence of solutions inherited from machine learning, in particular deep learning, has changed the field since 2012. One reason for the effectiveness of these methods is the amount of data analyzed but above all the ability to teach these algorithms on dedicated architectures, including graphics cards (GPUs). Indeed, this architecture allows an algorithm to be parallelized into a multitude of simple and independent subprograms that allow speed gains of around 6x to 10x to be achieved over traditional visual information processing architectures. We are now using these architectures excessively to test new image processing models - targeting applications in both neuroscience and machine learning.\nDans le domaine des neurosciences, la modélisation a longtemps permis d’effectuer des validations et prédictions sur les théories du traitement de l’information dans les réseaux de neurones qui constitue le cerveau. L’émergence récente de solutions héritées de l’apprentissage machine, en particulier l’apprentissage profond (Deep Learning) est venu bouleverser le champ depuis 2012. Une raison de l’efficacité de ces méthodes est la quantité de données analysées mais surtout la capacité de faire apprendre ces algorithmes sur des architectures dédiées, notamment les cartes graphiques (GPU). En effet cette architecture permet de paralléliser un algorithme en une multitude de sous programmes simples et indépendants qui permettent d’atteindre des gains de vitesse de l’ordre de 6x à 10x sur des architectures classiques de traitement de l’information visuelle. Nous utilisons maintenant excessivement ces architectures pour tester de nouveaux modèles de traitement des images - en visant des applications aussi bien aux neurosciences qu’en apprentissage machine.\nAcknowledgement: This work was granted access to the HPC resources of Aix-Marseille Université financed by the project Equip@Meso (ANR-10-EQPX-29-01) of the program « Investissements d’Avenir » supervised by the Agence Nationale de la Recherche.»\n","date":1568851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568851200,"objectID":"6a346209c6e42b0d1a344e57cf81372d","permalink":"https://laurentperrinet.github.io/grant/mesocentre/","publishdate":"2019-09-19T00:00:00Z","relpermalink":"/grant/mesocentre/","section":"grant","summary":"MesoCentre (2018/2022) : access to the HPC resources of Aix-Marseille Université.","tags":["grant","past-grant"],"title":"MesoCentre (2018/2022)","type":"grant"},{"authors":["Jean Martinet","Laurent U Perrinet"],"categories":null,"content":" Le projet APROVIS3D est lauréat de l’appel à projets 2018 CHIST-ERA :    APROVIS3D project targets analog computing for artificial intelligence in the form of Spiking Neural Networks (SNNs) on a mixed analog and digital architecture. The project includes including field programmable analog array (FPAA) and SpiNNaker applied to a stereopsis system dedicated to coastal surveillance using an aerial robot. Computer vision systems widely rely on artificial intelligence and especially neural network based machine learning, which recently gained huge visibility. The training stage for deep convolutional neural networks is both time and energy consuming. In contrast, the human brain has the ability to perform visual tasks with unrivalled computational and energy efficiency. It is believed that one major factor of this efficiency is the fact that information is vastly represented by short pulses (spikes) at analog – not discrete – times. However, computer vision algorithms using such representation still lack in practice, and its high potential is largely underexploited. Inspired from biology, the project addresses the scientific question of developing a low-power, end-to-end analog sensing and processing architecture of 3D visual scenes, running on analog devices, without a central clock and aims to validate them in real-life situations. More specifically, the project will develop new paradigms for biologically inspired vision, from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes. The ambitious long-term vision of the project is to develop the next generation AI paradigm that will eventually compete with deep learning. We believe that neuromorphic computing, mainly studied in EU countries, will be a key technology in the next decade. It is therefore both a scientific and strategic challenge for the EU to foster this technological breakthrough. The consortium from four EU countries offers a unique combination of expertise that the project requires. SNNs specialists from various fields, such as visual sensors (IMSE, Spain), neural network architecture and computer vision (Uni. of Lille, France) and computational neuroscience (INT, France) will team up with robotics and automatic control specialists (NTUA, Greece), and low power integrated systems designers (ETHZ, Switzerland) to help geoinformatics researchers (UNIWA, Greece) build a demonstrator UAV for coastal surveillance (TRL5). Adding up to the shared interest regarding analog based computing and computer vision, all team members have a lot to offer given their different and complementary points of view and expertise. Key challenges of this project will be end-to-end analog system design (from sensing to AI-based control of the UAV and 3D coastal volumetric reconstruction), energy efficiency, and practical usability in real conditions. We aim to show that such a bioinspired analog design will bring large benefits in terms of power efficiency, adaptability and efficiency needed to make coastal surveillance with UAVs practical and more efficient than digital approaches.\n Type de contrat : Subvention / Aide Durée: 3 ans, à partir du 1er avril 2020 (prolongation demandée) Budget total: 867 k€ , bugdget INT: 150 k€ Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE Objet : AAP 2019 - CHIST-ERA “Analog PROcessing of bioinspired VIsion Sensors for 3D reconstruction” ANR-19-CHR3-0008-03 Responsable Scientifique INT : PERRINET Laurent (UMR7289) “This project has received funding from the European Union’s ERA-NET CHIST-ERA 2018 research and innovation programme under grant agreement No ANR-19-CHR3-0008-03” Find more on the official website  ","date":1568109600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568109600,"objectID":"31b6a176175b3f139e3f5ff52cd84c59","permalink":"https://laurentperrinet.github.io/grant/aprovis-3-d/","publishdate":"2019-09-10T10:00:00Z","relpermalink":"/grant/aprovis-3-d/","section":"grant","summary":"Analog PROcessing of bioinspired VIsion Sensors for 3D reconstruction (APROVIS3D) is [2018 *CHIST-ERA* laureate](http://www.chistera.eu/projects/aprovis3d).","tags":["grant","current-grant"],"title":"APROVIS3D (2019/2023)","type":"grant"},{"authors":["Emmanuel Daucé","Pierre Albigès","Laurent U Perrinet"],"categories":null,"content":" download a preliminary PDF Emmanuel Daucé @ #CNS2019Barcelona speaks about our joint work on « Learning where to look: a foveated visuomotor control model » more info @ https://t.co/HREjuIgNCn @CNSorg pic.twitter.com/GbbXhWL1k1\n— laurentperrinet (@laurentperrinet) July 15, 2019    Problem setting: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. A) It is synthesized in the following experiment: After a fixation period of 200 ms, an observer is presented with a luminous display showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of 500 ms (light shaded area in B), that is enough to perform at most one saccade (here, successful) on the potential target. Finally, the observer has to identify the digit by a keypress. B) Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window and the temporal window during which a saccade is possible (green shaded area). C) Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display and after a saccade, the dashed red box indicating the visual area of the ``what’’ pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \\emph{before seeing it}, that is, before being able to actually identify the target’s category from a central fixation.    Results: success    Results: failure to classify    Results: failure to locate   ","date":1563193200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563193200,"objectID":"cc603b0227322578c492fe6d1bdbc55b","permalink":"https://laurentperrinet.github.io/talk/2019-07-15-cns/","publishdate":"2019-07-15T12:20:00Z","relpermalink":"/talk/2019-07-15-cns/","section":"talk","summary":"In computer vision, the visual search task consists in extracting a scarce and specific visual information (the target) from a large and crowded visual display. This task is usually implemented by scanning the different possible target identities at all possible spatial positions, hence with strong computational load. The human visual system employs a different strategy, combining a foveated sensor with the capacity to rapidly move the center of fixation using saccades. Saccade-based visual exploration can be idealized as an inference process, assuming that the target position and category are independently drawn from a common generative process. Knowing that process, visual processing is then separated in two specialized pathways, the where pathway mainly conveying information about target position in peripheral space, and the what pathway mainly conveying information about the category of the target. We consider here a dual neural network architecture learning independently where to look and then at what to see. This allows in particular to infer target position in retinotopic coordinates, independently to its category. This framework was tested on a simple task of finding digits in a large, cluttered image. Simulation results demonstrate the benefit of specifically learning where to look before actually knowing the target category. The approach is also energy-efficient as it includes the strong compression rate performed at the sensor level, by retina and V1 encoding, which is preserved up to the action selection level, highlighting the advantages of bio-mimetic strategies with regards to traditional computer vision when computing resources are at stake.","tags":["Active Inference","Deep Learning","Object localization","Visual search","Visuomotor control"],"title":"Learning where to look: a foveated visuomotor control model","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Publication d’un nouvel article généraliste autour des “Illusions et hallucinations visuelles” à découvrir sur le site TheConversation:\nIllusions et hallucinations visuelles : une porte sur la perception https://t.co/nOJFVRYDz7 pic.twitter.com/HiBPHbKdSA\n— The Conversation France (@FR_Conversation) June 6, 2019  Les objectifs sont :\n mieux comprendre la fonction de la perception visuelle en explorant certaines limites ; mieux comprendre l’importance de l’aspect dynamique de la perception ; mieux comprendre le rôle de l’action dans la perception.  Les #illusions et #hallucinations nous informent sur les #perceptions mais aussi sur la compréhension des mécanismes cérébraux.\nLaurent Perrinet, @imera_amu, nous explique comment et pourquoi une image peut tromper nos sens.\n🌪️https://t.co/5QSaezZonz#neurosciences #cerveau🧠 pic.twitter.com/t8ehsyoGt8\n— The Conversation France (@FR_Conversation) June 7, 2019  Une version étendue est accessible sur le repo GitHub, ainsi que les sources.\n","date":1560384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560384000,"objectID":"5ae819e22435e2db4eba93c6368e33a8","permalink":"https://laurentperrinet.github.io/post/2019-06-06-theconversation/","publishdate":"2019-06-13T00:00:00Z","relpermalink":"/post/2019-06-06-theconversation/","section":"post","summary":"Article de dissémination sur la perception visuelle vue à travers illusions et hallucinations.","tags":["neuroscience","vision","psychiatry"],"title":"Illusions et hallucinations visuelles : une porte sur la perception","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    This is part of the Active Inference symposium @ NeuroFrance SYMPOSIUM, Room 7 23.05.2019, 11:00 – 13:00 in french: Principes et psychophysique de l´Inférence Active dans l’estimation d’un biais dynamique et volatile de probabilité previous talk @ LAW, Lyon previous talk @ INVIBE FEST, Paris previous talk @ Brain workshop, Marseille previous talk @ LACONEU, Chile previous talk @ CAUSAL Kick-off, Marseille  ","date":1558575000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558575000,"objectID":"2b3cde0a0df30a30c33967b0941dffef","permalink":"https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/","publishdate":"2019-05-23T01:30:00Z","relpermalink":"/talk/2019-05-23-neurofrance/","section":"talk","summary":"Animal behavior must constantly adapt to changes, for example when the state of an environmental context changes unexpectedly. For an agent that interacts with this volatile setting, it is important to react accurately and as quickly as possible. For example, it has already been shown that when a random sequence of directions of motion to the right or left of a visual target is suddenly biased to one direction, human observers adapt to accurately anticipate it with their eye movements. Here, we prove that this ability extends to a volatile environment where probability biases could change at random switching times. In addition, we also recorded the level of confidence reported by human observers. These results were compared to those of a probabilistic agent that is optimal in relation to the event switching generating model. Compared to other models such as the leaky integrator, we found a better match between the behavioral response observed and that given by this agent. Furthermore, we were also able to fit the experimental data with different levels of switching volatility in the model and derive a common marker for the inter-variability of participants, by titrating their level of preference between exploration and exploitation. Such results prove that in such an unstable environment, human observers can still effectively represent an internal belief, and use this representation in their sensory-motor control system and for explicit judgments. This work offers an innovative approach to more generically test human cognitive abilities in uncertain and dynamic environments.","tags":null,"title":"Should I stay or should I go? Humans adapt to the volatility of visual motion properties, and know about it","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Active Inference: Bridging theoretical and experimental neurosciences. / Inference Active: Un pont entre neurosciences théoriques et expérimentales.   \n SYMPOSIUM S17 When: 23.05.2019 11:00-13:00h When: Endoume 1+2  S17.1 Active inference and Brain-Computer Interfaces / Inférence active et interfaces cerveau-machine  Mattout J. (Lyon, France), Mladenovic J. (Lyon, France), Frey J. (Bordeaux, France)3, Joffily M. (Lyon, France), Maby E. (Lyon, France), Lotte F. (Lyon, France)  Brain-Computer Interfaces (BCIs) devices bypass natural pathways to connect the brain with a machine, directly. They may rely on invasive or non-invasive measures of brain activity and applications cover a large domain, mostly but not restricted to clinical ones. A major objective is to restore communication and autonomy in heavily motor impaired patients. However, no BCI has made its way to a routinely used clinical application yet. One lead for improvement is to endow the machine with learning abilities so that it can optimize its decisions and adapt to changes in the user signals over time1. Several approaches have been proposed but a generic framework is still lacking to foster the development of efficient adaptive BCIs2. Initially proposed to model perception, learning and action by the brain, the Active Inference (AI) framework offers great promises in that aim3. It rests on an explicit generative model of the environment. In BCI, from the machine’s point of view, brain signals play the role of sensory inputs on which the machine’s perception of mental states will be based. Furthermore, the machine builds up decisions and trades between different actions such as: go on observing, deciding to decide, correcting its previous action or moving on. In this talk, I will present an instantiation of AI in the context of the EEG-based P300-speller BCI for communication, showing it can flexibly combine complementary adaptive features pertaining to both perception and action, and yield significant improvements as shown on realistic simulations. We will discuss perspectives to further extend the current model and performance as well as the challenges ahead to implement this framework online.\n Mattout, J. Brain-Computer Interfaces: A Neuroscience Paradigm of Social Interaction? A Matter of Perspective. Frontiers in Human Neuroscience 6, (2012). Mladenovic, J., Mattout, J. \u0026amp; Lotte, F. A Generic Framework for Adaptive EEG-Based BCI Training and Operation. in Brain-computer interfaces handbook: technological and theoretical advances (eds. Nam, C. S., Nijholt, A. \u0026amp; Lotte, F.) Chapter 31 (Taylor \u0026amp; Francis, CRC Press, 2018). Friston, K., Mattout, J. \u0026amp; Kilner, J. Action understanding and active inference. Biological Cybernetics 104, 137-160 (2011).  S17.2 Comparing active inference and reinforcement learning models of a Go NoGo task and their relationships to striatal dopamine 2 receptors assessed using PET / Comparaison des modèles d’inférence active et d’apprentissage par renforcement dans une tâche Go / NoGo : relation avec les récepteurs dopaminergiques D2 striataux évalués par TEP  R. Adams (London) https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1398  Adaptive behaviour includes the ability to choose actions that result in advantageous outcomes. It is key to survival and a fundamental function of nervous systems. Active inference (AI) and reinforcement learning (RL) are two influential models of how the brain might achieve this. A key AI parameter is the precision of beliefs about policies. Precision controls the stochasticity of action selection - similar to decision temperature in RL - and is thought to be encoded by striatal dopamine. 75 healthy subjects performed a ‘go/no-go’ task, and we measured striatal dopamine 2/3 receptor (D2/3R) availability in a subset of 25 using [11C]-(+)-PHNO positron emission tomography. In behavioural model comparison, RL performed best across the whole group but AI performed best in accurate subjects. D2/3R availability in the limbic striatum correlated with AI policy precision and also with RL irreducible decision ’noise’. Limbic striatal D2/3R availability also correlated with AI Pavlovian prior beliefs - i.e. the respective probabilities of making or withholding actions in rewarding or loss-avoiding contexts - and the RL learning rate. These findings are consistent with the notion that occupancy of inhibitory striatal D2/3Rs controls the variability of action selection.\nS17.3 Principles and psychophysics of active inference in anticipating a dynamic, switching probabilistic bias / Principes et psychophysique de l’inférence active dans l´estimation d’un biais dynamique et volatile de probabilité  L. Perrinet (Marseille) see more info on this talk  S17.4 Is laziness contagious? A computational approach to attitude alignment / La fainéantise est-elle contagieuse? Une approche computationnelle de l´alignement des attitudes  J. Daunizeau (Paris)  What do people learn from observing others´ …","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"a0b7a36433cf7e7c8e007741c44013fe","permalink":"https://laurentperrinet.github.io/post/2019-05-23-neurofrance/","publishdate":"2019-05-20T00:00:00Z","relpermalink":"/post/2019-05-23-neurofrance/","section":"post","summary":"We organized a Symposium at NeuroFrance 2019 entitled *Active Inference: Bridging theoretical and experimental neurosciences*. This is part of a series of theoretical neuroscience symposia organized in this international conference from the french Neursocience Society.","tags":["events","probalistic-inference"],"title":"2019-05-20: Symposium on Active Inference at NeuroFrance 2019","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Le texte de cette présentation est reprise dans cet article de The Conversation (lien direct). Voir la @ présentation au NeuroStories  ","date":1555592400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555592400,"objectID":"7a9755306a0dd5ee7ac75e5a138f0b8a","permalink":"https://laurentperrinet.github.io/talk/2019-04-18-jnlf/","publishdate":"2019-04-18T13:00:00Z","relpermalink":"/talk/2019-04-18-jnlf/","section":"talk","summary":"Les objectifs sont : -- mieux comprendre la fonction de la perception visuelle en explorant certaines limites ; -- mieux comprendre l'importance de l'aspect dynamique de la perception ; -- mieux comprendre le rôle de l'action dans la perception.","tags":[],"title":"Des illusions aux hallucinations visuelles: une porte sur la perception","type":"talk"},{"authors":null,"categories":null,"content":"Description  Le projet SpikeAI est lauréat de l’appel à projets 2019 Biomimétisme :  The SpikeAI project targets analog computing for artificial intelligence in the form of Spiking Neural Networks (SNNs). Computer vision systems widely rely on artificial intelligence and especially neural network based machine learning, which recently gained huge visibility. The training stage for deep convolutional neural networks is time-consuming and yields enormous energy consumption. In contrast, the human brain has the ability to perform visual tasks with unrivaled computational and energy efficiency. It is believed that one major factor of this efficiency is the fact that information is vastly represented by short pulses (spikes) at analog –not discrete– times. However, computer vision algorithms using such representation still lack in practice, and its high potential is largely underexploited. Inspired from biology, the project addresses the scientific question of developing a low-power, end-to-end analog sensing and processing architecture. This will be applied on the particular context of a field programmable analog array (FPAA) applied to a stereovision system dedicated to coastal surveillance using an aerial robot of 3D visual scenes, running on analog devices, without a central clock and to validate them in real-life situations. The ambitious long-term vision of the project is to develop the next generation AI paradigm that will at term compete with deep learning. We believe that neuromorphic computing, mainly studied in EU countries, will be a key technology in the next decade. It is therefore both a scientific and strategic challenge for France and EU to foster this technological breakthrough. This call will help kickstart collaboration within this European consortium to help leverage the chance to successfully apply to future large-scale grant proposals (e.g. ANR, CHIST-ERA, ERC).\n Get more information.  outcomes The main goal was mainly to build a network of actors and to answer to relevant calls in the field of biomimetic research. We have communicated through the diffusion of computational frameworks and actions which are gathered online @ https://github.com/SpikeAI.\nSummary of the actions taken:\n  We had a APROVIS3D FPP meeting April 23-24, 2019 in Lille with all partners. The call could support the travel of the 5 participants from outside Lille. During these two days, we had a first day to know each other better and a second day devoted to writing the grant proposal.\n  With the help of this call, we could kickstart a collaboration within this European consortium which helped successfully achieve a large-scale grant proposal (CHIST-ERA : https://laurentperrinet.github.io/grant/aprovis-3-d/ ).\n  We organized a tutorial on deep learning during the GDR Vision, see https://github.com/SpikeAI/2019-10-10_ML-tutorial and https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/\n  We could invite a major actor of the modeling of biomimetic computations, Ryad Benosman, to the GDR vision meeting. The call could support his travel and accommodation and was acknowledged in all communications (see https://gdrvision2019.sciencesconf.org/resource/page/id/6).\n  ","date":1555322400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555322400,"objectID":"986f8977b09cddc0bbe478e9bc9acf79","permalink":"https://laurentperrinet.github.io/grant/spikeai/","publishdate":"2019-04-15T10:00:00Z","relpermalink":"/grant/spikeai/","section":"grant","summary":"Algorithmes événementiels d’Intelligence Artificielle / Event-Based Artificial Inteligence (2019).","tags":["grant","past-grant"],"title":"SpikeAI: laureat du Défi Biomimétisme (2019)","type":"grant"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    previous talk @ LAW, Lyon previous talk @ INVIBE FEST, Paris previous talk @ Brain workshop, Marseille previous talk @ LACONEU, Chile next talk @ NeuroFrance, Marseille  ","date":1554479100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554479100,"objectID":"2b560cb4fbc59a53968d6a22b7b7d068","permalink":"https://laurentperrinet.github.io/talk/2019-04-05-bbcp-causal-kickoff/","publishdate":"2019-04-05T15:45:00Z","relpermalink":"/talk/2019-04-05-bbcp-causal-kickoff/","section":"talk","summary":"Animal behavior has to constantly adapt to changes, for instance when unexpectedly switching the state of an environmental context. For an agent interacting with this kind of volatile environment, it is important to respond to such switches accurately and with the shortest delay. However, this operation has in general to be performed in presence of noisy sensory inputs and solely based on the accumulated information. It has already been shown that human observers can accurately anticipate the motion direction of a visual target with their eye movements when this random sequence of rightward/leftward motions is defined by a bias in direction probability. Here, we generalized the capacity of these observers to anticipate different random biases within random-length contextual blocks. Experimental results were compared to those of a probabilistic agent which is optimal with respect to this switching model. We found a better fit between the behaviorally observed anticipatory response with that of the probabilistic agent compared to other models such as a leaky integrator model. Moreover, we could similarly fit the level of confidence reported by human observers with that provided by the model and derive a common marker for subject inter-variability, titrating their level of preference between exploration and exploitation. Such results provide evidence that in such a volatile environment human observers may still efficiently represent an internal belief, along with its precision, and use this representation for sensorimotor control as well as for explicit judgments. This work proposes a novel approach to more generically test human cognitive abilities in uncertain and dynamic environments.","tags":null,"title":"Should I stay or should I go? Adaption of human observers to the volatility of visual inputs","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1554307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554307200,"objectID":"153f2bc927caf2c897df838ab8ed138f","permalink":"https://laurentperrinet.github.io/talk/2019-04-03-a-course-on-vision-and-modelization/","publishdate":"2019-04-03T16:00:00Z","relpermalink":"/talk/2019-04-03-a-course-on-vision-and-modelization/","section":"talk","summary":"Visual areas are essential in transforming the raw luminous signal into a representation which efficiently conveys information about the environment. This process is constrained by various factors such as a wide variety of changes in the characteristics of the visual image but also by the necessity to be able to respond as quickly as possible to the incoming sensory stream, for instance to drive a movement of the eyes to the location of a potential danger. To achieve this, it is believed that the visual system takes advantage of the existence of a priori knowledge in the structure of visual information, such as the regularity in the shape and motion of visual objects. We will review different models around the predictive coding coding framework to offer a unified theory to explain many of the mechanisms at the different levels of the visual system and which were unveiled by decades of study in neurophysiology and psychophysics.","tags":null,"title":"From the retina to action: Understanding visual processing","type":"talk"},{"authors":["Victor Boutin","Angelo Franciosini","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1553524200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553524200,"objectID":"d4805bbc1b84af9a7343fdb4ef3e9e02","permalink":"https://laurentperrinet.github.io/talk/2019-03-25-hdr-robin-baures/","publishdate":"2019-03-25T14:30:00Z","relpermalink":"/talk/2019-03-25-hdr-robin-baures/","section":"talk","summary":"Visual areas are essential in transforming the raw luminous signal into a representation which efficiently conveys information about the environment. This process is constrained by various factors such as a wide variety of changes in the characteristics of the visual image but also by the necessity to be able to respond as quickly as possible to the incoming sensory stream, for instance to drive a movement of the eyes to the location of a potential danger. To achieve this, it is believed that the visual system takes advantage of the existence of a priori knowledge in the structure of visual information, such as the regularity in the shape and motion of visual objects. As such, the predictive coding coding framework offers a unified theory to explain many of the mechanisms at the different levels of the visual system and which were unveiled by decades of study in neurophysiology and psychophysics.","tags":null,"title":"From the retina to action: Predictive processing in the visual system","type":"talk"},{"authors":["Sandrine Chemla","Alexandre Reynaud","Matteo diVolo","Yann Zerlaut","Laurent U Perrinet","Alain Destexhe","Frédéric Chavane"],"categories":null,"content":"","date":1552867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552867200,"objectID":"f5aea229fcd43b197c49e06c6085ffc2","permalink":"https://laurentperrinet.github.io/publication/chemla-19/","publishdate":"2019-03-18T00:00:00Z","relpermalink":"/publication/chemla-19/","section":"publication","summary":"Traveling waves have recently been observed in different animal species, brain areas and behavioral states. However, it is still unclear what are their functional roles. In the case of cortical visual processing, waves propagate across retinotopic maps and can hereby generate interactions between spatially and temporally separated instances of feedforward driven activity. Such interactions could participate in processing long-range apparent motion stimuli, an illusion for which no clear neuronal mechanisms have yet been proposed. Using this paradigm in awake monkeys, we show that suppressive traveling waves produce to a spatio-temporal normalization of apparent motion stimuli. Our study suggests that cortical waves shape the representation of illusory moving stimulus within retinotopic maps for an straightforward read-out by downstream areas.","tags":["area-v1"],"title":"Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey V1","type":"publication"},{"authors":["Victor Boutin","Angelo Franciosini","Frédéric Chavane","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" presented during this talk see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1551571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551571200,"objectID":"70b29148173a7b87bd4942c2aeb2c58c","permalink":"https://laurentperrinet.github.io/publication/boutin-20-sigma/","publishdate":"2019-03-03T00:00:00Z","relpermalink":"/publication/boutin-20-sigma/","section":"publication","summary":"Both neurophysiological and psychophysical experiments have pointed out the crucial role of recurrent and feedback connections to process context-dependent information in the early visual cortex. While numerous models have accounted for feedback effects at either neural or representational level, none of them were able to bind those two levels of analysis. Is it possible to describe feedback effects at both levels using the same model? We answer this question by combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical and convolutional framework. In this Sparse Deep Predictive Coding (SDPC) model, the SC component models the internal recurrent processing within each layer, and the PC component describes the interactions between layers using feedforward and feedback connections. Here, we train a 2-layered SDPC on two different databases of images, and we interpret it as a model of the early visual system (V1~\u0026~V2). We first demonstrate that once the training has converged, SDPC exhibits oriented and localized receptive fields in V1 and more complex features in V2. Second, we analyze the effects of feedback on the neural organization beyond the classical receptive field of V1 neurons using interaction maps. These maps are similar to association fields and reflect the Gestalt principle of good continuation. We demonstrate that feedback signals reorganize interaction maps and modulate neural activity to promote contour integration. Third, we demonstrate at the representational level that the SDPC feedback connections are able to overcome noise in input images. Therefore, the SDPC captures the association field principle at the neural level which results in better disambiguation of blurred images at the representational level.","tags":["deep-learning","sparse coding"],"title":"Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system","type":"publication"},{"authors":["Cesar U Ravello","Laurent U Perrinet","Maria Jose Escobar","Adrián G Palacios"],"categories":null,"content":" Press release  Dès la rétine, le système visuel préfère des images naturelles Dans la rétine, au premier étage du traitement de l’image visuelle, on peut obtenir des représentations extrêmement fines. Une collaboration entre des chercheurs français et chiliens a permis de mettre en évidence que, dans la rétine de rongeurs, une représentation de la vitesse de l’image visuelle est précisément codée. Dans cette collaboration pluridisciplinaire, l’utilisation d’un modèle du fonctionnement de la rétine a permis de générer un nouveau type de stimuli visuels qui a révélé des résultats expérimentaux surprenants. Travail collaboratif et multi-disciplinaire entre @cesarravello @laurentperrinet María José Escobar et @APalacio_s librement disponible sur @SciReports - merci à l\u0026#39;@AgenceRecherche pour l\u0026#39;aide financière et à @CNRS @CNRS_dr12 + @univamu pour l\u0026#39;#InstitutDeNeurosciencesDeLaTimone https://t.co/YixRfpCrT3\n— laurentperrinet (@laurentperrinet) February 3, 2019  La rétine est la première étape du traitement visuel, aux capacités étonnantes. À la différence d’un simple capteur comme ceux qu’on trouve dans les appareils photographiques numériques, ce mince tissu neuronal est un système complexe et encore largement méconnu. Une meilleure connaissance de cette structure est essentielle pour la construction de capteurs du futur efficaces et économes -par exemple ceux qui équiperont les futures voitures autonomes- mais aussi pour mieux comprendre des pathologies comme la Déficience Maculaire Liée à l’Age (DMLA). Une des facettes méconnues de la rétine est sa capacité à détecter des mouvements et cet article permet de mieux comprendre une partie des mécanismes en jeu. New study on speed selectivity in the #retina showing that a majority of neurons prefer natural-like stimuli. Collaborative and multi-disciplinary work with @cesarravello @laurentperrinet María José Escobar and @APalacio_s available with @SciReports at https://t.co/Vb7GoRxjoT\n— 👉🏼Adrian Palacios (@APalacio_s) February 3, 2019  Retinal cell preference for natural-like stimuli. Very elegant work by @APalacio_s et al. #retina #neuroscience #decoding https://t.co/3xNWaZd5x6\n— Andres Canales-Johnson (@canalesjohnson) February 4, 2019  Conciliant modélisation et neurophysiologie, cette étude a permis de faire des prédictions sur le traitement de l’information rétinienne et en particulier de générer des textures synthétiques qui sont optimales pour ces modèles (voir film). Les enregistrements effectués sur la rétine de rongeurs diurnes Octodon degus ont ensuite permis de mesurer la sélectivité à la vitesse mais aussi de valider une nouvelle fois ces modèles en reconstruisant l’image d’entrée à partir de l’activité neurale. Le résultat le plus inattendu est la différence de sélectivité de certaines classes de neurones rétiniens par rapport à la complexité du stimulus présenté. En effet, la représentation de la vitesse est relativement peu précise si on utilise des réseaux de lignes (“Grating”), comme cela est d’habitude réalisé dans la plupart des expériences neurophysiologiques. Au contraire, elle devient plus précise si on utilise comme signaux visuels des textures artificielles ressemblant à des nuages en mouvement (“MC Narrow”). En particulier, plus cette texture est complexe, plus la représentation est précise (“MC Broad”). #RésultatScientifique 🔍| Dès la #rétine, le système #visuel préfère des images naturelles\n▶️ https://t.co/BBY2IpGum6\n📕 @SciReports | https://t.co/5mULuWTp3N\n🤝 @CNRS @CNRS_dr12 @univamu #InstitutDeNeurosciencesDeLaTimone #LaurentPerrinet pic.twitter.com/34R1URHUic\n— Biologie au CNRS (@INSB_CNRS) February 1, 2019  Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli. Beautiful article in Scientific Reports on an original animal model, the diurnal rodent Octodon degus. https://t.co/BdzyzEVYnX (open access) pic.twitter.com/1UaoMYTFd2\n— Stéphane Deny (@StephaneDeny) January 30, 2019  Ces textures complexes sont plus proches des images naturellement observées et ces résultats montrent donc que dès la rétine, le système visuel est particulièrement adapté à des stimulations naturelles. Ce résultat devrait pouvoir s’étendre à des textures encore plus complexes et encore plus proches d’images naturelles, mais aussi pouvoir se généraliser à d’autres aires visuelles plus complexes, comme le cortex visuel primaire, et à d’autres espèces.   Pour une cellule représentative, on montre ici la réponse au cours du temps sous forme d’impulsions pour différentes présentations (Trial) ainsi que la moyenne de cette réponse (Firing rate). Les différentes colonnes représentent différentes vitesses des stimulations sur la rétine. Les différentes lignes sont différentes stimulations. En bleu, une stimulation classique sous forme de réseaux de lignes (« Grating »). En vert et Orange, la réponse à une texture progressivement plus complexe (de « Mc Narrow » à « MC Broad »). Si les réponses aux différents stimulations …","date":1548288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548288000,"objectID":"99be955cfcda8bbb82ec76c8226d705d","permalink":"https://laurentperrinet.github.io/publication/ravello-19/","publishdate":"2019-01-24T00:00:00Z","relpermalink":"/publication/ravello-19/","section":"publication","summary":"Motion detection represents one of the critical tasks of the visual system and has motivated a large body of research. However, it remains unclear precisely why the response of retinal ganglion cells (RGCs) to simple artificial stimuli does not predict their response to complex, naturalistic stimuli. To explore this topic, we use Motion Clouds (MC), which are synthetic textures that preserve properties of natural images and are merely parameterized, in particular by modulating the spatiotemporal spectrum complexity of the stimulus by adjusting the frequency bandwidths. By stimulating the retina of the diurnal rodent, Octodon degus with MC we show that the RGCs respond to increasingly complex stimuli by narrowing their adjustment curves in response to movement. At the level of the population, complex stimuli produce a sparser code while preserving movement information; therefore, the stimuli are encoded more efficiently. Interestingly, these properties were observed throughout different populations of RGCs. Thus, our results reveal that the response at the level of RGCs is modulated by the naturalness of the stimulus -in particular for motion- which suggests that the tuning to the statistics of natural images already emerges at the level of the retina.","tags":["motion detection","motion-clouds","Retina"],"title":"Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    previous talk @ LAW, Lyon previous talk @ INVIBE FEST, Paris previous talk @ Brain workshop, Marseille next talk @ CAUSAL Kick-off, Marseille  ","date":1547808300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547808300,"objectID":"0416104d85ade3e818f62ef79567df23","permalink":"https://laurentperrinet.github.io/talk/2019-01-18-laconeu/","publishdate":"2019-01-18T10:45:00Z","relpermalink":"/talk/2019-01-18-laconeu/","section":"talk","summary":"See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.","tags":null,"title":"Should I stay or should I go? Adaption of human observers to the volatility of visual inputs","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1547721900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547721900,"objectID":"ecba56fdd3037896423fe1150ad94d6f","permalink":"https://laurentperrinet.github.io/talk/2019-01-17-laconeu/","publishdate":"2019-01-17T10:45:00Z","relpermalink":"/talk/2019-01-17-laconeu/","section":"talk","summary":"","tags":null,"title":"Role of dynamics in neural computations underlying visual processing","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1547635500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547635500,"objectID":"bed9dbde1994e2e3a6b3a666296ce185","permalink":"https://laurentperrinet.github.io/talk/2019-01-16-laconeu/","publishdate":"2019-01-16T10:45:00Z","relpermalink":"/talk/2019-01-16-laconeu/","section":"talk","summary":"","tags":null,"title":"Efficient coding of visual information in neural computations","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1547463600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547463600,"objectID":"00adf8aa2e3626ede4cd574f5b6c183a","permalink":"https://laurentperrinet.github.io/talk/2019-01-14-laconeu/","publishdate":"2019-01-14T11:00:00Z","relpermalink":"/talk/2019-01-14-laconeu/","section":"talk","summary":"","tags":null,"title":"Modelling spiking neural networks using Brian, Nest and pyNN","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Cinéma et sciences : rencontre avec les collégiens marseillais L’Association Polly Maggoo http://www.pollymaggoo.org/ met en place tout le long de l’année, des actions de culture scientifique et artistique en direction du grand public et des lycées, au cours desquelles l’association programme des films à caractère scientifique. Les projections se déroulent en présence de chercheurs et/ou de cinéastes dans la perspective d’un développement de la culture cinématographique et scientifique en direction des publics scolaires. Le jeudi 10 janvier 2019, je suis venu échanger au côté de Serge Dentin autour de films traitant du rapport fiction/réel, des illusion visuelles (\u0026#34; Qu’est ce qu’une image? “), des rapports d’échelles, de la perception, … et qui sont projetés lors de la séance, avec les élèves de deux classes de 4ème. Une occasion aussi de parler du métier de chercheur.\n Date\n10 janvier 2019 Location\ncollège André Malraux, Marseille Programmation\n“LAZARUS MIRAGES : TÉLÉPATHIE À L’UNIVERSITÉ DE SHANGAI” de Patric JEAN et Henry BROCH (France, 2012, documentaire, 3\u0026#39;21) /“CARLITOPOLIS” / / “BIG DATA, BIG BUSINESS” / “The Centrifuge Brain Project, A Short Film by Till Nowak”  ","date":1547112600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547112600,"objectID":"598419f7d590d83379643bd3e650962e","permalink":"https://laurentperrinet.github.io/talk/2019-01-10-polly-maggoo/","publishdate":"2019-01-10T09:30:00Z","relpermalink":"/talk/2019-01-10-polly-maggoo/","section":"talk","summary":"Le jeudi 10 janvier 2019, je suis venu échanger au côté de Serge Dentin autour de films traitant du rapport fiction/réel, des illusion visuelles (\\\" Qu'est ce qu'une image? \\\"), des rapports d'échelles, de la perception, ... et qui sont projetés lors de la séance, avec les élèves de deux classes de 4ème. Une occasion aussi de parler du métier de chercheur.","tags":null,"title":"Rencontre avec les collégiens marseillais","type":"talk"},{"authors":["Angelo Franciosini","Victor Boutin","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6e5238c8c20d9f0f77593c582fd6dee8","permalink":"https://laurentperrinet.github.io/publication/franciosini-perrinet-19-neurofrance/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/franciosini-perrinet-19-neurofrance/","section":"publication","summary":"Sparse coding holds the idea that signals can be concisely described as a linear mixture of few components (called atoms) picked from a bigger set of primary kernels (called dictionary). This framework has long been used to model the strategy employed by mammals´ primary visual cortex (V1) to detect low-level features, in particular, oriented edges in natural scenes. Differently, predictive coding is a prominent tool used to model hierarchical neural dynamics: high-level cortical layers predict at best the activity of lower-level ones and this prediction is sent back through of a feedback connection between the layers. This defines a recursive loop in which prediction error is integrated to the sensory input and fed forward to refine the quality of the prediction. We propose a Sparse Deep Predictive Coding algorithm (SDPC) that exploits convolutional dictionaries and a feedback information flow for meaningful, hierarchical feature learning in static images. The proposed architecture allows us to add arbitrary non-linear spatial transformation stages between each layer of the hierarchical sparse representations, such as Max-Pooling or Spatial Transformer layers. SPDC consists of a dynamical system in the form of a convolutional neural network, analogous to the model proposed by Rao and Ballard, 1999. The state variables are sparse feature maps encoding the input and the feedback signals while the parameters of the system are convolutional dictionaries optimized through Hebbian learning. We observed that varying the strength of the feedback modulates the overall sparsity of low-level representations (lower feedback scales correspond to a less sparse activity), but without changing the exponential shape of the distribution of the sparse prior. This model could shed light on the role of sparsity and feedback modulation in hierarchical feature learning with important applications in signal processing (data compression), computer vision (by extending it to dynamic scenes) and computational neuroscience, notably by using more complex priors like group sparsity to model topological organization in the brain cortex.","tags":["deep-learning","sparse coding"],"title":"A hierarchical, multi-layer convolutional sparse coding algorithm based on predictive coding","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"“An adaptive algorithm for unsupervised learning”   supplementary info : https://spikeai.github.io/HULK/ Abstract HTML PDF code for paper: https://github.com/SpikeAI/HULK code for framework: https://github.com/bicv/SparseHebbianLearning/ code for figures https://github.com/SpikeAI/HULK/blob/master/Annex.ipynb (which is rendered @ https://spikeai.github.io/HULK/ ) video abstract (and the code for generating it) previous publication :  Laurent U Perrinet  (2010). Role of homeostasis in learning sparse representations. Neural Computation.  Preprint  PDF  Cite  Code  DOI   $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$   ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"f098536b9c70b9f7bfd447dd076b7ceb","permalink":"https://laurentperrinet.github.io/publication/perrinet-19-hulk/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/perrinet-19-hulk/","section":"publication","summary":"The formation of structure in the visual system, that is, of the connections between cells within neural populations, is by large an unsupervised learning process: the emergence of this architecture is mostly self-organized. In the primary visual cortex of mammals, for example, one can observe during development the formation of cells selective to localized, oriented features which results in the development of a representation of contours in area V1. We modeled such a process using sparse Hebbian learning algorithms. These algorithms alternate a coding step to encode the information with a learning step to find the proper encoder. We identified here a major difficulty of classical solutions in their ability to deduce a good representation while knowing immature encoders, and to learn good encoders with a non-optimal representation. To solve this problem, we propose to introduce a new regulation process between learning and coding, called homeostasis. It is compatible with a neuromimetic architecture and allows for a more efficient emergence of localized filters sensitive to orientation. The key to this algorithm lies in a simple adaptation mechanism based on non-linear functions that reconciles the antagonistic processes that occur at the coding and learning time scales. We tested this unsupervised algorithm with this homeostasis rule for a series of learning algorithms coupled with different neural coding algorithms. In addition, we propose a simplification of this optimal homeostasis rule by implementing a simple heuristic on the probability of activation of neurons. Compared to the optimal homeostasis rule, we show that this heuristic allows to implement a faster unsupervised learning algorithm while retaining much of its effectiveness. These results demonstrate the potential application of such a strategy in computer vision and machine learning and we illustrate it with a result in a convolutional neural network.","tags":["area-v1","gain control","homeostasis","matching pursuit","sparse coding","sparse hebbian learning","unsupervised learning"],"title":"An adaptive homeostatic algorithm for the unsupervised learning of visual features","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Ce texte est disponible dans cet article de The Conversation. Voir la @ présentation au NeuroStories  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"283ce4a0e4d2f3a8f3739dcea08acf73","permalink":"https://laurentperrinet.github.io/publication/perrinet-19-illusions/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/perrinet-19-illusions/","section":"publication","summary":"Les illusions visuelles sont des créations d'artistes, de scientifiques et plus récemment, grâce aux réseaux sociaux, du grand public qui proposent des situations souvent incongrues, dans lesquelles l'eau remonte une cascade, les personnes volent dans les airs ou des serpents se mettent à tourner. Au-delà de leur indéniable coté ludique, ces illusions nous apprennent beaucoup sur le fonctionnement du cerveau, notamment quand celles-ci se transforment en hallucinations visuelles, dépassant ainsi les limites des capacités de notre perception. En tant que chercheur en Neurosciences à l'Institut de Neurosciences de la Timone à Marseille, je vous dévoilerai des aspects du fonctionnement du cerveau qui sont souvent méconnus. En particulier, nous verrons pourquoi un magicien peut tromper nos sens ou comment des objets peuvent voyager dans le temps. Surtout nous essaierons de comprendre le fonctionnement de notre perception visuelle sur les bases d'une théorie de la vision non pas comme une simple caméra qui enregistre des images mais comme un processus actif en relation avec le monde qui nous entoure.","tags":null,"title":"Illusions et hallucinations visuelles : une porte sur la perception","type":"publication"},{"authors":["Angelo Franciosini","Victor Boutin","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"d5f3217e86dc46c35c95a80725ecf27a","permalink":"https://laurentperrinet.github.io/publication/franciosini-perrinet-19-cns/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/franciosini-perrinet-19-cns/","section":"publication","summary":"see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.","tags":["deep-learning","sparse coding"],"title":"Modelling Complex Cells of Early Visual Cortex using Predictive Coding","type":"publication"},{"authors":["Hugo Ladret","Nelson Cortes","Frédéric Chavane","Laurent U Perrinet","Christian Casanova"],"categories":null,"content":"Interested in orientation selectivity in V1? at #sfn2019 ? We tested a model getting different precision levels and then tested these predictions in real neurons ! Check out poster 403.16 / P20 @ https://t.co/iHUv0AHuzl -\u0026gt; more info :https://t.co/JkXXgC5IVp\n🤝 @univamu @CNRS pic.twitter.com/MVBz0UGH70\n— laurentperrinet (@laurentperrinet) October 22, 2019   See a followup in Ladret and Perrinet, 2020  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"43a2701fb492339e1389e4695c6ad404","permalink":"https://laurentperrinet.github.io/publication/ladret-19-sfn/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/ladret-19-sfn/","section":"publication","summary":"A key property of the neurons in the primary visual cortex (V1) is their selectivity to oriented stimuli in the visual field. Orientation selectivity allows the segmentation of objects in natural visual scenes, which is the first step in building integrative representations from retinal inputs. As such, V1 has always been of central interest in creating artificial neural networks and the recent years have seen a growing interest in the creation of explainable yet robust and adaptive models of cortical visual processes, for fundamental or applied purposes. One notable challenge for those models is to behave reliably in generic natural environments, where information is usually hidden in noise, while most models are typically studied with oriented gratings. Here we show that a simple biologically inspired neural network accounts for orientation selectivity to natural-like textures in the cat's primary visual cortex. Our spiking neural network (SNN) is made of point neurons organized in recurrent and hierarchical layers based on the structure of cortical layers IV and II/III. We found that Spike-timing plasticity and synaptic recurrence allowed the SNN to self-organize its connections weights and reproduce the activity of neurons recorded with laminar probes in cortical areas 17 and 18 of cats, notably orientation tuning responses. After less than 5 seconds of stimulus presentation, the SNN displays narrow orientation selectivity (bandwidth = 10 degrees) characteristic of sparse representations, removes noise from the input and learns the structure of natural pattern repetitions. Our results support the use of natural stimuli to study theoretical and experimental cortical dynamics. Furthermore, this model encourages using SNNs to reduce complexity in cortical networks as a method to understand the separate contribution of different components in the laminar organization of the cortex. From an applied perspective, the computations this network performs could also be used as an alternative to classical blackbox Deep Learning models used in artificial vision.","tags":["area-v1"],"title":"Orientation selectivity to synthetic natural patterns in a cortical-like model of the cat primary visual cortex","type":"publication"},{"authors":["Victor Boutin","Angelo Franciosini","Frédéric Chavane","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":"If you’re at #sfn2019 and have an interest in #sparse #deep Predictive Coding, checkout @VictorBoutin ‘s poster 403.16 / P20:https://t.co/2VLEsl98oU\nIt shows today + comes with a (timely) preprint https://t.co/FfKi9tjqrN !\n#SfN19 #MachineLearning pic.twitter.com/ep0RrPjzzZ\n— laurentperrinet (@laurentperrinet) October 21, 2019   see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"4e21350d1e0ad26721c379582b2eaf02","permalink":"https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/","section":"publication","summary":"If you’re at #sfn2019 and have an interest in #sparse #deep Predictive Coding, checkout @VictorBoutin ‘s poster 403.16 / P20:https://t.co/2VLEsl98oU\nIt shows today + comes with a (timely) preprint https://t.co/FfKi9tjqrN !","tags":["deep-learning","sparse coding"],"title":"Sparse Deep Predictive Coding to model visual object recognition","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Un article dans The Conversation dont l’objectif est d’être accessible et réutilisable (dans des cours d’introduction aux neurosciences, sciences cognitives, vision, réseaux de neurones, intelligence artificielle). Le flash-lag effect original:  la même chose avec un arrêt:  pour illustrer la fleche du temps (\u0026#34; Or dans tout système, d’après le second principe de la thermodynamique, le désordre mesuré par l’entropie se doit d’augmenter. Voilà pourquoi il existe une asymétrie dans l’écoulement du temps, c’est-à-dire une flèche du temps. Résultat, si l’on filme une partie de billard, on trouvera incongru cette séquence si on la projette dans le sens inverse du temps. “), on peut aussi utiliser cette video d’un bocal qui se brise qu’il est aisé de lire dans le sens inverse du temps:    Neurostories: d’autres videos du flash-lag effect Laurent Perrinet a reçu des financements de l’Agence Nationale de la Recherche (ANR HOR-V1 ANR-17-CE37-0006) et du CNRS (SpikeAI). Cet article n’aurait pas vu le jour sans la journée des Neurostories de la NeuroSchool d’Aix-Marseille Université, ceux qui l’ont fait vivre et parmi eux: François Féron, Alexia Belleville, Jean-Marc Michelangeli, Camille Grasso, Daniele Schön, Anne-Marie François-Bellan, Jennifer Coull, Corine Sombrun et Francis Taulelle.  ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"2b8a77eead249bb4ee4b44c15236a8ad","permalink":"https://laurentperrinet.github.io/publication/perrinet-19-temps/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/perrinet-19-temps/","section":"publication","summary":"Lorsque nous observons un sablier, lorsque nous fixons notre regard sur les grains de sable qui tombent, nous avons le sentiment que le temps s'écoule de façon continue. Nous pensons qu'il en est ainsi depuis la naissance du monde, et que rien ne peut contredire cette vérité universelle. Pourtant, nos perceptions sensorielles et les neurones qui en sont à l'origine ont une toute autre manière de scander le temps. Une manière subjective et sensuelle, au sens propre du terme.","tags":null,"title":"Temps et cerveau : comment notre perception nous fait voyager dans le temps","type":"publication"},{"authors":["Victor Boutin","Angelo Franciosini","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8b176150c405322fb72113cdcdcb9503","permalink":"https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-19-gdr-robotics/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/boutin-franciosini-ruffier-perrinet-19-gdr-robotics/","section":"publication","summary":"The brain has to solve inverse problems to correctly interpret sensory data and infer the set of causes that generated the sensory inputs. When imposing sparse prior and hierarchical structure this problem is called Hierarchical Sparse Coding (HSC). Predictive Coding (PC) is a computational neuroscience framework suggesting that each layer  predicts the activity of the lower layer via feedback connections. The error between predicted and actual response is then sent back to the next higher level via feed-forward connections to correct the estimation of the representation. While computer scientists often solved HSC using a stacking of Lasso sub-problems that we will call Hierarchical Lasso (Hi-La), we propose to leverage PC into a hierarchical and sparse model called Sparse Deep Predictive Coding (SDPC) network. This poster shows computational differences between SDPC and Hi-La.","tags":["deep-learning","sparse coding"],"title":"Top-down connection in Hierarchical Sparse Coding","type":"publication"},{"authors":["Jonathan Vacher","Andrew Isaac Meso","Laurent U Perrinet","Gabriel Peyré"],"categories":null,"content":"","date":1542758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542758400,"objectID":"0cc2b9c7c446eae0899fbb1ae3f80308","permalink":"https://laurentperrinet.github.io/publication/vacher-16/","publishdate":"2018-11-21T00:00:00Z","relpermalink":"/publication/vacher-16/","section":"publication","summary":"A common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model. The present study details the complete formulation of such a generative model intended to probe visual motion perception. It is first derived in a set of axiomatic steps constrained by biological plausibility. We then extend previous contributions by detailing three equivalent formulations of the Gaussian dynamic texture model. First, the composite dynamic textures are constructed by the random aggregation of warped patterns, which can be viewed as 3D Gaussian fields. Second, these textures are cast as solutions to a stochastic partial differential equation (sPDE). This essential step enables real time, on-the-fly, texture synthesis using time-discretized auto-regressive processes. It also allows for the derivation of a local motion-energy model, which corresponds to the log-likelihood of the probability density. The log-likelihoods are finally essential for the construction of a Bayesian inference framework. We use the model to probe speed perception in humans psychophysically using zoom-like changes in stimulus spatial frequency content. The likelihood is contained within the generative model and we chose a slow speed prior consistent with previous literature. We then validated the fitting process of the model using synthesized data. The human data replicates previous findings that relative perceived speed is positively biased by spatial frequency increments. The effect cannot be fully accounted for by previous models, but the current prior acting on the spatio-temporal likelihoods has proved necessary in accounting for the perceptual bias.","tags":["Bayesian model","motion detection","motion-clouds","psychophysics"],"title":"Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"2018-11-09 : “Retinal computations” by Maria José Escobar (Chile) During a seminar at the Institute of Neurosciences Timone in Marseille, María José Escobar, Ph.D. :\n “Retinal computations” : The retina is part of the nervous system and consists in well-organized layers of different cell types and functions. Those cells have been vastly studied in various animal models, and also the circuits conveying to different functional categories. All these different types of either physiological properties or computation equivalents revealed the retina as not a single light to electricity encoder but a pre-processing layer, which is in charge to extract relevant visual signals from the environment that are critical for animal survival. During this seminar, we describe some of the computations performed by the retina, and how this knowledge can be applied to solve engineering problems, such as image processing and robot controllers.\n    ","date":1541721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541721600,"objectID":"a9b689ddc96066063565c8b7ab57d7ba","permalink":"https://laurentperrinet.github.io/post/2018-11-09_seminaire-escobar/","publishdate":"2018-11-09T00:00:00Z","relpermalink":"/post/2018-11-09_seminaire-escobar/","section":"post","summary":"A seminar by [María José Escobar, Ph.D.](http://profesores.elo.utfsm.cl/~mjescobar/) at the Institute of Neurosciences Timone in Marseille.","tags":["events"],"title":"2018-11-09 : Retinal computations","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"La modélisation biomorphique de la perception visuelle in “La modélisation de la genèse physico-mathématique du vivant” BIOMORPHISME ET CREATION ARTISTIQUE – Session 3  Date\n11 Octobre 2018 Atelier\nSéminaire/workshop organisé dans le cadre du projet Biomorphisme. Approches sensibles et conceptuelles des formes du vivant http://lesa.univ-amu.fr/?q=node/391 http://centregranger.cnrs.fr Location\nBâtiment Egger, dans la salle E 215 (2ème étage côté voie ferrée) - 3 avenue R. Schuman - Aix-en-Provence Visuels\nHTML Organisation\nJean Arnaud, PR arts plastiques au LESA-AMU ; Julien Bernard, MCF philosophe des sciences au Centre GG Granger-AMU ; Sylvie Pic, artiste Résumé\nLa vision utilise un faisceau d’informations de différentes qualités pour atteindre une perception unifiée du monde environnant. Elle interagit avec lui en créant son propre modèle génératif de sa structure physico-mathématique. Avec Etienne Rey de l’atelier Ondes Parallèles, nous avons utilisé lors de plusieurs projets art-science (voir https://github.com/NaturalPatterns) des installations permettant de manipuler explicitement des composantes de ce flux d’information et de révéler des ambiguités dans notre perception. Dans l’installation Tropique, des faisceaux de lames lumineuses sont arrangés dans l’espace assombri de l’installation. Les spectateurs les observent grâce à leur interaction avec une brume invisible qui est diffusée dans l’espace. L’ensemble des faisceaux évolue comme autant de lames lumineuses à partir de 6 video-projecteurs placés dans l’espace de l’installation, suivant une dynamique autonome. En même temps, la position des spectateurs est captée et permet d’alterner entre une vision de ces sculptures d’un point de vue introceptif à un point de vue exteroceptif. Dans «Trame Élasticité», 25 parallélépipèdes de miroirs (3m de haut) sont arrangés verticalement sur une ligne horizontale. Ces lames sont rotatives et leurs mouvements est synchronisé. Suivant la dyamique qui est imposé à ces lames, la perception de l’espace environnent fluctue conduisant à recomposer l’espace de la concentration à l’expansion, ou encore à générer un surface semblant transparente ou inverser la visons de ce qui est située devant et derrière l’observateur. Enfin, dans «Trames», nous explorons l’interaction de séries périodiques de points placées sur des surfaces transparentes. À partir de premières expérimentations utilisant une technique novatrice de sérigraphie, ces trames de points sont placées afin de faire émerger des structures selon le point de vue du spectateur. Ce qui est en jeu ici c’est l’émergence de l’apparition de motifs virtuels résultat de la relation entre une réalité physique, la grandeur et l’ordonnancement de trames et notre physiologie qui conduit à cette état de perception. Lorsqu’on est fasse à ces motifs ce qui saute au yeux plus que le motif réel c’est sa résultante, instable et éphémère qui fait apparaitre une richesse de figures géométriques qui se transforment et évoluent en fonction du temps d’observation et du point de vue. Sur ce principe de dispositif optique, le travail de chacun des motifs, lié à un séquençage de trames conduit à faire apparaitre une composition et des émergences de formes spécifiques. L’expérience de perception de chacun des motifs explore les notions d’instabilité, de flux, d’émergences … dont l’expérience donne à entrevoir des formes que l’on retrouve dans la nature ou les phénomènes naturels: le dessin du pelage d’un zèbre, une accumulation de bulles de savons, ou plus généralement dans les compositions chimiques issue de la théorie de la morphogénèse de Turing. De manière générale, nous montrerons ici les différentes méthodes utilisées, comme l’utilisation des limites perceptives, et aussi les résultats apportés par une telle collaboration. Mots-Clés\nart cinétique ; science ; vision ; perception ; modèle interne  ","date":1539282600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539282600,"objectID":"cec549e55afbe5f87cfb70fe82a77e8b","permalink":"https://laurentperrinet.github.io/talk/2018-10-11-bio-morphisme/","publishdate":"2018-10-11T18:30:00Z","relpermalink":"/talk/2018-10-11-bio-morphisme/","section":"talk","summary":"La modélisation biomorphique de la perception visuelle in “La modélisation de la genèse physico-mathématique du vivant” BIOMORPHISME ET CREATION ARTISTIQUE – Session 3  Date\n11 Octobre 2018 Atelier\nSéminaire/workshop organisé dans le cadre du projet Biomorphisme.","tags":null,"title":"La modélisation biomorphique de la perception visuelle","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"FÊTE DE LA SCIENCE 2018 : Alcazar / MERLAN L’Association Polly Maggoo http://www.pollymaggoo.org/ met en place tout le long de l’année, des actions de culture scientifique et artistique en direction du grand public et des lycées, au cours desquelles l’association programme des films à caractère scientifique. Les projections se déroulent en présence de chercheurs et/ou de cinéastes dans la perspective d’un développement de la culture cinématographique et scientifique en direction des publics scolaires. Le samedi 6 octobre et le mercredi 10 octobre, je suis venu échanger au côté de Serge Dentin autour de films traitant du rapport fiction/réel, des illusion visuelles (\u0026#34; Qu’est ce qu’une image? “), des rapports d’échelles, de la perception, … et qui sont projetés lors de la séance, avec tout public (samedi) ou des élèves de lycée (mercredi). Une occasion aussi de parler du métier de chercheur.\n Date\n6 octobre 2018 Location\nbibliothèque de l’Alcazar (BMVR), Marseille Programmation\nSAMSUNG GALAXY de Romain CHAMPALAUNE (France, 2015), documentaire-fiction, 7′ / LA DRÔLE DE GUERRE D’ALAN TURING de Denis VAN WAEREBEKE (France, 2014), documentaire, 60’ URL\nhttp://pollymaggoo.org/fete-de-la-science-2018-alcazar-bmvr/ Date\n10 octobre 2018 Location\nbibliothèque du Merlan, Marseille Programmation\nSAMSUNG GALAXY de Romain CHAMPALAUNE (France, 2015), documentaire-fiction, 7′ / “JE TE SUIS (JAG FÖLJER DIG)” / “OS Love_EN” / “BIG DATA, BIG BUSINESS” / COPIER-CLONER / et en bonus “The Centrifuge Brain Project, A Short Film by Till Nowak”  ","date":1539196200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539196200,"objectID":"eff12f045da400b4f922413fbaa1ff49","permalink":"https://laurentperrinet.github.io/talk/2018-10-10-polly-maggoo/","publishdate":"2018-10-10T18:30:00Z","relpermalink":"/talk/2018-10-10-polly-maggoo/","section":"talk","summary":"FÊTE DE LA SCIENCE 2018 : Alcazar / MERLAN L’Association Polly Maggoo http://www.pollymaggoo.org/ met en place tout le long de l’année, des actions de culture scientifique et artistique en direction du grand public et des lycées, au cours desquelles l’association programme des films à caractère scientifique.","tags":null,"title":"Intervention fête de la science 2018","type":"talk"},{"authors":["Jean-Bernard Damasse","Laurent U Perrinet","Laurent Madelain","Anna Montagnini"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"26b8a2fc0cacfa12844d3cc81e66a163","permalink":"https://laurentperrinet.github.io/publication/damasse-18/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/damasse-18/","section":"publication","summary":"","tags":["eye movements"],"title":"Reinforcement effects in anticipatory smooth eye movements","type":"publication"},{"authors":null,"categories":null,"content":"In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 …\nTip 2 …\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://laurentperrinet.github.io/tutorial/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I’ll share my top 10 tips for getting started with Academic:\nTip 1 …\nTip 2 …","tags":null,"title":"Example Page","type":"docs"},{"authors":["Laurent U Perrinet","Chloé Pasturel","Anna Montagnini"],"categories":null,"content":" See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    previous talk @ LAW, Lyon previous talk @ INVIBE FEST, Paris next talk @ LACONEU, Chile next talk @ CAUSAL Kick-off, Marseille next talk @ NeuroFrance, Marseille  ","date":1522936800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522936800,"objectID":"839508cf4cbdeb199d2539fcb76ababe","permalink":"https://laurentperrinet.github.io/talk/2018-04-05-bcp-talk/","publishdate":"2018-04-05T14:00:00Z","relpermalink":"/talk/2018-04-05-bcp-talk/","section":"talk","summary":"See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.","tags":null,"title":"Principles and psychophysics of Active Inference in anticipating a dynamic, switching probabilistic bias","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"   Probabilities and Optimal Inference to understand the Brain a 2-day workshop at the Institute of Neurosciences Timone in Marseille     Date April 5-6th 2018 Location  Institute of Neurosciences Timone in Marseille in the south of France\n Main site  https://opt-infer-brain.sciencesconf.org/\n Full program  https://opt-infer-brain.sciencesconf.org/program/details.\n Organizing committee Paul Apicella, Frederic Danion, Nicole Malfait, Anna Montagnini and Laurent Perrinet     ","date":1522886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522886400,"objectID":"818a91c1040cae9047160c744a1fd20a","permalink":"https://laurentperrinet.github.io/post/2018-04-05_optimal-inference-brain-workshop/","publishdate":"2018-04-05T00:00:00Z","relpermalink":"/post/2018-04-05_optimal-inference-brain-workshop/","section":"post","summary":"We organize a Symposium at NeuroFrance 2019 entitled Active Inference: Bridging theoretical and experimental neurosciences. This is part of a series of theoretical neuroscience symposia organized in this international conference from the french Neursocience Society.","tags":["events","probalistic-inference"],"title":"2018-04-05 : *Probabilities and Optimal Inference to understand the Brain* Workshop","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1522069200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522069200,"objectID":"fa7c6aef44d8cdf7b8a3bccd2850f603","permalink":"https://laurentperrinet.github.io/talk/2018-03-26-cours-neuro-comp-fep/","publishdate":"2018-03-26T13:00:00Z","relpermalink":"/talk/2018-03-26-cours-neuro-comp-fep/","section":"talk","summary":"","tags":null,"title":"Probabilities, Bayes and the Free-energy principle","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"PhD Program: course in Computational Neuroscience Context\nComputational neuroscience is an expending field that is proving to be essential in neurosciences. The aim of this course will be to provide a common solid background in computational neurosciences. The course will comprise historical recall of the field and a description of the different modelling approaches that are currently developed, including details about their specificities, limits and advantages.\nObjective\nThe course aims at introducing students with the major tools that will be necessary during their thesis to model or analyze their neuroscientific results. While it will start by a short, generic introduction, we will then explore different systems at different scales. On the first day, we will study the different possible regimes in which a single neuron can behave, while progressively introducing the theory of dynamical systems to understand these more globally. Then, during the second day, we will introduce methods to analyze neuroscientific data in general, such as Bayesian methods and information theory. This will be implemented by simple practical examples.\nLanguage of intervention\nEnglish\nNumber of hours\n~20 hours (session 1=7 + session 2=7 + session 3=4)\nMax participants\n15 for the practical sessions (afternoon Day 2 and Day 3), unlimited for theoretical courses\nPublic priority\nPhD students\nPublic concerned\nPhD students, interested M2 students and postdocs\nLocation\nInstitut des Neurosciences de la Timone (INT)\nKeywords\nneuronal modelling, neural circuit modelling, information theory, decoding and encoding\nTargets\nUnderstanding how computational modelling can be used to formulate and solve neuroscience problems at different spatial and temporal scales; learning the formal notions of information, encoding and decoding and experimenting their use on toy datasets\nProgram\nFirst session: Introduction to modeling single neurons (morning); An introduction to neural masses: modeling assemblies of neurons up to capturing collective oscillations and resting state dynamics in a mean-field model - presentation of the Virtual Brain software (afternoon) - Second session: An overview on “What is encoding?” “What is decoding?”: formalization of the notion of information in neural activity; shared and transferred information; integration, segregation and complexity (morning). Bayesian probabilities, the Free-energy principle and Active Inference, with practical demonstrations in python (afternoon). Third session: the problem of information estimation in practice. Practical exercices in Matlab: estimating entropy and stimulus decodability from spike trains; comparing coding hypotheses (morning).\nPre-required\nBasic knowledge of statistics and probability and calculus (differential equations,…) is useful, but steps will be explained and complex math avoided as much as possible. Practical exercises are in python and/or MATLAB, so basic knowledge of these environments is a plus.\nprogram day 1 : 2018-03-26 : an introduction to Computational Neuroscience   09:30-12:30 = Introduction to modeling single neurons (LaP)\n  14:00-17:00 = An introduction to neural masses: modeling assemblies of neurons up to capturing resting state dynamics in a mean-field model - presentation of the Virtual Brain software (DaB)\n  day 2 : 2018-03-27 : Information theory / bayesian models   09:15-10:30 = An overview on “What is encoding?” “What is decoding?”: formalization of the notion of information in neural activity (DaB)\n  11:00-12:15 = (…continued after the coffee break: ) Live information! From sharing information to transferring information (and a glimpse into the zoo of higher-order friends) (DaB)\n  14:00-17:10 = Probabilities, the Free-energy principle and Active Inference (LuP).\n  day 3 : 2018-03-28 : Practical course on Information theory  09:30-12:30 = Practical course on Information theory (DaB)  More material related to the course day 1 - morning : the single neuron   site du livre de Gerstner et al “Neuronal Dynamics”: http://neuronaldynamics.epfl.ch/\n  A (longer) introduction to the Hodgkin-Huxley model in three steps by Dr Stefano Luccioli\n  http://neuro.fi.isc.cnr.it/uploads/TALKS/lez1.pdf\n  http://neuro.fi.isc.cnr.it/uploads/TALKS/lez2.pdf\n  http://neuro.fi.isc.cnr.it/uploads/TALKS/lez3.pdf\n    An interactive course with Wulfram Gerstner https://www.edx.org/course/neuronal-dynamics-computational-epflx-bio465-1x\n  His book ONLINE http://cn.epfl.ch/~gerstner/NeuronalDynamics-MOOC1.html\n  day 1 - afternoon : neural mass models   Another interactive course @ Washington University https://www.coursera.org/course/compneuro\n  Collection of didactic material for the EU FP7 ITN Neural Engineering Transformative Technology http://www.neural-engineering.eu/training/index.html\n  Didactic material from Lab in Computational Neuroscience http://neuro.fi.isc.cnr.it/index.php?page=didactic-material\n  A open source simulator of a whole brain which runs on your laptop, “The Virtual …","date":1522022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522022400,"objectID":"70fe469e0be591ed747d286985d2714c","permalink":"https://laurentperrinet.github.io/post/2018-03-26-cours-neuro-comp-fep/","publishdate":"2018-03-26T00:00:00Z","relpermalink":"/post/2018-03-26-cours-neuro-comp-fep/","section":"post","summary":"PhD Program: course in Computational Neuroscience.","tags":["events"],"title":"2018-03-26 : PhD Program: course in Computational Neuroscience","type":"post"},{"authors":["Laurent U Perrinet","Chloé Pasturel","Anna Montagnini"],"categories":null,"content":" See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    previous talk @ LAW, Lyon next talk @ Brain workshop, Marseille next talk @ LACONEU, Chile next talk @ CAUSAL Kick-off, Marseille next talk @ NeuroFrance, Marseille  ","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"4e8bf1408568147693b31da10683e0c8","permalink":"https://laurentperrinet.github.io/talk/2018-02-01-bcp-invibe-fest/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/talk/2018-02-01-bcp-invibe-fest/","section":"talk","summary":"See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.","tags":null,"title":"Estimating and anticipating a dynamic probabilistic bias in visual motion direction","type":"talk"},{"authors":["Laurent U Perrinet","Étienne Rey"],"categories":null,"content":"Meetup Art et Neurosciences  Quoi\nMeetup Art et Neurosciences Qui\nAssociation NeuroNautes Quand\n25 Janvier 2018 Où\nSalle des voutes campus Saint Charles Support visuel\nhttps://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html (notes: la présentation peut mettre un certain temps à charger. Une fois que le titre apparait, appuyer sur la touche “F” pour mettre en plein écran)   Elasticité dynamique est composée des pièces Expansion, Trame et Lignes sonores. Volume hexagonal en miroir de 7 mètres de diamètre, Expansion fonctionne comme une chambre d’écho. A l’intérieur de ce volume se situe Trame. Constituée de 25 lames de miroir en rotation, cette pièce réoriente continuellement le regard. Quant à Lignes sonores, elle est formée de quatre monolithes orientés vers Expansion et émet des sons qui se réorientent en fonction du mouvement des lames. (© Etienne Rey, Adagp Paris    ","date":1516905000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516905000,"objectID":"6c5991b7ec7db2470e754d0f4bca4833","permalink":"https://laurentperrinet.github.io/talk/2018-01-25-meetup-neuronautes/","publishdate":"2018-01-25T18:30:00Z","relpermalink":"/talk/2018-01-25-meetup-neuronautes/","section":"talk","summary":"Meetup Art et Neurosciences  Quoi\nMeetup Art et Neurosciences Qui\nAssociation NeuroNautes Quand\n25 Janvier 2018 Où\nSalle des voutes campus Saint Charles Support visuel\nhttps://laurentperrinet.github.io/sciblog/files/2018-01-25_meetup-neuronautes.html (notes: la présentation peut mettre un certain temps à charger.","tags":["Biologically Inspired Computer vision"],"title":"Expériences autour de la perception de la forme en art et science","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" poster presented @ [[https://gdrvision2018.sciencesconf.org/|GDR vision, Paris]]. program : https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf poster : https://github.com/laurentperrinet/Perrinet18gdr/raw/master/Perrinet18gdr.pdf poster (code) : https://github.com/laurentperrinet/Perrinet18gdr/ source code for this framework: https://github.com/laurentperrinet/CatchTheEye  ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"6573bf2d6c2a5a33ed3f6a53d2948ff9","permalink":"https://laurentperrinet.github.io/publication/perrinet-18-gdr/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/perrinet-18-gdr/","section":"publication","summary":"Recording eye movements is a technique that attracts an increasing number of scientists, but also in the general public. Indeed, this allows to quantitatively measure a number of useful dimensions of perception and behavior in general. However, most existing trackers rely on expensive or technically complex solutions. Here, we propose a simple framework to record eye movements using any camera, such as a webcam. As a proof of concept, the recorded image is processed in real-time to detect from a simple sub-set of eye movements : left, center, right or blink. The processing is based on two stages. First, we use a pre-trained computer vision algorithm to extract the image of the face. Second, we used a classical deep-learning architecture to learn to classify these sub-images. This network is a 3 layered convolutional neural network, for which we optimized performance as measured by the accuracy with cross-validation on a wide range of the network's hyper-parameters. Over a dataset of more than 1000 images, this network achieves an average accuracy of approximately 97 percent. We also provide with an integration with the psychopy library which shows that frames can be processed on a standard laptop at a rate of approximately 25 Hz.","tags":["deep-learning","motion anticipation"],"title":"A low-cost, accessible eye tracking framework","type":"publication"},{"authors":["Chloé Pasturel","Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":" see a write-up in “Humans adapt their anticipatory eye movements to the volatility of visual motion properties” as presented at https://eyemovements.sciencesconf.org/ get the poster code : https://github.com/invibe/ANEMO/  ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"7a0358a28d771c41f8d83c636f1103d3","permalink":"https://laurentperrinet.github.io/publication/pasturel-18-anemo/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/pasturel-18-anemo/","section":"publication","summary":" see a write-up in “Humans adapt their anticipatory eye movements to the volatility of visual motion properties” as presented at https://eyemovements.sciencesconf.org/ get the poster code : https://github.com/invibe/ANEMO/  ","tags":["motion anticipation"],"title":"ANEMO: Quantitative tools for the ANalysis of Eye MOvements","type":"publication"},{"authors":["Laurent U Perrinet","Chloé Pasturel","Anna Montagnini"],"categories":null,"content":" see a write-up in “Humans adapt their anticipatory eye movements to the volatility of visual motion properties” as presented at https://eyemovements.sciencesconf.org/ get the poster code : https://github.com/chloepasturel/AnticipatorySPEM/  ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"f6c68beb1b5da8bb5034d852976f029d","permalink":"https://laurentperrinet.github.io/publication/pasturel-18-grenoble/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/pasturel-18-grenoble/","section":"publication","summary":"Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high*resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye*to*target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate \\\"how much they are confident that the target will move to the right or left in the next trial\\\"  and to adjust the cursor's position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random*direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch*up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile).","tags":["motion anticipation"],"title":"Estimating and anticipating a dynamic probabilistic bias in visual motion direction","type":"publication"},{"authors":["Chloé Pasturel","Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":" see a write-up in “Humans adapt their anticipatory eye movements to the volatility of visual motion properties”  ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"66f5a2c510e4f723453f38c17e094f91","permalink":"https://laurentperrinet.github.io/publication/pasturel-18/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/pasturel-18/","section":"publication","summary":"Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high-resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye*to*target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp*motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate \\\"how much they are confident that the target will move to the right or left in the next trial\\\"  and to adjust the cursor's position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch*up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile).","tags":["motion anticipation"],"title":"Estimating and anticipating a dynamic probabilistic bias in visual motion direction","type":"publication"},{"authors":["Victor Boutin","Angelo Franciosini","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" accepted submission @ iTWIST: international Traveling Workshop on Interactions between low-complexity data models and Sensing Techniques, 21 - 23 November​, 2018 poster session scheduled on Thursday, November 22th, from 10h30 till 12h00. CIRM, Marseille, France.  get the full proceedings Poster as PDF see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"03fdaa73568d78beffa4372b0ceae4d9","permalink":"https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-itwist/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/boutin-franciosini-ruffier-perrinet-18-itwist/","section":"publication","summary":"The formation of connections between neural cells is essentially emerging from an unsupervised learning process. During the development of primary visual cortex (V1) of mammals, for example, one may observe the emergence of cells selective to localized and oriented features. This leads to the development of a rough contour-based representation of the retinal image in area V1. We modeled the formation of this representation along the thalamo-cortical pathway using a sparse unsupervised learning algorithm in a hierarchical network. This algorithm alternates (i) a coding phase to encode the information and (ii) a learning phase to find the proper encoder (also called dictionary). We replicated and adapted the Multi-Layer Convolutional Sparse Coding (ML-CSC) model from Michael Elad's group.  As an application, we have trained our implementation on a database containing images from faces. The extracted features show similarities with some of the neuron's receptive field found in V1 and beyond. Furthermore, our results demonstrate the potential application of such a strategy to the fast classification of images, for example in hierarchical and dynamical architectures.","tags":["deep-learning","sparse coding"],"title":"From biological vision to unsupervised hierarchical sparse coding","type":"publication"},{"authors":["Julien Dupeyroux","Victor Boutin","Julien R Serres","Laurent U Perrinet","Stéphane Viollet"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"559cfb037bc9509a1596fe0097482e4a","permalink":"https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/dupeyroux-boutin-serres-perrinet-viollet-18/","section":"publication","summary":"This paper presents for the first time the embedded stand-alone version of the bio-inspired M2APix (Michaelis-Menten auto-adaptive pixels) sensor as a ventral optic flow sensor to endow glider-type unmanned aerial vehicles with autonomous landing ability. Assuming the aircraft is equipped with any reliable speed measurement system such as a global positioning system or an inertial measurement unit, we can use the velocity of the glider to determine with high precision its height while landing. This information is robust to different outdoor lighting conditions and over different kinds of textured ground, a crucial property to control the landing phase of the aircraft.","tags":["Biologically Inspired Computer vision"],"title":"M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation","type":"publication"},{"authors":["Angelo Franciosini","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"eaa303ad04a603870c98601405c6e30c","permalink":"https://laurentperrinet.github.io/publication/franciosini-perrinet-18-cs/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/franciosini-perrinet-18-cs/","section":"publication","summary":" It is widely assumed that visual processing follows a forward sequence of processing steps along a hierarchy of laminar sub-populations of the neural system. Taking the example of the early visual system of mammals, most models are consequently organized in layers from the retina to visual cortical areas, until a decision is taken using the representation that is formed in the highest layer. Typically, features of higher complexity (position, orientation, size, curvature, ...) are successively extracted in distinct layers (Carandini, 2012). This is prevalent in most deep learning algorithms and stems from a long history of feed-forward architectures. Though this proved to be highly successful, the origin of such architectures is not known i̧teSerre07. Using a generic unsupervised learning algorithm, we first trained a simple one-layer convolutional neural network on a seta of natural images with a growing number of neurons. By doing this, we could quantitatively manipulate the complexity of the representation that emerges from such learning and analyze if sub-populations within the layer could be grouped by their similarity, hence justifying the emergence of a hierarchical processing. As shown in previous studies (Olshausen, 1996), such an algorithm converges to a weight matrix that has strong analogies with the receptive fields of simple cells located in the Primary Visual Cortex of mammals (V1).  This result extends naturally to a cortical representation of the input image that encodes second-order features (edges) as neural responses arranged in a three-dimensional space, where the third dimension can be seen as a model of hyper-columns of the Primary Visual Cortex. From this bio-inspired encoding, we were able to define contours in images as simple smooth trajectories in a cortical representation space. This simple model shows that hierarchical processing may originate from the neural encoding of different visual transformations within natural images: respectively translation, rotations and zooms, which correspond to rigid translation in the cortical space. The model can be further extended to reproduce the effect of complex cells in V1 (max pooling) and feedback signals from higher cortical areas. We predict that invariance to more complex transformation like shearing (perspective) and viewpoint changes (looming) will emerge as these additional steps in sensory processing are taken into account.  Indeed, a higher level of complexity can be introduced as the cortical representation is extended from smooth trajectories (space domain) to smooth surfaces (space-time domain). As such, this justifies the extension of a simple sparse network formalism to translation invariant neural networks (such as the convolutional neural networks used in deep learning) that is able to generalize geometrical transformations, such as translation, rotations, and zooms, in an invariant bio-inspired representation (Perrinet, 2015). This should provide some key insights into higher-order features such as co-occurrences, but also to novel categorization architectures. Indeed, such features were recently found to be sufficient to allow the categorization of images containing an animal (Perrinet and Bednar, 2015). Crucially, as the geometrical transformations develop in time, we expect that the detection of these features is made robust by dynamical processes.","tags":["deep-learning","sparse coding"],"title":"On the Origins of Hierarchy in Visual Processing","type":"publication"},{"authors":["Hugo Ladret","Laurent U Perrinet"],"categories":null,"content":" poster présenté au GDR vision, Paris. program : https://gdrvision2018.sciencesconf.org/data/pages/posters_GDRVision2018.pdf Poster (pdf) code : https://github.com/hugoladret/InternshipM1  ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"ed0340c8bcb3d1a047d8c2f4b3ee3309","permalink":"https://laurentperrinet.github.io/publication/ladret-18-gdr/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/ladret-18-gdr/","section":"publication","summary":"The selectivity of the visual system to oriented patterns is very well documented in a wide range of species, especially in mammals. In particular, neurons of the primary visual cortex are anatomically grouped by their preference to a given oriented visual stimulus. Interactions between such groups of neurons have been successfully modeled using recurrently-connected network of spiking neurons, so called \\\"ring models\\\". Nonetheless, this selectivity is most often studied with crystal-like patterns such as gratings. Here, we studied the ability of human observers to discriminate texture-like patterns for which we could quantitatively tune the precision of their oriented content and we propose a generic model to explain such results. The first contribution shows that the discrimination threshold as a function of the precision did not vary smoothly as would be expected, but more in a binary, \\\"all or none\\\" fashion. Our second contribution is to propose a novel model of orientation selectivity that is based on deep-learning techniques, which performance we evaluated in the same task. This model has human-like performance in term of accuracy and exhibits qualitatively similar psychophysical curves. One hypothesis that such a structure allows for the system to be robust to noise in its visual inputs.","tags":["area-v1","deep-learning","orientation","psychophysics"],"title":"Selectivity to oriented patterns of different precisions","type":"publication"},{"authors":["Kiana Mansour-Pour","Nikos Gekas","Pascal Mamassian","Laurent U Perrinet","Anna Montagnini","Guillaume S Masson"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"b9c63d00934a3a1c7c1ee7255795d611","permalink":"https://laurentperrinet.github.io/publication/mansour-18-vss/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/mansour-18-vss/","section":"publication","summary":"It is still not fully understood how visual system integrates motion energy across different spatial and temporal frequencies to build a coherent percept of the global motion under the complex, noisy naturalistic conditions. We addressed this question by manipulating local speed variability distribution (i. e. speed bandwidth) using a well-controlled class of broadband random-texture stimuli called Motion Clouds (MCs) with continuous naturalistic spatiotemporal frequency spectra (Sanz-Leon et al., 2012, ; Simoncini et al., 2012). In a first 2AFC experiment on speed discrimination, participants had to compare the speed of a broad speed bandwidth MC (range: 0.05-8$,^∘$/s) moving at 1 of 5 possible mean speeds (ranging from 5 to 13 $,^∘$/s) to that of another MC with a small speed bandwidth (SD: 0.05 $,^∘$/s), always moving at a mean speed of 10$,^∘$/s . We found that MCs with larger speed bandwidth (between 0.05-0.5$,^∘$/s) were perceived moving faster. Within this range, speed uncertainty results in over-estimating stimulus velocity. However, beyond a critical bandwidth (SD: 0.5 $,^∘$/s), perception of a coherent speed was lost. In a second 2AFC experiment on direction discrimination, participants had to estimate the motion direction of moving MCs with different speed bandwidths. We found that for large band MCs participant could no longer discriminate motion direction. These results suggest that when increasing speed bandwidth from small to large range, the observer experiences different perceptual regimes. We then decided to run a Maximum Likelihood Difference Scaling (Knoblauch \u0026 Maloney, 2008) experiment with our speed bandwidth stimuli to investigate these different possible perceptual regimes. We identified three regimes within this space that correspond to motion coherency, motion transparency and motion incoherency. These results allow to further characterize the shape of the interactions kernel observed between different speed tuned channels and different spatiotemporal scales (Gekas et al ., 2017) that underlies global velocity estimation.","tags":["motion detection"],"title":"Speed uncertainty and motion perception with naturalistic random textures","type":"publication"},{"authors":["Victor Boutin","Angelo Franciosini","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"7c46e53e913046689b8e874a1a2b68fd","permalink":"https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-18-doctoral-day/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/boutin-franciosini-ruffier-perrinet-18-doctoral-day/","section":"publication","summary":" The brain has to solve inverse problems to correctly interpret sensory data and infer the set of causes that generated the sensory inputs. Such a problem is typically ill-posed, and thus requires constraint the narrow down the number of solutions. Predictive coding (PC)is a computational neuroscience framework that finds the most likely causes for the sensory input by minimizing the mismatch between the sensory data and the predicted input. Such a framework could be used to build sparse hierarchical internal representations of a given input.","tags":["deep-learning","sparse coding"],"title":"Unsupervised Hierarchical Sparse Coding algorithm inspired by Biological Vision","type":"publication"},{"authors":["Victor Boutin","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" see a related work describing SDPC in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1511528400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511528400,"objectID":"011aba6e7c12aacce6e65b1a90b2ceae","permalink":"https://laurentperrinet.github.io/talk/2017-11-24-neurosciences-robotique/","publishdate":"2017-11-24T13:00:00Z","relpermalink":"/talk/2017-11-24-neurosciences-robotique/","section":"talk","summary":"see a related work describing SDPC in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.","tags":["sparse coding"],"title":"Unsupervised learning applied to robotic vision","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"FESTIVAL INTERFÉRENCES​ Cinéma Documentaire et Débat Public    FESTIVAL INTERFÉRENCES​  Le collectif Scènes Publiques composé de citoyens, chercheurs et cinéastes, organise la deuxième édition du Festival Interférences du 8 au 18 novembre 2017 à Lyon. J’ai eu la chance de pouvoir participer au jury autour de documentaires avec un regard scientifiques. Une occasion aussi de parler du métier de chercheur.\n Date\n17 et 18 Novembre 2017 Location\nLyon Programmation\nhttp://www.lacitedoc.com/interferences-programmation  ","date":1510943400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510943400,"objectID":"e44ef1d91774ad4e2641d4e6c6ccb427","permalink":"https://laurentperrinet.github.io/talk/2017-11-17-festival-interferences/","publishdate":"2017-11-17T18:30:00Z","relpermalink":"/talk/2017-11-17-festival-interferences/","section":"talk","summary":"FESTIVAL INTERFÉRENCES​ Cinéma Documentaire et Débat Public    FESTIVAL INTERFÉRENCES​  Le collectif Scènes Publiques composé de citoyens, chercheurs et cinéastes, organise la deuxième édition du Festival Interférences du 8 au 18 novembre 2017 à Lyon.","tags":null,"title":"Participation au jury","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"References  unsupervised learning : Perrinet (2010) Biologically inspired computer vision supervised learning : https://www.nature.com/articles/srep11400 (more info) dynamics: Khoei et al (2017) - http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005068 ( more info )  ","date":1510750800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510750800,"objectID":"245a573fb4b294b4a9ab406b3ab75468","permalink":"https://laurentperrinet.github.io/talk/2017-11-15-colloque-master/","publishdate":"2017-11-15T13:00:00Z","relpermalink":"/talk/2017-11-15-colloque-master/","section":"talk","summary":"This seminar is an exercise to introduce the AMU masters into the format of international conferences. As such, we will try to introduce new concepts and results which will not be found in textbooks.","tags":null,"title":"What dynamic neural codes for efficient visual processing","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1498654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498654800,"objectID":"ccb6df4a5b4bbeccf18d3e1e82d16e5e","permalink":"https://laurentperrinet.github.io/talk/2017-06-28-telluride/","publishdate":"2017-06-28T13:00:00Z","relpermalink":"/talk/2017-06-28-telluride/","section":"talk","summary":"","tags":null,"title":"Back to the present: dealing with delays in biological and neuromorphic systems","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1498654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498654800,"objectID":"02783b93a8f78b8feaf49f74ff4f16d2","permalink":"https://laurentperrinet.github.io/talk/2017-06-30-telluride/","publishdate":"2017-06-28T13:00:00Z","relpermalink":"/talk/2017-06-30-telluride/","section":"talk","summary":"","tags":null,"title":"Tutorial on predictive coding","type":"talk"},{"authors":["Mina A Khoei","Guillaume S Masson","Laurent U Perrinet"],"categories":null,"content":" Press release  Visual illusions: their origin lies in prediction    Flash-Lag Effect. When a visual stimulus moves along a continuous trajectory, it may be seen ahead of its veridical position with respect to an unpredictable event such as a punctuate flash. This illusion tells us something important about the visual system: contrary to classical computers, neural activity travels at a relatively slow speed. It is largely accepted that the resulting delays cause this perceived spatial lag of the flash. Still, after several decades of debates, there is no consensus regarding the underlying mechanisms.  Researchers from the Timone Institute of Neurosciences bring a new theoretical hypothesis on a visual illusion discovered at the beginning of the 20th century. This illusion remained misunderstood while it poses fundamental questions about how our brains represent events in space and time. This study published on January 26, 2017 in the journal PLOS Computational Biology, shows that the solution lies in the predictive mechanisms intrinsic to the neural processing of information. New Research: The Flash-Lag Effect as a Motion-Based Predictive Shift https://t.co/K3KWPO8l4a Khoei et al. #vision #motion #neuralnetworks pic.twitter.com/RElm4Qqo58\n— PLOS Comp Biol (@PLOSCompBiol) February 8, 2017  Visual illusions are still popular: in a quasi-magical way, they can make objects appear where they are not expected… They are also excellent opportunities to question the constraints of our perceptual system. Many illusions are based on motion, such as the flash-lag effect. Observe a luminous dot that moves along a rectilinear trajectory. If a second light dot is flashed very briefly just above the first, the moving point will always be perceived in front of the flash while they are vertically aligned.   Fig 2. Diagonal Markov chain. In the current study, the estimated state vector z = {x, y, u, v} is composed of the 2D position (x and y) and velocity (u and v) of a (moving) stimulus. (A) First, we extend a classical Markov chain using Nijhawan’s diagonal model in order to take into account the known neural delay τ: At time t, information is integrated until time t − τ, using a Markov chain and a model of state transitions p(zt|zt−δt) such that one can infer the state until the last accessible information p(zt−τ|I0:t−τ). This information can then be “pushed” forward in time by predicting its trajectory from t − τ to t. In particular p(zt|I0:t−τ) can be predicted by the same internal model by using the state transition at the time scale of the delay, that is, p(zt|zt−τ). This is virtually equivalent to a motion extrapolation model but without sensory measurements during the time window between t − τ and t. Note that both predictions in this model are based on the same model of state transitions. (B) One can write a second, equivalent “pull” mode for the diagonal model. Now, the current state is directly estimated based on a Markov chain on the sequence of delayed estimations. While being equivalent to the push-mode described above, such a direct computation allows to more easily combine information from areas with different delays. Such a model implements Nijhawan’s “diagonal model”, but now motion information is probabilistic and therefore, inferred motion may be modulated by the respective precisions of the sensory and internal representations. (C) Such a diagonal delay compensation can be demonstrated in a two-layered neural network including a source (input) and a target (predictive) layer [44]. The source layer receives the delayed sensory information and encodes both position and velocity topographically within the different retinotopic maps of each layer. For the sake of simplicity, we illustrate only one 2D map of the motions (x, v). The integration of coherent information can either be done in the source layer (push mode) or in the target layer (pull mode). Crucially, to implement a delay compensation in this motion-based prediction model, one may simply connect each source neuron to a predictive neuron corresponding to the corrected position of stimulus (x + v ⋅ τ, v) in the target layer. The precision of this anisotropic connectivity map can be tuned by the width of convergence from the source to the target populations. Using such a simple mapping, we have previously shown that the neuronal population activity can infer the current position along the trajectory despite the existence of neural delays.  Processing visual information takes time and even if these delays are remarkably short, they are not negligible and the nervous system must compensate them. For an object that moves predictably, the neural network can infer its most probable position taking into account this processing time. For the flash, however, this prediction can not be established because its appearance is unpredictable. Thus, while the two targets are aligned on the retina at the time of the flash, the position of the moving object is …","date":1485388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485388800,"objectID":"bfd147642e8504d567d1d4628f16966a","permalink":"https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/","publishdate":"2017-01-26T00:00:00Z","relpermalink":"/publication/khoei-masson-perrinet-17/","section":"publication","summary":"Due to its inherent neural delays, the visual system has an outdated access to sensory information about the current position of moving objects. In contrast, living organisms are remarkably able to track and intercept moving objects under a large range of challenging environmental conditions. Physiological, behavioral and psychophysical evidences strongly suggest that position coding is extrapolated using an explicit and reliable representation of object's motion but it is still unclear how these two representations interact. For instance, the so-called flash-lag effect supports the idea of a differential processing of position between moving and static objects. Although elucidating such mechanisms is crucial in our understanding of the dynamics of visual processing, a theory is still missing to explain the different facets of this visual illusion. Here, we reconsider several of the key aspects of the flash-lag effect in order to explore the role of motion upon neural coding of objects' position. First, we formalize the problem using a Bayesian modeling framework which includes a graded representation of the degree of belief about visual motion. We introduce a motion-based prediction model as a candidate explanation for the perception of coherent motion. By including the knowledge of a fixed delay, we can model the dynamics of sensory information integration by extrapolating the information acquired at previous instants in time. Next, we simulate the optimal estimation of object position with and without delay compensation and compared it with human perception under a broad range of different psychophysical conditions. Our computational study suggests that the explicit, probabilistic representation of velocity information is crucial in explaining position coding, and therefore the flash-lag effect. We discuss these theoretical results in light of the putative corrective mechanisms that can be used to cancel out the detrimental effects of neural delays and illuminate the more general question of the dynamical representation of spatial information at the present time in the visual pathways.","tags":["Bayesian model","motion prediction","psychophysics"],"title":"The flash-lag effect as a motion-based predictive shift","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1484909100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484909100,"objectID":"a823b73a1499766ad4e4c538151477c3","permalink":"https://laurentperrinet.github.io/talk/2017-01-20-laconeu/","publishdate":"2017-01-20T10:45:00Z","relpermalink":"/talk/2017-01-20-laconeu/","section":"talk","summary":"","tags":null,"title":"Tutorial: Active inference for eye movements: Bayesian methods, neural inference, dynamics","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1484822700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484822700,"objectID":"16849b832a320a43870b886e3417b0d4","permalink":"https://laurentperrinet.github.io/talk/2017-01-19-laconeu/","publishdate":"2017-01-19T10:45:00Z","relpermalink":"/talk/2017-01-19-laconeu/","section":"talk","summary":"","tags":["Biologically Inspired Computer vision"],"title":"Tutorial: Sparse optimization in neural computations","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"   ","date":1484730000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484730000,"objectID":"b5b2261e3d5e714f398d8b91f9719733","permalink":"https://laurentperrinet.github.io/talk/2017-01-18-laconeu/","publishdate":"2017-01-18T09:00:00Z","relpermalink":"/talk/2017-01-18-laconeu/","section":"talk","summary":"   ","tags":null,"title":"Back to the present: how neurons deal with delays","type":"talk"},{"authors":["Victor Boutin","Angelo Franciosini","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    more about the role of top-down connections:  Victor Boutin, Angelo Franciosini, Franck Ruffier, Laurent U Perrinet  (2020). Effect of top-down connections in Hierarchical Sparse Coding. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"189b14135ac826959e97f0c00f210079","permalink":"https://laurentperrinet.github.io/publication/boutin-franciosini-ruffier-perrinet-17-doctoral-day/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/boutin-franciosini-ruffier-perrinet-17-doctoral-day/","section":"publication","summary":"Improve performances of existing recognition computer vision algorithms with biological concepts. The gain are expected in the following: Recognition latency and accuracy (faster and better), less data needed to train algorithms and of decreased power consumption.)","tags":["deep-learning","sparse coding"],"title":"Controlling an aerial robot with human gestures using bio-inspired algorithm","type":"publication"},{"authors":["Jean-Bernard Damasse","Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2b8e1a67c2eadb91c07060af4c8d27c9","permalink":"https://laurentperrinet.github.io/publication/damasse-17-vss/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/damasse-17-vss/","section":"publication","summary":"","tags":["eye movements","Smooth pursuit eye movement"],"title":"Dynamic modulation of volatility by reward contingencies: effects on anticipatory smooth eye movement","type":"publication"},{"authors":["Victor Boutin","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" This work is a followup of  Laurent U Perrinet  (2010). Role of homeostasis in learning sparse representations. Neural Computation.  Preprint  PDF  Cite  Code  DOI    the poster (PDF) will be presented Thursday, May 18 @ NeuroFrance, Bordeaux. see a follow-up publication on  Laurent U Perrinet  (2019). An adaptive homeostatic algorithm for the unsupervised learning of visual features. Vision.  Preprint  PDF  Cite  Code  DOI    see a related work describing SDPC in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"3e557550f0b6b8354cc084b48365d19f","permalink":"https://laurentperrinet.github.io/publication/boutin-ruffier-perrinet-17-neurofrance/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/boutin-ruffier-perrinet-17-neurofrance/","section":"publication","summary":"One core advantage of sparse representations is the efficient coding of complex signals using compact codes. For instance, it allows for the representation of any image as a combination of few elements drawn from a large dictionary of basis functions. In the context of the efficient processing of natural images, we propose here that such codes can be optimized by designing proper homeostatic rules between the elements of the dictionary. Indeed, a common design for unsupervised learning rules relies on a gradient descent over a cost measuring representation quality with respect to sparseness. The sparseness constraint introduces a competition which can be optimized by ensuring that each item in the dictionary is selected as often as others. We implemented this rule by introducing a gain normalization similar to what is observed in a balanced neural network. We validated this theoretical insight by challenging different sparse coding algorithms with the same learning rule but with or without homeostasis. The different sparse coding algorithms were chosen for their efficiency and generality. They include least-angle regression, orthogonal matching pursuit and basis pursuit. Simulations show that for a given homeostasis rule, gradient descent performed similarly the learning of a dataset of image patches. While the coding algorithm did not matter much, including homeostasis changed qualitatively the learned features. In particular, homeostasis results in a more homogeneous set of orientation selective filters, which is closer to what is found in the visual cortex of mammals. To further validate these results, we applied this algorithm to the optimization of a visual system embedded in an aerial robot. As a consequence, this biologically-inspired learning rule demonstrates that principles observed in neural computations can help improve real-life machine learning algorithms.","tags":["deep-learning","sparse coding"],"title":"Efficient learning of sparse image representations using homeostatic regulation","type":"publication"},{"authors":["Victor Boutin","Franck Ruffier","Laurent U Perrinet"],"categories":null,"content":" This work is a followup of Perrinet, 2010, Neural Computation code is available @ https://github.com/laurentperrinet/BoutinRuffierPerrinet17spars and heavily uses https://github.com/bicv/SparseHebbianLearning the poster (PDF) will be presented Thursday, June 8 @ SPARS, Lisbon. see a related work describing SDPC in:  Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet  (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"ed03afbc2b20ef244da286e8e708d5df","permalink":"https://laurentperrinet.github.io/publication/boutin-ruffier-perrinet-17-spars/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/boutin-ruffier-perrinet-17-spars/","section":"publication","summary":"One core advantage of sparse representations is the efficient coding of complex signals using compact codes. For instance, it allows for the representation of any image as a combination of few elements drawn from a large dictionary of basis functions. In the context of the efficient processing of natural images, we propose here that such codes can be optimized by designing proper homeostatic rules between the elements of the dictionary. Indeed, a common design for unsupervised learning rules relies on a gradient descent over a cost measuring representation quality with respect to sparseness. The sparseness constraint introduces a competition which can be optimized by ensuring that each item in the dictionary is selected as often as others. We implemented this rule by introducing a gain normalization similar to what is observed in a balanced neural network. We validated this theoretical insight by challenging different sparse coding algorithms with the same learning rule but with or without homeostasis. The different sparse coding algorithms were chosen for their efficiency and generality. They include least-angle regression, orthogonal matching pursuit and basis pursuit. Simulations show that for a given homeostasis rule, gradient descent performed similarly the learning of a dataset of image patches. While the coding algorithm did not matter much, including homeostasis changed qualitatively the learned features. In particular, homeostasis results in a more homogeneous set of orientation selective filters, which is closer to what is found in the visual cortex of mammals. To further validate these results, we applied this algorithm to the optimization of a visual system embedded in an aerial robot. As a consequence, this biologically-inspired learning rule demonstrates that principles observed in neural computations can help improve real-life machine learning algorithms.","tags":["deep-learning","sparse coding"],"title":"Efficient learning of sparse image representations using homeostatic regulation","type":"publication"},{"authors":["Chloé Pasturel","Jean-Bernard Damasse","Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":" see a write-up in “Humans adapt their anticipatory eye movements to the volatility of visual motion properties”  ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f70a6b2a346f729469842fccdf858dea","permalink":"https://laurentperrinet.github.io/publication/pasturel-17-gdr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pasturel-17-gdr/","section":"publication","summary":" see a write-up in “Humans adapt their anticipatory eye movements to the volatility of visual motion properties”  ","tags":["Bayesian model","motion detection"],"title":"Estimating and anticipating a dynamic probabilistic bias in visual motion direction","type":"publication"},{"authors":["Laurent U Perrinet","Étienne Rey"],"categories":null,"content":"Expériences autour de la perception de la forme en art et science La vision utilise un faisceau d’informations de différentes qualités pour atteindre une perception unifiée du monde environnant. Nous avons utilisé lors de plusieurs projets art-science (voir https://github.com/NaturalPatterns) des installations permettant de manipuler explicitement des composantes de ce flux d’information et de révéler des ambiguités dans notre perception.     Dans l’installation «Tropique», des faisceaux de lames lumineuses sont arrangés dans l’espace assombri de l’installation. Les spectateurs les observent grâce à leur interaction avec une brume invisible qui est diffusée dans l’espace. Dans «Trame Élasticité», 25 parallélépipèdes de miroirs (3m de haut) sont arrangés verticalement sur une ligne horizontale. Ces lames sont rotatives et leurs mouvements est synchronisé. Suivant la dyamique qui est imposé à ces lames, la perception de l’espace environnent fluctue conduisant à recomposer l’espace de la concentration à l’expansion, ou encore à générer un surface semblant transparente ou inverser la visons de ce qui est située devant et derrière l’observateur. Enfin, dans «Trame instabilité», nous explorons l’interaction de séries périodiques de points placées sur des surfaces transparentes. À partir de premières expérimentations utilisant une technique novatrice de sérigraphie, ces trames de points sont placées afin de faire émerger des structures selon le point de vue du spectateur. De manière générale, nous montrerons ici les différentes méthodes utilisées, comme l’utilisation des limites perceptives, et aussi les résultats apportés par une telle collaboration.      poster présenté au GDR vision 2017, Lille. abstract: https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017abstract_168363.pdf poster : https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017poster.pdf poster (code) : https://github.com/NaturalPatterns/2017-10-12_GDR/blob/master/2017-10-12_PerrinetRey2017poster.ipynb more code : https://github.com/NaturalPatterns  ","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"a79bc98e429a1deb9d93dade630a7ffc","permalink":"https://laurentperrinet.github.io/publication/perrinet-17-gdr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/perrinet-17-gdr/","section":"publication","summary":"La vision utilise un faisceau d'informations de différentes qualités pour atteindre une perception unifiée du monde environnant. Nous avons utilisé lors de plusieurs projets art-science (voir https://github.com/NaturalPatterns) des installations permettant de manipuler explicitement des composantes de ce flux d'information et de révéler des ambiguités dans notre perception. Dans l'installation Tropique, des faisceaux de lames lumineuses sont arrangés dans l'espace assombri de l'installation. Les spectateurs les observent grâce à leur interaction avec une brume invisible qui est diffusée dans l'espace. Dans Trame Élasticité, 25 parallélépipèdes de miroirs (3m de haut) sont arrangés verticalement sur une ligne horizontale. Ces lames sont rotatives et leurs mouvements est synchronisé. Suivant la dyamique qui est imposé à ces lames, la perception de l'espace environnent fluctue conduisant à recomposer l'espace de la concentration à l'expansion, ou encore à générer un surface semblant transparente ou inverser la visons de ce qui est située devant et derrière l'observateur. Enfin, dans Trame instabilité, nous explorons l'interaction de séries périodiques de points placées sur des surfaces transparentes. À partir de premières expérimentations utilisant une technique novatrice de sérigraphie, ces trames de points sont placées afin de faire émerger  des structures selon le point de vue du spectateur. De manière générale, nous montrerons ici les différentes méthodes utilisées, comme l'utilisation des limites perceptives, et aussi les résultats apportés par une telle collaboration.","tags":["motion anticipation"],"title":"Expériences autour de la perception de la forme en art et science","type":"publication"},{"authors":["Kiana Mansour-Pour","Laurent U Perrinet","Guillaume S Masson","Anna Montagnini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2258506451c47b9ea4010eba86b3a29c","permalink":"https://laurentperrinet.github.io/publication/mansour-17-ecvp/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mansour-17-ecvp/","section":"publication","summary":"","tags":["eye movements","motion detection","motion-clouds"],"title":"How the dynamics of human smooth pursuit is influenced by speed uncertainty","type":"publication"},{"authors":["Kiana Mansour-Pour","Laurent U Perrinet","Guillaume S Masson","Anna Montagnini"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"2c4ed9ecd6f3377df41dfc4f621b2ab6","permalink":"https://laurentperrinet.github.io/publication/mansour-17-gdr/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/mansour-17-gdr/","section":"publication","summary":"The properties of motion processing for driving smooth eye movements have bee investigated using simple, artificial stimuli such as gratings, small dots or random dot patterns. Motion processing in the context of complex, natural images is less known. We have previously investigated the human ocular following responses to a novel class of random texture stimuli of parameterized naturalistic statistics: the Motion Clouds. In Fourier space, these dynamical textures are designed with a log normal distribution of spatial frequencies power multiplied by a pink noise power spectral density that reduces the high frequency contents of the stimulus (Sanz-Leon et al. 2011). We have previously shown that the precision of reflexive tracking increases with the spatial frequency bandwidth of large ( 30 deg diameter) patterns (i.e. the width of the spatial frequency distribution around a given mean spatial frequency; Simoncini et al. 2012). Now, we extend this approach to voluntary tracking and focused on the effects of spatial frequency bandwidth upon the initial phase of smooth pursuit eye movements. Participants were instructed to pursue a large patch of moving clouds (mean speeds: 5, 10 or 20 deg/s) embedded within a smoothing Gaussian window of standard deviation 5 deg. The motion stimuli were presented with four different spatial frequency bandwidths and two different mean spatial frequencies (0.3 and 1 cpd). We observed that smaller bandwidth textures exhibit a stronger spectral energy within the low spatial frequency range (below 1cpd), yielding to shorter latency of smooth pursuit eye movements. A weak and less consistent effect was found on initial eye acceleration, contrary what was previously observed with OFR. After 400ms, the steady-state tracking velocity matched the mean visual motion speed and pursuit performance was comparable with that observed with a control, small dot motion. Motion Clouds offer an efficient tool to probe the optimal window of visibility for human smooth pursuit through the manipulation of both the mean and the variability of spatial frequency.","tags":["eye movements","motion detection","motion-clouds"],"title":"Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"RENCONTRES INTERNATIONALES SCIENCES \u0026amp; CINÉMAS cinéma les Variétés    http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg  L’Association Polly Maggoo http://www.pollymaggoo.org/ programme la 10e édition des RENCONTRES INTERNATIONALES SCIENCES \u0026amp; CINÉMAS (RISC) à Marseille, au cours desquelles l’association programme des films à caractère scientifique. Les projections se déroulent en présence de chercheurs et/ou de cinéastes dans la perspective d’un développement de la culture cinématographique et scientifique en direction des publics scolaires. Ce dimanche 20 novembre, je suis venu échanger au côté de Serge Dentin et Caroline Renard (Maître de conférences en études cinématographiques à Aix-Marseille Université), autour de films traitant du rapport fiction/réel, de la mémoire, et du temps. Une occasion aussi de parler du métier de chercheur.\n Date\n25 Avril 2016 Location\ncinéma les Variétés Programmation\n“addendum” court métrage de Jérôme Lefdup et “Poétique du cerveau” long métrage de Nurith Aviv  entretien avec Clara Delmon L’occasion aussi d’un entretien avec Clara Delmon dans le cadre de son mémoire de DSAA (Diplôme Supérieur d’Arts Appliqués) mention Design Graphique à Marseille, disponible sur http://www.tonerkebab.fr/wiki/doku.php/wiki:proto-memoires:clara-delmon:clara-delmon et https://www.behance.net/claradelmon “L’échec de la perception”. Entretien avec Laurent PERRINET, rencontré à la 10e édition des RISC (Rencontres Internationales de la Science et du Cinéma) chercheur au CNRS (Centre National de la Recherche Scientifque) à l’Institut de Neurosciences de la Timone à Marseille, spécialisé en perception visuelle.\nEntretien Entretien avec Laurent PERRINET, rencontré à la 10e édition des RISC (Rencontres Internationales de la Science et du Cinéma)chercheur au CNRS (Centre National de la Recherche Scientifque) à l’Institut de Neurosciences de la Timone à Marseille, spécialisé en perception visuelle. ﻿ 1 / Vous faites les Rencontres Internationales de la Science et du Cinéma depuis quelques années déjà, la science est de plus en plus présente dans les arts, comme avec certains courants artistiques comme l’Art Cinétique ou l’Art Optique, pourquoi pensez-vous qu’une telle interaction est présente à notre époque ? J’ai la sensation qu’il y a un intérêt grandissant pour l’étude du cerveau dans le domaine des arts et de la communication. À votre avis, pourquoi un tel besoin de donner de la poésie au cerveau, (ou du cerveau à la poésie) ? En effet, je participe aux Rencontres Internationales de la Science et du Cinéma depuis déjà deux ans déjà. Le but est simplement de rentrer en contact avec le grand public et partager ma passion pour l’étude de la perception visuelle et du cerveau plus généralement. J’attache beaucoup d’importance à ces rencontres car elle nous permettent aussi de mieux comprendre l’intérêt public pour le cerveau dans son fonctionnement normal mais aussi dans ses dysfonctions. C’est aussi une source d’inspiration pour savoir dans quelle direction il est important de plus creuser nos recherches. 2 / Vous travaillez notamment avec Etienne Rey sur des installations interactives, où la place et le ressenti du spectateur font l’œuvre. La vue est alors votre outil de travail essentiel, pourquoi ce sens est-il plus sensiblement exposé à l’expérience de l’illusion ? Qu’apporte l’expérience perceptive au spectateur ? En effet, en parallèle de ces actions de partage avec le public, je travaille aussi avec Étienne Rey, un artiste plasticien résidant à la Friche Belle de mai à Marseille. Notre travail s’articule autour de l’ambiguïté de l’expérience perceptive du spectateur. Est-il en train de se regarder lui-même dans un miroir ou le miroir est-il lui-même une œuvre d’art ? 3 / Les graphistes d’aujourd’hui ont tendance à brouiller les codes, déformer, rendre illisible, en bref utiliser la complexité de l’image pour en complexifier la lecture. Pensez-vous qu’une image où on ne voit rien puisse en dire plus ? C’est-à-dire, pensez-vous qu’en accentuant l’acte de lecture, le designer graphique amène à son lecteur une activité qui consisterait non plus seulement à déchiffrer un message (présentation d’un évènement, publicité…) mais à s’observer lui-même en tant que lecteur ? Le travail du système visuel est de décoder les messages ambigus qui lui sont délivrés par la rétine. En créant des oeuvres graphiques qui brouillent les codes et en les déformants, on oblige le cerveau à avoir une démarche plus active par rapport au décodage du message fourni. Tout le travail du graphiste consiste donc à indiquer ce processus actif tout en conservant l’intégrité du message. 4 / Ces images utilisent le plus souvent des trames, des rayures, des distorsions qui captent notre attention. Pourquoi notre œil est plus attiré par ce qui est en mouvement ? Notre oeil est attiré par tout ce qui est surprenant. Cela inclut donc tout ce …","date":1479632400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479632400,"objectID":"bf6f177f738f6339f1d0f17668bf849b","permalink":"https://laurentperrinet.github.io/talk/2016-11-20-polly-maggoo/","publishdate":"2016-11-20T09:00:00Z","relpermalink":"/talk/2016-11-20-polly-maggoo/","section":"talk","summary":"RENCONTRES INTERNATIONALES SCIENCES \u0026 CINÉMAS cinéma les Variétés    http://pollymaggoo.org/wp-content/uploads/2016/10/RISC2016_A3-724x1024.jpg  L’Association Polly Maggoo http://www.pollymaggoo.org/ programme la 10e édition des RENCONTRES INTERNATIONALES SCIENCES \u0026 CINÉMAS (RISC) à Marseille, au cours desquelles l’association programme des films à caractère scientifique.","tags":null,"title":"Participation au jury et entretien avec Clara Delmon","type":"talk"},{"authors":["Cesar U Ravello","Maria Jose Escobar","Adrián G Palacios","Laurent U Perrinet"],"categories":null,"content":"See supplementray code.\nHow does the retina respond to stimuli with different sparseness? This stimulus is generated simply using the Motion Clouds library by defining a sparse draw of events:\nimport numpy as np import MotionClouds as mc import matplotlib.pyplot as plt # PARAMETERS seed = 2042 np.random.seed(seed=seed) N_sparse = 5 sparse_base = 2.e5 sparseness = np.logspace(-1, 0, N_sparse, base=sparse_base, endpoint=True) print(sparseness) # TEXTON N_X, N_Y, N_frame = 256, 256, 1 fx, fy, ft = mc.get_grids(N_X, N_Y, 1) mc_i = mc.envelope_gabor(fx, fy, ft, sf_0=0.05, B_sf=0.025, B_theta=np.inf) values = np.random.randn(N_X, N_Y, N_frame) chance = np.argsort(-np.abs(values.ravel())) chance = np.array(chance, dtype=np.float) chance /= chance.max() chance = chance.reshape((N_X, N_Y, N_frame)) fig, axs = plt.subplots(1, N_sparse, figsize=(fig_width, fig_width/N_sparse)) for i_ax, l0_norm in enumerate(sparseness): threshold = 1 - l0_norm mask = np.zeros_like(chance) mask[chance \u0026gt; threshold] = 1. im = 2*mc.rectif(mc.random_cloud(mc_i, events=mask*values))-1 axs[i_ax].imshow(im[:, :, 0], vmin=-1, vmax=1, cmap=plt.gray()) #axs[i_ax].text(9, 80, r\u0026#39;$n=%.0f\\%%$\u0026#39; % (noise*100), color=\u0026#39;white\u0026#39;, fontsize=10) axs[i_ax].text(4, 40, r\u0026#39;$\\epsilon=%.0e$\u0026#39; % l0_norm, color=\u0026#39;white\u0026#39;, fontsize=8) axs[i_ax].set_xticks([]) axs[i_ax].set_yticks([]) plt.tight_layout() fig.subplots_adjust(hspace = .0, wspace = .0, left=0.0, bottom=0., right=1., top=1.) ","date":1479600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479600000,"objectID":"0a50e0e556c2bbe845d4b751e803c9a5","permalink":"https://laurentperrinet.github.io/publication/ravello-16-droplets/","publishdate":"2016-11-20T00:00:00Z","relpermalink":"/publication/ravello-16-droplets/","section":"publication","summary":"See supplementray code.\nHow does the retina respond to stimuli with different sparseness? This stimulus is generated simply using the Motion Clouds library by defining a sparse draw of events:","tags":["Biologically Inspired Computer vision","Image texture","Retina","sparse coding"],"title":"Differential response of the retinal neural code with respect to the sparseness of natural images","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" Based on Perrinet et al, 2012 and Khoei et al, 2013 See a followup in Khoei et al, 2017  ","date":1478178000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478178000,"objectID":"a21afa2114711607d191aa6dd6e579f6","permalink":"https://laurentperrinet.github.io/talk/2016-11-03-sigma/","publishdate":"2016-11-03T13:00:00Z","relpermalink":"/talk/2016-11-03-sigma/","section":"talk","summary":" Based on Perrinet et al, 2012 and Khoei et al, 2013 See a followup in Khoei et al, 2017  ","tags":null,"title":"The flash-lag effect as a motion-based predictive shift","type":"talk"},{"authors":["Jean-Bernard Damasse","Laurent U Perrinet","Jeremie Jozefowiez","Laurent Madelain","Anna Montagnini"],"categories":null,"content":"","date":1478131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478131200,"objectID":"7a4b4cda2e35089fa388c7bb5bb5f8a4","permalink":"https://laurentperrinet.github.io/talk/2016-11-03-gdr/","publishdate":"2016-11-03T00:00:00Z","relpermalink":"/talk/2016-11-03-gdr/","section":"talk","summary":"Natural environments potentially contain several interesting targets for goal-directed behavior. Thus sensorimotor systems need to operate a competitive selection based on behaviorally meaningful parameters. Recently, it has been observed that voluntary eye movements such as saccades and smooth pursuit can be considered as operant behaviors (Madelain et al, 2011). Indeed, parameters of saccades such as peak-velocity or latency (Montagnini et al, 2005) as well as smooth pursuit behavior during transient blanking (Madelain et al, 2003) or visually-guided pursuit of ambiguous stimuli (Schutz et al, 2015) can be modified by reinforcement contingencies. Here we address the question of whether expectancy-based anticipatory smooth pursuit can be modulated by reinforcement contingencies. When predictive information is available, anticipatory smooth pursuit eye movements (aSPEM) is frequently observed before target appearance. Actions that occur at some distance in time from the reinforcement outcome, such as aSPEM -which occurs without any concurrent sensory feedback suffer of the well-known credit assignment problem (Kaelbling et al, 1996). We designed a direction-bias task as a baseline and modified it by setting an implicit eye velocity criterion during anticipation. The nature of the following trial-outcome (reward or punishment) was contingent to the online criterion matching. We observed a dominant graded effect of motion-direction bias and a small modulational effect of reinforcement on aSPEM velocity. A yoked-control paradigm corroborated this result showing a strong reduction in anticipatory behavior when the reward/punishment schedule was not contingent to behavior. An additional classical conditioning paradigm confirmed that reinforcement contingencies have to be operant to be effective and that they have a role in solving the credit assignment problem during aSPEM.","tags":null,"title":"Reinforcement contingencies modulate anticipatory smooth eye movements","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1477486800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477486800,"objectID":"b5d9424071f3a6b742d8b48c868a9d9e","permalink":"https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/","publishdate":"2016-10-26T13:00:00Z","relpermalink":"/talk/2016-10-26-perrinet-16-euvip/","section":"talk","summary":"","tags":["Biologically Inspired Computer vision"],"title":"Biologically-inspired characterization of sparseness in natural images","type":"talk"},{"authors":["Lionel Fillatre","Michel Barlaud","Laurent U Perrinet"],"categories":null,"content":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1477486800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477486800,"objectID":"59929f1297bfcec1f03b17a14bc8b53a","permalink":"https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/","publishdate":"2016-10-26T13:00:00Z","relpermalink":"/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/","section":"talk","summary":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","tags":["Biologically Inspired Computer vision"],"title":"Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"2016-10-26 : EUVIP Special Session on Biologically Inspired Computer Vision description of the session Recent advances in imaging technologies have yielded scientific data at unprecedented detail and volume, leading to the need of a shift of paradigm in image processing and computer vision. Beyond the usual classical von Neumann architecture, one strategy that is emerging in order to process and interpret this amount of data follows from the architecture of biological organisms and shows for instance computational paradigms implementing asynchronous communication with a high degree of local connectivity in sensors or brain tissues. This session aims at bringing together researchers from different fields of Biologically Inspired Computer Vision to present latest results in the field, from fundamental to more specialized topics, including visual analysis based on a computational level, hardware implementation, and the design of new more advanced vision sensors. It is expected to provide a comprehensive overview in the computer area of biologically motivated vision. On the one hand, biological organisms can provide a source of inspiration for new computationally efficient and robust vision models and on the other hand machine vision approaches can provide new insights for understanding biological visual systems. This session covers a wide range of topics from fundamental to more specialized topics, including visual analysis based on a computational level, hardware implementation, and the design of new more advanced vision sensors. In particular, we expect to provide an overview of a few representative applications and current state of the art of the research in this area.\n  URL http://www-l2ti.univ-paris13.fr/euvip2016/index.php/86-euvip2016/129-tentative-technical-program-in-detail\n  date October 26th, 2016\n  Location Ecole Centrale Marseille\n  Address 38 rue Frédéric Joliot-Curie 13013 Marseille, France Phone : +33 (0)4 91 05 45 45\n  Programme\n  13.50 Visual System Inspired Algorithm For Contours, Corner And T Junction Detection, Antoni Buades, Rafael Grompone Von Gioi\n  13.50 Biologically-inspired characterization of sparseness in natural images, Laurent Perrinet\n  14.10 Color filter array imitating the random nature of color arrangement in the human cone mosaic, Prakhar Amba, David Alleysson\n  14.30 An Illuminant-Independent Analysis Of Reflectance As Sensed By Humans, And Its Applicability To Computer Vision, Alban Flachot, Phelma, J.Kevin O’Regan, Edoardo Provenzi\n  14.50 Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor, Lionel Fillatre, Michel Barlaud, Laurent Perrinet\n  ","date":1477440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477440000,"objectID":"f231f03505d9ad9330adc3eff9e7a3c6","permalink":"https://laurentperrinet.github.io/post/2016-10-26_euvip-bicv/","publishdate":"2016-10-26T00:00:00Z","relpermalink":"/post/2016-10-26_euvip-bicv/","section":"post","summary":"EUVIP Session 7: Biologically Inspired Computer Vision (Special Session).","tags":["events","Biologically Inspired Computer Vision"],"title":"2016-10-26 : EUVIP BICV","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1476921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476921600,"objectID":"7086ec2f451d080124c1424ed13c5705","permalink":"https://laurentperrinet.github.io/publication/perrinet-16-euvip/","publishdate":"2016-10-20T00:00:00Z","relpermalink":"/publication/perrinet-16-euvip/","section":"publication","summary":"Natural images follow statistics inherited by the structure of our physical (visual) environment. In particular, a prominent facet of this structure is that images can be described by a relatively sparse number of features. We designed a sparse coding algorithm biologically-inspired by the architecture of the primary visual cortex. We show here that coefficients of this representation exhibit a power-law distribution. For each image, the exponent of this distribution characterizes sparseness and varies from image to image. To investigate the role of this sparseness, we designed a new class of random textured stimuli with a controlled sparseness value inspired by measurements of natural images. Then, we provide with a method to synthesize random textures images with a given sparseness statistics that match that of some class of natural images and provide perspectives for their use in neurophysiology.","tags":["Biologically Inspired Computer vision","Image texture","sparse coding"],"title":"Biologically-inspired characterization of sparseness in natural images","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    next talk @ INVIBE FEST, Paris next talk @ Brain workshop, Marseille next talk @ LACONEU, Chile next talk @ CAUSAL Kick-off, Marseille next talk @ NeuroFrance, Marseille  ","date":1476352800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476352800,"objectID":"b63a543e96a8f1e83b19bebf67fee739","permalink":"https://laurentperrinet.github.io/talk/2016-10-13-law/","publishdate":"2016-10-13T10:00:00Z","relpermalink":"/talk/2016-10-13-law/","section":"talk","summary":"See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.","tags":null,"title":"Eye movements as a model for active inference","type":"talk"},{"authors":["Jean-Bernard Damasse","Laurent U Perrinet","Jeremie Jozefowiez","Laurent Madelain","Anna Montagnini"],"categories":null,"content":"","date":1474329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1474329600,"objectID":"9f06218457c3203f5ecd260bf3b11009","permalink":"https://laurentperrinet.github.io/publication/damasse-16-vss/","publishdate":"2016-09-20T00:00:00Z","relpermalink":"/publication/damasse-16-vss/","section":"publication","summary":"","tags":["eye movements","Smooth pursuit eye movement"],"title":"Operant reinforcement versus reward expectancy: effects on anticipatory eye movements","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" See a followup in Perrinet et al, 2012  ","date":1467896400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467896400,"objectID":"1650b9d411a521d7f949d363a900f3e1","permalink":"https://laurentperrinet.github.io/talk/2016-07-07-edp-proba/","publishdate":"2016-07-07T13:00:00Z","relpermalink":"/talk/2016-07-07-edp-proba/","section":"talk","summary":" See a followup in Perrinet et al, 2012  ","tags":null,"title":"Modelling the dynamics of cognitive processes: from the Bayesian brain to particles","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Les illusions visuelles, un révélateur du fonctionnement de notre cerveau Cycle de conférences “Tous connectés”, Bibliothèque de Méjanes     Date\n28 Avril 2016 Location\nBibliothèque de Méjanes Visuels\nHTML  ","date":1461868200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461868200,"objectID":"4eaa55f1632515d70345839ae22861df","permalink":"https://laurentperrinet.github.io/talk/2016-04-28-mejanes/","publishdate":"2016-04-28T18:30:00Z","relpermalink":"/talk/2016-04-28-mejanes/","section":"talk","summary":"Les illusions visuelles sont des créations d'artistes, de scientifiques et plus récemment, grâce aux réseaux sociaux, du grand public qui proposent des situations souvent incongrues, dans lesquelles l'eau remonte une cascade, les personnes volent dans les airs ou des serpents se mettent à tourner. Au-delà de leur indéniable coté ludique, ces illusions nous apprennent beaucoup sur le fonctionnement du cerveau. En tant que chercheur en Neurosciences à l'Institut de Neurosciences de la Timone à Marseille, je vous dévoilerai des aspects du fonctionnement du cerveau qui sont souvent méconnus. En particulier, nous verrons pourquoi un magicien peut tromper nos sens ou comment des objets peuvent voyager dans le temps. Surtout nous essaierons de comprendre le fonctionnement de notre perception visuelle sur les bases d'une théorie de la vision non pas comme une simple caméra qui enregistre des images mais comme un processus actif en relation avec le monde qui nous entoure.","tags":null,"title":"Les illusions visuelles, un révélateur du fonctionnement de notre cerveau","type":"talk"},{"authors":["Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":"ANR BalaV1: Balanced states in area V1 (2013/2016)  Official website  In carnivores and primates the orientation selectivity (OS) of the cells in the primary visual cortex (V1) is organized in maps in which preferred orientations (POs) of the cells change gradually except near “pin- wheels”, around which all orientations are present. Over the last half-century the mechanism for OS has been hotly debated. However the theories that purport to explain OS have almost all considered cortical networks in which the neurons receive input preferentially from cells with similar PO. Such theories certainly capture the connectivity for neurons in orientation domains where neurons are surrounded by other cells with similar PO. However this does not necessarily hold near pinwheels: because of the discontinuous change in orientation preference at the pinwheel, neurons in this area are surrounded by cells of all preferred orientations. Thus if the probability of connection is solely dependent on anatomical distance, the inputs that these neurons receive should represent all orientations by roughly the same amount. Thus one may expect that the response of the cells near pinwheels should hardly vary with orientation, in contrast to experimental data. As a result, the common belief is that, at least near pinwheels, the connectivity depends also on the differences between preferred orientation. The situation near pinwheels in V1 of carnivores and primates is similar to that in the whole of V1 of rodents. In these species, neurons in V1 are OS but the network does not exhibit an orientation map and the surround of the cells represents all orientations roughly equally. In a recent theoretical paper (Hansel and van Vreeswijk 2012) we have demonstrated that in this situation, the response of the cells can still be orientation selective provided that the network operates in the balanced regime. Here we hypothesize that V1 with an orientation map operates in the balanced regime and therefore neurons can exhibit OS near pinwheels even in the absence of functional specific connectivity. The goal of this interdisciplinary project is to investigate whether the “balance hypothesis” holds for layer 2/3 in V1 of primate and carnivore and whether the functional organization observed in that layer can be accounted for without feature specific connectivity. We will combine modeling and experiments to investigate how the response of the neurons – the mean firing, the mean voltage, the inhibitory and excitatory conductances and importantly, the power spectrum of their fluctuations – vary with the location in the map, and also how a population of neurons – LFP, voltage-sensitive dye imaging or 2 photons – is affected by the various para- meters used to test the system. Whether V1 indeed operates in the balanced regime in more realistic conditions will be further investigated by determining how the local network responds to visual stimuli beyond the classical receptive field. We will investigate this issue in models of layer 2/3 representing multiple hyper- columns to characterize center-surround interactions and their dependence on the long-range connectivity. This will provide us with predictions for center-surround interactions for cells near pinwheels and in orientation domains. These predictions will be tested experimentally.\nThe proposed project is new and ambitious. It aims at building a comprehensive and coherent understand- ing of the physiology of V1 layer 2/3 on several spatial scales from single cells to several hypercolumns and to account for this in mechanistic models. To accomplish these ambitious aims, we propose a combination of experimental and computational studies that take advantage of the unique strengths and the complementarity of expertise of 3 research teams. The Paris team has extensive experience in large-scale modeling of V1. The Toulouse and Marseille teams master both intra- and extracellular electrophysiology. In addition, the Marseille team is expert in microscopic and mesoscopic imaging techniques in V1.\nAcknowledgement\nThis work was supported by ANR project \u0026#34;BalaV1\u0026#34; N° ANR-13-BSV4-0014-02.  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e4d34358ccdcefa5fcf4857b90f0798a","permalink":"https://laurentperrinet.github.io/grant/anr-bala-v1/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/anr-bala-v1/","section":"grant","summary":"ANR BalaV1: Balanced states in area V1 (2013--2016)","tags":["grant","past-grant"],"title":"ANR BalaV1 (2013/2016)","type":"grant"},{"authors":null,"categories":null,"content":"With Andrea Brovelli (INT), Mateus Joffily (GATE)…\nSee https://anr.fr/Project-ANR-18-CE28-0016\nHumans have an extraordinary capacity to infer cause-effect relations. In particular, we excel in forming ​beliefs ​about the ​causal effect of actions​. Causal learning provides the basis for rational decision-making and allows people to engage in meaningful life and social interactions. Causal learning is a form of goal-directed learning, defined as the capacity to rapidly learn the consequence of actions and to select behaviours according to goals and motivational state. This ability is based on internal models of the consequence of our behaviors​ and relies on learning rules determined by the​ contingency between actions and outcomes​. At a first approximation, contingency Δ​P ​is operationalized as the difference between two conditional probabilities: i) P(O|A), the probability of outcome O given action A; ii) P(O|¬A), the probability of the outcome when the action is withheld. In everyday life, people perceive their actions as causing a given outcome if the contingency is positive, whereas they perceive them as preventing​ ​it​ ​if​ ​negative;​ ​when​ ​P(O|A)​ ​and​ ​P(O|¬A)​ ​are​ ​equal,​ ​people​ ​report​ ​no​ ​causal​ ​effect​​ ​. Despite the centrality of causal learning, a clear understanding of both the internal computations and neural substrates (the so-called ​cognitive architectures​) is currently missing. ​Our project will therefore address​ ​two​ ​key​ ​questions:\n  What are the key ​internal representations of causal beliefs and what are the ​computational processes​​ ​that​ ​enable​ ​their​ ​formation​ ​during​ ​learning?\n  How ​​are ​​internal​ ​representations​ ​and ​​computational​​ processes​ ​​implemented​ ​​in ​​the ​​brain? CausaL​ ​​will​ ​address​ ​these​ ​two​ ​objectives​ ​through​ ​two​ ​dedicated​ ​research​ ​work​ ​packages​ ​(WPs).\n  Acknowledgement : This work was supported by ANR project ANR-18-AAPG–“CAUSAL, Cognitive Architectures of Causal Learning”.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"36d7a65e0b74327bc83cd0a3c1f88c20","permalink":"https://laurentperrinet.github.io/grant/anr-causal/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/anr-causal/","section":"grant","summary":"ANR CausaL (2018/2020) : Cognitive​ ​architectures​ ​of​ Causal​ ​Learning.","tags":["grant","past-grant"],"title":"ANR CausaL (2018/2020)","type":"grant"},{"authors":["Yves Fregnac","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":" Description on the official website of the ANR  The Horizontal-V1 project aims at understanding the emergence of sensory predictions linking local shape attributes (orientation, contour) to global indices of movement (direction, speed, trajectory) at the earliest stage of cortical processing (primary visual cortex, i.e. V1). We will study how the long-distance “horizontal” connectivity, intrinsic to V1 and the feedback from higher cortical areas contribute to a dynamic processing of local-to-global features as a function of the context (eg displacement along a trajectory; during reafference change induced by eye-movements…). We will search to characterize the dynamic processes based on lateral propagation intra-V1, through which spatio-temporal inferences (continuous movement or apparent motion sequences) facilitating spatial (“filling-in”) or positional (“flash-lag”) future expected responses may be generated. The project will use a variety of animations of local oriented stimuli forming, according to their spatial and temporal coherence, predictable global patterns, apparent motion sequences and/or continuous trajectories. We will measure the cortical dynamics at two scales of neuronal integration, from micro- (intracellular, SUA) to meso-scopic levels (multi-electrode arrays (MEA) and voltage sensitive dye imaging (VSDI)) in the anesthetized (cat, marmoset) and awake fixating animal (macaca mulata). In a second step, we will combine these multiscale observations to constrain a structuro-functional model of low-level perception, integrating the micro-meso constraints. Two laboratories will participate in synergy to the project: UNIC-Gif (Dir. Yves Frégnac, DRCE2 CNRS, coordinator) and INT-Marseille (NeOpTo Team Dir. Frédéric Chavane, DR2 CNRS).\nWP3 - Design of novel visual paradigms, probabilistic model of V1 and data-driven simulations - co lead UNIC-INT. Objectives : This WP will have two primary goals. The first one is theoretically driven, and for sake of simplicity will ignore the dynamic features of neural integration (as expected from a statistical model of image analysis). Binding the different features of visual objects at the local scale (contours) as well as a more global level involves understanding the statistical regularities of the sensory inflow. In particular, titrating the predictions that can be done at the statistical level can be seen as a first pass to better search for critical parameters constraining the network behaviour. From these, we will build probabilistic predictive models optimized for edge co-occurrence classification and generate novel visual statistics 1) which obey rules imposed by the functional horizontal connectivity anisotropies, such as co- circularity, and 2) which facilitate binding in the orientation domain, such as log-polar planforms. These statistics generated in the first half of the grant will be implemented and tested experimentally in the second half of the grant. The second one is more data-driven (as well as phenomenological for feedback from higher cortical areas, since it will not be explored in the grant). Since model fitting will depend on close interactions with WP1 and WP2 measurements, it will be done in the second half of the grant.\nWP3-Task 1: Theoretically oriented workplan – Lead INT (Laurent Perrinet)   WP3-Task 1.1 - theory : we will exploit our current expertise in integrating these statistics in the form of probabilistic models to make predictions both at the physiological and modelling levels. First, we will take advantage of our previous work on the quantification of the association field in different classes of natural images (Perrinet \u0026amp; Bednar, 2015). Using an existing library (https://github.com/bicv/SparseEdges), we will use the sparse representation of static natural images to compute histograms of edge co-occurrences. Using an existing algorithm for unsupervised learning (https://github.com/bicv/SparseHebbianLearning), we will learn the different independent components of edge co-occurrences. Such an algorithm fits well a traditional deep-learning convolutional neural network, but, in addition, will include constraints imposed by intra-layer horizontal connectivity. We expect that relevant features will be co-linear or co-circular pairs of edges, but also T-junctions or end-stopping features.\n  WP3-Task 1.2 - image/film synthesis : We have previously found that random synthetic textures, coined “Motion Clouds”, can be used to quantify V1 implication in visual motion perception (Leon et al, 2012; Simoncini et al, 2012). Recently, the INT and UNIC, partners proved mathematically that these stimuli were optimal with respect to some common geometrical transformations, such as translation, zoom or rotations (Vacher et al, 2015). A main characteristic of these textures is to be generated with a maximally entropic arrangement of elementary textures (so-called textons).\n  ** Informed by the generative model of edge co-occurrences studied …","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"41e2feb90580d53e6cf9614ae014fde2","permalink":"https://laurentperrinet.github.io/grant/anr-horizontal-v1/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/anr-horizontal-v1/","section":"grant","summary":"Connectivité Horizontale et Prédiction de Cohérences dans l'Intégration de Contour et Mouvement dans le Cortex Visuel Primaire","tags":["grant","past-grant"],"title":"ANR Horizontal-V1 (2017/2021)","type":"grant"},{"authors":["Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":"The objectives of PREDICTEYE is to rigorously test and define the functional and neurophysiological grounds of probabilistic oculomotor internal models by investigating the multiple timescales at which the trajectory of a moving target is learned and represented in a probabilistic framework (Aim #1). Second, we will investigate the role of (pre)frontal oculomotor networks in building such probabilistic representations and their impact upon two of their downstream neural targets of the brainstem premotor centers (superior colliculus for saccades; NRTP for pursuit) (Aim #2). Our third objective is to model and simulate the dynamics of target motion prediction and eye movement performance. A key question is to unveil how probabilistic information about target timing and motion (i.e. direction and speed) is sampled over trial history by neuronal populations and integrated with Prior knowledge (i.e. sequence properties and rules of conditional probabilities) in order to coordinate saccades and pursuit and optimize their precisions (Aim #3).\n  ANR-2018 Project PREDICTEYE - Agence Nationale de la Recherche (2018-2022). This project starts november 2018, for 4 years. It will investigate the neural networks in human volunteers supporting anticipatory pursuit eye movements using magnetic transcranial stimulation (TMS) to perturb frontal networks during ocular tracking of predictable targets. In complementary studies conducted in macaque monkeys, perturbations will be applied pharmacologically to subcortical targets of this frontal network, namely superior colliculus and NTRP, a pontine nucleus relaying information to the pursuit networks of the cerebellum. The project involves 4 CNRS permanent researchers from the INVIBE team headed by G Masson. The funding is 507K€ for 4 years. PI: G Masson, co-PI: A Montagnini, L Perrinet, L Goffart\n  related grant by the Fondation pour le Recherche Médicale, under the program Équipe FRM (DEQ20180339203/PredictEye/PI: G Masson/ A. Montagnini and L. Perrinet as participants).\n  Acknowledgement\nThis work was supported by ANR project \u0026#34;PredictEye\u0026#34; ANR-XXXX.  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ff269cbba8f72317d77037993d278463","permalink":"https://laurentperrinet.github.io/grant/anr-predicteye/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/anr-predicteye/","section":"grant","summary":"ANR PredictEye (2018/2020) : Mapping and predicting trajectories for eye  movements","tags":["grant","past-grant"],"title":"ANR PredictEye (2018/2020)","type":"grant"},{"authors":null,"categories":null,"content":"   Reinforcement learning theory provides a general conceptual framework to account for behavioral changes. Recently the idea that reinforcement may be used to explain learning in motor responses has emerged. In particular, there is a growing interest in studying the effects of reinforcement learning in arm movements trajectories (Dam, Kording, \u0026amp; Wei, 2013), pointing movements (Trommershauser, Landy, \u0026amp; Maloney, 2006), or eye movements (Madelain, Champrenaut, \u0026amp; Chauvin, 2007; Madelain \u0026amp; Krauzlis, 2003b; Madelain, Paeye, \u0026amp; Wallman, 2011; Sugrue, Corrado, \u0026amp; Newsome, 2004; Takikawa, Kawagoe, Itoh, Nakahara, \u0026amp; Hikosaka, 2002; Xu-Wilson, Zee, \u0026amp; Shadmehr, 2009). However, and despite these few seminal studies, much is still unknown about both the details of the effects of reward on motor control and the underlying mechanisms. This proposal aims at a better understanding of how skilled motor responses are learned focusing on voluntary eye movements.\nAlthough learning is often regarded as a restricted period of time during which a behavior undergo some changes we view learning as a continuously ongoing process. In the case of motor control every instance of a behavior is followed by some consequences that will affect some dimensions of the future response. These changes will in return affect the functional relations with the environment and this feedback process continues through lifetime. Therefore we do not regard motor learning as a special phase that allows the emergence of a particular motor response but as a continuous adaptation to the changes within the organism that affect the functional relations with her environment. This distinction is important because the learning situations that are experimentally tested over a short period of time may then be viewed as a condensed version of motor learning in the real life: the same adaptive processes are responsible for the changes in the response in both situations.\nAn important aspect of this fundamental research project is that the theoretical propositions addressed provide a new view on motor learning that departs from conventional wisdom. We expect to gain considerable knowledge on learning by constructing new experimental paradigms to collect behavioural data, implementing new learning models based on Bayesian theories and testing dynamical mathematical models of behavioural changes. Whichever way the results turn out, we anticipate that these studies will provide a better understanding of motor learning and provide a well-defined and solid framework for studying other forms of motor plasticity. If eye movement learning follows the rules of other operant responses (i.e. responses reinforced by their consequences), this will constitute a minor revolution in the study of motor control, both at the behavioral and neural levels, with important implications for the understanding of plasticity in other motor systems.\nAcknowledgement\nThis work was supported by ANR project ANR-13-APPR-0008 \u0026#34;ANR R.E.M.\u0026#34;.  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"19e847530b09d262346fb5e668715a6b","permalink":"https://laurentperrinet.github.io/grant/anr-rem/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/anr-rem/","section":"grant","summary":"ANR REM : Renforcement et mouvements oculaires (2013/2016).","tags":["grant","past-grant"],"title":"ANR REM (2013/2016)","type":"grant"},{"authors":null,"categories":null,"content":"Measuring speed and direction of moving objects is an essential computational step in order to move our eyes, hands or other body parts with respect to the environment. Whereas encoding and decoding of direction information is now largely understood in various neuronal systems, how the human brain accurately represents speed information remains largely unknown. Speed tuned neurons have been identified in several early cortical visual areas in monkeys. However, how such speed tuning emerges is not yet understood. A working hypothesis is that speed tuned neurons nonlinearly combine motion information extracted at different spatial and temporal scales, taking advantage of the statistical spatiotemporal properties of natural scenes. However, such pooling of information must be context dependent, varying with the spatial perceptual organization of the visual scenes. Furthermore, the population code underlying perceived speed is not elucidated either and therefore we are still far from understanding how speed information is decoded to drive and control motor responses or perceptual judgments.\nRecently, we have proposed that speed estimation is intrinsically a multi-scale, task-dependent problem (Simoncini et al., Nature Neuroscience 2012) and we have defined a new set of motion stimuli, constructed as random phase dynamical textures that mimic the statistics of natural scenes (Sanz-Leon et al., Journal of Neurophysiology 2012). This approach has proved to be fruitful to investigate nonlinear properties of motion integration.\nThe current proposal brings together psychophysicists, oculomotor scientists and modelers to investigate speed processing in human. We aim at expanding this framework in order to understand how tracking eye movements and motion perception can take advantage of multiple scale processing for estimating target speed. We will design sets of high dimensional stimuli by extending our generative model. Using these natural-statistics stimuli, we will investigate how speed information is encoded by computing motion energy across different spatial and temporal filters. By analysing both perceptual and oculomotor responses we will probe the nonlinear mechanisms underlying the integration of the outputs of multiple spatiotemporal filters and implement these processes in a refined version of our model. Furthermore, we will test our working hypothesis that in natural scenes such nonlinear integration provides precise and reliable motion estimates, which leads to efficient motion-based behaviors. By comparing tracking responses with perception, we will also test a second critical hypothesis, that nonlinear speed computations are task-dependent. In particular, we will explore the extent to which the geometrical structures of visual scenes are decisive for perception beyond the motion energy computation used for early sensorimotor transformation. Finally we will investigate the role of contextual and extra-retinal, predictive information in building an efficient dynamic estimate of objects’ speed for perception and action.\nAcknowledgement\nThis work was supported by ANR project \u0026#34;ANR Speed\u0026#34; ANR-13-BSHS2-0006.  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"3a71cb86ed5a08c844723ee42c35c0b8","permalink":"https://laurentperrinet.github.io/grant/anr-speed/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/anr-speed/","section":"grant","summary":"ANR SPEED: Traitement de la vitesse dans les scènes visuelles naturelles (2013/2016).","tags":["grant","past-grant"],"title":"ANR SPEED (2013/2016)","type":"grant"},{"authors":["Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":"Global motion processing is a major computational task of biological visual systems. When an object moves across the visual field, the sequence of visited positions is strongly correlated in space and time, forming a trajectory. These correlated images generate a sequence of local activation of the feed-forward stream. Local properties such as position, direction and orientation can be extracted at each time step by a feed-forward cascade of linear filters and static non-linearities. However such local, piecewise, analysis ignores the recent history of motion and faces several difficulties, such as systematic delays, ambiguous information processing (e.g., aperture and correspondence problems61) high sensitivity to noise and segmentation problems when several objects are present. Indeed, two main aspects of visual processing have been largely ignored by the dominant, classical feed-forward scheme. First, natural inputs are often ambiguous, dynamic and non-stationary as, e.g., objects moving along complex trajectories. To process them, the visual system must segment them from the scene, estimate their position and direction over time and predict their future location and velocity. Second, each of these processing steps, from the retina to the highest cortical areas, is implemented by an intricate interplay of feed-forward, feedback and horizontal interactions1. Thus, at each stage, a moving object will not only be processed locally, but also generate a lateral propagation of information. Despite decades of motion processing research, it is still unclear how the early visual system processes motion trajectories. We, among others, have proposed that anisotropic diffusion of motion information in retinotopic maps can contribute resolving many of these difficulties25 13. Under this perspective, motion integration, anticipation and prediction would be jointly achieved through the interactions between feed-forward, lateral and feedback propagations within a common spatial reference frame, the retinotopic maps.\nAddressing this question is particularly challenging, as it requires to probe these sequences of events at multiple scales (from individual cells to large networks) and multiple stages (retina, primary visual cortex (V1)). “TRAJECTORY” proposes such an integrated approach. Using state-of-the-art micro- and mesoscopic recording techniques combined with modeling approaches, we aim at dissecting, for the first time, the population responses at two key stages of visual motion encoding: the retina and V1. Preliminary experiments and previous computational studies demonstrate the feasibility of our work. We plan three coordinated physiology and modeling work-packages aimed to explore two crucial early visual stages in order to answer the following questions: How is a translating bar represented and encoded within a hierarchy of visual networks and for which condition does it elicit anticipatory responses? How is visual processing shaped by the recent history of motion along a more or less predictable trajectory? How much processing happens in V1 as opposed to simply reflecting transformations occurring already in the retina?\nThe project is timely because partners master new tools such as multi-electrode arrays and voltage-sensitive dye imaging for investigating the dynamics of neuronal populations covering a large segment of the motion trajectory, both in retina and V1. Second, it is strategic: motion trajectories are a fundamental aspect of visual processing that is also a technological obstacle in computer vision and neuroprostheses design. Third, this project is unique by proposing to jointly investigate retinal and V1 levels within a single experimental and theoretical framework. Lastly, it is mature being grounded on (i) preliminary data paving the way of the three different aims and (ii) a history of strong interactions between the different groups that have decided to join their efforts.\nThe Marseille team   Frédéric Chavane (DR, CNRS, NEOPTO team) is working in the field of vision research for about 20 years with a special interest in the role of lateral interactions in the integration of sensory input in the primary visual cortex. His recent work suggest that lateral interactions mediated by horizontal intracortical connectivity participates actively in the input normalization that controls a wide range of function, from the contrast-response gain to the representation of illusory or real motion. His expertise range from microscopic (intracellular recordings) to mesoscopic (optical imaging, multi-electrode array) recording scales in the primary visual cortex of anesthetized and awake behaving animals.\n  Laurent Perrinet (CR, CNRS, NEOPTO team). His scientific interests focus on bridging computational understanding of neural dynamics and low-level sensory processing by focusing on motion perception. He is the author of papers in machine learning, computational neuroscience and behavioral psychology. One …","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"300d70c191b69dd6dc2720ff8f091a30","permalink":"https://laurentperrinet.github.io/grant/anr-trajectory/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/anr-trajectory/","section":"grant","summary":"ANR TRAJECTORY (2016/2019).","tags":["grant","past-grant"],"title":"ANR TRAJECTORY (2016/2019)","type":"grant"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"16e3279184925dbcf18a07345d906626","permalink":"https://laurentperrinet.github.io/project/art-science/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/art-science/","section":"project","summary":"Liste d'actions entre art et sciences.","tags":["research-interests"],"title":"Art \u003c\u003e Sciences","type":"project"},{"authors":["Victor Boutin","Laurent U Perrinet"],"categories":null,"content":"DOC2AMU is co-funded by the prestigious Marie Skłodowska-Curie COFUND action within the H2020 Research and Innovation programme of the European Union and by the Regional Council of Provence-Alpes-Côte d’Azur, with a contribution from A*MIDEX Foundation.\nWithin this programme, the PhD fellows will sign a three-year work contract with one of the 12 Doctoral Schools of AMU. Numerous advantages\nThese PhD fellowships are remunerated above that of a standard French PhD contract with a gross monthly salary of 2600 € and a gross monthly mobility allowance of 300 €, which after standard deductions will amount to a net salary of approximately 1625€/month (net amount may vary slightly). A 500€ travel allowance per year and per fellow is also provided for the fellows to travel between Marseille and their place of origin. Tailored training and personalised mentoring: Fellows will define and follow a Personal Career Development Plan at the beginning of their Doctoral thesis and will have access to a variety of training options and workshops. Financial support for international research training and conferences participations. A contribution to the research costs will be provided for the benefit of the fellow.\n“This work was supported by the Doc2Amu project which received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 713750. Projet cofinancé par le Conseil Régional Provence-Alpes-Côte d’Azur. Projet cofinancé par le Conseil Régional Provence-Alpes-Côte d’Azur, la commission européenne et les Investissements d’Avenir.”\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"62a9b58a3fee98bde0872e764cec79b8","permalink":"https://laurentperrinet.github.io/grant/doc-2-amu/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/doc-2-amu/","section":"grant","summary":"DOC2AMU: An Excellence Fellowship (2016/2019).","tags":["grant","past-grant"],"title":"DOC2AMU (2016/2019)","type":"grant"},{"authors":null,"categories":null,"content":"To enable the dissemination of the knowledge that is produced in our lab, we share all source code with open source licences. This includes code to reproduce results obtained in papers (e.g. (Perrinet, Adams and Friston, 2015), (Perrinet and Bednar, 2015), (Khoei et, 2017), (Perrinet, 2019), (Pasturel et al, 2020), (Daucé et al, 2020)) or courses and slides (e.g. 2019-04-03: vision and modelization, 2019-04-18_JNLF, …) and also the development of the following libraries on GitHub.\nFollow @laurentperrinet\n Bayesian Change Point A python implementation of Adams \u0026amp; MacKay 2007 “Bayesian Online Changepoint Detection” for binary inputs in  Python.\n Source code See the final publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ANEMO: Quantitative tools for the ANalysis of Eye MOvements This implementation proposes a set of robust fitting methods for the extraction of eye movements parameters.\n Source code See a poster @ Pasturel, Montagnini and Perrinet (2018) This library was used in the following publication @  Chloé Pasturel, Anna Montagnini, Laurent U Perrinet  (2020). Humans adapt their anticipatory eye movements to the volatility of visual motion properties. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     LeCheapEyeTracker Work-in-progress : an eye tracker based on webcams.\n Source code  Biologically inspired computer vision (  Python) SLIP: a Simple Library for Image Processing This library collects different Image Processing tools for use with the LogGabor and SparseEdges libraries.\n Web-site Source code  LogGabor: a Simple Library for Image Processing This library defines the set of LogGabor kernels. These are generic edge-like filters at different scales, phases and orientations. The library develops a simple method to construct a simple multi-scale linear transform.\n Web-site Source code This library is detailed in the following publication  Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal  (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision.  PDF  Cite  Code  DOI    LogGabor filters are used in numerous computer vision applications and reaches 177 citations on Google Scholar (last updated 22/10/2021).  SparseEdges: sparse coding of natural images Our goal here is to build practical algorithms of sparse coding for computer vision.\nThis class exploits the SLIP and LogGabor libraries to provide with a sparse representation of edges in images.\n Web-site Source code This algorithm was presented in the following paper, which is available as a reprint  Laurent U Perrinet  (2015). Sparse Models for Computer Vision. Biologically Inspired Computer Vision.  Preprint  PDF  Cite  Code  DOI    It was notably used in the following paper  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     Sparse Hebbian Learning : unsupervised learning of natural images This is a collection of python scripts to test learning strategies to efficiently code natural image patches. This is here restricted to the framework of the SparseNet algorithm from Bruno Olshausen (http://redwood.berkeley.edu/bruno/sparsenet/).\n Source code This algorithm was presented in the following paper  Laurent U Perrinet  (2010). Role of homeostasis in learning sparse representations. Neural Computation.  Preprint  PDF  Cite  Code  DOI    54 citations on Google Scholar (last updated 22/10/2021) Follow-up paper  Laurent U Perrinet  (2019). An adaptive homeostatic algorithm for the unsupervised learning of visual features. Vision.  Preprint  PDF  Cite  Code  DOI     MotionClouds MotionClouds are random dynamic stimuli optimized to study motion perception.\n Web-site Source code using  Python. This algorithm was presented in the following paper  Paula Sanz Leon, Ivo Vanzetta, Guillaume S Masson, Laurent U Perrinet  (2012). Motion Clouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception. Journal of Neurophysiology.  Preprint  PDF  Cite  DOI    37 citations on Google Scholar (last updated 22/10/2021) Follow-up paper  This library was notably used in the following paper  Claudio Simoncini, Laurent U Perrinet, Anna Montagnini, Pascal Mamassian, Guillaume S Masson  (2012). More is not always better: dissociation between perception and action explained by adaptive gain control. Nature Neuroscience.  PDF  Cite  DOI     PyNN PyNN is a simulator-independent language for building neuronal network models using  Python.\n Web-site Source code This algorithm was presented in the following paper  Andrew P Davison, Daniel Bruderle, Jochen Eppler, Jens Kremkow, Eilif Muller, Dejan Pecevski, Laurent U Perrinet, Pierre Yger  (2008). …","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"505583b2371f2a63507bb831a9d4a60b","permalink":"https://laurentperrinet.github.io/project/open-science/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/open-science/","section":"project","summary":"To enable the dissemination of the knowledge that is produced in our lab, we share all source code with open source licences.","tags":["log-gabor","psychophysics","motion-clouds"],"title":"Open Science","type":"project"},{"authors":["Angelo Franciosini","Laurent U Perrinet"],"categories":null,"content":"Description The Ph.D. program in Integrative and Clinical Neuroscience (Aix-Marseille University) is offering in 2017 three Ph.D. scholarships to Master students graduated from highly ranked international universities (outside France). We were awarded with one PhD position for Angelo Franciosini at the “Institut de Neurosciences de la Timone” (team “Inference and Visual Behavior”), CNRS, Marseille (France) to study trajectories in natural images and the sensory processing of contours.\n##Funding\nThis project is funded by the Aix-Marseille Université, which was awarded the prestigious status of “Excellence Initiative” (A*MIDEX) by the French Government and considering interdisciplinary studies as one of its main axes of growth. Within this program, the PhD fellow will sign a three-year work contract. They will enroll the ICN PhD program offering personalized follow-up to the students, a wide spectrum of scientific and professional training activities including specialized courses and career development activities and interactions with multi-disciplinary researchers at Aix-Marseille University and top world-wide visiting speakers, in a vibrant international community of students.\nAcknowledgement This work has received support from the French government under the Programme Investissements d’Avenir, Initiative d’Excellence d’Aix-Marseille Université via A*Midex (AMX-19-IET-004) and ANR (ANR-17-EURE-0029) funding.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"5180f685c974623d9956559759606de2","permalink":"https://laurentperrinet.github.io/grant/phd-icn/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/grant/phd-icn/","section":"grant","summary":"A grant from the Ph.D. program in Integrative and Clinical Neuroscience (PhD position, 2017 / 2021).","tags":["grant","past-grant","phd-icn"],"title":"PhD ICN (2017 / 2021)","type":"grant"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"63e1fd879d909485461334c861ad9b77","permalink":"https://laurentperrinet.github.io/project/tout-public/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/tout-public/","section":"project","summary":"Liste d'actions destinées à la culture scientifique et au public en général.","tags":["research-interests","EtienneRey","PollyMaggoo"],"title":"Tout public!","type":"project"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Les illusions visuelles, un révélateur du fonctionnement de notre cerveau Cinésciences, collège Clair Soleil L’Association Polly Maggoo http://www.pollymaggoo.org/ met en place tout le long de l’année, des actions de culture scientifique et artistique en direction des collèges et des lycées, les Cinésciences, au cours desquelles l’association programme des films à caractère scientifique, au sein d’établissements scolaires. Les projections se déroulent en présence de chercheurs et/ou de cinéastes dans la perspective d’un développement de la culture cinématographique et scientifique en direction des publics scolaires.\n Date\n25 Avril 2016 Location\ncollège Clair Soleil, Marseille Visuels\nHTML  ","date":1461574800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461574800,"objectID":"785e955b92a4f66b8bba5eb8e5ddced6","permalink":"https://laurentperrinet.github.io/talk/2016-04-25-polly-maggoo/","publishdate":"2016-04-25T09:00:00Z","relpermalink":"/talk/2016-04-25-polly-maggoo/","section":"talk","summary":"Ce lundi 25 avril de 9h à 12h, je suis venu échanger au côté de Serge Dentin autour de films traitant du rapport fiction/réel, des illusion visuelles (\\\" Qu'est ce qu'une image? \\\"), des rapports d'échelles, de la perception, ... et qui sont projetés lors de la séance, avec des élèves de 4e lors d'une séance Cinésciences au collège Clair Soleil, 53 Boulevard Charles Moretti, 13014 Marseille. Une occasion aussi de parler du métier de chercheur.","tags":null,"title":"Les illusions visuelles, un révélateur du fonctionnement de notre cerveau","type":"talk"},{"authors":["Laurent U Perrinet","Étienne Rey"],"categories":null,"content":"Elasticité dynamique @ Fondation Vasarely à Aix-en-Provence    2016, https://github.com/NaturalPatterns/elasticite  L’installation ‘‘Elasticité dynamique’’ agit comme un filtre et génère de nouveaux espaces démultipliés, comme un empilement quasi infini d’horizons. Par principe de réflexion, la pièce absorbe l’image de l’environnement et accumule les points de vue ; le mouvement permanent requalifie continuellement ce qui est regardé et entendu.\n DIMENSIONS : 3 m de haut 5 m de large, INOX POLI MIROIR / ALUMINIUM / ACIER / MOTEURS / PROGRAMME TEMPS RÉEL LIEU : Fondation Vasarely EXPOSITION : Multiplicité, Fondation Vasarely dans le cadre de l’Hommage à Victor Vasarely EXPOSITION : DU 2 juin au 2 octobre 2016 VIDEOS : http://vimeo.com/198189587 Crédits : © Etienne Rey  Composé d’une succession de lames de miroirs, verticales et rotatives, l’installation Trame se joue des reflets et de la démultiplication de l’espace, offrant au spectateur une multiplicité de points de vue dans lesquels il peut se perdre à loisir. Par un effet de « porosité » recherché par l’artiste, le dispositif dialogue intensément avec les Intégrations.\nDevant l’œuvre en constante métamorphose, l’alphabet plastique de Vasarely se recompose ainsi à l’infini comme un jeu de construction renouvelable. Dans cette œuvre, Etienne Rey explore en profondeur les possibilités offertes par le mouvement, la lumière, et surtout l’interaction entre l’œuvre, le public et l’espace, ouvrant sur de nouveaux rapports sensibles et sensoriels au monde.\n(Texte : Véronique Baton)\nElasticité dynamique @ 104 (Paris)    LIEU:: NEMO, BIENNALE INTERNATIONALE DES ARTS NUMERIQUES - CENTQUATRE - 104 EXPOSITION : Prosopopées : Quand les objets prennent vie VERNISSAGE : SAMEDI 5 DÉCEMBRE 14h \u0026gt; 23h30 EXPOSITION : DU 6 DÉCEMBRE 2015 AU 18 JANVIER 2016 VIDEOS : https://vimeo.com/150654250 : installation; https://vimeo.com/146242233 : simulation, Crédits : Elasticité dynamique © Etienne Rey, Adagp Paris 2015       Elasticité dynamique est composée des pièces Expansion, Trame et Lignes sonores. Volume hexagonal en miroir de 7 mètres de diamètre, Expansion fonctionne comme une chambre d\u0026#39;écho. A l\u0026#39;intérieur de ce volume se situe Trame. Constituée de 25 lames de miroir en rotation, cette pièce réoriente continuellement le regard. Quant à Lignes sonores, elle est formée de quatre monolithes orientés vers Expansion et émet des sons qui se réorientent en fonction du mouvement des lames. (© Etienne Rey, Adagp Paris 2015)\u0026#39;\u0026#39;|width=\u0026#34;100%\u0026#34;}}Elasticité dynamique est composée des pièces Expansion, Trame et Lignes sonores. \u0026lt;\u0026lt;BR\u0026gt;\u0026gt; Volume hexagonal en miroir de 7 mètres de diamètre, Expansion fonctionne comme une chambre d\u0026#39;écho. A l\u0026#39;intérieur de ce volume se situe Trame. Constituée de 25 lames de miroir en rotation, cette pièce réoriente continuellement le regard. Quant à Lignes sonores, elle est formée de quatre monolithes orientés vers Expansion et émet des sons qui se réorientent en fonction du mouvement des lames. \u0026lt;\u0026lt;BR\u0026gt;\u0026gt;(© Etienne Rey, Adagp Paris 2015)  EQUIPE  Etienne Rey : Artiste plasticien Wilfried Wendling : Compositeur Laurent Perrinet : Chercheur en Neurosciences à l’INT / CNRS-AMU Atelier Ni : Accompagnement conception Gauthier Le Rouzic : Électronique Lucie Evans : Assistante Remerciements : Guillaume Stagnaro  PRODUCTION DÉLÉGUÉE  Seconde Nature  SOUTIENS  DRAC PACA _aide individuelle à la création REGION PACA _CAC art visuel  CO-PRODUCTION  ARCADI La Muse en Circuit. Centre National de Création Musicale CNRS-AMU / INT, Institut de Neurosciences de la Timone  ","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"093476882707c90c74e5697348d6fafa","permalink":"https://laurentperrinet.github.io/post/2016-06-02_elasticite/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/2016-06-02_elasticite/","section":"post","summary":"L'installation ''Elasticité dynamique'' agit comme un filtre et génère de nouveaux espaces démultipliés, comme un empilement quasi infini d'horizons. Par principe de réflexion, la pièce absorbe l'image de l'environnement et accumule les points de vue ; le mouvement permanent requalifie continuellement ce qui est regardé et entendu.","tags":["art-science"],"title":"Elasticité dynamique","type":"post"},{"authors":["Laurent U Perrinet","Étienne Rey"],"categories":null,"content":"Instabilité (series) Installation (sérigraphie, dessin mural, lumière), 2018; Guest artist, Mécènes du Sud / Art-O-Rama (Fair), Marseille I 2018 Mécènes du Sud invite chaque année un artiste lauréat pour concevoir un stand-projet au sein du salon d’art contemporain ART-O-RAMA : \u0026#34; Le travail d’Etienne Rey, lauréat 2011, explore la notion même d’espace. Il détourne des phénomènes physiques pour faire surgir par des biais perceptifs une conscience “d’être là”. Ses installations sont expérientielles par nature. Elles ne proposent pas une expérience elles la contiennent. Ce sont des oeuvres actives qui impliquent présence et espace. Instabilités, l’ensemble présenté ici, issu de recherches en cours de développement, met en tension espace et temps dans une expérience vibratoire. Chaos et cristal attisent des forces instables dans un rapport de contraintes qui détermine la complexité des oeuvres présentées.\u0026#34; Bénédicte Chevallier\n En collaboration avec le chercheur Laurent Perrinet, CNRS-AMU / Institut de Neurosciences de la Timone  Links / Liens:  ART-O-RAMA - http://art-o-rama.fr/en https://www.lemonde.fr/argent/article/2018/08/19/avec-art-o-rama-marseille-se-demarque-sur-le-marche-de-l-art_5343904_1657007.html https://www.apollo-magazine.com/why-manifesta-makes-sense-in-marseille/ Retour par En revenant de l’expo !      SALON INTERNATIONAL D’ART CONTEMPORAIN ART-O-RAMA SALON : 31 AOÛT \u0026gt; 2 SEPTEMBRE 2018 EXPOSITION \u0026gt; 9 SEPTEMBRE 2018 ESPACE PARTENAIRES - J1, QUAI DE LA JOLIETTE, MARSEILLE 2e  ","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"1d8f82066b4bfb55cefdeca0193a2eca","permalink":"https://laurentperrinet.github.io/post/2018-09-09_artorama/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/2018-09-09_artorama/","section":"post","summary":"Installation (sérigraphie, dessin mural, lumière), 2018;  Guest artist, Mécènes du Sud / Art-O-Rama (Fair), Marseille I 2018.","tags":["art-science"],"title":"Instabilité (series) @ Art-O-Rama","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"  “Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.” Edsger Dijkstra\n  “Si les cochons pouvaient regarder en l’air, on en ferait des marins…” (anonyme)\n  L’art optique, c’est : « ce qui se passe dans l’esprit du spectateur quand son œil est obligé d’organiser un champ perceptif tel qu’il est nécessairement instable» Viktor Vasarely\n  “To us, probability is the very guide of life.” Marcus Tullius Cicero (106 BCE - 43 BCE)\n  “Notre mère stérile réclame un enfant. Mon ami, mon amour d’ami, Que cela soit terrible ou sublime, Ce n’est pas moi qui clame, c’est la terre qui tonne” Attila József (1924, traduit dans la chanson éponyme de Noir Désir)\n  “Savoir marcher sur le fil tendu entre la frontière des densités humaines sauve de l’isolement.” Babouillec\n  « Ne pense pas mais regarde plutôt ! » Ludwig Wittgenstein (Remarques philosophiques, fragment 66)\n  “Free will, we’re determined to have it”\n  “Scientist would rather borrow the toothbrush of other scientists than their words” - G. Edelman\n  “Slow is smooth, smooth is fast.” Emily Harrington on El Cap\n  “The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.” - Edsger W. Dijkstra\n  “We are all prodigious Olympians in perceptual and motor areas, so good that we make the difficult look easy.” (Hans Moravec)\n  “Science is like sex: sometimes something useful comes out, but that is not the reason we are doing it.” (Richard Feynman)\n  “Mathematics is no more computation than typing is literature.” (John Allen Paulos)\n  “C’est ce que je fais qui m’apprend ce que je cherche.” (Pierre Soulages)\n  “Harry Potter: Is this real? Or has this been happening inside my head? Professor Albus Dumbledore: Of course it is happening inside your head, Harry, but why on earth should that mean that it is not real?” ― (J.K. Rowling, Harry Potter and the Deathly Hallows)\n  “I do know what time is,” Tubby declared. He paused. “Time,” he added slowly – “time is what keeps everything from happening at once. I know that–I seen it in print too.” (Ray Cummings)\n  “You don’t see it because it’s there, it’s there because you see it.”\n  “Look for the bare necessities / The simple bare necessities / Forget about your worries and your strife / I mean the bare necessities / Old Mother Nature’s recipes / That bring the bare necessities of life” – Baloo’s song [The Jungle Book]\n  “Ni rire, ni pleurer, ni haïr, mais comprendre” (Baruch Spinoza)\n  “For years there has been a theory that millions of monkeys typing at random on millions of typewriters would reproduce the entire works of Shakespeare. The Internet has proven this theory to be untrue.” (???, ???)\n  “Je me suis endormie, ce matin/en pensant/sur tes lèvres.” (Anonyme, sur les murs, Marseille, 2017)\n  “If you can look into the seeds of time, And say which grain will grow and which will not; Speak…” (Shakespeare, Macbeth, Act I, Scene 3)\n  “Here, too, the honorable finds its due, and there are tears for passing things; here, too, things mortal touch the mind.” (Virgil, Aeneid, 29-19 BC)\n  “Man’s maturity is to have regained the seriousness that he had as a child at play.” (Friedrich Nietzsche)\n  “There are 10 types of people in the world. Those who understand binary, those who don’t, those who weren’t expecting a base 8 joke, and 5 other types of people.” John Story\n  “Ce qui fut se refait; tout coule comme une eau / Et rien dessous le Ciel ne se voit de nouveau/ Mais la forme se change en une autre nouvelle/ Et ce changement-là. Vivre au monde s’appelle” - Ronsard, Hymnes.\n  “Hanlon’s Razor: Never attribute to malice what is adequately explained by stupidity.”\n  “Essentially, all models are wrong, but some are useful.” Box, George E. P.; Norman R. Draper (1987). Empirical Model-Building and Response Surfaces, p. 424, Wiley. ISBN 0471810339\n  “Computers were around for 50 years before we figured out how to create the internet for example.”\n  “In sum, the physicist can never subject an isolated hypothesis to experimental test, but only a whole group of hypotheses; when the experiment is in disagreement with his predictions, what he learns is that at least one of the hypotheses constituting this group is unacceptable and ought to be modified; but the experiment does not designate which one should be changed.” (P. Duhem, The Aim and Structure of Physical Theory, 1914)\n  “Un bon maître a ce souci constant : enseigner à se passer de lui.” - André Gide\n  “Truth in science can be defined as the working hypothesis best suited to open the way to the next better one.”(Konrad Lorenz)\n  “all motion is illusion” (Zeno of Elea, 490 BC )\n  “Be humble for you are made of dung. Be noble for you are made of stars.” (Serbian proverb)\n  “voir des objets ne consiste pas à en extraire des traits visuels, mais à guider visuellement l’action dirigée vers eux.” (Francisco Varela in ‘‘L’inscription …","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"1395225e7ad60a54933bb58000170e80","permalink":"https://laurentperrinet.github.io/post/proverbes-et-citations/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/proverbes-et-citations/","section":"post","summary":"Proverbes et citations glanés au cours des années","tags":null,"title":"Proverbes Et Citations","type":"post"},{"authors":["Laurent U Perrinet","Étienne Rey"],"categories":null,"content":"   « Densité flou » (2019)   Étienne Rey - Horizon faille - Densité flou, 2019 - Sans gravité - une poétique de l’air à - Ardenome - Avignon © https://www.enrevenantdelexpo.com  « Tension superficielle » (2019)   Étienne Rey – Horizon faille – Tension superficielle, 2019 – Sans gravité – une poétique de l’air à – Ardenome – Avignon © https://www.enrevenantdelexpo.com  dans le cadre de “SANS GRAVITÉ, UNE POÉTIQUE DE L’AIR - ETIENNE REY / MATHILDE LAVENNE / HUGO DEVERCHÈRE / EDITH DEKYNDT - 23 MARS \u0026gt; 22 JUIN 2019”  Sans Gravité a été conçue par EDIS dans le cadre de “Chroniques, Biennale des Imaginaires Numériques”, qui réunit un ensemble d’institutions culturelles de la Région PACA. Après Aix-Marseille, l’Ardenome à Avignon est la seconde étape de ce parcours régional.\n  Horizon faille est une installation globale qui cherche à défier la gravité de la nature. Prenant appui sur deux notions dont l’artiste en a fait ses motifs principaux - les failles du paysage et l’immatérielle ligne d’horizon. La notion de faille exprime la fracture, au sens géologique. Elle est à considérer comme un interstice, une zone de transformation, un passage d’un état à un autre. De même, l’horizon scinde la terre du ciel dans une tentative de géométrisation de l’univers, de mise en espace des éléments naturels. Intouchable ligne de partage, ce filin tendu désigne aussi le seuil de vision du paysage. C’est la ligne imaginaire qui se forme à partir de notre position dans l’espace. Elle est ce qui échappe à la vue ou à la représentation.\n  L’ensemble de ces œuvres résonnent en échos visuels les unes avec les autres. Elles décrivent des mouvements, des points de rupture, forment des zones de passage, explorent des états de métamorphose issus d’un paysage initial dont les perspectives ont été dépliés. En parcourant du regard ces différentes propositions plastiques, le visiteur prend conscience de l’espace qui l’entoure et le trouble à la fois. Un espace élastique, démultiplicateur qui lui offre une diversité d’angles dans lesquels il peut se perdre à loisir, comme en état d’apesanteur. Le monde n’est pas figé, il est une dérive constante, une recomposition permanente. Véronique Baton\n  Accompagnement Réalisation : Atelier Ni et Guillaume Stagnaro Variable Density et Tension Superficielle en collaboration avec Laurent Perrinet, chercheur Institut de neurosciences de la Timone, INT. Tirage sérigraphique Atelier Tchikebe.  Links / Liens:   ardemone https://www.ardenome.fr/sans-gravite-ardenome-chroniques\n  Retour par En revenant de l’expo !\n  SALON INTERNATIONAL D’ART CONTEMPORAIN ART-O-RAMA\n  SALON : 31 AOÛT \u0026gt; 2 SEPTEMBRE 2018\n  EXPOSITION \u0026gt; 9 SEPTEMBRE 2018\n  ESPACE PARTENAIRES - J1, QUAI DE LA JOLIETTE, MARSEILLE 2e\n  ","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"3005367f2fe26923e015fe5bfc4fbe18","permalink":"https://laurentperrinet.github.io/post/2019-06-22_ardemone/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/2019-06-22_ardemone/","section":"post","summary":"Sans gravité – une poétique de l’air – Ardenome à Avignon.","tags":["art-science"],"title":"Sans gravité – une poétique de l’air","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"TRAMES   À la Fondation Vasarely à Aix-en-Provence, Etienne Rey a choisi d’installer dans la salle des Intégrations architectoniques un ballet visuel hypnotique.\n«Trame instabilité» est un travail en cours de recherche. Le projet est basé sur des principes d’occultations partielles en couches associées à des trames qui font émerger une dimension immatérielle. L’expérience de perception de ces motifs produit un sentiment de basculement de la perception dans le sens où le motifs réel passe au second plan pour laisser place à l’émergence d’une figure du vide, c’est dans les blancs immatériel que des formes apparaissent et vacillent occupant tout notre champ visuel. Ces apparitions virtuelles, purs phénomènes optiques n’existent pas dans notre monde «physique», réel. «BR» Ce qui est en jeu ici c’est l’émergence de l’apparition de motifs virtuels résultat de la relation entre une réalité physique, la grandeur et l’ordonnancement de trames et notre physiologie qui conduit à cette état de perception. Lorsqu’on est fasse à ces motifs ce qui saute au yeux plus que le motif réel c’est sa résultante, instable et éphémère qui fait apparaitre une richesse de figures géométriques qui se transforment et évoluent en fonction du temps d’observation et du point de vue. Sur ce principe de dispositif optique, le travail de chacun des motifs, lié à un séquençage de trames conduit à faire apparaitre une composition et des émergences de formes spécifiques. L’expérience de perception de chacun des motifs explore les notions d’instabilité, de flux, d’émergences … dont l’expérience donne à entrevoir des formes que l’on retrouve dans la nature ou les phénomènes naturels: le dessin du pelage d’un zèbre, une accumulation de bulles de savons, ou plus généralement dans les compositions chimiques issue de la théorie de la morphogénèse de Turing.\n   En collaboration avec le chercheur Laurent Perrinet, CNRS-AMU / Institut de Neurosciences de la Timone.\n Trame, Élasticité \u0026amp; Écran n°3 étaient aussi présentés au Festival Ososphère, Strasbourg en Avril 2017.  ","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"dbf3eca19c7eba6705c4104c9f348a78","permalink":"https://laurentperrinet.github.io/post/2018-04-10_trames/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/2018-04-10_trames/","section":"post","summary":"À la Fondation Vasarely à Aix-en-Provence, Etienne Rey a choisi d’installer dans la salle des Intégrations architectoniques un ballet visuel hypnotique.","tags":["art-science"],"title":"TRAMES","type":"post"},{"authors":["Laurent U Perrinet","Étienne Rey"],"categories":null,"content":"TROPIQUE    https://github.com/NaturalPatterns/Tropique        ‘‘Etienne Rey investigates the invisible and mutual relationships which take place between human and his environment. Prize-winner of the 1st call for projects of the RAN, its project of immersive installation Tropique puts in link the perception of the space connected to the movement, to the light and to the sound. Within the framework of a residence of creation in the Centre des arts, the object of which is ” to sculpt the light “, he presents a work in progress of this installation.’’\n      Tropique est une installation environnementale, un espace vide de matière, qui se densiﬁe en ondes sonores et lumineuses, activées et modulées par la présence et l’activité humaines. Ce projet met en lien la perception de l’espace articulée au mouvement, à la lumière et au son. Les personnes qui se situent dans l’espace sont entourées d’une aura lumineuse et sonore qui ﬂuctue en fonction des mouvements et de la proximité des corps.\n   Etienne Rey investigates the invisible and mutual relationships which take place between human and his environment. Prize-winner of the 1st call for projects of the RAN, its project of immersive installation Tropique puts in link the perception of the space connected to the movement, to the light and to the sound. Within the framework of a residence of creation in the Centre des arts, the object of which is ” to sculpt the light “, he presents a work in progress of this installation.\u0026#39;\u0026#39;\n        “Tropique plonge le visiteur au coeur d’un espace embrumé, sculpté par la lumière et le son. Expérience sensorielle, le monde de Tropique évolue dans un entre-deux, rendant palpables des matérialités d’ordinaires invisibles. Expérience perceptive, le dispositif irradie l’espace, l’incorpore, l’amalgame, le dilate. La lumière se diffuse jusqu’à modifier notre rapport à l’espace, elle le redessine à travers notre propre vision et provoque une expérience personnelle, une émotion visuelle. L’environnement réagit aux variations de l’activité dans l’installation, à la façon dont nous l’habitons et le transformons. Tropique contruit un espace dynamique une architecture mobile à l’état de l’air, en miroir à notre présence. Ce qui est révélé est un ensemble vivant, la plupart du temps imperceptible, comme une mise une lumière de notre écosystème et de ses interrelations. Ce projet est élaboré avec le concours d’une équipe pluridisciplinaire composée d’un chercheur en Neuroscience : Laurent Perrinet, d’un compositeur : Wilfried Wendling d’un ingénieur : Julien Marro Dauzat.”\n   Accueilli en résidence dans le cadre des résidences de recherche de l’IMéRA pendant 6 mois (3 périodes de 2 mois),et soutenu dans le cadre d’un Atelier de l’!EuroMéditerranée, ce projet est élaboré en collaboration avec des chercheurs. Nous abordons ainsi les questions de la cognition et de la perception de l’espace liées à la vue, à l’audition, et au déplacement, auxquelles nous lierons les questions relatives à la diffusion de phénomènes ondulatoires.\nnews    space odyssée à l’institut francais en Coree du Sud de juin a octobre 2016 Un entretien d’ER durant le Mois multi 2015     Un entretien avec Roger Malina     Installation Space Odyssey : Tournée en Corée avec Mac de Créteil en 2016 / dates et lieux à venir. Installation Space Odyssey : FESTIVAL VIA / MAUBEUGE / 12 AU 22 MARS 2015 - FESTIVAL EXIT / CRÉTEIL / 26 MARS AU 05 AVRIL 2015 - LE PRINTEMPS À SAINT SAUVEUR / LILLE / 27 AVRIL 2016 AU 28 AOÛT 2016 : Home Cinema. À lire, sur digitatarti. Installation Tropique : Festival international d’arts multidisciplinaires et électroniques Le Mois Multi 16 / Québec du 4 février au 1 mars 2015     Installation Space Odyssey : Lille 3000, du 26 SEPT 2015 \u0026gt; 17 JAN 2016 Installation Space Odyssey : FESTIVAL INTERNATIONAL EXIT 2015 Créteil du 26 MARS -\u0026gt; 05 AVRIL 2015 Installation Space Odyssey : Festival Via 2015, Maubeuge du jeudi 12 mars 2015 au dimanche 22 mars 2015 Du 13 au 23 Mars 2014: TROPIQUE @ FESTIVAL VIA : Depuis presque 30 ans, le Festival VIA flirte avec les frontières des territoires artistiques, à la croisée des arts de la scène, de la création technologique et numérique. Toujours plus international et interdisciplinaire, VIA traduit, à Maubeuge et à Mons, la vitalité de la scène contemporaine. Il est également à noter que cette édition préfigure la Capitale européenne de la culture 2015 qui se déroulera dans ces deux villes l’année prochaine. Pour cette nouvelle édition de VIA sera présentée l’installation Tropique d’Étienne Rey, produite par Seconde Nature et récemment dévoilée lors du festival « Chroniques des Mondes Possibles » dans le cadre d’E-topie, Marseille-Provence 2013. 16 novembre au 15 décembre 2013: présentation à Paris dans le cadre du festival nemo ; écouter la tête au carré sur France Inter 10 Octobre au 10 novembre 2013: présentation finale à la fondation Vasarely à Aix-en-Provence dans le cadre du festival e-Topie, cf cahier beaux-arts de libé, la …","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"31e54815e17de7441bcd80f031187290","permalink":"https://laurentperrinet.github.io/post/2013-10-10_tropique/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/2013-10-10_tropique/","section":"post","summary":"Tropique is an immersive installation which puts in link the perception of the space connected to the movement, to the light and to the sound.","tags":["art-science"],"title":"TROPIQUE","type":"post"},{"authors":["Étienne Rey"],"categories":null,"content":"Turbulences   Installation in situ, 2018; Collection of the François Schneider Foundation, Wattwiller I 2018  “En agissant sur les variations et les rayonnements d’ordre physique, Etienne Rey fait apparaître l’épaisseur existentielle du vide.” Bénédicte Chevallier, Mécènes du Sud\n L’installation Turbulences explore l’émergence de caustiques, phénomènes caractéristiques de la relation entre l’eau, la lumière et l’air. Le mouvement y perturbe un état optique stable. La turbulence des plis lumineux donne l’illusion d’un corps flottant.\nInstallation immersive plastique et sonore, l’expérience de l’oeuvre se joue dans l’exploration de dimensions immatérielles par l’observateur.\nCréation originale pour la Fondation François Schneider Production : Quatre 4.0 / L’Ososphère, en partenariat avec la Fondation François Schneider et le soutien de la Région Grand Est Accompagnement, production déléguée : bOssa\n INAUGURATION : SAMEDI 27 OCTOBRE 2018 EXPOSITION \u0026gt; 20 JANVIER 2019 FONDATION FRANÇOIS SCHNEIDER - 27 RUE DE LA PREMIÈRE ARMÉE, 68700 WATTWILLER  ","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"f5167cd8b8cfe95928f6c3e3a6a8a061","permalink":"https://laurentperrinet.github.io/post/2018-01-20_turbulences/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/2018-01-20_turbulences/","section":"post","summary":"L’installation Turbulences explore l’émergence de caustiques, phénomènes caractéristiques de la relation entre l’eau, la lumière et l’air. Le mouvement y perturbe un état optique stable. La turbulence des plis lumineux donne l’illusion d’un corps flottant.","tags":["art-science"],"title":"Turbulences","type":"post"},{"authors":["Wahiba Taouali","Giacomo Benvenuti","Pascal Wallisch","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":"","date":1453420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453420800,"objectID":"bd64c4a35030fe282cdf6f718bb8cac8","permalink":"https://laurentperrinet.github.io/publication/taouali-16/","publishdate":"2016-01-22T00:00:00Z","relpermalink":"/publication/taouali-16/","section":"publication","summary":"The repeated presentation of an identical visual stimulus in the receptive field of a neuron may evoke different spiking patterns at each trial. Probabilistic methods are essential to understand the functional role of this variance within the neural activity. In that case, a Poisson process is the most common model of trial-to-trial variability. For a Poisson process, the variance of the spike count is constrained to be equal to the mean, irrespective of the duration of measurements. Numerous studies have shown that this relationship does not generally hold. Specifically, a majority of electrophysiological recordings show an \\\" over-dispersion \\\" effect: Responses that exhibit more inter-trial variability than expected from a Poisson process alone. A model that is particularly well suited to quantify over-dispersion is the Negative-Binomial distribution model. This model is well-studied and widely used but has only recently been applied to neuroscience. In this paper, we address three main issues. First, we describe how the Negative-Binomial distribution provides a model apt to account for overdispersed spike counts. Second, we quantify the significance of this model for any neurophysiological data by proposing a statistical test, which quantifies the odds that over-dispersion could be due to the limited number of repetitions (trials). We apply this test to three neurophysiological tests along the visual pathway. Finally, we compare the performance of this model to the Poisson model on a population decoding task. We show that the decoding accuracy is improved when accounting for over-dispersion, especially under the hypothesis of tuned over-dispersion. ","tags":["coding decoding","spike"],"title":"Testing the odds of inherent vs. observed overdispersion in neural spike counts","type":"publication"},{"authors":["Wahiba Taouali","Giacomo Benvenuti","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in this poster This is a followup in Perrinet et al, 2012  ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ae433a6885751e176896111ed33940e6","permalink":"https://laurentperrinet.github.io/publication/taouali-15-vss/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/taouali-15-vss/","section":"publication","summary":"Natural scenes generally contain objects in motion. The local orientation of their contours and the direction of motion are two essential components of visual information which are processed in parallel in the early visual areas. Focusing on the primary visual cortex of the macaque monkey (V1), we challenged different models for the joint representation of orientation and direction within the neural activity. Precisely, we considered the response of V1 neurons to an oriented moving bar to investigate whether, and how, the information about the bar's orientation and direction could be encoded dynamically at the population activity level. For that purpose, we used a decoding approach based on a space-time receptive field model that encodes jointly orientation and direction. We based our decoding approach on the statistics of natural scenes by first determining optimal space-time receptive fields (RFs) that encode orientation and direction. For this, we first derived a set of dynamic filters from a database of natural images~[1] and following an unsupervised learning rule~[2]. More generally, this allows us to propose a dynamic generative model for the joint coding of orientation and direction. Then, using this model and a maximum likelihood paradigm, we infer the most likely representation for a given network activity~[3, 4]. We tested this model on surrogate data and on extracellular recordings in area emphV1 (67 cells) of awake macaque monkeys in response to oriented bars moving in $12$ different directions. Using a cross-validation method we could robustly decode both the orientation and the direction of the bar within the classical receptive field (cRF). Furthermore, this decoding approach shows different properties: First, information about the orientation of the bar is emerging ıt before entering the cRF if the trajectory of the bar is long enough. Second, when testing different orientations with the same direction, our approach unravels that we can decode the direction and the orientation independently. Moreover, we found that, similarly to orientation decoding, the decoding of direction is dynamic but weaker. Finally, our results demonstrate that the orientation and the direction of motion of an ambiguous moving bar can be progressively decoded in V1. This is a signature of a dynamic solution to the aperture problem in area V1, similarly to what was already found in area MT~[5]. $[1]$ J. Burge, W. Geisler. Optimal speed estimation in natural image movies predicts human performance. Nature Communications, 6, 7900. http://doi.org/10.1038/ncomms8900, 2015.  $[2]$ L. Perrinet. Role of homeostasis in learning sparse representations. ıt Neural Computation, 22(7):1812--36, 2010.  $[3]$ M. Jazayeri and J.A. Movshon. Optimal representation of sensory information by neural populations. ıt Nature Neuroscience, 9(5):690--696, 2006. $[4]$ W. Taouali, G. Benvenuti, P. Wallisch, F. Chavane, L. Perrinet. Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts. ıt Journal of Neurophysiology, 2015. $[5]$ C. Pack, R. Born. Temporal dynamics of a neural solution to the aperture problem in visual area MT of macaque brain. ıt Nature, 409(6823), 1040--1042. 2001.","tags":["coding decoding"],"title":"A dynamic model for decoding direction and orientation in macaque primary visual cortex","type":"publication"},{"authors":["Wahiba Taouali","Giacomo Benvenuti","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"9bdab2eb5d93df2c1ee02a5319953dd1","permalink":"https://laurentperrinet.github.io/publication/taouali-16-areadne/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/taouali-16-areadne/","section":"publication","summary":"Natural scenes generally contain objects in motion. The local orientation of their contours and the direction of motion are two essential components of visual information which are processed in parallel in the early visual areas. Focusing on the primary visual cortex of the macaque monkey (V1), we challenged different models for the joint representation of orientation and direction within the neural activity. Precisely, we considered the response of V1 neurons to an oriented moving bar to investigate whether, and how, the information about the bar's orientation and direction could be encoded dynamically at the population activity level. For that purpose, we used a decoding approach based on a space-time receptive field model that encodes jointly orientation and direction. We based our decoding approach on the statistics of natural scenes by first determining optimal space-time receptive fields (RFs) that encode orientation and direction. For this, we first derived a set of dynamic filters from a database of natural images~[1] and following an unsupervised learning rule~[2]. More generally, this allows us to propose a dynamic generative model for the joint coding of orientation and direction. Then, using this model and a maximum likelihood paradigm, we infer the most likely representation for a given network activity~[3, 4]. We tested this model on surrogate data and on extracellular recordings in area emphV1 (67 cells) of awake macaque monkeys in response to oriented bars moving in $12$ different directions. Using a cross-validation method we could robustly decode both the orientation and the direction of the bar within the classical receptive field (cRF). Furthermore, this decoding approach shows different properties: First, information about the orientation of the bar is emerging ıt before entering the cRF if the trajectory of the bar is long enough. Second, when testing different orientations with the same direction, our approach unravels that we can decode the direction and the orientation independently. Moreover, we found that, similarly to orientation decoding, the decoding of direction is dynamic but weaker. Finally, our results demonstrate that the orientation and the direction of motion of an ambiguous moving bar can be progressively decoded in V1. This is a signature of a dynamic solution to the aperture problem in area V1, similarly to what was already found in area MT~[5]. $[1]$ J. Burge, W. Geisler. Optimal speed estimation in natural image movies predicts human performance. Nature Communications, 6, 7900. http://doi.org/10.1038/ncomms8900, 2015.  $[2]$ L. Perrinet. Role of homeostasis in learning sparse representations. ıt Neural Computation, 22(7):1812--36, 2010.  $[3]$ M. Jazayeri and J.A. Movshon. Optimal representation of sensory information by neural populations. ıt Nature Neuroscience, 9(5):690--696, 2006. $[4]$ W. Taouali, G. Benvenuti, P. Wallisch, F. Chavane, L. Perrinet. Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts. ıt Journal of Neurophysiology, 2015. $[5]$ C. Pack, R. Born. Temporal dynamics of a neural solution to the aperture problem in visual area MT of macaque brain. ıt Nature, 409(6823), 1040--1042. 2001.","tags":["coding decoding"],"title":"A dynamic model for decoding direction and orientation in macaque primary visual cortex","type":"publication"},{"authors":["Laurent U Perrinet","Rick A Adams","Karl Friston"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"875e8c9a6676162c4f4f5816896d2780","permalink":"https://laurentperrinet.github.io/publication/perrinet-16-networks/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/perrinet-16-networks/","section":"publication","summary":"We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.","tags":["active inference"],"title":"Compensation of oculomotor delays in the visual system's network","type":"publication"},{"authors":["Anna Montagnini","Jean-Bernard Damasse","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ccd74a7dd78c79db61db6426d44a90ad","permalink":"https://laurentperrinet.github.io/publication/montagnini-16-ecvp/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/montagnini-16-ecvp/","section":"publication","summary":"","tags":["eye movements"],"title":"Effects of motion predictability on anticipatory and visually-guided eye movements: a common prior for sensory processing and motor control?","type":"publication"},{"authors":["Jean-Bernard Damasse","Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"bba457f04c75675e91f3d893a33785fc","permalink":"https://laurentperrinet.github.io/publication/damasse-16-ecvp/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/damasse-16-ecvp/","section":"publication","summary":"","tags":["eye movements","Smooth pursuit eye movement"],"title":"Modeling the effect of dynamic contingencies on anticipatory eye movements","type":"publication"},{"authors":["Jens Kremkow","Laurent U Perrinet","Cyril Monier","Jose-Manuel Alonso","Ad M Aertsen","Yves Fregnac","Guillaume S Masson"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"dbce997abfca067b735b3a9e77df0091","permalink":"https://laurentperrinet.github.io/publication/kremkow-16/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/kremkow-16/","section":"publication","summary":"Neurons in the primary visual cortex are known for responding vigorously but with high variability to classical stimuli such as drifting bars or gratings. By contrast, natural scenes are encoded more efficiently by sparse and temporal precise spiking responses. We used a conductance-based model of the visual system in higher mammals to investigate how two specific features of the thalamo-cortical pathway, namely push-pull receptive field organization and synaptic depression, can contribute to this contextual reshaping of V1 responses. By comparing cortical dynamics evoked respectively by natural vs. artificial stimuli in a comprehensive parametric space analysis, we demonstrate that the reliability and sparseness of the spiking responses during natural vision is not a mere consequence of the increased bandwidth in the sensory input spectrum. Rather, it results from the combined impacts of synaptic depression and push-pull inhibition, the later acting for natural scenes as a form of ``effective'' feed-forward inhibition as demonstrated in other sensory systems. Thus, the combination of feedforward-like inhibition with fast thalamo-cortical synaptic depression by simple cells receiving a direct structured input from thalamus composes a generic computational mechanism for generating a sparse and reliable encoding of natural sensory events.","tags":["area-v1","statistics of natural images"],"title":"Push-Pull Receptive Field Organization and Synaptic Depression: Mechanisms for Reliably Encoding Naturalistic Stimuli in V1","type":"publication"},{"authors":["Kiana Mansour-Pour","Laurent U Perrinet","Guillaume S Masson","Anna Montagnini"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"122ce2e7c14ef823da56ee097c7a9236","permalink":"https://laurentperrinet.github.io/publication/mansour-16-ecvp/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/mansour-16-ecvp/","section":"publication","summary":"","tags":["eye movements","motion detection","motion-clouds"],"title":"Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit","type":"publication"},{"authors":["Kiana Mansour-Pour","Laurent U Perrinet","Guillaume S Masson","Anna Montagnini"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"2d06995d916b105e915cc3a717ceed3f","permalink":"https://laurentperrinet.github.io/publication/mansour-16-gdr/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/mansour-16-gdr/","section":"publication","summary":"","tags":["eye movements","motion detection","motion-clouds"],"title":"Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit","type":"publication"},{"authors":["Kiana Mansour-Pour","Laurent U Perrinet","Guillaume S Masson","Anna Montagnini"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"91036ffd42e7f31a24efd73660a365c6","permalink":"https://laurentperrinet.github.io/publication/mansour-16-sfn/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/mansour-16-sfn/","section":"publication","summary":"","tags":["eye movements","motion detection","motion-clouds"],"title":"Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit","type":"publication"},{"authors":["Gabriel Cristóbal","Laurent U Perrinet","Matthias S Keil"],"categories":null,"content":"   Mindmap of the book contents. Cross-links between chapters have been indicated as thin lines.  ","date":1447977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1447977600,"objectID":"b50e63f4dc0332662df9a7de8634d9df","permalink":"https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv-chap-1/","publishdate":"2015-11-20T00:00:00Z","relpermalink":"/publication/cristobal-perrinet-keil-15-bicv-chap-1/","section":"publication","summary":"This is the introductory chapter of the book, which serves as a comprehensive but rigorous reference in the area of biologically inspired computer vision modeling. Biological vision shows excellence in terms of performance and robustness. Biologically inspired vision, that is, the study of visual systems of living beings, can be considered as a two-way process. The book often follows Marr's classical, three-level approach to vision, but also goes beyond Marr's approach in the design of novel and more advanced vision sensors. It provides an overview of a few representative applications and current state of the art of the research in this area. The book also provides an overview of bioinspired computer vision, starting from fundamentals to the most recent advances and applications in the field.","tags":["Biologically Inspired Computer vision"],"title":"Introduction","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1447977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1447977600,"objectID":"2b1f4daf1a63e8432cafb7feac19fceb","permalink":"https://laurentperrinet.github.io/publication/perrinet-15-bicv/","publishdate":"2015-11-20T00:00:00Z","relpermalink":"/publication/perrinet-15-bicv/","section":"publication","summary":"The representation of images in the brain is known to be sparse. That is, as neural activity is recorded in a visual area, for instance the primary visual cortex of primates, only a few neurons are active at a given time with respect to the whole population. It is believed that such a property reflects the efficient match of the representation with the statistics of natural scenes. Applying such a paradigm to computer vision therefore seems a promising approach towards more biomimetic algorithms. Herein, we will describe a biologically-inspired approach to this problem. First, we will describe an unsupervised learning paradigm which is particularly adapted to the efficient coding of image patches. Then, we will outline a complete multi-scale framework (SparseLets) implementing a biologically inspired sparse representation of natural images. Finally, we will propose novel methods for integrating prior information into these algorithms and provide some preliminary experimental results. We will conclude by giving some perspective on applying such algorithms to computer vision. More specifically, we will propose that bio-inspired approaches may be applied to computer vision using predictive coding schemes, sparse models being one simple and efficient instance of such schemes.","tags":["Biologically Inspired Computer vision","sparse coding"],"title":"Sparse Models for Computer Vision","type":"publication"},{"authors":["Anna Montagnini","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1447977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1447977600,"objectID":"61cb6f383891ef5679c5cfe487534495","permalink":"https://laurentperrinet.github.io/publication/montagnini-15-bicv/","publishdate":"2015-11-20T00:00:00Z","relpermalink":"/publication/montagnini-15-bicv/","section":"publication","summary":"","tags":["Biologically Inspired Computer vision"],"title":"Visual motion processing and human tracking behavior","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1446728400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446728400,"objectID":"396d7acc9b8c526046f7762b3b64e0f2","permalink":"https://laurentperrinet.github.io/talk/2015-11-05-chile/","publishdate":"2015-11-05T13:00:00Z","relpermalink":"/talk/2015-11-05-chile/","section":"talk","summary":"We stand at a point in history where our phones have become smart but lack a feature which prevails in most forms of living intelligence: vision. The ability to see is indeed an essential facet of intelligence which is developed in an autonomous manner even in young human infants. I will focus here on a particular problem: how do we estimate motion in a visual image? I will explain why for this problem, it is crucial to understand how the visual system might overcome temporal delays and will demonstrate at different levels of description, from probabilistic models to neuromorphic hardware,  a surprising solution: The visual system models the world and uses the eye to probe this model","tags":null,"title":"Motion-based prediction with neuromorphic hardware","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1444222800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1444222800,"objectID":"f339e96811000581db6927de6d63fae1","permalink":"https://laurentperrinet.github.io/talk/2015-10-07-gdr-bio-comp/","publishdate":"2015-10-07T13:00:00Z","relpermalink":"/talk/2015-10-07-gdr-bio-comp/","section":"talk","summary":"We stand at a point in history where our phones have become smart but lack a feature which prevails in most forms of living intelligence: vision. The ability to see is indeed an essential facet of intelligence which is developed in an autonomous manner even in young human infants. I will focus here on a particular problem: how do we estimate motion in a visual image? I will explain why for this problem, it is crucial to understand how the visual system might overcome temporal delays and will demonstrate at different levels of description, from probabilistic models to neuromorphic hardware,  a surprising solution: The visual system models the world and uses the eye to probe this model.","tags":null,"title":"Motion-based prediction with neuromorphic hardware","type":"talk"},{"authors":["Gabriel Cristóbal","Laurent U Perrinet","Matthias S Keil"],"categories":null,"content":"   Biologically Inspired Computer vision  Biologically Inspired Computer Vision As state-of-the-art imaging technologies becomes more and more advanced, yielding scientific data at unprecedented detail and volume, the need to process and interpret all the data has made image processing and computer vision also increasingly important. Sources of data that have to be routinely dealt with today applications include video transmission, wireless communication, automatic fingerprint processing, massive databanks, non-weary and accurate automatic airport screening, robust night vision to name a few. Multidisciplinary inputs from other disciplines such as computational neuroscience, cognitive science, mathematics, physics and biology will have a fundamental impact in the progress of imaging and vision sciences. One of the advantages of the study of biological organisms is to devise very diﬀerent type of computational paradigms beyond the usual von Neumann e.g. by implementing a neural network with a high degree of local connectivity.   This is a comprehensive and rigorous reference in the area of biologically motivated vision sensors. The study of biologically visual systems can be considered as a two way avenue. On the one hand, biological organisms can provide a source of inspiration for new computational efficient and robust vision models and on the other hand machine vision approaches can provide new insights for understanding biological visual systems. Along the different chapters, this book covers a wide range of topics from fundamental to more specialized topics, including visual analysis based on a computational level, hardware implementation, and the design of new more advanced vision sensors. The last two sections of the book provide an overview of a few representative applications and current state of the art of the research in this area. This makes it a valuable book for graduate, Master, PhD students and also researchers in the field. This book contains 17 chapters that have been organized in four different parts:\n Fundamentals Sensing Modeling Applications See the Table of contents.    ","date":1444176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1444176000,"objectID":"1279272fa85e2d8ded3ce02fcb3913c8","permalink":"https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/","publishdate":"2015-10-07T00:00:00Z","relpermalink":"/publication/cristobal-perrinet-keil-15-bicv/","section":"publication","summary":"As the state-of-the-art imaging technologies became more and more advanced, yielding scientific data at unprecedented detail and volume, the need to process and interpret all the data has made image processing and computer vision also increasingly important. Sources of data that have to be routinely dealt with today applications include video transmission, wireless communication, automatic fingerprint processing, massive databases, non-weary and accurate automatic airport screening, robust night vision to name a few. Multidisciplinary inputs from other disciplines such as computational neuroscience, cognitive science, mathematics, physics and biology will have a fundamental impact in the progress of imaging and vision sciences. One of the advantages of the study of biological organisms is to devise very different type of computational paradigms beyond the usual von Neumann e.g. by implementing a neural network with a high degree of local connectivity. This is a comprehensive and rigorous reference in the area of biologically motivated vision sensors. The study of biologically visual systems can be considered as a two way avenue. On the one hand, biological organisms can provide a source of inspiration for new computational efficient and robust vision models and on the other hand machine vision approaches can provide new insights for understanding biological visual systems. Along the different chapters, this book covers a wide range of topics from fundamental to more specialized topics, including visual analysis based on a computational level, hardware implementation, and the design of new more advanced vision sensors. The last two sections of the book provide an overview of a few representative applications and current state of the art of the research in this area. This makes it a valuable book for graduate, Master, PhD students and also researchers in the field.","tags":["Biologically Inspired Computer vision"],"title":"Biologically Inspired Computer Vision","type":"publication"},{"authors":["Laurent U Perrinet","James A Bednar"],"categories":null,"content":"","date":1440028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440028800,"objectID":"7c132b903380a9f883be7d43306c0643","permalink":"https://laurentperrinet.github.io/publication/perrinet-15-eusipco/","publishdate":"2015-08-20T00:00:00Z","relpermalink":"/publication/perrinet-15-eusipco/","section":"publication","summary":"Oriented edges in images of natural scenes tend to be aligned in co-linear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (the good continuation law of Gestalt psychology). The visual system appears to take advantage of this prior knowledge about natural images, with human contour detection and grouping performance well predicted by such an asociation field between edge elements. Geisler et al (2001) have estimated this prior information available to the visual system by extracting contours from a database of natural images, and showed that these statistics could predict behavioral data from humans in a line completion task. In this paper, we show that an association field of this type can be used for the sparse representation of natural images.","tags":["association field","Biologically Inspired Computer vision","coding decoding","lateral connections","sparse coding","sparselets","statistics of natural images"],"title":"Sparse Coding Of Natural Images Using A Prior On Edge Co-Occurences","type":"publication"},{"authors":null,"categories":null,"content":"List of publications that were funded by European Union’s project Number FP7-269921, “BrainScales”.\nSee also:\n  The FACETS research project which ended on 31 August 2010.\n  The FACETS-ITN Marie-Curie initital training network for graduate training continues until August 2013\n  The BrainScaleS project builds on and extends the research done in FACETS. This 4 year project started on January 1st, 2011.\n  ","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"d05539cf40d9b472c3daf44914dced78","permalink":"https://laurentperrinet.github.io/grant/brain-scales/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/grant/brain-scales/","section":"grant","summary":"BrainScaleS: Brain-inspired multiscale computation in neuromorphic hybrid systems (2011/2014).","tags":["grant","past-grant"],"title":"BrainScaleS (2011/2014) ","type":"grant"},{"authors":null,"categories":null,"content":"The CODDE network studies the links between sensory input, brain activity and motor output. It does this by combining behavioural techniques, brain imaging, movement recording and computational modelling.\n","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"d97acb2eae7ee8262689a71b17a8ff81","permalink":"https://laurentperrinet.github.io/grant/codde/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/grant/codde/","section":"grant","summary":"CODDE: understanding brain and behaviour (2008/2012).","tags":["grant","past-grant"],"title":"CODDE (2008/2012)","type":"grant"},{"authors":null,"categories":null,"content":"List of publications that were funded by the FACETS project (more info).\n also available on the FACET’s website  See also:\n  The FACETS research project which ended on 31 August 2010.\n  The FACETS-ITN Marie-Curie initital training network for graduate training continues until August 2013\n  The BrainScaleS project builds on and extends the research done in FACETS. This 4 year project started on January 1st, 2011.\n  ","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"542ce231efbb54a433b7d4dd186d2195","permalink":"https://laurentperrinet.github.io/grant/facets/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/grant/facets/","section":"grant","summary":"FACETS: Fast Analog Computing with Emergent Transient States (2006/2010).","tags":["grant","past-grant"],"title":"FACETS (2006/2010)","type":"grant"},{"authors":null,"categories":null,"content":"FACETS-ITN: From Neuroscience to neuro-inspired computing (2010/2013) \nFACETS ITN project (EU funding, grant number 237955) is a ‘Marie-Curie Initial Training Network’ involves 15 groups at European Research Universities, Research Centers and Industrial Partners in 6 countries. 22 Ph.D. Positions are funded in the FACETS-ITN project in the following scientific work areas: Neurobiology of Cells and Networks, Modelling of Neural Systems, Neuromorphic Hardware, Neuro-Electronic Interfaces, Computational Principles in Neural Architectures, Mechanisms of Learning and Plasticity. \nSee also:\n  The FACETS research project which ended on 31 August 2010.\n  The FACETS-ITN Marie-Curie initital training network for graduate training continues until August 2013\n  The BrainScaleS project builds on and extends the research done in FACETS. This 4 year project started on January 1st, 2011.\n  ","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"bbe1361102f5a99b82200aa3d462fd5e","permalink":"https://laurentperrinet.github.io/grant/facets-itn/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/grant/facets-itn/","section":"grant","summary":"FACETS-ITN: From Neuroscience to neuro-inspired computing (2010/2013)","tags":["grant","past-grant"],"title":"FACETS-ITN (2010/2013)","type":"grant"},{"authors":["Anna Montagnini","Laurent U Perrinet"],"categories":null,"content":"The PACE ITN project involved over 50 researchers spread across 10 full and 5 associated partners, from academia and the private sector, established in 7 different European and Associated countries, the PACE network gathers a broad range of expertise from experimental psychology, cognitive neurosciences, brain imaging, technology and clinical sciences.\nThe PACE Project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 642961\n","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"c64cccb5c00ca0df4c1c2da7560cb550","permalink":"https://laurentperrinet.github.io/grant/pace-itn/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/grant/pace-itn/","section":"grant","summary":"PACE-ITN: ITN Marie Curie network (2015/2019).","tags":["grant","past-grant"],"title":"PACE-ITN (2015/2019)","type":"grant"},{"authors":["Jonathan Vacher","Andrew Isaac Meso","Laurent U Perrinet","Gabriel Peyré"],"categories":null,"content":" See a followup in  Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré  (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"c2804cac3cc2125f9fd4a939c4a99c36","permalink":"https://laurentperrinet.github.io/publication/vacher-15-icms/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/vacher-15-icms/","section":"publication","summary":" See a followup in  Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré  (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation.  Preprint  PDF  Cite  DOI     ","tags":["motion-clouds","psychophysics"],"title":"A Mathematical Account of Dynamic Texture Synthesis for Probing Visual Perception","type":"publication"},{"authors":["Anna Montagnini","Jean-Bernard Damasse","Laurent U Perrinet","Laurent Madelain"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"8e34cf520a701244575b6a6bfe81c459","permalink":"https://laurentperrinet.github.io/publication/montagnini-15-sfn/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/montagnini-15-sfn/","section":"publication","summary":"","tags":["Bayesian model"],"title":"Anticipating a moving target: role of vision and reinforcement","type":"publication"},{"authors":["Jean-Bernard Damasse","Laurent Madelain","Laurent U Perrinet","Anna Montagnini"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"4a954e30c0af2a4d0f3381f267fd2a82","permalink":"https://laurentperrinet.github.io/publication/damasse-15-vss/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/damasse-15-vss/","section":"publication","summary":"When an object is moving in the visual field, we are able to accurately track it with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling visual analysis with high acuity. Importantly, when predictive information is available about the target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM (baseline condition). This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. To further understand the nature of this process, we investigate the effects of reinforcement on aSPEM with two distinct experiments. First, it has been previously shown that several properties of eye movements can be modulated by reinforcement paradigms based on monetary reward (Madelain et al. 2011). We adapted and extended this framework to prediction-based aSPEM, by associating a monetary reward to a criterion-matching anticipatory velocity, in the gap before the target onset. Second, it has also been reported that accurate perception per se can play the role of an efficient ecological reinforcer for visually guided saccades (Montagnini \u0026 Chelazzi, 2005). With a gaze-contingent procedure, we manipulated the discriminability of a perceptual target (appearing during the pursuit trial and followed by a discrimination task) The difficulty level of this task has been matched depending on the velocity of aSPEM. This experiment taps on the very reason to produce anticipatory tracking movement, that is to grant a quicker high-acuity vision of the moving target. We compare predictive anticipatory eye movements across these conditions.","tags":["eye movements","Smooth pursuit eye movement"],"title":"Anticipatory smooth eye movements and reinforcement","type":"publication"},{"authors":["Jean-Bernard Damasse","Laurent Madelain","Laurent U Perrinet","Anna Montagnini"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"898c5896316050e1132aa76ecb74d7e4","permalink":"https://laurentperrinet.github.io/publication/damasse-15-gdr/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/damasse-15-gdr/","section":"publication","summary":"","tags":["eye movements","Smooth pursuit eye movement"],"title":"Anticipatory smooth eye movements as operant behavior","type":"publication"},{"authors":["Jonathan Vacher","Andrew Isaac Meso","Laurent U Perrinet","Gabriel Peyré"],"categories":null,"content":" See a followup in  Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré  (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"5271085a92f2cbb7f8091cd3cfe0ded3","permalink":"https://laurentperrinet.github.io/publication/vacher-15-nips/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/vacher-15-nips/","section":"publication","summary":" See a followup in  Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré  (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation.  Preprint  PDF  Cite  DOI     ","tags":["motion-clouds"],"title":"Biologically Inspired Dynamic Textures for Probing Motion Perception","type":"publication"},{"authors":["Laurent U Perrinet","James A Bednar"],"categories":null,"content":" Press release communiqué de presse supplementary information supplementary material  A study of how people can quickly spot animals by sight is helping uncover the workings of the human brain. Scientists examined why volunteers who were shown hundreds of pictures - some with animals and some without - were able to detect animals in as little as one-tenth of a second. They found that one of the first parts of the brain to process visual information - the primary visual cortex - can control this fast response. More complex parts of the brain are not required at this stage, contrary to what was previously thought. New info published on how the human brain processes visual information from @EdinburghUni and @uniamu stuidy http://t.co/KUicugL8P7\n— EdinUniNeuro (@EdinUniNeuro) June 22, 2015   Edge co-occurrences (A) An example image with the list of extracted edges overlaid. Each edge is represented by a red line segment which represents its position (center of segment), orientation, and scale (length of segment). We controlled the quality of the reconstruction from the edge information such that the residual energy was less than 5%. (B) The relationship between a reference edge A and another edge B can be quantified in terms of the difference between their orientations $\\theta$, ratio of scale $\\sigma$, distance $d$ between their centers, and difference of azimuth (angular location) $\\phi$. Additionally, we define $\\psi=\\phi - \\theta/2$, which is symmetric with respect to the choice of the reference edge; in particular, $\\psi=0$ for co-circular edges. % (see text). As in~\\citet{Geisler01}, edges outside a central circular mask are discarded in the computation of the statistics to avoid artifacts. (Image credit: Andrew Shiva, Creative Commons Attribution-Share Alike 3.0 Unported license). This is used to compute the chevron map in Figure~2.  動物か否かの見分け方。http://t.co/TTY8MwZGoO　引用されてるけど、Thorpe (1996)の150msで区別されてるって話(なつかしい)と関係ありそう。\n— Makito Oku (@okumakito) June 22, 2015   The probability distribution function $p(\\psi, \\theta)$ represents the distribution of the different geometrical arrangements of edges’ angles, which we call a chevron map. We show here the histogram for non-animal natural images, illustrating the preference for co-linear edge configurations. For each chevron configuration, deeper and deeper red circles indicate configurations that are more and more likely with respect to a uniform prior, with an average maximum of about $3$ times more likely, and deeper and deeper blue circles indicate configurations less likely than a flat prior (with a minimum of about $0.8$ times as likely). Conveniently, this chevron map shows in one graph that non-animal natural images have on average a preference for co-linear and parallel edges, (the horizontal middle axis) and orthogonal angles (the top and bottom rows),along with a slight preference for co-circular configurations (for $\\psi=0$ and $\\psi=\\pm \\frac \\pi 2$, just above and below the central row). We compare chevron maps in different image categories in Figure~3.  Edge co-occurrences can account for rapid categorization of natural versus animal imageshttp://t.co/NY9HapBx2S pic.twitter.com/rKQ8I5i6Ty\n— Francis Villatoro (@emulenews) June 22, 2015   As for Figure 2, we show the probability of edge configurations as chevron maps for two databases (man-made, animal). Here, we show the ratio of histogram counts relative to that of the non-animal natural image dataset. Deeper and deeper red circles indicate configurations that are more and more likely (and blue respectively less likely) with respect to the histogram computed for non-animal images. In the left plot, the animal images exhibit relatively more circular continuations and converging angles (red chevrons in the central vertical axis) relative to non-animal natural images, at the expense of co-linear, parallel, and orthogonal configurations (blue circles along the middle horizontal axis). The man-made images have strikingly more co-linear features (central circle), which reflects the prevalence of long, straight lines in the cage images in that dataset. We use this representation to categorize images from these different categories in Figure~4.   Classification results. To quantify the difference in low-level feature statistics across categories (see Figure~3, we used a standard Support Vector Machine (SVM) classifier to measure how each representation affected the classifier’s reliability for identifying the image category. For each individual image, we constructed a vector of features as either (FO) the histogram of first-order statistics as the histogram of edges’ orientations, (CM) the chevron map subset of the second-order statistics, (i.e., the two-dimensional histogram of relative orientation and azimuth; see Figure 2 ), or (SO) the full, four-dimensional histogram of second-order statistics (i.e., all parameters of the edge co-occurrences). We gathered these vectors for each different class of …","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"6c5d7302877e74ebd56153473901a0df","permalink":"https://laurentperrinet.github.io/publication/perrinet-bednar-15/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/perrinet-bednar-15/","section":"publication","summary":"Making a judgment about the semantic category of a visual scene, such as whether it contains an animal, is typically assumed to involve high-level associative brain areas. Previous explanations require progressively analyzing the scene hierarchically at increasing levels of abstraction, from edge extraction to mid-level object recognition and then object categorization. Here we show that the statistics of edge co-occurrences alone are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. We first extracted the edges from images using a scale-space analysis coupled with a sparse coding algorithm. We then computed the ``association field'' for different categories (natural, man-made, or containing an animal) by computing the statistics of edge co-occurrences. These differed strongly, with animal images having more curved configurations. We show that this geometry alone is sufficient for categorization, and that the pattern of errors made by humans is consistent with this procedure. Because these statistics could be measured as early as the primary visual cortex, the results challenge widely held assumptions about the flow of computations in the visual system. The results also suggest new algorithms for image classification and signal processing that exploit correlations between low-level structure and the underlying semantic category.","tags":["association field","Biologically Inspired Computer vision","sparse coding"],"title":"Edge co-occurrences can account for rapid categorization of natural versus animal images","type":"publication"},{"authors":["Fréderic Danion","Caroline Landelle","Anna Montagnini","Laurent U Perrinet","Laurent Madelain"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"fbfe473d9830ee0090419be17949c1ad","permalink":"https://laurentperrinet.github.io/publication/danion-15-sfn/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/danion-15-sfn/","section":"publication","summary":"","tags":["Bayesian model"],"title":"Eye tracking a self-moved target with complex hand-target dynamics","type":"publication"},{"authors":["Wahiba Taouali","Giacomo Benvenuti","Pascal Wallisch","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in this publication  ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"2da4f922f8f79c6abb06801a3cd00d33","permalink":"https://laurentperrinet.github.io/publication/taouali-15-icmns/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/taouali-15-icmns/","section":"publication","summary":"The repeated presentation of an identical visual stimulus in the receptive field of a neuron may evoke different spiking patterns at each trial. Probabilistic methods are essential to understand its functional role within the neural activity. In that case, a Poisson process is the most common model of trial-to-trial variability. However, the variance of the spike count is constrained to be equal to the mean, irrespective of measurement's duration. Numerous studies have shown that this relationship does not generally hold. Specifically, a majority of electrophysiological recordings show an ``em overdispersion'' effect: Responses that exhibit more inter-trial variability than expected from a Poisson process alone. A model that is particularly well suited to quantify overdispersion is the Negative-Binomial distribution model. This model is largely applied and studied but has only recently been applied to neuroscience. In this paper, we address three main issues. First, we describe how the Negative-Binomial distribution provides a model apt to account for overdispersed spike counts. Second, we quantify the significance of this model for any neurophysiological data by proposing a statistical test, which quantifies the odds that overdispersion could be due to the limited number of repetitions (trials). We apply this test to three neurophysiological tests along the visual pathway. Finally, we compare the performance of this model to the Poisson model on a population decoding task. This shows that more knowledge about the form of dispersion tuning is necessary to have a significant gain, uncovering a possible feature of the neural spiking code.","tags":["coding decoding"],"title":"On overdispersion in neuronal evoked activity","type":"publication"},{"authors":["Cesar U Ravello","F. Olivares","R. Herzog","Laurent U Perrinet","Maria Jose Escobar","Adrián G Palacios"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"1d2b4cb8d6d9bc223d51c325dd0e8909","permalink":"https://laurentperrinet.github.io/publication/ravello-15/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/ravello-15/","section":"publication","summary":"","tags":["Retina"],"title":"Spatiotemporal tuning of retinal ganglion cells dependent on the context of signal presentation","type":"publication"},{"authors":["Laurent U Perrinet","Rick A Adams","Karl Friston"],"categories":null,"content":"Active Inference, tracking eye movements and oculomotor delays Tracking eye movements face a difficult task: they have to be fast while they suffer inevitable delays. If we focus on area MT of humans for instance as it is crucial for detecting the motion of visual objects, sensory information coming to this area is already lagging some 35 milliseconds behind operational time – that is, it reflects some past information. Still the fastest action that may be done there is only able to reach the effector muscles of the eyes some 40 milliseconds later – that is, in the future. The tracking eye movement system is however able to respond swiftly and even to anticipate repetitive movements (e.g. Barnes et al, 2000 – refs in manuscript). In that case, it means that information in a cortical area is both predicted from the past sensory information but also anticipated to give an optimal response in the future. Even if numerous models have been described to model different mechanisms to account for delays, no theoretical approach has tackled the whole problem explicitly. In several areas of vision research, authors have proposed models at different levels of abstractions from biomechanical models, to neurobiological implementations (e.g. Robinson, 1986) or Bayesian models. This study is both novel and important because – using a neurobiologically plausible hierarchical Bayesian model – it demonstrates that using generalized coordinates to finesse the prediction of a target’s motion, the model can reproduce characteristic properties of tracking eye movements in the presence of delays. Crucially, the different refinements to the model that we propose – pursuit initiation, smooth pursuit eye movements, and anticipatory response – are consistent with the different types of tracking eye movements that may be observed experimentally.   (A) This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. (B) This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss. \n","date":1418688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1418688000,"objectID":"da20b43c63c3d8b3822f4c1abb68c266","permalink":"https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/","publishdate":"2014-12-16T00:00:00Z","relpermalink":"/publication/perrinet-adams-friston-14/","section":"publication","summary":"This paper considers the problem of sensorimotor delays in the optimal control of (smooth) eye movements under uncertainty. Specifically, we consider delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple way of compensating for both sensory and oculomotor delays. The efficacy of this scheme is illustrated using neuronal simulations of pursuit initiation responses, with and without compensation. We then consider an extension of the generative model to simulate smooth pursuit eye movements in which the visuo-oculomotor system believes both the target and its centre of gaze are attracted to a (hidden) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can recognise and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.","tags":["active inference","Bayesian model","Biologically Inspired Computer vision","eye movements","free energy","motion detection"],"title":"Active inference, eye movements and oculomotor delays","type":"publication"},{"authors":["Andrew Isaac Meso","Claudio Simoncini","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1408665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408665600,"objectID":"ae53f3d7541e45df40358b9436f2a144","permalink":"https://laurentperrinet.github.io/publication/meso-14-vss/","publishdate":"2014-08-22T00:00:00Z","relpermalink":"/publication/meso-14-vss/","section":"publication","summary":"Estimating object speed in visual scenes is a critical part of perception. While various aspects of speed computation including discrimination thresholds, neural mechanisms and spatial integration mechanisms have been studied, there remain areas to elucidate. One is the integration of information across spatio-temporal frequency channels to compute speed. We probe this integration with a 2-AFC psychophysical task in which moving random phase noise stimuli are used with experimenter defined frequency parameters and bandwidths to target specific neural populations. They are presented for 300ms in a large square aperture with smooth eye movements recorded while speed discrimination judgements are made over two intervals. There is no instruction to observers to pursue the stimuli and no pre trial saccade to induce a classic ocular following response. After a latency, eye movements follow the stimulated direction presumably to facilitate the speed judgement. Within each of the two intervals, we randomly vary a range of spatial frequency and speed parameters respectively such that stimuli at the centre of the ranges are identical. The aim is to characterise the speed response of the eye movements recorded in a context which creates an ocular motor 'action' during a perceptual task instead of artificially separating the two. Within the speed varied intervals, averaged eye movements are systematically modulated in strength by stimulus speed. Within the spatial frequency intervals, higher frequencies perceived as faster in discrimination responses interestingly show no corresponding strengthening of eye responses particularly at higher contrasts where they may be weaker. Thus for a pair of stimuli matched for contrast and perceived speed, this early eye response appears to be driven by a contrast dependent low level motion energy like computation. We characterise an underlying spatial frequency response which is shifted towards lower frequencies, unlike the perceptual responses and is probably separate from perception.","tags":["motion-clouds"],"title":"Beyond simply faster and slower: exploring paradoxes in speed perception","type":"publication"},{"authors":["Laurent U Perrinet","James A Bednar"],"categories":null,"content":" see a follow-up:  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1408665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408665600,"objectID":"b32e5513d77529f7cccf22f1b6663ff0","permalink":"https://laurentperrinet.github.io/publication/perrinet-bednar-14-vss/","publishdate":"2014-08-22T00:00:00Z","relpermalink":"/publication/perrinet-bednar-14-vss/","section":"publication","summary":"Analysis and interpretation of a visual scene to extract its category, such as whether it contains an animal, is typically assumed to involve higher-level associative brain areas. Previous proposals have been based on a series of processing steps organized in a multi-level hierarchy that would progressively analyze the scene at increasing levels of abstraction, from contour extraction to low-level object recognition and finally to object categorization (Serre, PNAS 2007). We explore here an alternative hypothesis that the statistics of edge co-occurences are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. The method is based on a realistic model of image analysis in the primary visual cortex that extends previous work from Geisler et al. (Vis. Res. 2001). Using a scale-space analysis coupled with a sparse coding algorithm, we achieved detailed and robust extraction of edges in different sets of natural images. This edge-based representation allows for a simple characterization of the ``association field'' of edges by computing the statistics of co-occurrences. We show that the geometry of angles made between edges is sufficient to distinguish between different sets of natural images taken in a variety of environments (natural, man-made, or containing an animal). Specifically, a simple classifier, working solely on the basis of this geometry, gives performance similar to that of hierarchical models and of humans in rapid-categorization tasks. Such results call attention to the importance of the relative geometry of local image patches in visual computation, with implications for designing efficient image analysis systems. Most importantly, they challenge assumptions about the flow of computations in the visual system and emphasize the relative importance in this process of associative connections, and in particular of intra-areal lateral connections.","tags":["association field","Biologically Inspired Computer vision","sparse coding"],"title":"Edge co-occurrences are sufficient to categorize natural versus animal images","type":"publication"},{"authors":["Mina A Khoei","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1408665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408665600,"objectID":"8bfc879d951c9f5f391701db53807991","permalink":"https://laurentperrinet.github.io/publication/khoei-14-vss/","publishdate":"2014-08-22T00:00:00Z","relpermalink":"/publication/khoei-14-vss/","section":"publication","summary":"The flash lag effect (FLE) is a well known visual illusion that reveals the perceptual difference in position coding of moving and stationary flashed objects. It has been reproduced experimentally in retina and V1 along with some relevant evidences about motion based position coding in areas MT and MT+. Numerous hypotheses for mechanisms underlying FLE such as motion extrapolation, latency difference, position persistence, temporal averaging and postdiction have been under debate for last two decades. Here, we have challenged our previous motion-based prediction model to understand FLE, consistently with the motion extrapolation account proposed by Nijhawan. Our hypothesis is based on predictability of motion trajectory and importance of motion signal in manipulation of receptive field shape for moving objects. Using a probabilistic framework, we have implemented motion-based prediction (MBP) and simulated three different demonstrations of FLE including standard, flash initiated and flash terminated cycles. This method allowed us to compare the shape of the characteristic receptive fields for moving and stationary flashed dots in the case of rightward and leftward motions. As control model, we have eliminated velocity signal from motion estimation and simulated position-based (PX) model of FLE. Results of MBP model suggest that above a minimal time for duration of flash, the development of predictive component for the moving object is sufficient to shift in the direction of motion and to produce flash lag effect. MBP model reproduces experimental data of FLE and its dependence to the contrast of flash. Against what has been argued as shortage of motion extrapolation account, in our results spatial lead of moving object is also evident in flash initiated cycle. Our model, without being restricted to one special visual area, provides a generic account for FLE by emphasize on different manipulation of stationary objects and trajectory motion by the sensory system.","tags":["motion detection","motion prediction"],"title":"Motion-based prediction model for flash lag effect","type":"publication"},{"authors":["Claudio Simoncini","Anna Montagnini","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1408665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1408665600,"objectID":"8437d9dae0e9884eb37137c1c8d7de85","permalink":"https://laurentperrinet.github.io/publication/simoncini-14-vss/","publishdate":"2014-08-22T00:00:00Z","relpermalink":"/publication/simoncini-14-vss/","section":"publication","summary":"Under natural viewing conditions, large eye movements are interspace by small eye movements (microsaccade). Recent works have shown that these two kinds of eye movements are generate by the same oculomotor mechanisms (Goffart et al., 2012) and are driven from the same visual information (Simoncini et al., VSS 2012 abstract). These results seem to demonstrate that microsaccade and saccade represent a continuum of the same ocular movement. However, if the role played in vision perception by large saccades is clearly identified, the role of the microsaccade is not clearly defined. In order to investigate the role of microsaccade, we measured pattern discrimination performance using an ABX match-to-sample task during the presentation of 1/f natural statistics texture where we varied the spatial frequency contents. We compared perceptual performance with eye movements recorded during the task. We found that the rate of microsaccadic movements changed as a function of the subjects task strategy. In particular, in the trials where the perception of the difference between the stimuli was simple (low spatial frequency) the subjects used the information provided by all the stimuli to do the task and the microsaccadic rate for all the stimuli (ABX) was the same. However, when the perception of the difference between the stimuli was harder (for instance for high spatial frequency), the subjects rather used the information provided by the last two stimuli only and the microsaccadic rate for the image BX increased respect at the image A. These results demonstrate that microsaccadic eye movements also play a role during the analysis of the visual scene and that such experiments can help decipher their participation to perception of the scene. Goffart L., Hafed Z.M., Krauzlis R.J. 2012. Visual fixation as equilibrium: evidence from superior colliculus inactivation. (31) 10627-10636.","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"The characteristics of microsaccadic eye movements varied with the change of strategy in a match-to-sample task","type":"publication"},{"authors":["Bernhard A Kaplan","Mina A Khoei","Anders Lansner","Laurent U Perrinet"],"categories":null,"content":" see Kaplan and al, 2014  ","date":1398384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398384000,"objectID":"a3f87a92498704e2789b5b39a0c72599","permalink":"https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/","publishdate":"2014-04-25T00:00:00Z","relpermalink":"/talk/2014-04-25-kaplan-beijing/","section":"talk","summary":" see Kaplan and al, 2014  ","tags":["Bayesian model","Biologically Inspired Computer vision","dynamics","motion detection"],"title":"Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Quand: le 17 avril 2014 de 14 H30 à 16 H 30,\nQuoi: “Codage prédictif dans les transformations visuo-motrices”.\nLieu: salle Henri Gastaut, au rez de chaussée de l’INT (how to get there)\nLa soutenance est suivie d’un pot au R+4 de l’Institut de Neurosciences de la Timone (how to get there)\nJury La soutenance est ouverte à tous, merci d’annoncer votre présence à laurent.perrinet@univ-amu.fr\nLe jury est composé par::\n Prof. Laurent Madelain, Université Lille III Dr. Alain Destexhe, Université Paris XI (Rapporteur) Prof. Gustavo Deco, Universitat Pompeu Fabra, Barcelona (Rapporteur) Dr. Guillaume Masson, Aix-Marseille Université Dr. Viktor Jirsa, Aix-Marseille Université (Rapporteur) Prof. J.-L. Mege, Aix-Marseille Université  ","date":1397692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397714400,"objectID":"ca9b99eb215e7aa76f7c09c7c9824216","permalink":"https://laurentperrinet.github.io/post/2014-04-17_hdr/","publishdate":"2014-04-17T00:00:00Z","relpermalink":"/post/2014-04-17_hdr/","section":"post","summary":"J'ai soutenu mon habilitation à diriger des recherche (HDR) le 17 avril 2014, celle-ci ayant pour titre: **Codage prédictif dans les transformations visuo-motrices**","tags":["events"],"title":"2014-04-17: Soutenance d'habilitation à diriger des recherches (HDR)","type":"post"},{"authors":["Laurent U Perrinet","Bernhard A Kaplan","Mina A Khoei","Anders Lansner","Guillaume S Masson"],"categories":null,"content":"","date":1395320400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395320400,"objectID":"bb9a96df5b7a5cc74b7057c3306548e7","permalink":"https://laurentperrinet.github.io/talk/2014-03-20-manchester/","publishdate":"2014-03-20T13:00:00Z","relpermalink":"/talk/2014-03-20-manchester/","section":"talk","summary":"The question how the visual system is able to create a coherent representation of a rapidly changing environment in the presence of neural delays is not fully resolved. In this paper we use an abstract probabilistic framework and a spiking neural network (SNN) implementation to investigate the role of motion-based prediction in estimating motion trajectories with delayed information sampling. In particular, we investigate how anisotropic diffusion of information may explain the development of anticipatory response in neural populations to an approaching stimulus. Inspired by a mechanism proposed by Nijhawan [2009], we use a Bayesian particle filter framework and introduce a diagonal motion-based prediction model which extrapolates the estimated response to delayed stimulus in the direction of the trajectory. In the SNN implementation, we have used anisotropic recurrent connections between excitatory cells as mechanism for motion-extrapolation. Consistent with recent experimental data collected in extracellular recordings of macaque primary visual cortex [Benvenuti et al 2011], we have simulated different trajectory lengths and have explored how anticipatory response may be dependent to the information accumulated along the trajectory. We show that both our probabilistic framework and the SNN can replicate the experimental data qualitatively. Most importantly, we highlight requirements for the development of a trajectory-dependent anticipatory response, and in particular the anisotropic nature of the diagonal motion extrapolation mechanism.","tags":null,"title":"WP5 - Demo 1.3 : Spiking model of motion-based prediction","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1389312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1389312000,"objectID":"b06c12a6cd7613b644aa6355fd83cf1a","permalink":"https://laurentperrinet.github.io/talk/2014-01-10-int-fest/","publishdate":"2014-01-10T00:00:00Z","relpermalink":"/talk/2014-01-10-int-fest/","section":"talk","summary":"Moving objects generate sensory information that may be noisy and ambiguous, yet it is important to be able to reconstruct object speed as fast as possible. One unsolved question is to understand how the brain pools motion information to give an efficient response. I will present computational models of motion detection in the visual system that try to answer this question. First, sensory information is grabbed by pooling the information from different sensory neurons. This pooling is modulated by the precision of information and we will present some recent model-based behavioural results. Then I will focus on a novel model of motion-based prediction that allows to track objects on smooth trajectories. This model gives an economical description of neural mechanisms associated with the processing underlying motion detection. Finally, we will propose an exploratory hypothesis such that eye movements may be understood as the prospective response to this dynamical sensory response knowing oculomotor constraints such as delays. This line of research aims at showing that through the convergent use of models, electrophysiology or behavioural responses, the study of motion detection is an essential tool in our understanding of neural computations.","tags":["Bayesian model","dynamics","eye movements","free energy","motion detection","predictive coding"],"title":"Axonal delays and on-time control of eye movements","type":"talk"},{"authors":["Wahiba Taouali","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in this publication  ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"cf663492ba0ff12f7ba5e1ed3ec38e6c","permalink":"https://laurentperrinet.github.io/publication/taouali-14-areadne/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/taouali-14-areadne/","section":"publication","summary":" see a follow-up in this publication  ","tags":["coding decoding"],"title":"A Simple Model of Orientation Encoding Accounting For Multivariate Neural Noise","type":"publication"},{"authors":["Wahiba Taouali","Laurent U Perrinet"],"categories":null,"content":" see a follow-up in this publication  ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"2a49f426812d6c6cd3d14fae11873ba2","permalink":"https://laurentperrinet.github.io/publication/taouali-14-neurocomp/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/taouali-14-neurocomp/","section":"publication","summary":" see a follow-up in this publication  ","tags":["coding decoding"],"title":"A Simple Model of Orientation Encoding Accounting For Multivariate Neural Noise","type":"publication"},{"authors":["Jonathan Vacher","Andrew Isaac Meso","Laurent U Perrinet","Gabriel Peyré"],"categories":null,"content":" See a followup in  Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré  (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation.  Preprint  PDF  Cite  DOI     ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"be668460d100a02c9706494ffc120932","permalink":"https://laurentperrinet.github.io/publication/vacher-14-ihp/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/vacher-14-ihp/","section":"publication","summary":"This work extends the MotionClouds dynamic texture model testing aspects of its parametrization with an application in psychophysics.","tags":["motion-clouds","psychophysics"],"title":"Dynamic Textures For Probing Motion Perception","type":"publication"},{"authors":["Jean-Bernard Damasse","Laurent Madelain","Laurent U Perrinet","Anna Montagnini"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"fd8d482f02cb58c638177fcb821bb2a0","permalink":"https://laurentperrinet.github.io/publication/damasse-14-gdr/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/damasse-14-gdr/","section":"publication","summary":"","tags":["eye movements","Smooth pursuit eye movement"],"title":"On the nature of anticipatory eye movements and the factors affecting them","type":"publication"},{"authors":["P Philipp Rudiger","Jean-Luc Stevens","Bharath Chandra Talluri","Laurent U Perrinet","James A Bednar"],"categories":null,"content":" see a follow-up:  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"18975c86f97ed0d13e7ebe1d90b5b0ec","permalink":"https://laurentperrinet.github.io/publication/rudiger-14-cosyne/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/rudiger-14-cosyne/","section":"publication","summary":" see a follow-up:  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","tags":["statistics of natural images"],"title":"Relationship between natural image statistics and lateral connectivity in the primary visual cortex","type":"publication"},{"authors":["Bernhard A Kaplan","Mina A Khoei","Anders Lansner","Laurent U Perrinet"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     Figure 4: Rasterplot of input and output spikes. The raster plot from excitatory neurons is ordered according to their position. Each input spike is a blue dot and each output spike is a black dot. While input is scattered during blanking periods (Figure 1), the network output shows shows some tuned activity during the blank (compare with the activity before visual stimulation). To decode such patterns of activity we used a maximum-likelihood estimation technique based on the tuning curve of the neurons.   ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"87ac758310b5becccd093c1e8955bd91","permalink":"https://laurentperrinet.github.io/publication/kaplan-khoei-14/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/publication/kaplan-khoei-14/","section":"publication","summary":"As it is confronted to inherent neural delays, how does the visual system create a coherent representation of a rapidly changing environment? In this paper, we investigate the role of motion-based prediction in estimating motion trajectories compensating for delayed information sampling. In particular, we investigate how anisotropic diffusion of information may explain the development of anticipatory response as recorded in a neural populations to an approaching stimulus. We validate this using an abstract probabilistic framework and a spiking neural network (SNN) model. Inspired by a mechanism proposed by Nijhawan [1], we first use a Bayesian particle filter framework and introduce a diagonal motion-based prediction model which extrapolates the estimated response to a delayed stimulus in the direction of the trajectory. In the SNN implementation, we have used this pattern of anisotropic, recurrent connections between excitatory cells as mechanism for motion-extrapolation. Consistent with recent experimental data collected in extracellular recordings of macaque primary visual cortex [2], we have simulated different trajectory lengths and have explored how anticipatory responses may be dependent on the information accumulated along the trajectory. We show that both our probabilistic framework and the SNN model can replicate the experimental data qualitatively. Most importantly, we highlight requirements for the development of a trajectory-dependent anticipatory response, and in particular the anisotropic nature of the connectivity pattern which leads to the motion extrapolation mechanism.","tags":["Bayesian model","motion detection","motion prediction","pynn"],"title":"Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network","type":"publication"},{"authors":["Bernhard A Kaplan","Laurent U Perrinet"],"categories":null,"content":" Together with Bernhard Kaplan, we talked about how we aim at “compiling” a predictive motion-based approach as a spiking neural networks and then as a parallel wafer systems in the BrainscaleS project (Demo 1, Task4). (private to the consortium: https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showMeetingInfoPage\u0026amp;meetingID=52 https://brainscales.kip.uni-heidelberg.de/internal/jss/AttendMeeting?m=showAgenda\u0026amp;meetingID=52 including copies of the slides)  ","date":1385424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1385424000,"objectID":"f393075ce250fe84ec9bf77587d30821","permalink":"https://laurentperrinet.github.io/talk/2013-11-26-brain-scales-demos/","publishdate":"2013-11-26T00:00:00Z","relpermalink":"/talk/2013-11-26-brain-scales-demos/","section":"talk","summary":"Together with Bernhard Kaplan, we talked about how we aim at “compiling” a predictive motion-based approach as a spiking neural networks and then as a parallel wafer systems in the BrainscaleS project (Demo 1, Task4).","tags":null,"title":"Demo 1, Task4: Implementation of models showing emergence of cortical fields and maps","type":"talk"},{"authors":["Mina A Khoei","Guillaume S Masson","Laurent U Perrinet"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI    Based on Perrinet et al, 2012 See a followup in Khoei et al, 2017   Figure 1: The problem of fragmented trajectories and motion extrapolation. As an object moves in visual space (as represented here for commodity by the red trajectory of a tennis ball in a space–time diagram with a one-dimensional space on the vertical axis), the sensory flux may be interrupted by a sudden and transient blank (as denoted by the vertical, gray area and the dashed trajectory). How can the instantaneous position of the dot be estimated at the time of reappearance? This mechanism is the basis of motion extrapolation and is rooted on the prior knowledge on the coherency of trajectories in natural images. We show below the typical eye velocity profile that is observed during Smooth Pursuit Eye Movements (SPEM) as a prototypical sensory response. It consists of three phases: first, a convergence of the eye velocity toward the physical speed, second, a drop of velocity during the blank and finally, a sudden catch-up of speed at reappearance (Becker and Fuchs, 1985).   ","date":1384905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1384905600,"objectID":"1de874f4c0bd9dcade9bc4bf5626442f","permalink":"https://laurentperrinet.github.io/publication/khoei-13-jpp/","publishdate":"2013-11-20T00:00:00Z","relpermalink":"/publication/khoei-13-jpp/","section":"publication","summary":"During normal viewing, the continuous stream of visual input is regularly interrupted, for instance by blinks of the eye. Despite these frequents blanks (that is the transient absence of a raw sensory source), the visual system is most often able to maintain a continuous representation of motion. For instance, it maintains the movement of the eye such as to stabilize the image of an object. This ability suggests the existence of a generic neural mechanism of motion extrapolation to deal with fragmented inputs. In this paper, we have modeled how the visual system may extrapolate the trajectory of an object during a blank using motion-based prediction. This implies that using a prior on the coherency of motion, the system may integrate previous motion information even in the absence of a stimulus. In order to compare with experimental results, we simulated tracking velocity responses. We found that the response of the motion integration process to a blanked trajectory pauses at the onset of the blank, but that it quickly recovers the information on the trajectory after reappearance. This is compatible with behavioral and neural observations on motion extrapolation. To understand these mechanisms, we have recorded the response of the model to a noisy stimulus. Crucially, we found that motion-based prediction acted at the global level as a gain control mechanism and that we could switch from a smooth regime to a binary tracking behavior where the dot is tracked or lost. Our results imply that a local prior implementing motion-based prediction is sufficient to explain a large range of neural and behavioral results at a more global level. We show that the tracking behavior deteriorates for sensory noise levels higher than a certain value, where motion coherency and predictability fail to hold longer. In particular, we found that motion-based prediction leads to the emergence of a tracking behavior only when enough information from the trajectory has been accumulated. Then, during tracking, trajectory estimation is robust to blanks even in the presence of relatively high levels of noise. Moreover, we found that tracking is necessary for motion extrapolation, this calls for further experimental work exploring the role of noise in motion extrapolation.","tags":["motion detection","motion prediction"],"title":"Motion-based prediction explains the role of tracking in motion extrapolation","type":"publication"},{"authors":["Bernhard A Kaplan","Anders Lansner","Guillaume S Masson","Laurent U Perrinet"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI   lication/khoei-13-jpp\u0026#34; view=“4” \u0026gt;}}  ","date":1379376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379376000,"objectID":"69cb1f17a99fb25bc6a452138ff47478","permalink":"https://laurentperrinet.github.io/publication/kaplan-13/","publishdate":"2013-09-17T00:00:00Z","relpermalink":"/publication/kaplan-13/","section":"publication","summary":"Predictive coding hypothesizes that the brain explicitly infers upcoming sensory input to establish a coherent representation of the world. Although it is becoming generally accepted, it is not clear on which level spiking neural networks may implement predictive coding and what function their connectivity may have. We present a network model of conductance-based integrate-and-fire neurons inspired by the architecture of retinotopic cortical areas that assumes predictive coding is implemented through network connectivity, namely in the connection delays and in selectiveness for the tuning properties of source and target cells. We show that the applied connection pattern leads to motion-based prediction in an experiment tracking a moving dot. In contrast to our proposed model, a network with random or isotropic connectivity fails to predict the path when the moving dot disappears. Furthermore, we show that a simple linear decoding approach is sufficient to transform neuronal spiking activity into a probabilistic estimate for reading out the target trajectory.","tags":["Bayesian model","large-scale_networks","motion detection","motion prediction","predictive coding","pynn","spike"],"title":"Anisotropic connectivity implements motion-based prediction in a spiking neural network","type":"publication"},{"authors":["Laurent U Perrinet","David Fitzpatrick","James A Bednar"],"categories":null,"content":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1373029200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1373029200,"objectID":"59dc893df81bf8ba56840abde41daa96","permalink":"https://laurentperrinet.github.io/talk/2013-07-05-cerco/","publishdate":"2013-07-05T13:00:00Z","relpermalink":"/talk/2013-07-05-cerco/","section":"talk","summary":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","tags":["Biologically Inspired Computer vision"],"title":"Edge co-occurrences and categorizing natural images","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" see also:  Andrew P Davison, Daniel Bruderle, Jochen Eppler, Jens Kremkow, Eilif Muller, Dejan Pecevski, Laurent U Perrinet, Pierre Yger  (2008). PyNN: A Common Interface for Neuronal Network Simulators. Frontiers in Neuroinformatics.  Preprint  PDF  Cite  Project  DOI     ","date":1363824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363824000,"objectID":"ff7d7a2adb075b00990c7c8a43a62f2a","permalink":"https://laurentperrinet.github.io/talk/2013-03-21-marseille/","publishdate":"2013-03-21T00:00:00Z","relpermalink":"/talk/2013-03-21-marseille/","section":"talk","summary":"This session aims at presenting new ideas that emerged during the first years of BrainScaleS. Indeed, the collaborations that were initiated within the consortium led to the creation of novel tools as planned in the proposal but also some of which were unforeseen, like the Motion Clouds that we presented previously. We present here some prototypical and inspiring examples of such collaborative work on: 1) tool chains from experimental (Davison), computational (Antolik) or integrative (Petrovici) perspectives, 2) original methods inspired by novel types of analysis for propagating waves (Schmidt, Muller) or by novel magnetrodes (Pannetier Lecoeur).","tags":null,"title":"Why methods and tools are the key to artificial brain-like systems","type":"talk"},{"authors":["Laurent U Perrinet","Rick A Adams","Karl Friston"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"00d7f86918f76c4162fa1e0a49f2499c","permalink":"https://laurentperrinet.github.io/publication/perrinet-13-cns/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/perrinet-13-cns/","section":"publication","summary":"We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.","tags":["active inference"],"title":"Active inference, eye movements and oculomotor delays","type":"publication"},{"authors":["Laurent U Perrinet","Rick A Adams","Karl Friston"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"47e3f7af224cf7e12e3529e3ea4ca19c","permalink":"https://laurentperrinet.github.io/publication/perrinet-13-jffos/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/perrinet-13-jffos/","section":"publication","summary":"We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.","tags":["active inference"],"title":"Active inference, eye movements and oculomotor delays","type":"publication"},{"authors":["Rodrigo Nava","J Victor Marcos","Boris Escalante-Ramirez","Gabriel Cristóbal","Laurent U Perrinet","Raúl S J Estépar"],"categories":null,"content":" relies on log-Gabor filters:  Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal  (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision.  PDF  Cite  Code  DOI     ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"f231603f8147d552da5b7f3ece074388","permalink":"https://laurentperrinet.github.io/publication/nava-13/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/nava-13/","section":"publication","summary":"In recent years, with the advent of High-resolution Computed Tomography (HRCT), there has been an increased interest for diagnosing Chronic Obstructive Pulmonary Disease (COPD), which is commonly presented as emphysema. Since low-attenuation areas in HRCT images describe different emphysema patterns, the discrimination problem should focus on the characterization of both local intensities and global spatial variations. We propose a novel texture-based classification framework using complex Gabor filters and local binary patterns. We also analyzed a set of global and local texture descriptors to characterize emphysema morphology. The results have shown the effectiveness of our proposal and that the combination of descriptors provides robust features that lead to an improvement in the classification rate.","tags":["Biologically Inspired Computer vision","Image texture","sparse coding"],"title":"Advances in Texture Analysis for Emphysema Classification","type":"publication"},{"authors":["Andrew Isaac Meso","Claudio Simoncini","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"dcdcbae8385a42c124806425d5c22a3f","permalink":"https://laurentperrinet.github.io/publication/meso-13-vss/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/meso-13-vss/","section":"publication","summary":"Humans are able to interact successfully with moving objects in our dynamic world and the visual system effi ciently performs the motion computation that makes this possible. Object speed and direction are estimated following the integration of information across cortical motion sensitive channels. Speed estimation along this system is not fully understood, particularly the mapping function between the actual speed of viewed objects and that perceived by observers, a question we address in this work. It has been demonstrated that perceived speed is profoundly influenced by object contrast, spatial frequency, stimulus complexity and frequency bandwidth. In a 2 interval forced choice speed discrimination task, we present a random phase textured motion stimulus to probe small shifts in perceived speed measured using fi xed stimulus sets as reference scales while mean spatial frequency and bandwidths serve as the dependent variable in a probe. The presentations are short (200ms). Using a scale of narrowband stimuli (0.2 octaves), we measured a shift in perceived speed; higher frequencies are seen as faster moving than lower ones. On the scale of broader bandwidth (1 octave), this difference across frequency was reduced and perceived speed seems to converge on a slower representation. From these results we estimated this mapping between perceived and veridical stimulus speeds. In direct comparisons, the relative speed is faster for high frequencies and increases in bandwidth make stimuli appear slower. During this early computation, when presented with a random phase stimulus it appears that the visual systems makes assumptions about expected speeds based on the richness of the frequency content and the veridical speed is not explicitly computed. In this first 200ms, the perceptual system perhaps underestimates some speeds in an optimal response for initially stabilizing the scene. Acknowledgement: CNRS \u0026 Brainscales FP7","tags":["motion-clouds"],"title":"How and why do image frequency properties influence perceived speed?","type":"publication"},{"authors":["Claudio Simoncini","Laurent U Perrinet","Anna Montagnini","Guillaume S Masson"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"488ccebd1c69fe4332b2eefdc6be296e","permalink":"https://laurentperrinet.github.io/publication/simoncini-13-vss/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/simoncini-13-vss/","section":"publication","summary":"The visual system does not process information instantaneously, but rather integrates over time. Integration occurs both for stationary objects and moving objects, with very similar time constants (Burr, 1981). We measured, as a function of exposure duration, speed discrimination and ocular following performance for rich textured motion stimuli of varying spatial frequency bandwidth. Psychometric sensitivity and Oculometric sensitivity for these patterns increased with exposure duration. However the best stimuli for ocular following (namely those with a large bandwidth for spatial frequency) was well integrated up to about 150 - 200 msec, while the best stimuli for speed discrimination (small bandwidth) was well integrated up to about 300 msec. Interestingly, discriminability of ocular tracking eye movements follow a non-monotonic time course, due to the contribution of motor noise. These results suggest that although perception and action relies work in synergy, they may be described by two different integrating mechanisms: A low level, fast one guiding the ocular movement to enable one to catch stimuli in the visual fi eld quickly; and a slower one being able to measure the speed difference between two objects translating in the visual fi eld. Burr, D.C. (1981). Temporal summation of moving images by the human visual system. Proceedings of Royal Society, B211, 321 - 339","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception","type":"publication"},{"authors":["Mina A Khoei","Giacomo Benvenuti","Frédéric Chavane","Laurent U Perrinet"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"efbcb3b5e4f45306df2de57ee15ac36d","permalink":"https://laurentperrinet.github.io/publication/khoei-13-cns/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/khoei-13-cns/","section":"publication","summary":"Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013).","tags":["Bayesian model","motion detection","motion prediction"],"title":"Motion-based prediction and development of the response to an 'on the way' stimulus","type":"publication"},{"authors":["Rick A Adams","Laurent U Perrinet","Karl Friston"],"categories":null,"content":"   ","date":1351209600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351209600,"objectID":"17347db40bbc868085f66a637a026e7b","permalink":"https://laurentperrinet.github.io/publication/adams-12/","publishdate":"2012-10-26T00:00:00Z","relpermalink":"/publication/adams-12/","section":"publication","summary":"This paper introduces a model of oculomotor control during the smooth pursuit of occluded visual targets. This model is based upon active inference, in which subjects try to minimise their (proprioceptive) prediction error based upon posterior beliefs about the hidden causes of their (exteroceptive) sensory input. Our model appeals to a single principle -the minimisation of variational free energy - to provide Bayes optimal solutions to the smooth pursuit problem. However, it tries to accommodate the cardinal features of smooth pursuit of partially occluded targets that have been observed empirically in normal subjects and schizophrenia. Specifically, we account for the ability of normal subjects to anticipate periodic target trajectories and emit pre-emptive smooth pursuit eye movements -prior to the emergence of a target from behind an occluder. Furthermore, we show that a single deficit in the postsynaptic gain of prediction error units (encoding the precision of posterior beliefs) can account for several features of smooth pursuit in schizophrenia: namely, a reduction in motor gain and anticipatory eye movements during visual occlusion, a paradoxical improvement in tracking unpredicted deviations from target trajectories and a failure to recognise and exploit regularities in the periodic motion of visual targets. This model will form the basis of subsequent (dynamic causal) models of empirical eye tracking measurements, which we hope to validate, using psychopharmacology and studies of schizophrenia.","tags":["active inference","Bayesian model","eye movements","motion detection"],"title":"Smooth Pursuit and Visual Occlusion: Active Inference and Oculomotor Control in Schizophrenia","type":"publication"},{"authors":["Laurent U Perrinet","David Fitzpatrick","James A Bednar"],"categories":null,"content":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1336654800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1336654800,"objectID":"15cc78c0cd9cde8c3ef7f2f17f739eb1","permalink":"https://laurentperrinet.github.io/talk/2012-05-10-itwist/","publishdate":"2012-05-10T13:00:00Z","relpermalink":"/talk/2012-05-10-itwist/","section":"talk","summary":"Oriented edges in images of natural scenes tend to be aligned in collinear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (Geisler et al., Vis Res 41:711-24, 2001). The visual system appears to take advantage of this prior information, and human contour detection and grouping performance is well predicted by such an \\\"association field\\\" (Field et al., Vis Res 33:173-93, 1993). One possible candidate substrate for implementing an association field in mammals is the set of long-range lateral connections between neurons in the primary visual cortex (V1), which could act to facilitate detection of contours matching the association field, and/or inhibit detection of other contours (Choe and Miikkulainen, Biol Cyb 90:75-88, 2004). To fill this role, the lateral connections would need to be orientation specific and aligned along contours, and indeed such an arrangement has been found in tree shrew primary visual cortex (Bosking et al., J Neurosci 17:2112-27, 1997). However, it is not yet known whether these patterns develop as a result of visual experience, or are simply hard-wired to be appropriate for the statistics of natural scenes. To investigate this issue, we examined the properties of the visual environment of laboratory animals, to determine whether the observed connection patterns are more similar to the statistics of the rearing environment or of a natural habitat. Specifically, we analyzed the cooccurence statistics of edge elements in images of natural scenes, and compared them to corresponding statistics for images taken from within the rearing environment of the animals in the Bosking et al. (1997) study. We used a modified version of the algorithm from Geisler et al. (2001), with a more general edge extraction algorithm that uses sparse coding to avoid multiple responses to a single edge. Collinearity and co-circularity results for natural images replicated qualitatively the results from Geisler et al. (2001), confirming that prior information about continuations appeared consistently in natural images. However, we find that the largely man-made environment in which these animals were reared has a significantly higher probability of collinear edge elements. We thus predict that if the lateral connection patterns are due to visual experience, the patterns in wild-raised tree shrews would be very different from those measured by Bosking et al. (1997), with shorter-range correlations and less emphasis on collinear continuations. This prediction can be tested in future experiments on matching groups of animals reared in different environments.","tags":["sparse coding"],"title":"Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1332507600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1332507600,"objectID":"d669f50125739371dffa2b04f6e93788","permalink":"https://laurentperrinet.github.io/talk/2012-03-23-juelich/","publishdate":"2012-03-23T13:00:00Z","relpermalink":"/talk/2012-03-23-juelich/","section":"talk","summary":"","tags":null,"title":"Apparent motion in V1 - Probabilistic approaches","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1332424800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1332424800,"objectID":"3adb6588f649f5e9d3f10885e1f90f2a","permalink":"https://laurentperrinet.github.io/talk/2012-03-22-juelich/","publishdate":"2012-03-22T14:00:00Z","relpermalink":"/talk/2012-03-22-juelich/","section":"talk","summary":"","tags":["motion-clouds"],"title":"MotionClouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception","type":"talk"},{"authors":["Guillaume S Masson","Laurent U Perrinet"],"categories":null,"content":"   ","date":1332288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1332288000,"objectID":"4623e59bf94b6cdc78f759c6f3d99658","permalink":"https://laurentperrinet.github.io/publication/masson-12/","publishdate":"2012-03-21T00:00:00Z","relpermalink":"/publication/masson-12/","section":"publication","summary":"Short-latency ocular following are reflexive, tracking eye movements that are observed in human and non-human primates in response to a sudden and brief translation of the image. Initial, open-loop part of the eye acceleration reflects many of the properties attributed to low-level motion processing. We review a very large set of behavioral data demonstrating several key properties of motion detection and integration stages and their dynamics. We propose that these properties can be modeled as a behavioral receptive field exhibiting linear and nonlinear mechanisms responsible for context-dependent spatial integration and gain control. Functional models similar to that used for describing neuronal properties of receptive fields can then be applied successfully.","tags":["eye movements","motion detection"],"title":"The behavioral receptive field underlying motion integration for primate tracking eye movements","type":"publication"},{"authors":["Paula Sanz Leon","Ivo Vanzetta","Guillaume S Masson","Laurent U Perrinet"],"categories":null,"content":"   MotionClouds are random dynamic stimuli optimized to study motion perception.\n Web-site Source code using  Python. 37 citations on Google Scholar (last updated 22/10/2021) Follow-up paper  Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré  (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation.  Preprint  PDF  Cite  DOI    Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré  (2015). Biologically Inspired Dynamic Textures for Probing Motion Perception. Advances in Neural Information Processing Systems.  Preprint  PDF  Cite    This library was notably used in the following paper  Claudio Simoncini, Laurent U Perrinet, Anna Montagnini, Pascal Mamassian, Guillaume S Masson  (2012). More is not always better: dissociation between perception and action explained by adaptive gain control. Nature Neuroscience.  PDF  Cite  DOI     Figure 4 Broadband vs. narrowband stimuli. From A through B to C, the frequency bandwidth Bf increases, while all other parameters (such as f0) are kept constant. The MC with the broadest bandwidth is thought to best represent natural stimuli, since, as those, it contains many frequency components. A: Bf = 0:05 (Supplemental Movie S4). B: Bf = 0:15 (Supplemental Movie S5). C: Bf = 0:4 (Supplemental Movie S6).   ","date":1331683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1331683200,"objectID":"9cfdc9231eeaf93544fd87541f7b0eba","permalink":"https://laurentperrinet.github.io/publication/sanz-12/","publishdate":"2012-03-14T00:00:00Z","relpermalink":"/publication/sanz-12/","section":"publication","summary":"Choosing an appropriate set of stimuli is essential to characterize the response of a sensory system to a particular functional dimension, such as the eye movement following the motion of a visual scene. Here, we describe a framework to generate random texture movies with controlled information content, i.e., Motion Clouds. These stimuli are defined using a generative model that is based on controlled experimental parametrization. We show that Motion Clouds correspond to dense mixing of localized moving gratings with random positions. Their global envelope is similar to natural-like stimulation with an approximate full-field translation corresponding to a retinal slip. We describe the construction of these stimuli mathematically and propose an open-source Python-based implementation. Examples of the use of this framework are shown. We also propose extensions to other modalities such as color vision, touch, and audition.","tags":["log-gabor","motion-clouds"],"title":"Motion Clouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1327622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1327622400,"objectID":"b2656750c668fe50aaf184284e6180ca","permalink":"https://laurentperrinet.github.io/talk/2012-01-27-fil/","publishdate":"2012-01-27T00:00:00Z","relpermalink":"/talk/2012-01-27-fil/","section":"talk","summary":"Moving objects generate sensory information that may be noisy and ambiguous, yet it is important to be able to reconstruct object speed as fast as possible. One unsolved question is to understand how the brain pools motion information to give an efficient response. I will present computational models of motion detection in the visual system that try to answer this question. First, sensory information is grabbed by pooling the information from different sensory neurons. This pooling is modulated by the precision of information and we will present some recent model-based behavioural results. Then I will focus on a novel model of motion-based prediction that allows to track objects on smooth trajectories. This model gives an economical description of neural mechanisms associated with the processing underlying motion detection. Finally, we will propose an exploratory hypothesis such that eye movements may be understood as the prospective response to this dynamical sensory response knowing oculomotor constraints such as delays. This line of research aims at showing that through the convergent use of models, electrophysiology or behavioural responses, the study of motion detection is an essential tool in our understanding of neural computations.","tags":["Bayesian model","dynamics","eye movements","free energy","motion detection","predictive coding"],"title":"Grabbing, tracking and sniffing as models for motion detection and eye movements","type":"talk"},{"authors":["Laurent U Perrinet","David Fitzpatrick","James A Bednar"],"categories":null,"content":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1327410000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1327410000,"objectID":"3d468e49e4c231119204172a710f8bd3","permalink":"https://laurentperrinet.github.io/talk/2012-01-24-edinburgh/","publishdate":"2012-01-24T13:00:00Z","relpermalink":"/talk/2012-01-24-edinburgh/","section":"talk","summary":"Oriented edges in images of natural scenes tend to be aligned in collinear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (Geisler et al., Vis Res 41:711-24, 2001). The visual system appears to take advantage of this prior information, and human contour detection and grouping performance is well predicted by such an \\\"association field\\\" (Field et al., Vis Res 33:173-93, 1993). One possible candidate substrate for implementing an association field in mammals is the set of long-range lateral connections between neurons in the primary visual cortex (V1), which could act to facilitate detection of contours matching the association field, and/or inhibit detection of other contours (Choe and Miikkulainen, Biol Cyb 90:75-88, 2004). To fill this role, the lateral connections would need to be orientation specific and aligned along contours, and indeed such an arrangement has been found in tree shrew primary visual cortex (Bosking et al., J Neurosci 17:2112-27, 1997). However, it is not yet known whether these patterns develop as a result of visual experience, or are simply hard-wired to be appropriate for the statistics of natural scenes. To investigate this issue, we examined the properties of the visual environment of laboratory animals, to determine whether the observed connection patterns are more similar to the statistics of the rearing environment or of a natural habitat. Specifically, we analyzed the cooccurence statistics of edge elements in images of natural scenes, and compared them to corresponding statistics for images taken from within the rearing environment of the animals in the Bosking et al. (1997) study. We used a modified version of the algorithm from Geisler et al. (2001), with a more general edge extraction algorithm that uses sparse coding to avoid multiple responses to a single edge. Collinearity and co-circularity results for natural images replicated qualitatively the results from Geisler et al. (2001), confirming that prior information about continuations appeared consistently in natural images. However, we find that the largely man-made environment in which these animals were reared has a significantly higher probability of collinear edge elements. We thus predict that if the lateral connection patterns are due to visual experience, the patterns in wild-raised tree shrews would be very different from those measured by Bosking et al. (1997), with shorter-range correlations and less emphasis on collinear continuations. This prediction can be tested in future experiments on matching groups of animals reared in different environments.","tags":null,"title":"Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" See a followup in Perrinet et al, 2012  ","date":1326387600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1326387600,"objectID":"d56f0438532461d81423509c263d46b8","permalink":"https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/","publishdate":"2012-01-12T17:00:00Z","relpermalink":"/talk/2012-01-12-vision-at-ucl/","section":"talk","summary":"In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of  two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is  implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations.","tags":["Bayesian model","center-surround interactions","dynamics","eye movements","motion detection","motion prediction"],"title":"Motion-based prediction is sufficient to solve the aperture problem","type":"talk"},{"authors":["Laurent U Perrinet","Rick A Adams","Karl Friston"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"9108fe20d3d5a1d3d5dc6c774a66d030","permalink":"https://laurentperrinet.github.io/publication/perrinet-12-areadne/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/perrinet-12-areadne/","section":"publication","summary":"We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals. This work was supported from the European Community's Seventh Framework Program FP7/2007-2013 under grant agreement number 214728-2, (CODDE)","tags":["active inference"],"title":"Active inference, smooth pursuit and oculomotor delays","type":"publication"},{"authors":["Nicole Voges","Laurent U Perrinet"],"categories":null,"content":"    Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI     ","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"11b59f2a8ee40366aa15a0c1a0c0ee74","permalink":"https://laurentperrinet.github.io/publication/voges-12/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/voges-12/","section":"publication","summary":"Most studies on the dynamics of recurrent cortical networks are either based on purely random wiring or neighborhood couplings. Neuronal cortical connectivity, however, shows a complex spatial pattern composed of local and remote patchy connections. We ask to what extent such geometric traits influence the '' idle'' dynamics of two-dimensional (2d) cortical network models composed of conductance-based integrate-and-fire (iaf) neurons. In contrast to the typical 1 mm2 used in most studies, we employ an enlarged spatial set-up of 25 mm2 to provide for long-range connections. Our models range from purely random to distance-dependent connectivities including patchy projections, i.e., spatially clustered synapses. Analyzing the characteristic measures for synchronicity and regularity in neuronal spiking, we explore and compare the phase spaces and activity patterns of our simulation results. Depending on the input parameters, different dynamical states appear, similar to the known synchronous regular '' SR'' or asynchronous irregular '' AI'' firing in random networks. Our structured networks, however, exhibit shifted and sharper transitions, as well as more complex activity patterns. Distance-dependent connectivity structures induce a spatio-temporal spread of activity, e.g., propagating waves, that random networks cannot account for. Spatially and temporally restricted activity injections reveal that a high amount of local coupling induces rather unstable AI dynamics. We find that the amount of local versus long-range connections is an important parameter, whereas the structurally advantageous wiring cost optimization of patchy networks has little bearing on the phase space.","tags":["lateral connections"],"title":"Complex dynamics in recurrent cortical networks based on spatially realistic connectivities","type":"publication"},{"authors":["Claudio Simoncini","Anna Montagnini","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"efe92d24d9f32dc735f83ebfb549a481","permalink":"https://laurentperrinet.github.io/publication/simoncini-12-vss/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/simoncini-12-vss/","section":"publication","summary":"Under natural viewing conditions, small movements of the eyes prevent the maintenance of a steady direction of gaze. It is unclear how the spatiotemporal content of the fixated scene has an impact on the properties of miniatures, fixational eye movements. We have investigated the characteristics of fixational eye movements recorded while human subjects are instructed to fixate natural statistics random textures (Motion Clouds) in which we manipulated the spatial frequency content. We used long presentations (5 sec) of Motion Clouds stimuli (Schrater et al. 2000) of varying spatial frequency bandwidths (Bsf) around different central spatial frequency (Sf0). We found that central spatial frequency has an effect upon microsaccadic eye movements. In particular, smaller saccadic amplitudes were associated with high spatial frequencies, and larger saccades with low spatial frequencies. Broadening the spatial frequency bandwidth also changed the distribution of microsaccade amplitudes. A lower spatial frequencies, larger Bsf resulted in a large reduction of microsaccades amplitude while fixation behavior for high spatial frequencies texture was not affected. Relationship between microsaccade rate and intersaccadic timing was also dependent upon Bsf. These results suggest that the spatial frequency content of the fixated images have a strong impact upon fixation instability.","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"Effect of image statistics on fixational eye movements","type":"publication"},{"authors":["Claudio Simoncini","Laurent U Perrinet","Anna Montagnini","Pascal Mamassian","Guillaume S Masson"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"9781fb51459379d278f099d11bb9bd31","permalink":"https://laurentperrinet.github.io/publication/simoncini-12-coding/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/simoncini-12-coding/","section":"publication","summary":"To measure speed and direction of moving objects, the cortical motion system pools information across different spatiotemporal channels. One yet unsolved question is to understand how the brain pools this information and whether this pooling is generic or adaptive at the behavioral contexts. Here, we investigate in humans this integration process for two different tasks: psychophysical speed discrimination and ocular following eye movements, which are a probe of early motion detection and integration (Masson \u0026 Perrinet, 2011). For both tasks, we used short presentations of ``moving textures'' stimuli (Schrater et al., 2000) in which the width of the spatial frequency distribution (Bsf) was varied. We found that larger Bsf elicited stronger initial eye velocity during the open-loop part of tracking responses. Moreover, richer stimuli resulted in more accurate and reliable motor responses. By contrast, larger Bsf had a detrimental effect upon speed discrimination performance: speed discrimination thresholds linearly decreased when the width of spatial frequency distribution increased. These opposite results can be explained by a different decoding strategy where speed information is under the control of different gain setting mechanisms. We tested this model by measuring contrast response functions of both ocular following and speed discrimination for each Bsf. We found that varying spatial frequency distribution had opposite effect upon contrast gain control. Increasing Bsf lowered half-saturation contrast for ocular following but increased it for perception. Our results supports the view that speed-based perception and tracking eye movements are under the control of different early decoding mechanism. References Masson, G.S. \u0026 Perrinet, L.U. The behavioural receptive field underlying motion integration for primate tracking eye movements. Neurosci. BioBehav. Review 36, 1-25 (2011). Schrater, P.R., Knill, D.C. \u0026 Simoncelli, E.P. Mechanism of visual motion detection. Nat. Neurosci. 3, 64-68 (2000).","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception.","type":"publication"},{"authors":["Claudio Simoncini","Laurent U Perrinet","Anna Montagnini","Pascal Mamassian","Guillaume S Masson"],"categories":null,"content":"     Band-pass motion stimuli for perception and action tasks. (a) In the space representing temporal against spatial frequency, each line going through the origin corresponds to stimuli moving at the same speed. A simple drifting grating is a single point in this space. Our moving texture stimuli had their energy distributed within an ellipse elongated along a given speed line, keeping constant the mean spatial and temporal frequencies. The spatio-temporal bandwidth was manipulated by co-varying Bsf and Btf as illustrated by the (x,y,t) examples. Human performance was measured for two different tasks, run in parallel blocks. (b) For ocular tracking, motion stimuli were presented for a short duration (200ms) in the wake of a centering saccade to control both attention and fixation states. (c) For speed discrimination, test and reference stimuli were presented successively for the same duration and subjects were instructed to indicate whether the test stimulus was perceived as slower or faster than reference. \n","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"5f0fa9e3eaa10f282d732e1eff204ce1","permalink":"https://laurentperrinet.github.io/publication/simoncini-12/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/simoncini-12/","section":"publication","summary":"Moving objects generate motion information at different scales, which are processed in the visual system with a bank of spatiotemporal frequency channels. It is not known how the brain pools this information to reconstruct object speed and whether this pooling is generic or adaptive; that is, dependent on the behavioral task. We used rich textured motion stimuli of varying bandwidths to decipher how the human visual motion system computes object speed in different behavioral contexts. We found that, although a simple visuomotor behavior such as short-latency ocular following responses takes advantage of the full distribution of motion signals, perceptual speed discrimination is impaired for stimuli with large bandwidths. Such opposite dependencies can be explained by an adaptive gain control mechanism in which the divisive normalization pool is adjusted to meet the different constraints of perception and action.","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"More is not always better: dissociation between perception and action explained by adaptive gain control","type":"publication"},{"authors":["Guillaume S Masson","Laurent U Perrinet"],"categories":null,"content":" See a followup in Perrinet et al, 2012  ","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"9ae1d748edd216b5ada59bd55ce9b503","permalink":"https://laurentperrinet.github.io/publication/masson-12-areadne/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/masson-12-areadne/","section":"publication","summary":"In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations.","tags":["aperture problem","Bayesian model","motion prediction","predictive coding"],"title":"Motion-based prediction is sufficient to solve the aperture problem","type":"publication"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"     The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: it’s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.    Figure 1: (A) The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: it’s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. (A-inset) On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). (B) We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t + dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). (C) Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). (C-Inset) The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. (D) As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).    Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).    Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. (Right) Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. (Left) We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).    Figure 4: (Top) Prediction implements a competition between different trajectories. …","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"b1a5f2187cfabe1c229fc75c6b18bbd0","permalink":"https://laurentperrinet.github.io/publication/perrinet-12-pred/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/perrinet-12-pred/","section":"publication","summary":"In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations.","tags":["aperture problem","Bayesian model","motion prediction","predictive coding"],"title":"Motion-based prediction is sufficient to solve the aperture problem","type":"publication"},{"authors":["Claudio Simoncini","Anna Montagnini","Laurent U Perrinet","Pascal Mamassian","Guillaume S Masson"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"3bb39bcd3af7b5e6b254d234ae6745fd","permalink":"https://laurentperrinet.github.io/publication/simoncini-11-vss/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/simoncini-11-vss/","section":"publication","summary":"In order to analyze the characteristics of a rich dynamic visual environment, the visual system must integrate information collected at different scales through different spatiotemporal frequency channels. Still, it remains unclear how reliable representations of motion direction or speed are elaborated when presented with large bandwidth motion stimuli or natural statistics. Last year, we have shown that broadening the spatiotemporal frequency content of a textured pattern moving at constant speed leads to different results on a reflexive tracking task and a speed discrimination task. Larger bandwidth stimuli increase response amplitude and sensitivity of ocular following, consistently with a maximum-likelihood (ML) model of motion decoding. In contrast, larger bandwidth stimuli impair speed discrimination performance, suggesting that the perceptual system cannot take advantage of such additional, redundant information. Instead of ML, a gain control decoding mechanism can explain the drop in performance, suggesting that action and perception rely on different decoding mechanisms. To further investigate such task-dependant pooling of motion information, we measured pattern discrimination performance using these textured stimuli. Two noise patterns were presented sequentially for 250 ms on a CRT monitor (1280 × 1024 @ 100 Hz) and covered 47$,^∘$ of visual angle with identical properties (mean SF, bandwidth SF, speed) except for a randomized phase spectrum. A test pattern was then presented and subjects were asked to match it with one or the other reference stimulus (ABX task). At small bandwidth and optimal mean spatial frequency (0.3 cpd), subjects were able to discriminate the two patterns with high accuracy. Performance dropped to chance level as spatial frequency bandwidth increased. Increasing the mean spatial frequency decreased the overall performance. Again, these results suggest that perceptual performance is deteriorated in presence of larger information.","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize","type":"publication"},{"authors":["Karl Friston","Rick A Adams","Laurent U Perrinet","Michael Breakspear"],"categories":null,"content":"     This schematic shows the dependencies among various quantities that are assumed when modeling the exchanges of a self organizing system like the brain with the environment. The top panel describes the states of the environment and the system or agent in terms of a probabilistic dependency graph, where connections denote directed dependencies. The quantities are described within the nodes of this graph with exemplar forms for their dependencies on other variables (see main text). Here, hidden and internal states are separated by action and sensory states. Both action and internal states encoding a conditional density minimize free energy, while internal states encoding prior beliefs maximize salience. Both free energy and salience are defined in terms of a generative model that is shown as fictive dependency graph in the lower panel. Note that the variables in the real world and the form of their dynamics are different from that assumed by the generative model; this is why external states are in bold. Furthermore, note that action is a state in the model of the brain but is replaced by hidden controls in the brain’s model of its world. This means that the agent is not aware of action but has beliefs about hidden causes in the world that action can fulfill through minimizing free energy. These beliefs correspond to prior expectations that sensory states will be sampled in a way that optimizes conditional confidence or salience. \n","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"4b757a5fa945f21e226cfddda93c96e9","permalink":"https://laurentperrinet.github.io/publication/friston-12/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/friston-12/","section":"publication","summary":"If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.","tags":["active inference","Bayesian model","eye movements","free energy","psychophysics"],"title":"Perceptions as Hypotheses: Saccades as Experiments","type":"publication"},{"authors":["Mina A Khoei","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376000,"objectID":"4f13ffb3c1ee0ba853c14874b30997ac","permalink":"https://laurentperrinet.github.io/publication/khoei-12-sfn/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/khoei-12-sfn/","section":"publication","summary":"Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013).","tags":["Bayesian model","motion prediction"],"title":"Role of motion-based prediction in motion extrapolation","type":"publication"},{"authors":["Laurent U Perrinet","David Fitzpatrick","James A Bednar"],"categories":null,"content":" Abstract Control Number: 17671 Presentation Number: 530.04 Presentation Time: 8:45am - 9:00am session: Session Type: Nanosymposium Session Number: 530 Session Title: Development of Motor and Sensory Systems See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1321346700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1321346700,"objectID":"8d620d7ff55576be59c02ad748829f9c","permalink":"https://laurentperrinet.github.io/talk/2011-11-15-sfn/","publishdate":"2011-11-15T08:45:00Z","relpermalink":"/talk/2011-11-15-sfn/","section":"talk","summary":"Oriented edges in images of natural scenes tend to be aligned in collinear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (Geisler et al., Vis Res 41:711-24, 2001). The visual system appears to take advantage of this prior information, and human contour detection and grouping performance is well predicted by such an \\\"association field\\\" (Field et al., Vis Res 33:173-93, 1993). One possible candidate substrate for implementing an association field in mammals is the set of long-range lateral connections between neurons in the primary visual cortex (V1), which could act to facilitate detection of contours matching the association field, and/or inhibit detection of other contours (Choe and Miikkulainen, Biol Cyb 90:75-88, 2004). To fill this role, the lateral connections would need to be orientation specific and aligned along contours, and indeed such an arrangement has been found in tree shrew primary visual cortex (Bosking et al., J Neurosci 17:2112-27, 1997). However, it is not yet known whether these patterns develop as a result of visual experience, or are simply hard-wired to be appropriate for the statistics of natural scenes. To investigate this issue, we examined the properties of the visual environment of laboratory animals, to determine whether the observed connection patterns are more similar to the statistics of the rearing environment or of a natural habitat. Specifically, we analyzed the cooccurence statistics of edge elements in images of natural scenes, and compared them to corresponding statistics for images taken from within the rearing environment of the animals in the Bosking et al. (1997) study. We used a modified version of the algorithm from Geisler et al. (2001), with a more general edge extraction algorithm that uses sparse coding to avoid multiple responses to a single edge. Collinearity and co-circularity results for natural images replicated qualitatively the results from Geisler et al. (2001), confirming that prior information about continuations appeared consistently in natural images. However, we find that the largely man-made environment in which these animals were reared has a significantly higher probability of collinear edge elements. We thus predict that if the lateral connection patterns are due to visual experience, the patterns in wild-raised tree shrews would be very different from those measured by Bosking et al. (1997), with shorter-range correlations and less emphasis on collinear continuations. This prediction can be tested in future experiments on matching groups of animals reared in different environments.","tags":null,"title":"Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1317819600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1317819600,"objectID":"b02a0e97c577e41ed01bd6b36b02d311","permalink":"https://laurentperrinet.github.io/talk/2011-10-05-brain-scales-ess/","publishdate":"2011-10-05T13:00:00Z","relpermalink":"/talk/2011-10-05-brain-scales-ess/","section":"talk","summary":"","tags":["sparse coding"],"title":"Demo 1, Task4: Implementation of models showing emergence of cortical fields and maps","type":"talk"},{"authors":["Laurent U Perrinet","David Fitzpatrick","James A Bednar"],"categories":null,"content":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1317214800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1317214800,"objectID":"00238f6edc317a6e8afb08920badae3a","permalink":"https://laurentperrinet.github.io/talk/2011-09-28-ermites/","publishdate":"2011-09-28T13:00:00Z","relpermalink":"/talk/2011-09-28-ermites/","section":"talk","summary":"Oriented edges in images of natural scenes tend to be aligned in collinear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (Geisler et al., Vis Res 41:711-24, 2001). The visual system appears to take advantage of this prior information, and human contour detection and grouping performance is well predicted by such an \\\"association field\\\" (Field et al., Vis Res 33:173-93, 1993). One possible candidate substrate for implementing an association field in mammals is the set of long-range lateral connections between neurons in the primary visual cortex (V1), which could act to facilitate detection of contours matching the association field, and/or inhibit detection of other contours (Choe and Miikkulainen, Biol Cyb 90:75-88, 2004). To fill this role, the lateral connections would need to be orientation specific and aligned along contours, and indeed such an arrangement has been found in tree shrew primary visual cortex (Bosking et al., J Neurosci 17:2112-27, 1997). However, it is not yet known whether these patterns develop as a result of visual experience, or are simply hard-wired to be appropriate for the statistics of natural scenes. To investigate this issue, we examined the properties of the visual environment of laboratory animals, to determine whether the observed connection patterns are more similar to the statistics of the rearing environment or of a natural habitat. Specifically, we analyzed the cooccurence statistics of edge elements in images of natural scenes, and compared them to corresponding statistics for images taken from within the rearing environment of the animals in the Bosking et al. (1997) study. We used a modified version of the algorithm from Geisler et al. (2001), with a more general edge extraction algorithm that uses sparse coding to avoid multiple responses to a single edge. Collinearity and co-circularity results for natural images replicated qualitatively the results from Geisler et al. (2001), confirming that prior information about continuations appeared consistently in natural images. However, we find that the largely man-made environment in which these animals were reared has a significantly higher probability of collinear edge elements. We thus predict that if the lateral connection patterns are due to visual experience, the patterns in wild-raised tree shrews would be very different from those measured by Bosking et al. (1997), with shorter-range correlations and less emphasis on collinear continuations. This prediction can be tested in future experiments on matching groups of animals reared in different environments.","tags":["sparse coding"],"title":"Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1","type":"talk"},{"authors":["Claudio Simoncini","Anna Montagnini","Laurent U Perrinet","Pascal Mamassian","Guillaume S Masson"],"categories":null,"content":"","date":1316736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1316736000,"objectID":"b248011de47e25ba1dcc022d89d7ce05","permalink":"https://laurentperrinet.github.io/publication/simoncini-11-pattern/","publishdate":"2011-09-23T00:00:00Z","relpermalink":"/publication/simoncini-11-pattern/","section":"publication","summary":"In order to analyze the characteristics of a rich dynamic visual environment, the visual system must integrate information collected at different scales through different spatiotemporal frequency channels. Still, it remains unclear how reliable representations of motion direction or speed are elaborated when presented with large bandwidth motion stimuli or natural statistics. Last year, we have shown that broadening the spatiotemporal frequency content of a textured pattern moving at constant speed leads to different results on a reflexive tracking task and a speed discrimination task. Larger bandwidth stimuli increase response amplitude and sensitivity of ocular following, consistently with a maximum-likelihood (ML) model of motion decoding. In contrast, larger bandwidth stimuli impair speed discrimination performance, suggesting that the perceptual system cannot take advantage of such additional, redundant information. Instead of ML, a gain control decoding mechanism can explain the drop in performance, suggesting that action and perception rely on different decoding mechanisms. To further investigate such task-dependant pooling of motion information, we measured pattern discrimination performance using these textured stimuli. Two noise patterns were presented sequentially for 250 ms on a CRT monitor (1280 × 1024 @ 100 Hz) and covered 47$,^∘$ of visual angle with identical properties (mean SF, bandwidth SF, speed) except for a randomized phase spectrum. A test pattern was then presented and subjects were asked to match it with one or the other reference stimulus (ABX task). At small bandwidth and optimal mean spatial frequency (0.3 cpd), subjects were able to discriminate the two patterns with high accuracy. Performance dropped to chance level as spatial frequency bandwidth increased. Increasing the mean spatial frequency decreased the overall performance. Again, these results suggest that perceptual performance is deteriorated in presence of larger information.","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"La finalité de cette manifestation est de permettre à nos chercheurs de se réunir en groupes de travail et en ateliers afin de découvrir la thématique des neurosciences et son interdisciplinarité. La manifestation se tient dans le cadre des activités du laboratoire LAMS, de ABC MATHINFO, du GDRI NeurO et du réseau méditerranéen NeuroMed.\n related publication @ SPIE 2008 see this more recent talk @ UCL, London  ","date":1309564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309564800,"objectID":"b00dcb58190c63e3a496865a06cab61d","permalink":"https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/","publishdate":"2011-07-02T00:00:00Z","relpermalink":"/talk/2011-07-02-neuro-med-talk/","section":"talk","summary":"Sensory informations such as visual images are inherently variable. We use probabilistic models to describe how the low-level visual system could describe superposed and ambiguous information. This allows to describe the interactions of neighboring populations of neurons as inference rules that dynamically build up the overall description of the visual scene. We focus here on temporal prediction, that is by the transport of information based on an estimate of local motion in the image.","tags":["Bayesian model","dynamics","eye movements","motion detection","motion prediction"],"title":"Propriétés émergentes d'un modèle de prédiction probabiliste utilisant un champ neural","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Qui créera le premier ordinateur intelligent? Les ordinateurs classiques sont de plus en plus puissants, mais restent toujours aussi « stupides ». Impossible d’en trouver un avec lequel on puisse dialoguer de façon naturelle. Aucun système visuel artificiel ne voit aussi bien que nous, ou qu’une mouche ! Alors qui inventera le premier calculateur intelligent ?   Le code neural (En haut : © F. Chavane, en bas : © T. Bal). Le code neural est mieux compris grâce aux techniques d’imagerie récentes. Les neurosciences computationnelles permettent d’étudier les propriétés des réseaux de neurones.\n","date":1308528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1308528000,"objectID":"36a83650ded03fa0f345808b4f6ceb37","permalink":"https://laurentperrinet.github.io/publication/perrinet-10-doc-sciences/","publishdate":"2011-06-20T00:00:00Z","relpermalink":"/publication/perrinet-10-doc-sciences/","section":"publication","summary":"Qui créera le premier ordinateur intelligent? Les ordinateurs classiques sont de plus en plus puissants, mais restent toujours aussi « stupides ». Impossible d’en trouver un avec lequel on puisse dialoguer de façon naturelle.","tags":null,"title":"Qui créera le premier ordinateur intelligent?","type":"publication"},{"authors":["Amarender Bogadhi","Anna Montagnini","Pascal Mamassian","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"    See a followup in  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite     ","date":1303430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1303430400,"objectID":"9f582a70800dd6acf11cfbbb452809a7","permalink":"https://laurentperrinet.github.io/publication/bogadhi-11/","publishdate":"2011-04-22T00:00:00Z","relpermalink":"/publication/bogadhi-11/","section":"publication","summary":"Accuracy in estimating an object's global motion over time is not only affected by the noise in visual motion information but also by the spatial limitation of the local motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that during the initial stages of the motion information processing, 1D motion cues related to the object's edges have a dominating influence over the estimate of the object's global motion. However, during the later stages, 2D motion cues related to terminators (edge-endings) progressively take over, leading to a final correct estimate of the object's global motion. Here, we propose a recursive extension to the Bayesian framework for motion processing (Weiss, Simoncelli, Adelson, 2002) cascaded with a model oculomotor plant to describe the dynamic integration of 1D and 2D motion information in the context of smooth pursuit eye movements. In the recurrent Bayesian framework, the prior defined in the velocity space is combined with the two independent measurement likelihood functions, representing edge-related and terminator-related information, respectively to obtain the posterior. The prior is updated with the posterior at the end of each iteration step. The maximum-a posteriori (MAP) of the posterior distribution at every time step is fed into the oculomotor plant to produce eye velocity responses that are compared to the human smooth pursuit data. The recurrent model was tuned with the variance of pursuit responses to either p̈ure1̈D or r̈̈e ̈̈motion. The oculomotor plant was tuned with an independent set of oculomotor data, including the effects of line length (i.e. stimulus energy) and directional anisotropies in the smooth pursuit responses. The model not only provides an accurate qualitative account of dynamic motion integration but also a quantitative account that is close to the smooth pursuit response across several conditions (three contrasts and three speeds) for two human subjects.","tags":["Bayesian model","predictive coding"],"title":"Pursuing motion illusions: a realistic oculomotor framework for Bayesian inference","type":"publication"},{"authors":["Jérôme Fleuriet","Sandrine Hugues","Laurent U Perrinet","Laurent Goffart"],"categories":null,"content":"","date":1296518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296518400,"objectID":"e2f9027a08b80eb0e1dd8d394ca94c9b","permalink":"https://laurentperrinet.github.io/publication/fleuriet-11/","publishdate":"2011-02-01T00:00:00Z","relpermalink":"/publication/fleuriet-11/","section":"publication","summary":"When generating a saccade toward a moving target, the target displacement that occurs during the period spanning from its detection to the saccade end must be taken into account to accurately foveate the target and to initiate its pursuit. Previous studies have shown that these saccades are characterized by a lower peak velocity and a prolonged deceleration phase. In some cases, a second peak eye velocity appears during the deceleration phase, presumably reflecting the late influence of a mechanism that compensates for the target displacement occurring before saccade end. The goal of this work was to further determine in the head restrained monkey the dynamics of this putative compensatory mechanism. A step-ramp paradigm, where the target motion was orthogonal to a target step occurring along the primary axes, was used to estimate from the generated saccades: a component induced by the target step and another one induced by the target motion. Resulting oblique saccades were compared with saccades to a static target with matched horizontal and vertical amplitudes. This study permitted to estimate the time taken for visual motion-related signals to update the programming and execution of saccades. The amplitude of the motion-related component was slightly hypometric with an undershoot that increased with target speed. Moreover, it matched with the eccentricity that the target had 40-60 ms before saccade end. The lack of significant difference in the delay between the onsets of the horizontal and vertical components between saccades directed toward a static target and those aimed at a moving target questions the late influence of the compensatory mechanism. The results are discussed within the framework of the dual drive and mapping hypotheses.","tags":["eye movements","motion detection"],"title":"Saccadic foveation of a moving visual target in the rhesus monkey","type":"publication"},{"authors":["Laurent U Perrinet","David Fitzpatrick","James A Bednar"],"categories":null,"content":" See a followup in  Laurent U Perrinet, James A Bednar  (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports.  Preprint  PDF  Cite  Code  DOI     ","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"7b940d9dc657b1dcfc5e9e186df69091","permalink":"https://laurentperrinet.github.io/publication/perrinet-11-sfn/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/perrinet-11-sfn/","section":"publication","summary":"Oriented edges in images of natural scenes tend to be aligned in collinear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (Geisler et al., Vis Res 41:711-24, 2001). The visual system appears to take advantage of this prior information, and human contour detection and grouping performance is well predicted by such an association field (̈Field et al., Vis Res 33:173-93, 1993). One possible candidate substrate for implementing an association field in mammals is the set of long-range lateral connections between neurons in the primary visual cortex (V1), which could act to facilitate detection of contours matching the association field, and/or inhibit detection of other contours (Choe and Miikkulainen, Biol Cyb 90:75-88, 2004). To fill this role, the lateral connections would need to be orientation specific and aligned along contours, and indeed such an arrangement has been found in tree shrew primary visual cortex (Bosking et al., J Neurosci 17:2112-27, 1997). However, it is not yet known whether these patterns develop as a result of visual experience, or are simply hard-wired to be appropriate for the statistics of natural scenes. To investigate this issue, we examined the properties of the visual environment of laboratory animals, to determine whether the observed connection patterns are more similar to the statistics of the rearing environment or of a natural habitat. Specifically, we analyzed the cooccurence statistics of edge elements in images of natural scenes, and compared them to corresponding statistics for images taken from within the rearing environment of the animals in the Bosking et al. (1997) study. We used a modified version of the algorithm from Geisler et al. (2001), with a more general edge extraction algorithm that uses sparse coding to avoid multiple responses to a single edge. Collinearity and co-circularity results for natural images replicated qualitatively the results from Geisler et al. (2001), confirming that prior information about continuations appeared consistently in natural images. However, we find that the largely man-made environment in which these animals were reared has a significantly higher probability of collinear edge elements. We thus predict that if the lateral connection patterns are due to visual experience, the patterns in wild-raised tree shrews would be very different from those measured by Bosking et al. (1997), with shorter-range correlations and less emphasis on collinear continuations. This prediction can be tested in future experiments on matching groups of animals reared in different environments. W.H. Bosking and Y. Zhang and B. Schofield and D. Fitzpatrick (1997) Orientation selectivity and the arrangement of horizontal connections in tree shrew striate cortex Journal of Neuroscience 17:2112-27. E.M. Callaway and L.C. Katz (1990) Emergence and refinement of clustered horizontal connections in cat striate cortex. Journal of Neuroscience 10:1134--53. Y. Choe and R. Miikkulainen (2004) Contour integration and segmentation with self-organized lateral connections Biological Cybernetics 90:75-88. D.J. Field, A. Hayes, and R.F. Hess (1993) Contour integration by the human visual system: Evidence for a local s̈̈ociation field̈s̈̈ion Research 33:173--93. W.S. Geisler, J.S. Perry, B.J. Super, and D.P. Gallogly (2001) Edge co-occurrence in natural images predicts contour grouping performance. Vision Research 41:711-24.","tags":["motion-clouds"],"title":"Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1","type":"publication"},{"authors":["Mina A Khoei","Laurent U Perrinet","Amarender Bogadhi","Anna Montagnini","Guillaume S Masson"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1293840000,"objectID":"ef8cb606f06b1f688e0102b9c83bb2a1","permalink":"https://laurentperrinet.github.io/publication/khoei-11-ecvp/","publishdate":"2011-01-01T00:00:00Z","relpermalink":"/publication/khoei-11-ecvp/","section":"publication","summary":"Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013).","tags":["bayesian model","motion prediction"],"title":"Role of motion inertia in dynamic motion integration for smooth pursuit","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"An event ranging “From Mathematical Image Analysis to Neurogeometry of the Brain” LADISLAV TAUC \u0026amp; GDR MSPC NEUROSCIENCES CONFERENCE.\n related publication from Mina Khoei @ TAUC 2012 see this more recent talk @ UCL, London  ","date":1292544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1292544000,"objectID":"5321a4470a7b1725567777cf2376cca7","permalink":"https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/","publishdate":"2010-12-17T00:00:00Z","relpermalink":"/talk/2010-12-17-tauc-talk/","section":"talk","summary":"Sensory informations such as visual images are inherently variable. We use probabilistic models to describe how the low-level visual system could describe superposed and ambiguous information. This allows to describe the interactions of neighboring populations of neurons as inference rules that dynamically build up the overall description of the visual scene. We focus here on temporal prediction, that is by the transport of information based on an estimate of local motion in the image.","tags":["Bayesian model","center-surround interactions","dynamics","eye movements","motion detection","motion prediction"],"title":"Probabilistic models of the low-level visual system: the role of prediction in detecting motion","type":"talk"},{"authors":["Nicole Voges","Laurent U Perrinet"],"categories":null,"content":"    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012). Complex dynamics in recurrent cortical networks based on spatially realistic connectivities. Frontiers in Computational Neuroscience.  PDF  Cite  DOI     ","date":1289347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1289347200,"objectID":"b7bb150a5d688f60d3d034d64f3faff2","permalink":"https://laurentperrinet.github.io/publication/voges-10-jpp/","publishdate":"2010-11-10T00:00:00Z","relpermalink":"/publication/voges-10-jpp/","section":"publication","summary":"We study cortical network dynamics for a spatially embedded network model. It represents, in terms of spatial scale, a large piece of cortex allowing for long-range connections, resulting in a rather sparse connectivity. The spatial embedding also permits us to include distance-dependent conduction delays. We use two different types of conductance-based I\u0026F neurons as excitatory and inhibitory units, as well as specific connection probabilities. In order to remain computationally tractable, we reduce neuron density, modelling part of the missing internal input via external poissonian spike trains. Compared to previous studies, we observe significant changes in the dynamical phase space: Altered activity patterns require another regularity measures than the coefficient of variation. Hence, we compare three different regularity measure on the basis of artificial inter-spike-interval distributions. We identify two types of mixed states, where different phases coexist in certain regions of the phase space. More notably, our boundary between high and low activity states depends predominantly on the relation between excitatory and inhibitory synaptic strength instead of the input rate.","tags":["area-v1","association field"],"title":"Phase space analysis of networks based on biologically realistic parameters","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"    related publication :  Laurent U Perrinet  (2019). An adaptive homeostatic algorithm for the unsupervised learning of visual features. Vision.  Preprint  PDF  Cite  Code  DOI       ","date":1279324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1279324800,"objectID":"93f60419c723dc647421eded04272c00","permalink":"https://laurentperrinet.github.io/publication/perrinet-10-shl/","publishdate":"2010-07-17T00:00:00Z","relpermalink":"/publication/perrinet-10-shl/","section":"publication","summary":"Neurons in the input layer of primary visual cortex in primates develop edge-like receptive fields. One approach to understanding the emergence of this response is to state that neural activity has to efficiently represent sensory data with respect to the statistics of natural scenes. Furthermore, it is believed that such an efficient coding is achieved using a competition across neurons so as to generate a sparse representation, that is, where a relatively small number of neurons are simultaneously active. Indeed, different models of sparse coding coupled with Hebbian learning and homeostasis have been proposed that successfully match the observed emergent response. However, the specific role of homeostasis in learning such sparse representations is still largely unknown. By quantitatively assessing the efficiency of the neural representation during learning, we derive a cooperative homeostasis mechanism which optimally tunes the competition between neurons within the sparse coding algorithm. We apply this homeostasis while learning small patches taken from natural images and compare its efficiency with state-of-the-art algorithms. Results show that while different sparse coding algorithms give similar coding results, the homeostasis provides an optimal balance for the representation of natural images within the population of neurons. Competition in sparse coding is optimized when it is fair: By contributing to optimize statistical competition across neurons, homeostasis is crucial in providing a more efficient solution to the emergence of independent components.","tags":["association field","coding decoding","eye movements","homeostasis","matching pursuit","motion-clouds","receptive field","sparse coding","sparse hebbian learning","statistics of natural images","unsupervised learning"],"title":"Role of homeostasis in learning sparse representations","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"FACETS Code Jam Workshop #4 We held a CodeJam 22nd-24th June 2010, in Marseille.\n     From the website:\n The goal of the FACETS CodeJam workshops is to catalyze open-source, collaborative software development in computational and systems neuroscience and neuroinformatics, by bringing together researchers, students and engineers to share ideas, present their work, and write code together. The general format of the workshops is to dedicate the mornings to invited and contributed talks, leaving the afternoons free for discussions and code sprints.  For the 4th FACETS CodeJam, the main theme of the meeting will be workflows: what are the best practices for combining different tools (simulators, analysis tools, visualization tools, databases etc.) to ensure the efficient and reproducible flow of data and information from experiment conception to publication and archiving?  (…)  The meeting organizers gratefully acknowledge the support of the European Union through the FACETS Project (grant no. IST-2005-15879), and the International Neuroinformatics Co-ordinating Facility (INCF). We also wish to express our great appreciation to the DyVA team at the Institut de Neurosciences Cognitives de la Méditerranée for providing us with a great location and much assistance.\n   http://neuralensemble.org/meetings/CodeJam4.html\n  http://neuralensemble.org/meetings/CJ4_Program_v2.pdf\n  FACETS code jam #4{.https}\n     ","date":1277164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1277164800,"objectID":"11549efd3f5fd0f1f2bdf427f9aa2852","permalink":"https://laurentperrinet.github.io/post/2010-06-22_codejam-nr4/","publishdate":"2010-06-22T00:00:00Z","relpermalink":"/post/2010-06-22_codejam-nr4/","section":"post","summary":"We held a CodeJam 22nd-24th June 2010, in Marseille.","tags":["events"],"title":"2010-06-22 : CodeJamNr4","type":"post"},{"authors":["Jens Kremkow","Laurent U Perrinet","Guillaume S Masson","Ad M Aertsen"],"categories":null,"content":"   ","date":1276992000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1276992000,"objectID":"2590699cb285c082e8a08c05d6fc292b","permalink":"https://laurentperrinet.github.io/publication/kremkow-10-jcns/","publishdate":"2010-06-20T00:00:00Z","relpermalink":"/publication/kremkow-10-jcns/","section":"publication","summary":"Neurons in the neocortex receive a large number of excitatory and inhibitory synaptic inputs. Excitation and inhibition dynamically balance each other, with inhibition lagging excitation by only few milliseconds. To characterize the functional consequences of such correlated excitation and inhibition, we studied models in which this correlation structure is induced by feedforward inhibition (FFI). Simple circuits show that an effective FFI changes the integrative behavior of neurons such that only synchronous inputs can elicit spikes, causing the responses to be sparse and precise. Further, effective FFI increases the selectivity for propagation of synchrony through a feedforward network, thereby increasing the stability to background activity. Last, we show that recurrent random networks with effective inhibition are more likely to exhibit dynamical network activity states as have been observed in vivo. Thus, when a feedforward signal path is embedded in such recurrent network, the stabilizing effect of effective inhibition creates an suitable substrate for signal propagation. In conclusion, correlated excitation and inhibition support the notion that synchronous spiking may be important for cortical processing.","tags":["center-surround interactions","coding decoding","pynn","sparse coding"],"title":"Functional consequences of correlated excitatory and inhibitory conductances in cortical networks","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"2008-10-08 : Deuxième conférence française de Neurosciences Computationnelles, “Neurocomp08” La deuxième conférence française de Neurosciences Computationnelles, “Neurocomp08”, s’est déroulée à la Faculté de Médecine de Marseille du 8 au 11 octobre 2008. Cette conférence, organisée par le groupe de travail Neurocomp, a permis de réunir les principaux acteurs français du domaine (francophones ou non). Le champ des Neurosciences Computationnelles porte sur l’étude des mécanismes de calcul qui sont à l’origine de nos capacités cognitives. Cette approche nécessite l’intégration constructive de nombreux domaines disciplinaires, du neurone au comportement, des sciences du vivant à la modélisation numérique. Avec ce colloque, nous avons offert un lieu d’échanges afin de favoriser des collaborations interdisciplinaires entre des équipes relevant des neurosciences, des sciences de l’information, de la physique statistique, de la robotique. Cette édition a également été l’occasion d’ouvrir le cadre à de nouveaux domaines (modèles pour l’imagerie, interfaces cerveau-machine,…) notamment grâce à des ateliers thématiques (une nouveauté dans cette édition). Certains des principaux enjeux du domaine ont été présentés par quatre conférenciers invités : Ad Aertsen (Freiburg, Allemagne), Gustavo Deco (Barcelone, Espagne), Gregor Schöner (Bochum, Allemagne), Andrew B. Schwartz (Pittsburgh, USA). Des interventions orale plus courtes et plus spécifiques étaient également au programme, sur la base d’une sélection du comité de lecture. Une cinquantaine de posters ont également été présentés au cours de ces journées. Le premier jour était consacré aux modèles de la cellule neurale, aux modèles des traitements visuels et corticaux, ainsi qu’aux modèles de réseaux de neurones bio-mimétiques. La seconde journée était consacrée aux interfaces cerveau-machine, à la dynamique des grands ensembles de neurones, à la plasticité fonctionnelle et aux interfaces neurales. Enfin, la journée de samedi était consacrée à des ateliers thématiques, l’un sur les interfaces cerveau-machine, l’autre sur la vision computationnnelle. Cette conférence a connu un beau succès de par l’affluence (200 personnes environ) et la qualité des interventions. Ce succès tient également au fort soutien financier et organisationnel qu’elle a obtenu de ses partenaires. Les organisateurs remercient le CNRS, la Société des neurosciences, le conseil régional de la région Provence Alpes Côte d’Azur, le conseil général des Bouches de Rhône, la mairie de Marseille, l’université de Provence, l’IFR “Sciences du cerveau et de la cognition”, l’INRIA, ainsi que la faculté de médecine de Marseille et l’université de la Méditerranée qui ont hébergé la conférence.\n Les actes de la conférence regroupant les 68 contributions sont disponibles sur le serveur HAL dédié.     ","date":1274918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1274918400,"objectID":"346849abafe43ea052d070058280fc89","permalink":"https://laurentperrinet.github.io/post/2008-10-08_neurocomp/","publishdate":"2010-05-27T00:00:00Z","relpermalink":"/post/2008-10-08_neurocomp/","section":"post","summary":"La deuxième conférence française de Neurosciences Computationnelles, Neurocomp08, s'est déroulée à la Faculté de Médecine de Marseille du 8 au 11 octobre 2008.","tags":["events","computational-neuroscience"],"title":"2010-05-27 : Neurocomp08","type":"post"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Diffraction monochromatique, spectre audiographique     Diffraction est une sculpture en suspension composée d’une multitude de plaques de matière transparente et réfléchissante. L’installation met en jeu notre perception de l’espace par des phénomènes de résonance et de réflection de la lumière. Chaque lieu d’exposition donne à expérimenter et à élaborer, in situ, de nouvelles formes. A Seconde Nature, Etienne Rey abordera la relation entre le volume et le son en prenant comme base de construction un spectre audio, en collaboration avec l’artiste sonore Mathias Delplanque. Live de Mathias Delplanque et rencontre autour de Diffraction, le Mercredi 14 avril 2010: A l’occasion de cette rencontre publique, quatre chercheurs spécialistes de l’architecture, de la perception, du son, et de la lumière exposeront depuis leurs domaines de recherches les processus engagés autour de Diffraction.` Farid Ameziane, Ecole Nationale Supérieure d’Architecture de Marseille Luminy (EAML), Directeur de l’InsARTis, Marseille Guillaume Bonello, Chargé de mission, POPsud, co/OAMP, Marseille Fabrice Mortessagne, Directeur du laboratoire de Physique de la Matière Condensée (LPMC), Nice-Sophia Antipolis Laurent Perrinet, Chercheur à l’Institut de Neurosciences Cognitives de Méditerranée, Equipe DyVA, Marseille Modératrice : Colette Tron, Fondatrice d’Alphabetville, Marseille Entrée libre \u0026amp; gratuite - 19h, durée 2h. Renseignements pratiques : Espace Sextius investi par Seconde Nature : 27bis rue du 11 novembre, 13100 Aix-en-Provence (!) visitez le site de Seconde Nature  notes de l’intervention de Laurent Perrinet  Qu’est-ce que voir? En perception, les neurones « parlent » tous en même temps par de brèves impulsions électrochimiques, générant un mélange de signaux, un bruit. Pourtant c’est par eux que nous pensons, voyons, sentons. Les ordinateurs sont différents, plus rapides. Ils sont construits avec pour modèle la grammaire humaine autour d’une unité centrale, car on imaginait la cognition sous cet angle à leur invention. Le bit est le quantum d’un algorithme mécanique (thèse de Church-Turing). Une théorie tranche par rapport à la précédente, proposée par «von Neumann» : beaucoup d’unités sont présentes dans le cerveau. Comparée à la chaîne logique du langage, dans cet algorithme, beaucoup d’autres chaînes et logiques se mêlent. Comment vont-elles « parler » entre elles ? Existe-t-il des algorithmes biologiques ?   Définir ce « langage », c’est comprendre comment une somme d’informations locales peut produire une perception globale. Comment en jouant avec les atomes du code, en les superposant, les « cassant » pour les mettre en résonance, les neurosciences et l’artiste questionnent le langage de notre pensée ? Quel est le code utilisé par les neurones pour communiquer (code neuronal ? existe-t-il un même vocabulaire au sens homomorphique ?). En pratique, on apprend par exemple la sélectivité à l’orientation. Les phénomènes d’orientation sont radicaux à la fin de l’expérience, « gelant » son évolution. Un lien évident avec l’installation Phytosphère d’Etienne Rey. L’information dans le cerveau se propage par diffusion, par diffraction (contamination des informations entre neurones pour occuper l’espace), en lien avec le travail sur la lumière d’Etienne Rey. L’image a besoin de 30 millisecondes pour se diffuser de l’œil vers l’arrière du crâne et 85 millisecondes pour produire un réflexe oculaire. Les neurosciences cherchent à savoir comment comprendre la globalité par l’émergence. Il y a donc une superposition d’états, comme dans la diffraction d’Etienne Rey. En perception, le mécanisme neuronal cherche à sortir de l’ambiguïté première quand il connaît une image : il superpose des particules élémentaires d’information, les diffuse pour les prendre toutes. Ce qui émerge est non linéaire. Le cerveau interfère ces particules, donc les met en compétition, en coopération (voir expérience plus haut avec les neurones rouges et bleus), dans une dynamique où ces particules se réorientent elles-mêmes. Elles créent des phénomènes d’organisation, se collent, deviennent plus lumineuses. La perception n’est donc pas séquentielle mais fluide et la sortie de l’ambiguité depuis l’image pixel vient de l’introduction de ces contraintes. Ainsi quand nous voyons un objet, nous le « capturons ». Quand nous sommes vus, nous cherchons à nous séparer de cette capture. Un problème classique est l’ambiguité du monde sensible. Une couleur que l’on ne voit pas va apparaître visuellement. L’inpainting créé une œuvre qui correspond à un mécanisme neuronal, cherchant à reproduire toujours une même structure. La mémoire iconique du monde extérieur va imprégner le cerveau, s’y figer. Tout le problème de la perception pour les neurosciences repose sur deux dialectiques. La première présente une analogie avec les images informatiques par pixels : ce serait en neurosciences une métaphore de la sensation pure. La seconde rappelle l’image vectorisée : pour s’extraire …","date":1271271600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1271271600,"objectID":"fa6ec1e78b05f18a3b0f83b838ae7ceb","permalink":"https://laurentperrinet.github.io/talk/2010-04-14-ondes-paralleles/","publishdate":"2010-04-14T19:00:00Z","relpermalink":"/talk/2010-04-14-ondes-paralleles/","section":"talk","summary":"En perception, les neurones « parlent » tous en même temps par de brèves impulsions électrochimiques, générant un mélange de signaux, un bruit. Pourtant c'est par eux que nous pensons, voyons, sentons. Les ordinateurs sont différents, plus rapides. Ils sont construits avec pour modèle la grammaire humaine autour d'une unité centrale, car on imaginait la cognition sous cet angle à leur invention. Le bit est le quantum d'un algorithme mécanique (thèse de Church-Turing). Une théorie tranche par rapport à la précédente, proposée par «von Neumann» : beaucoup d'unités sont présentes dans le cerveau. Comparée à la chaı̂ne logique du langage, dans cet algorithme, beaucoup d'autres chan̂es et logiques se mêlent. Comment vont-elles « parler » entre elles ? Existe-t-il des algorithmes biologiques ? ","tags":null,"title":"Diffraction monochromatique, spectre audiographique","type":"talk"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":" see this more recent talk @ UCL, London  ","date":1262908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262908800,"objectID":"2a62ff6216b24f758b5b2983b53eb822","permalink":"https://laurentperrinet.github.io/talk/2010-01-08-facets/","publishdate":"2010-01-08T00:00:00Z","relpermalink":"/talk/2010-01-08-facets/","section":"talk","summary":" see this more recent talk @ UCL, London  ","tags":["Bayesian model","center-surround interactions","dynamics","eye movements","motion detection","motion prediction"],"title":"Models of low-level vision: linking probabilistic models and neural masses","type":"talk"},{"authors":["Amarender Bogadhi","Anna Montagnini","Pascal Mamassian","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"ee628b17e38527b78a891e55524304f8","permalink":"https://laurentperrinet.github.io/publication/bogadhi-10-vss/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/bogadhi-10-vss/","section":"publication","summary":"","tags":["Bayesian model"],"title":"A recurrent Bayesian model of dynamic motion integration for smooth pursuit","type":"publication"},{"authors":["Emmanuel Daucé","Laurent U Perrinet"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"bfd3f7e2cf62354c33d072a05e6eb5bc","permalink":"https://laurentperrinet.github.io/publication/dauce-10/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/dauce-10/","section":"publication","summary":"Despite the long and fruitful history of neuroscience, a global, multi-level description of cardinal brain functions is still far from reach. Using analytical or numerical approaches, emphComputational Neuroscience aims at the emergence of such common principles by using concepts from Dynamical Systems and Information Theory. The aim of this Special Issue of the Journal of Physiology (Paris) is to reflect the latest advances in this field which has been presented during the NeuroComp08 conference that took place in October 2008 in Marseille (France). By highlighting a selection of works presented at the conference, we wish to illustrate the intrinsic diversity of this field of research but also the need of an unification effort that is becoming more and more necessary to understand the brain in its full complexity, from multiple levels of description to a multi-level understanding.","tags":["computational neuroscience"],"title":"Computational Neuroscience, from Multiple Levels to Multi-level","type":"publication"},{"authors":["Claudio Simoncini","Laurent U Perrinet","Anna Montagnini","Pascal Mamassian","Guillaume S Masson"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"4209f18f080e6a1faf5e9653c9b66752","permalink":"https://laurentperrinet.github.io/publication/simoncini-10-vss/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/simoncini-10-vss/","section":"publication","summary":"","tags":["eye movements","motion detection","motion-clouds","psychophysics"],"title":"Different pooling of motion information for perceptual speed discrimination and behavioral speed estimation","type":"publication"},{"authors":["Mina A Khoei","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":" Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013). Motion-based prediction explains the role of tracking in motion extrapolation. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up on the flash-lag effect:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2017). The flash-lag effect as a motion-based predictive shift. PLoS Computational Biology.  Preprint  PDF  Cite  Code  DOI     ","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"fd687563f57e9ea2c566a716a7b3ed61","permalink":"https://laurentperrinet.github.io/publication/khoei-10-tauc/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/khoei-10-tauc/","section":"publication","summary":"Based on  Laurent U Perrinet, Guillaume S Masson  (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation.  Preprint  PDF  Cite    see follow-up on motion extrapolation:  Mina A Khoei, Guillaume S Masson, Laurent U Perrinet  (2013).","tags":["Bayesian model","center-surround interactions","motion prediction","Ocular Following Response","tracking eye movements","Visual perception"],"title":"Dynamical emergence of a neural solution for motion integration","type":"publication"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"b7edc2252253d5c7852e9bad57b381f1","permalink":"https://laurentperrinet.github.io/publication/perrinet-10-areadne/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/perrinet-10-areadne/","section":"publication","summary":"","tags":["Bayesian model","center-surround interactions","eye movements","lateral connections","motion detection","motion prediction","visual perception"],"title":"Dynamical emergence of a neural solution for motion integration","type":"publication"},{"authors":["Nicole Voges","Laurent U Perrinet"],"categories":null,"content":" Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012). Complex dynamics in recurrent cortical networks based on spatially realistic connectivities. Frontiers in Computational Neuroscience.  PDF  Cite  DOI     ","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"7d31054e93fa240cd1174bb52db1c750","permalink":"https://laurentperrinet.github.io/publication/voges-10-neurocomp/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/voges-10-neurocomp/","section":"publication","summary":"We study cortical network dynamics for a more realistic network model. It represents, in terms of spatial scale, a large piece of cortex allowing for long-range connections, resulting in a rather sparse connectivity. We use two different types of conductance-based I\u0026F neurons as excitatory and inhibitory units, as well as specific connection probabilities. In order to remain computationally tractable, we reduce neuron density, modelling part of the missing internal input via external poissonian spike trains. Compared to previous studies, we observe significant changes in the dynamical phase space: Altered activity patterns require another regularity measure than the coefficient of variation. We identify two types of mixed states, where different phases coexist in certain regions of the phase space. More notably, our boundary between high and low activity states depends predominantly on the relation between excitatory and inhibitory synaptic strength instead of the input rate. Key words:Artificial neural networks, Data analysis, Simulation, Spiking neurons. This work is supported by EC IP project FP6-015879 (FACETS).","tags":["lateral connections"],"title":"Phase space analysis of networks based on biologically realistic parameters","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" See a followup in Perrinet et al, 2012  ","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"84890dad39fa2bbe18058579a212dc8d","permalink":"https://laurentperrinet.github.io/publication/perrinet-10-tauc/","publishdate":"2010-01-01T00:00:00Z","relpermalink":"/publication/perrinet-10-tauc/","section":"publication","summary":"Sensory informations such as visual images are inherently variable. We use probabilistic models to describe how the low-level visual system could describe superposed and ambiguous information. This allows to describe the interactions of neighboring populations of neurons as inference rules that dynamically build up the overall description of the visual scene. We focus here on temporal prediction, that is by the transport of information based on an estimate of local motion in the image.","tags":["Bayesian model","center-surround interactions","coding decoding","eye movements","motion detection"],"title":"Probabilistic models of the low-level visual system: the role of prediction in detecting motion","type":"publication"},{"authors":["Laurent U Perrinet","Alexandre Reynaud","Frédéric Chavane","Guillaume S Masson"],"categories":null,"content":" see this more recent poster @ VSS  ","date":1259539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1259539200,"objectID":"3b24d7e8c9c3569bb708d752f7b34616","permalink":"https://laurentperrinet.github.io/talk/2009-11-30-vss/","publishdate":"2009-11-30T00:00:00Z","relpermalink":"/talk/2009-11-30-vss/","section":"talk","summary":"Short presentation of a large moving pattern elicits an ocular following response that exhibits many of the properties attributed to low-level motion processing such as spatial and temporal integration, contrast gain control and divisive interaction between competing motions. Similar mechanisms have been demonstrated in V1 cortical activity in response to center-surround gratings patterns measured with real-time optical imaging in awake monkeys (see poster of Reynaud et al., VSS09). Based on a previously developed Bayesian framework, we have developed an optimal statistical decoder of such an observed cortical population activity as recorded by optical imaging. This model aims at characterizing the statistical dependance between early neuronal activity and ocular responses and its performance was analyzed by comparing this neuronal read-out and the actual motor responses on a trial-by-trial basis. First, we show that relative performance of the behavioral contrast response function is similar to the best estimate obtained from the neural activity. In particular, we show that the latency of ocular response increases with low contrast conditions as well as with noisier instances of the behavioral task as decoded by the model. Then, we investigate the temporal dynamics of both neuronal and motor responses and show how motion information as represented by the model is integrated in space to improve population decoding over time. Lastly, we explore how a surrounding velocity non congruous with the central excitation information shunts the ocular response and how it is topographically represented in the cortical activity. Acknowledgement: European integrated project FACETS IST-15879.","tags":null,"title":"Reading out the dynamics of lateral interactions in the primary visual cortex from VSD data","type":"talk"},{"authors":["Laurent U Perrinet","Thierry Viéville"],"categories":null,"content":"Nous parlerons de cette partie “mécanique” du cerveau animal ou humain qui permet de percevoir les mouvements et de … survivre au sein de l’environnement. On verra, par exemple, que notre cerveau peut-être plus rapide que nous, qu’il y a des solutions “stupides” qui marchent remarquablement bien pour sortir d’un labyrinthe, et qui si la grenouille sait gober une mouche bien mieux qu’un robot … elle n’est pas plus maligne ! Parce que ce qu’il ne faut pas confondre ici c’est la différence entre calculer et penser, entre intelligence et algorithmes. En comprenant cela, avec Alan Mathison Turing, le Gutenberg du XXème siècle, l’humanité a basculé des temps modernes à l’ère du numérique.\n (!) visitez le site d’interstices!  ","date":1259087400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1259087400,"objectID":"c4a915873bb826fc5d6107a312e1f1e1","permalink":"https://laurentperrinet.github.io/talk/2009-11-24-intelligence-mecanique/","publishdate":"2009-11-24T18:30:00Z","relpermalink":"/talk/2009-11-24-intelligence-mecanique/","section":"talk","summary":"Nous parlerons de cette partie \\\"mécanique\\\" du cerveau animal ou humain qui permet de percevoir les mouvements et de ... survivre au sein de l'environnement. On verra, par exemple, que notre cerveau peut-être plus rapide que nous, qu'il y a des solutions \\\"stupides\\\" qui marchent remarquablement bien pour sortir d'un labyrinthe, et qui si la grenouille sait gober une mouche bien mieux qu'un robot ... elle n'est pas plus maligne ! Parce que ce qu'il ne faut pas confondre ici c'est la différence entre calculer et penser, entre intelligence et algorithmes. En comprenant cela, avec Alan Mathison Turing, le Gutenberg du XXème siècle, l'humanité a basculé des temps modernes à l'ère du numérique. ","tags":null,"title":"Peut-on parler d'intelligence mécanique?","type":"talk"},{"authors":["Jens Kremkow","Laurent U Perrinet","Cyril Monier","Yves Fregnac","Guillaume S Masson","Ad M Aertsen"],"categories":null,"content":" see this subsequent paper in the Journal of Computational Neuroscience  ","date":1247875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1247875200,"objectID":"750e4d6767a99d36a559f028d662ef89","permalink":"https://laurentperrinet.github.io/talk/2009-07-18-kremkow-09-cnstalk/","publishdate":"2009-07-18T00:00:00Z","relpermalink":"/talk/2009-07-18-kremkow-09-cnstalk/","section":"talk","summary":" see this subsequent paper in the Journal of Computational Neuroscience  ","tags":null,"title":"Control of the temporal interplay between excitation and inhibition by the statistics of visual input","type":"talk"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":" see this more recent talk @ UCL, London  ","date":1238544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1238544000,"objectID":"e3581a0596c779480981f6f16287d7bf","permalink":"https://laurentperrinet.github.io/talk/2009-04-01-int/","publishdate":"2009-04-01T00:00:00Z","relpermalink":"/talk/2009-04-01-int/","section":"talk","summary":" Moving the eyes rapidly to track a visual object moving in a cluttered environment is an essential function. However, doing so rapidly and efficiently is constrained by a number of noise sources in the visual system and by the fact that information is collected locally before giving raise to a global signal. After reviewing some results made in the modeling of low-level sensory areas, I will expose a method to decode low-level neural information as describing visual information using a probabilistic representation. Decisions will therefore correspond to statistical inferences which are dynamically resolving the veridical speed of a moving object. We will illustrate this method by showing how ambiguous local information can be merged to give raise to a global response which resolves the aperture problem. Using this theoretical approach \\\"in computo\\\", we will illustrate how we may better understand results which are observed \\\"in vivo\\\" (optical imaging) as a neural code linking actively sensation and behavior.","tags":null,"title":"Decoding low-level neural information to track visual motion","type":"talk"},{"authors":["Laurent U Perrinet","Nicole Voges","Jens Kremkow","Guillaume S Masson"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"0af404da37a4044072479d12631dc57a","permalink":"https://laurentperrinet.github.io/publication/perrinet-09-cosyne/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/perrinet-09-cosyne/","section":"publication","summary":"Short presentation of a large moving pattern elicits an Ocular Following Response (OFR) that exhibits many of the properties attributed to low-level motion processing such as spatial and temporal integration, contrast gain control and divisive interaction between competing motions. Similar mechanisms have been demonstrated in V1 cortical activity in response to center-surround gratings patterns measured with real-time optical imaging in awake monkeys. More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the global spatial integration of motion from an intermediate map of possible local translation velocities: (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuli [Barthelemy06,Barthelemy07]. In fact, the machinery behind the visual perception of motion and the subsequent sensorimotor transformation is confronted to uncertainties which are efficiently resolved in the primate's visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theory [Weiss02] and we extended in the dynamical domain the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We proved that this model is successfully adapted to model the OFR for the different experiments [Perrinet07neurocomp], that is for different levels of noise with full field gratings, with disks of various sizes and also for the effect of a flickering surround. However, another emphad hoc inhibitory mechanism has to be added in this model to account for suppressive effects of the surround. We explore here an hypothesis where this could be understood as the effect of a recurrent prediction of information in the velocity map. In fact, in previous models, the integration step assumes independence of the local information while natural scenes are very predictable: Due to the rigidity and inertia of physical objects in visual space, neighboring local spatiotemporal information is redundant and one may introduce this empha priori knowledge of the statistics of the input in the ideal observer model. We implement this in a realistic model of a layer representing velocities in a map of cortical columns, where predictions are implemented by lateral interactions within the cortical area. First, raw velocities are estimated locally from images and are propagated to this area in a feed-forward manner. Using this velocity map, we progressively learn the dependance of local velocities in a second layer of the model. This algorithm is cyclic since the prediction is using the local velocities which are themselves using both the feed-forward input and the prediction: We control the convergence of this process by measuring results for different learning rate. Results show that this simple model is sufficient to disambiguate characteristic patterns such as the Barber-Pole illusion. Due to the recursive network which is modulating the velocity map, it also explains that the representation may exhibit some memory, such as when an object suddenly disappears or when presenting a dot followed by a line (line-motion illusion). Finally, we applied this model that was tuned over a set of natural scenes to gratings of increasing sizes. We observed first that the feed-forward response as tuned to neurophysiological data gave lower responses at higher eccentricities, and that this effect was greater for higher grating frequencies. Then, we observed that depending on the size of the disk and on its spatial frequency, the recurrent network of lateral interactions Lastly, we explore how a surrounding velocity non congruous with the central excitation information shunts the ocular response and how it is topographically represented in the cortical activity.","tags":["Bayesian model","center-surround interactions","eye movements","lateral connections","motion detection","visual perception"],"title":"Decoding center-surround interactions in population of neurons for the ocular following response","type":"publication"},{"authors":["Nicole Voges","Laurent U Perrinet"],"categories":null,"content":" Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012). Complex dynamics in recurrent cortical networks based on spatially realistic connectivities. Frontiers in Computational Neuroscience.  PDF  Cite  DOI     ","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"8527e135d8e8743af128baea9dcdd297","permalink":"https://laurentperrinet.github.io/publication/voges-09-cosyne/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/voges-09-cosyne/","section":"publication","summary":"Most studies of cor tical network dynamics are either based on purely random wiring or neighborhood couplings, e.g., [Kumar, Schrader, Aer tsen, Rotter, 2008, Neural Computation 20, 1--43]. Neuronal connections in the cor tex, however, show a complex spatial pattern composed of local and long-range connections, the latter featuring a so-called patchy projection pattern, i.e., spatially clustered synapses [Binzegger, Douglas, Martin, 2007, J. Neurosci. 27(45), 12242--12254]. The idea of our project is to provide and to analyze probabilistic network models that more adequately represent horizontal connectivity in the cor tex. In particular, we investigate the effect of specific projection patterns on the dynamical state space of cor tical networks. Assuming an enlarged spatial scale we employ a distance dependent connectivity that reflects the geometr y of dendrites and axons. We simulate the network dynamics using a neuronal network simulator NEST/PyNN. Our models are composed of conductance based integrate-and-fire neurons, representing fast spiking inhibitor y and regular spiking excitator y cells. In order to compare the dynamical state spaces of previous studies with our network models we consider the following connectivity assumptions: purely random or purely local couplings, a combination of local and distant synapses, and connectivity structures with patchy projections. Similar to previous studies, we also find different dynamical states depending on the input parameters: the external input rate and the numerical relation between excitator y and inhibitor y synaptic weights. These states, e.g., synchronous regular (SR) or asynchronous irregular (AI) firing, are characterized by measures like the mean firing rate, the correlation coefficient, the coefficient of variation and so for th. On top of identified biologically realistic background states (AI), stimuli are applied in order to analyze their stability. Comparing the results of our different network models we find that the parameter space necessar y to describe all possible dynamical states of a network is much more concentrated if local couplings are involved. The transition between different states is shifted (with respect to both input parameters) and shar pened in dependence of the relative amount of local couplings. Local couplings strongly enhance the mean firing rate, and lead to smaller values of the correlation coefficient. In terms of emergence of synchronous states, however, networks with local versus non-local or patchy versus random remote connections exhibit a higher probability of synchronized spiking. Concerning stability, preliminar y results indicate that again networks with local or patchy connections show a higher probability of changing from the AI to the SR state. We conclude that the combination of local and remote projections bears important consequences on the activity of network: The apparent differences we found for distinct connectivity assumptions in the dynamical state spaces suggest that network dynamics strongly depend on the connectivity structure. This effect might be even stronger with respect to the spatio-temporal spread of signal propagation. This work is suppor ted by EC IP project FP6-015879 (FACETS).","tags":["lateral connections"],"title":"Dynamical state spaces of cortical networks representing various horizontal connectivities","type":"publication"},{"authors":["Nicole Voges","Laurent U Perrinet"],"categories":null,"content":" Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012). Complex dynamics in recurrent cortical networks based on spatially realistic connectivities. Frontiers in Computational Neuroscience.  PDF  Cite  DOI     ","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"df78c8b991931379aaa22a1a44f9b3f8","permalink":"https://laurentperrinet.github.io/publication/voges-09-gns/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/voges-09-gns/","section":"publication","summary":"Most studies of cortical network dynamics are either based on purely random wiring or neighborhood couplings [1], focussing on a rather local scale. Neuronal connections in the cortex, however, show a more complex spatial pattern composed of local and long-range patchy connections [2,3] as shown in the figure: It represents a tracer injection (gray areas) in the GM of a flattened cortex (top view): Black dots indicate neuron positions, blue lines their patchy axonal ramifications, and red lines represent the local connections. Moreover, to include distant synapses, one has to enlarge the spatial scale from the typically assumed 1mm to 5mm side length. As it is our aim to analyze more realistic network models of the cortex we assume a distance dependent connectivity that reflects the geometry of dendritesand axons [3]. Here, we ask to what extent the assumption of specific geometric traits influences the resulting dynamical behavior of these networks. Analyzing various characteristic measures that describe spiking neurons (e.g., coefficient of variation, correlation coefficient), we compare the dynamical state spaces of different connectivity types: purely random or purely local couplings, a combination of local and distant synapses, and connectivity structures with patchy projections. On top of biologically realistic background states, a stimulus is applied in order to analyze their stabilities. As previous studies [1], we also find different dynamical states depending on the external input rate and the numerical relation between excitatory and inhibitory synaptic weights. Preliminary results indicate, however, that transitions between these states are much sharper in case of local or patchy couplings. This work is supported by EU Grant 15879 (FACETS). Thanks to Stefan Rotter who supervised the PhD project [3] this work is based on. Network dynamics are simulated with NEST/PyNN [4]. [1] A. Kumar, S. Schrader, A. Aertsen and S. Rotter, Neural Computation 20, 2008, 1-43. [2] T. Binzegger, R.J. Douglas and K.A.C. Martin, J. of Neurosci., 27(45), 2007, 12242-12254. [3] Voges N, Fakultaet fuer Biologie, Albert-Ludwigs-Universitaet Freiburg, 2007. [4] NEST. M.O. Gewaltig and M. Diesmann, Scholarpedia 2(4):1430.","tags":["lateral connections"],"title":"Dynamics of cortical networks including long-range patchy connections","type":"publication"},{"authors":["Jens Kremkow","Laurent U Perrinet","Guillaume S Masson","Ad M Aertsen"],"categories":null,"content":" see this subsequent paper in the Journal of Computational Neuroscience  ","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"f13bb4ecdf7a4aba73051b0bd5ddefc1","permalink":"https://laurentperrinet.github.io/publication/kremkow-09-gns/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/kremkow-09-gns/","section":"publication","summary":"Neurons receive a large number of excitatory and inhibitory synaptic inputs whose temporal interplay determines their spiking behavior. On average, excitation (Gexc) and inhibition (Ginh) balance each other, such that spikes are elicited by fluctuations [1]. In addition, it has been shown in vivo that Gexc and Ginh are correlated, with Ginh lagging Gexc only by few milliseconds (6ms), creating a small temporal integration window [2,3]. This correlation structure could be induced by feed-forward inhibition (FFI), which has been shown to be present at many sites in the central nervous system. To characterize the functional consequences of the FFI, we first modeled a simple circuit using spiking neurons with conductance based synapses and studied the effect on the single neuron integration. We then coupled many of such circuits to construct a feed-forward network (synfire chain [4,5]) and investigated the effect of FFI on signal propagation along such feed-forward network. We found that the small temporal integration window, induced by the FFI, changes the integrative properties of the neuron. Only transient stimuli could produce a response when the FFI was active whereas without FFI the neuron responded to both steady and transient stimuli. Due to the increase in selectivity to transient inputs, the conditions of signal propagation through the feed-forward network changed as well. Whereas synchronous inputs could reliable propagate, high asynchronous input rates, which are known to induce synfire activity [6], failed to do so. In summary, the FFI increased the stability of the synfire chain. Supported by DFG SFB 780, EU-15879-FACETS, BMBF 01GQ0420 to BCCN Freiburg [1] Kumar A., Schrader S., Aertsen A. and Rotter S. (2008). The high-conductance state of cortical networks. Neural Computation, 20(1):1--43. [2] Okun M. and Lampl I. (2008). Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities. Nat Neurosci, 11(5):535--7. [3] Baudot P., Levy M., Marre O., Monier C. and Frégnac (2008). [4] Abeles M. (1991). Corticonics: Neural circuits of the cerebral cortex. Cambridge, UK [5] Diesmann M., Gewaltig M-O and Aertsen A. (1999). Stable propagation of synchronous spiking in cortical neural networks. Nature, 402(6761):529--33. [6] Kumar A., Rotter S. and Aertsen A. (2008), Conditions for propagating synchronous spiking and asynchronous firing rates in a cortical network model. J Neurosci 28 (20), 5268--80. Preliminary Program","tags":["feed-forward_inhibition","large-scale_networks"],"title":"Functional consequences of correlated excitation and inhibition on single neuron integration and signal propagation through synfire chains","type":"publication"},{"authors":["Laurent U Perrinet","Alexandre Reynaud","Frédéric Chavane","Guillaume S Masson"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"6dbb076b213757c360836710b2fb0876","permalink":"https://laurentperrinet.github.io/publication/perrinet-09-vss/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/perrinet-09-vss/","section":"publication","summary":"Short presentation of a large moving pattern elicits an ocular following response that exhibits many of the properties attributed to low-level motion processing such as spatial and temporal integration, contrast gain control and divisive interaction between competing motions. Similar mechanisms have been demonstrated in V1 cortical activity in response to center-surround gratings patterns measured with real-time optical imaging in awake monkeys (see poster of Reynaud et al., VSS09). Based on a previously developed Bayesian framework, we have developed an optimal statistical decoder of such an observed cortical population activity as recorded by optical imaging. This model aims at characterizing the statistical dependence between early neuronal activity and ocular responses and its performance was analyzed by comparing this neuronal read-out and the actual motor responses on a trial-by-trial basis. First, we show that relative performance of the behavioral contrast response function is similar to the best estimate obtained from the neural activity. In particular, we show that the latency of ocular response increases with low contrast conditions as well as with noisier instances of the behavioral task as decoded by the model. Then, we investigate the temporal dynamics of both neuronal and motor responses and show how motion information as represented by the model is integrated in space to improve population decoding over time. Lastly, we explore how a surrounding velocity non congruous with the central excitation information shunts the ocular response and how it is topographically represented in the cortical activity. Acknowledgment: European integrated project FACETS IST-15879.","tags":["motion detection"],"title":"Inferring monkey ocular following responses from V1 population dynamics using a probabilistic model of motion integration","type":"publication"},{"authors":["Pierre Yger","Daniel Bruderle","Jochen Eppler","Jens Kremkow","Dejan Pecevski","Laurent U Perrinet","Michael Schmuker","Eilif Muller","Andrew P Davison"],"categories":null,"content":" see a follow-up:  Andrew P Davison, Daniel Bruderle, Jochen Eppler, Jens Kremkow, Eilif Muller, Dejan Pecevski, Laurent U Perrinet, Pierre Yger  (2008). PyNN: A Common Interface for Neuronal Network Simulators. Frontiers in Neuroinformatics.  Preprint  PDF  Cite  Project  DOI     ","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"f279066143657644f8c363ec4f1840c3","permalink":"https://laurentperrinet.github.io/publication/yger-09-gns/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/yger-09-gns/","section":"publication","summary":"NeuralEnsemble (http://neuralensemble.org) is a multilateral effort to coordinate and organise neuroscience software development efforts based around the Python programming language into a larger, meta-simulator software system. To this end, NeuralEnsemble hosts services for source code management and bug tracking (Subversion/Trac) for a number of open-source neuroscience tools, organizes an annual workshop devoted to collaborative software development in neuroscience, and manages a google-group discussion forum. Here, we present two NeuralEnsemble hosted projects: PyNN (http://neuralensemble.org/PyNN) is a package for simulator-independent specification of neuronal network models. You can write the code for a model once, using the PyNN API, and then run it without modification on any simulator that PyNN supports. Currently NEURON, NEST, PCSIM and a VLSI hardware implementation are fully supported. NeuroTools (http://neuralensemble.org/NeuroTools) is a set of tools to manage, store and analyse computational neuroscience simulations. It has been designed around PyNN, but can also be used for data from other simulation environments or even electrophysiological measurements. We will illustrate how the use of PyNN and NeuroTools ease the developmental process of models in computational neuroscience, enhancing collaboration between different groups and increasing the confidence in correctness of results. NeuralEnsemble efforts are supported by the European FACETS project (EU-IST-2005-15879)","tags":["pynn"],"title":"NeuralEnsemble: Towards a meta-environment for network modeling and data analysis","type":"publication"},{"authors":["Nicole Voges","Laurent U Perrinet"],"categories":null,"content":" Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012). Complex dynamics in recurrent cortical networks based on spatially realistic connectivities. Frontiers in Computational Neuroscience.  PDF  Cite  DOI     ","date":1224460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1224460800,"objectID":"797aa357e385bf8b9c03774629cfd63c","permalink":"https://laurentperrinet.github.io/publication/voges-08-neurocomp/","publishdate":"2008-10-20T00:00:00Z","relpermalink":"/publication/voges-08-neurocomp/","section":"publication","summary":"Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012).","tags":["lateral connections"],"title":"Analyzing cortical network dynamics with respect to different connectivity assumptions","type":"publication"},{"authors":["Jens Kremkow","Laurent U Perrinet","Ad M Aertsen","Guillaume S Masson"],"categories":null,"content":" see this subsequent paper in the Journal of Computational Neuroscience  ","date":1224460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1224460800,"objectID":"f5ed50bef60fa3752d01e7c5a3eadd58","permalink":"https://laurentperrinet.github.io/publication/kremkow-08-neurocomp/","publishdate":"2008-10-20T00:00:00Z","relpermalink":"/publication/kremkow-08-neurocomp/","section":"publication","summary":" see this subsequent paper in the Journal of Computational Neuroscience  ","tags":["feed-forward_inhibition","large-scale_networks"],"title":"Functional properties of feed-forward inhibition","type":"publication"},{"authors":["Laurent U Perrinet","Emmanuel Daucé"],"categories":null,"content":"","date":1224460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1224460800,"objectID":"b8c6195fd3292d27f6ea0ae3182c8ae3","permalink":"https://laurentperrinet.github.io/publication/perrinet-08-neurocomp/","publishdate":"2008-10-20T00:00:00Z","relpermalink":"/publication/perrinet-08-neurocomp/","section":"publication","summary":"","tags":["computational neuroscience"],"title":"Proceedings of the second french conference on Computational Neuroscience, Marseille","type":"publication"},{"authors":["Emmanuel Daucé","Laurent U Perrinet"],"categories":null,"content":"  Date: 27-28 May 2010\n  Location: Amphithéâtre Charve at the Saint-Charles’ University campus\n Métro : Line 1 et 2 (St Charles), a 5 minute walk from the railway station.  Map (Amphithéâtre Charve, University Main Entrance, etc.)  Metro, Bus and Tramway  Getting to Marseille from Airport    Computational Neuroscience emerges now as a major breakthrough in exploring cognitive functions. It brings together theoretical tools that elucidate fundamental mechanisms responsible for experimentally observed behaviour in the applied neurosciences. This is the second Computational Neuroscience Workshop organized by the “NeuroComp Marseille” network.\nIt will focus on latest advances on the understanding of how information may be represented in neural activity (1st day) and on computational models of learning, decision-making and motor control (2nd day). The workshop will bring together leading researchers in these areas of theoretical neuroscience. The meeting will consist of invited speakers with sufficient time to discuss and share ideas and data. All conferences were in English.\nProgram 27 May 2010 Neural representations for sensory information \u0026amp; the structure-function relation\n9h00-9h30\nReception and coffee\n9h30-10h00\nLaurent Perrinet Institut de Neurosciences Cognitives de la Méditerranée, CNRS and Université de la Méditerranée - Marseille «Presentation of the Workshop and Topic»\n10h00-11h00\nGabriel Peyré CNRS and Université Paris-Dauphine «Sparse Geometric Processing of Natural Images» In this talk, I will review recent works on the sparse representations of natural images. I will in particular focus on both the application of these emerging models to image processing problems, and their potential implication for the modeling of visual processing. Natural images exhibit a wide range of geometric regularities, such as curvilinear edges and oscillating textures. Adaptive image representations select bases from a dictionary of orthogonal or redundant frames that are parameterized by the geometry of the image. If the geometry is well estimated, the image is sparsely represented by only a few atoms in this dictionary. On an ingeniering level, these methods can be used to enhance the resolution of super-resolution inverse problems, and can also be used to perform texture synthesis. On a biological level, these mathematical representations share similarities with low level grouping processes that operate in areas V1 and V2 of the visual brain. We believe both processing and biological application of geometrical methods work hand in hand to design and analyze new cortical imaging methods.\n11h00-12h00\nJean Petitot Centre d’Analyse et de Mathématique Sociales, Ecole des Hautes Etudes en Sciences Sociales - Paris «Neurogeometry of visual perception» In relation with experimental data, we propose a geometric model of the functional architecture of the primary visual cortex (V1) explaining contour integration. The aim is to better understand the type of geometry algorithms implemented by this functional architecture. The contact structure of the 1-jet space of the curves in the plane, with its generalization to the roto-translation group, symplectifications, and sub-Riemannian geometry, are all neurophysiologically realized by long-range horizontal connections. Virtual structures, such as illusory contours of the Kanizsa type, can then be explained by this model.\n12h00\nLunch\n14h00-14h45\nPeggy Series Institute for Adaptive and Neural Computation, Edinburgh «Bayesian Priors in Perception and Decision Making» We’ll present two recent projects:\n The first project (with M. Chalk and A. R. Seitz) is an experimental investigation of the influence of expectations on the perception of simple stimuli. Using a simple task involving estimation and detection of motion random dots displays, we examined whether expectations can be developed quickly and implicitly and how they affect perception. We find that expectations lead to attractive biases such that stimuli appear as being more similar to the expected one than they really are, as well as visual hallucinations in the absence of a stimulus. We discuss our findings in terms of Bayesian Inference. In the second project (with A. Kalra and Q. Huys), we explore the concepts of optimism and pessimism in decision making. Optimism is usually assessed using questionnaires, such as the LOT-R. Here, using a very simple behavioral task, we show that optimism can be described in terms of a prior on expected future rewards. We examine the correlation between the shape of this prior for individual subjects and their scores on questionnaires, as well as with other measures of personality traits.  14h45-15h45\nHeiko Neumann (in collaboration with Florian Raudies) Inst. of Neural Information Processing, Ulm University Germany «Cortical mechanisms of transparent motion perception – a neural model» Transparent motion is perceived when multiple motions different in directions and/or speeds are presented in …","date":1223424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1223424000,"objectID":"9c813f8e3a9510958f0f51faf308d0f8","permalink":"https://laurentperrinet.github.io/post/2010-05-27_neurocomp-marseille-workshop/","publishdate":"2008-10-08T00:00:00Z","relpermalink":"/post/2010-05-27_neurocomp-marseille-workshop/","section":"post","summary":"Computational Neuroscience: From Representations to Behavior, the Second NeuroComp Marseille Workshop.","tags":["events","computational-neuroscience"],"title":"Computational Neuroscience: From Representations to Behavior","type":"post"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":" related publications @ FENS 2006, @ NeuroComp 2008 and @ AREADNE 2008 see this more recent talk @ UCL, London  ","date":1212278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212278400,"objectID":"05be8292fff5bca92f581e610b7bc35b","permalink":"https://laurentperrinet.github.io/talk/2008-06-01-ulm/","publishdate":"2008-06-01T00:00:00Z","relpermalink":"/talk/2008-06-01-ulm/","section":"talk","summary":"The machinery behind the visual perception of motion and the subsequent sensorimotor transformation, such as in Ocular Following Response (OFR), is confronted to uncertainties which are efficiently resolved in the primate's visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theory (Weiss et al., 2002) which we previously proved to be successfully adapted to model the OFR for different levels of noise with full field gratings or with disk of various sizes and the effect of a flickering surround (Perrinet and Masson, 2007). More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the global spatial integration of motion from an intermediate map of possible local translation velocities: (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuli (Barthelemy et al., 2006, 2007). Herein, we extended in the dynamical domain the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We present analytical results which show that the hypothesis of independence of local measures can describe the initial segment of spatial integration of motion signal. Within this framework, we successfully accounted for the dynamical contrast gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another inhibitory mechanism had to be added to account for suppressive effects of the surround. We explore here an hypothesis where this could be understood as the effect of a recurrent integration of information in the velocity map.  F. Barthelemy, L. U. Perrinet, E. Castet, and G. S. Masson. Dynamics of distributed 1D and 2D motion representations for short-latency ocular following. Vision Research, 48(4):501--22, feb 2007. doi: 10.1016/j.visres.2007.10.020.  F. V. Barthelemy, I. Vanzetta, and G. S. Masson. Behavioral receptive field for ocular following in humans: Dynamics of spatial summation and center-surround interactions. Journal of Neurophysiology, (95):3712--26, Mar 2006. doi: 10.1152/jn.00112.2006.  L. U. Perrinet and G. S. Masson. Modeling spatial integration in the ocular following response using a probabilistic framework. Journal of Physiology (Paris), 2007. doi: 10.1016/j.jphysparis.2007.10.011.  Y. Weiss, E. P. Simoncelli, and E. H. Adelson. Motion illusions as optimal percepts. Nature Neuroscience, 5(6):598--604, Jun 2002. doi: 10.1038/nn858.","tags":null,"title":"Decoding the population dynamics underlying ocular following response using a probabilistic framework","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1207008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1207008000,"objectID":"a95f91bf2cf5fb891d33649cfe94bf76","permalink":"https://laurentperrinet.github.io/talk/2008-04-01-incm/","publishdate":"2008-04-01T00:00:00Z","relpermalink":"/talk/2008-04-01-incm/","section":"talk","summary":"Computational Neuroscience is a synthetic, inter-disciplinary approach aiming at understanding cognition by analyzing the mechanisms underlying neural computations. We present in this seminar our attempt in modeling low-level vision by bridging different integration levels, from neural spiking activity to behavior. At the behavioral level, the Ocular Following Response recorded in the laboratory reveals how the brain may integrate local information (moving images on visual receptive fields) to produce a single behavioral response (the movement of the eye). Using a probabilistic representation, we provide a simple integrative mechanism that gives the ''ideal'' response to possibly noisy and ambiguous information, similarly to a Bayesian approach. This fits well the performance revealed by behavioral data and may act as a generic cortical ''module''. At the population level, these mechanisms may indeed be implemented for the coding of natural images and we will show the particular importance of spiking representations and lateral interactions for efficient and rapid responses. In particular, we will present an original unsupervised learning algorithm that we applied to a model of the primary visual cortex. Finally, at the neuronal level, I will present work done in the team showing how certain mechanisms at the level of the synapse and of the neuron are essential at the population level and how we may understand these mechanisms at the population level. This illustrates the importance of dynamical processes, distributed activity and recurrent connections to produce a cortical gain control mechanism. As a conclusion, this approach provides useful applications for image processing and possible valorization in future computer architectures. More generally, it proves that the use of a probabilistic representation is a particularly efficient method for bridging biological versus computational neuroscience and illustrates the advantage of such an interdisciplinary approach.","tags":null,"title":"From neural activity to behavior: computational neuroscience as a synthetic approach for understanding the neural code.","type":"talk"},{"authors":["Frédéric V Barthélemy","Laurent U Perrinet","Eric Castet","Guillaume S Masson"],"categories":null,"content":"","date":1203465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1203465600,"objectID":"19ae898b2c9e56a084d7b14f4de56a50","permalink":"https://laurentperrinet.github.io/publication/barthelemy-08/","publishdate":"2008-02-20T00:00:00Z","relpermalink":"/publication/barthelemy-08/","section":"publication","summary":"Integrating information is essential to measure the physical 2D motion of a surface from both ambiguous local 1D motion of its elongated edges and non-ambiguous 2D motion of its features such as corners or texture elements. The dynamics of this motion integration shows a complex time course as read from tracking eye movements: first, local 1D motion signals are extracted and pooled to initiate ocular responses, then 2D motion signals are integrated to adjust the tracking direction until it matches the surface motion direction. The nature of these 1D and 2D motion computations are still unclear. One hypothesis is that their different dynamics may be explained from different contrast sensitivities. To test this, we measured contrast-response functions of early, 1D-driven and late, 2D-driven components of ocular following responses to different motion stimuli: gratings, plaids and barberpoles. We found that contrast dynamics of 1D-driven responses are nearly identical across the different stimuli. On the contrary, late 2D-driven components with either plaids or barberpoles have similar latencies but different contrast dynamics. Temporal dynamics of both 1D- and 2D-driven responses demonstrates that the different contrast gains are set very early during the response time course. Running a Bayesian model of motion integration, we show that a large family of contrast-response functions can be predicted from the probability distributions of 1D and 2D motion signals for each stimulus and by the shape of the prior distribution. However, the pure delay (i.e. largely independent upon contrast) observed between 1D- and 2D-motion supports the fact that 1D and 2D probability distributions are computed independently. This two-pathway Bayesian model supports the idea that 1D and 2D mechanisms represent edges and features motion in parallel.","tags":["Bayesian model","eye movements","motion detection","motion-clouds"],"title":"Dynamics of distributed 1D and 2D motion representations for short-latency ocular following","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":" related publications @ FENS 2006, @ NeuroComp 2008 and @ AREADNE 2008  ","date":1201824000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1201824000,"objectID":"2fcff0a66fda90fd0fe4cfd8d2dcd05c","permalink":"https://laurentperrinet.github.io/talk/2008-02-01-toledo/","publishdate":"2008-02-01T00:00:00Z","relpermalink":"/talk/2008-02-01-toledo/","section":"talk","summary":" related publications @ FENS 2006, @ NeuroComp 2008 and @ AREADNE 2008  ","tags":null,"title":"Modeling of spikes, sparseness and adaptation in the primary visual cortex: applications to imaging","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"9462f242e261f60e9b9dc531e2ee68a0","permalink":"https://laurentperrinet.github.io/publication/perrinet-08-spie/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/perrinet-08-spie/","section":"publication","summary":"If modern computers are sometimes superior to cognition in some specialized tasks such as playing chess or browsing a large database, they can't beat the efficiency of biological vision for such simple tasks as recognizing a relative or following an object in a complex background. We present in this paper our attempt at outlining the dynamical, parallel and event-based representation for vision in the architecture of the central nervous system. We will illustrate this by showing that in a signal matching framework, a L/LN (linear/non-linear) cascade may efficiently transform a sensory signal into a neural spiking signal and we apply this framework to a model retina. However, this code gets redundant when using an over-complete basis as is necessary for modeling the primary visual cortex: we therefore optimize the efficiency cost by increasing the sparseness of the code. This is implemented by propagating and canceling redundant information using lateral interactions. We compare the efficiency of this representation in terms of compression as the reconstruction quality as a function of the coding length. This will correspond to a modification of the Matching Pursuit algorithm where the ArgMax function is optimized for competition, or Competition Optimized Matching Pursuit (COMP). We will particularly focus on bridging neuroscience and image processing and on the advantages of such an interdisciplinary approach.","tags":["association field","Biologically Inspired Computer vision","coding decoding","inhibition","matching pursuit","sparse coding","sparse hebbian learning","spike"],"title":"Adaptive Sparse Spike Coding : applications of Neuroscience to the compression of natural images","type":"publication"},{"authors":["Jens Kremkow","Laurent U Perrinet","Pierre Baudot","Manu Levy","Olivier Marre","Cyril Monier","Yves Fregnac","Guillaume S Masson","Ad M Aertsen"],"categories":null,"content":" see this subsequent paper in the Journal of Computational Neuroscience  ","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"6f891adfcd8747291a84855cd02d486e","permalink":"https://laurentperrinet.github.io/publication/kremkow-08-sfn/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/kremkow-08-sfn/","section":"publication","summary":"In the primary visual cortex (V1), single cell responses to simple visual stimuli (gratings) are usually dense but with a high trial-by-trial variability. In contrast, when exposed to full field natural scenes, the firing patterns of these neurons are sparse but highly reproducible over trials (Marre et al., 2005; Frégnac et al., 2006). It is still not understood how these two classes of stimuli can elicit these two distinct firing behaviours. A common model for simple-cell computation in layer 4 is the ``push-pull'' circuitry (Troyer et al. 1998). It accounts for the observed anti-phase behaviour between excitatory and inhibitory conductances in response to a drifting grating (Anderson et al., 2000; Monier et al., 2008), creating a wide temporal integration window during which excitation is integrated without the shunting or opponent effect of inhibition and allowed to elicit multiple spikes. This is in contrast to recent results from intracellular recordings in vivo during presentation of natural scenes (Baudot et al., 2013). Here the excitatory and inhibitory conductances were highly correlated, with inhibition lagging excitation only by few milliseconds (̃6 ms). This small lag creates a narrow temporal integration window such that only synchronized excitatory inputs can elicit a spike, similar to parallel observations in other cortical sensory areas (Wehr and Zador, 2003; Okun and Lampl, 2008). To investigate the cellular and network mechanisms underlying these two different correlation structures, we constructed a realistic model of the V1 network using spiking neurons with conductance based synapses. We calibrated our model to fit the irregular ongoing activity pattern as well as in vivo conductance measurements during drifting grating stimulation and then extracted predicted responses to natural scenes seen through eye-movements. Our simulations reproduced the above described experimental observation, together with anti-phase behaviour between excitation and inhibition during gratings and phase lagged activation during natural scenes. In conclusion, the same cortical network that shows dense and variable responses to gratings exhibits sparse and precise spiking to natural scenes. Work is under way to show to which extent this feature is specific for the feedforward vs recurrent nature of the modelled circuit.","tags":["feed-forward_inhibition","large-scale_networks"],"title":"Control of the temporal interplay between excitation and inhibition by the statistics of visual input: a V1 network modelling study","type":"publication"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"70ebc9151fa803247875baa3c1aa43c0","permalink":"https://laurentperrinet.github.io/publication/perrinet-08-areadne/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/perrinet-08-areadne/","section":"publication","summary":"The machinery behind the visual perception of motion and the subsequent sensorimotor transformation, such as in Ocular Following Response (OFR), is confronted to uncertainties which are efficiently resolved in the primate's visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theorỹWeiss02 which we previously proved to be successfully adapted to model the OFR for different levels of noise with full field gratings or with disk of various sizes and the effect of a flickering surround̃Perrinet07neurocomp. More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the global spatial integration of motion from an intermediate map of possible local translation velocities: (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuliB̃arthelemy06,Barthelemy07. Herein, we extended in the dynamical domain the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We present analytical results which show that the hypothesis of independence of local measures can describe the initial segment of spatial integration of motion signal. Within this framework, we successfully accounted for the dynamical contrast gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another inhibitory mechanism had to be added to account for suppressive effects of the surround. We explore here an hypothesis where this could be understood as the effect of a recurrent integration of information in the velocity map. F. Barthelemy, L. U. Perrinet, E. Castet, and G. S. Masson. Dynamics of distributed 1D and 2D motion representations for short-latency ocular following. Vision Research, 48(4):501--22, feb 2007. doi: 10.1016/j.visres.2007.10.020. F. V. Barthelemy, I. Vanzetta, and G. S. Masson. Behavioral receptive field for ocular following in humans: Dynamics of spatial summation and center-surround interactions. Journal of Neurophysiology, (95):3712--26, Mar 2006. doi: 10.1152/jn.00112.2006. L. U. Perrinet and G. S. Masson. Modeling spatial integration in the ocular following response using a probabilistic framework. Journal of Physiology (Paris), 2007. doi: 10.1016/j.jphysparis.2007.10.011. Y. Weiss, E. P. Simoncelli, and E. H. Adelson. Motion illusions as optimal percepts. Nature Neuroscience, 5(6):598--604, Jun 2002. doi: 10.1038/nn858. This work was supported by EC IP project FP6-015879, ''FACETS''.","tags":["Bayesian model","center-surround interactions","eye movements","lateral connections","motion detection","visual perception"],"title":"Decoding the population dynamics underlying ocular following response using a probabilistic framework","type":"publication"},{"authors":["Nicole Voges","Jens Kremkow","Laurent U Perrinet"],"categories":null,"content":" Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012). Complex dynamics in recurrent cortical networks based on spatially realistic connectivities. Frontiers in Computational Neuroscience.  PDF  Cite  DOI     ","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"2cdfa8cd9d92479d394886ccd2ae840b","permalink":"https://laurentperrinet.github.io/publication/voges-08/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/voges-08/","section":"publication","summary":"Based on  Nicole Voges, Laurent U Perrinet  (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris.  PDF  Cite  DOI    see follow-up :  Nicole Voges, Laurent U Perrinet  (2012).","tags":["lateral connections"],"title":"Dynamics of cortical networks based on patchy connectivity patterns","type":"publication"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"da8e2c2fd857c3f2e4a112b5a9db39fb","permalink":"https://laurentperrinet.github.io/publication/perrinet-08-a/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/perrinet-08-a/","section":"publication","summary":"","tags":["Bayesian model","center-surround interactions","eye movements","lateral connections","motion detection","visual perception"],"title":"Modeling spatial integration in the ocular following response to center-surround stimulation using a probabilistic framework","type":"publication"},{"authors":["Andrew P Davison","Daniel Bruderle","Jochen Eppler","Jens Kremkow","Eilif Muller","Dejan Pecevski","Laurent U Perrinet","Pierre Yger"],"categories":null,"content":"PyNN is a simulator-independent language for building neuronal network models using  Python.\n Web-site Source code 619 citations on Google Scholar (last updated 22/10/2021)  ","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"bd50966740ee38a62e92883c865e7b17","permalink":"https://laurentperrinet.github.io/publication/davison-08/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/davison-08/","section":"publication","summary":"Computational neuroscience has produced a diversity of software for simulations of networks of spiking neurons, with both negative and positive consequences. On the one hand, each simulator uses its own programming or configuration language, leading to considerable difficulty in porting models from one simulator to another. This impedes communication between investigators and makes it harder to reproduce and build on the work of others. On the other hand, simulation results can be cross-checked between different simulators, giving greater confidence in their correctness, and each simulator has different optimizations, so the most appropriate simulator can be chosen for a given modelling task. A common programming interface to multiple simulators would reduce or eliminate the problems of simulator diversity while retaining the benefits. PyNN is such an interface, making it possible to write a simulation script once, using the Python programming language, and run it without modification on any supported simulator (currently NEURON, NEST, PCSIM, Brian and the Heidelberg VLSI neuromorphic hardware). PyNN increases the productivity of neuronal network modelling by providing high-level abstraction, by promoting code sharing and reuse, and by providing a foundation for simulator-agnostic analysis, visualization and data-management tools. PyNN increases the reliability of modelling studies by making it much easier to check results on multiple simulators. PyNN is open-source software and is available from http://neuralensemble.org/PyNN.","tags":["computational neuroscience","pynn"],"title":"PyNN: A Common Interface for Neuronal Network Simulators","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"52f8c754a5aa992a2649244b91e12c2d","permalink":"https://laurentperrinet.github.io/publication/perrinet-08/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/perrinet-08/","section":"publication","summary":"","tags":["Bayesian model","coding decoding","sparse coding","sparse hebbian learning","spike"],"title":"What adaptive code for efficient spiking representations? A model for the formation of receptive fields of simple cells","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1196467200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1196467200,"objectID":"499c6c655be7c70209917aa38054b080","permalink":"https://laurentperrinet.github.io/talk/2007-12-01-rankprize/","publishdate":"2007-12-01T00:00:00Z","relpermalink":"/talk/2007-12-01-rankprize/","section":"talk","summary":"","tags":null,"title":"What efficient code for adaptive spiking representations?","type":"talk"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1188604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1188604800,"objectID":"312ecdcf96d9868859c14e4de1008a84","permalink":"https://laurentperrinet.github.io/talk/2007-09-01-mipm/","publishdate":"2007-09-01T00:00:00Z","relpermalink":"/talk/2007-09-01-mipm/","section":"talk","summary":"I will illustrate in this talk how computational neuroscience may inspire and be inspired by mathematical image processing. Focusing on efficiently representing natural images in the primary visual cortex, we derive an event-based adaptive algorithm inspired by statistical inference, Matching Pursuit and Hebbian learning. This algorithm allows to learn efficient \\\"edge-like\\\" receptive fields similarly to Independent Components Analysis. The correlation-based inhibition has been shown to be a necessary condition for the fomation of this type of receptive fields and shows the putative functional role of lateral propagation of information in cortical layers. I'll first present state-of-the-art neural algorithms for this task, the results of a detailed analysis of this Sparse Hebbian Learning algorithm and finally draw a comparison with similar strategies.","tags":null,"title":"Neural Codes for Adaptive Sparse Representations of Natural Images","type":"talk"},{"authors":["Jens Kremkow","Laurent U Perrinet","Arvind Kumar","Ad M Aertsen","Guillaume S Masson"],"categories":null,"content":" see this subsequent paper in the Journal of Computational Neuroscience  ","date":1183680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1183680000,"objectID":"3bd786fb0d499b5283b9f6891e6c48df","permalink":"https://laurentperrinet.github.io/publication/kremkow-07-cns/","publishdate":"2007-07-06T00:00:00Z","relpermalink":"/publication/kremkow-07-cns/","section":"publication","summary":" see this subsequent paper in the Journal of Computational Neuroscience  ","tags":["pynn"],"title":"Synchrony in thalamic inputs enhances propagation of activity through cortical layers","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"Dynamical Neural Networks (DyNNs) are a class of models for networks of neurons where particular focus is put on the role of time in the emergence of functional computational properties. The definition and study of these models involves the cooperation of a large range of scientific fields from statistical physics, probabilistic modelling, neuroscience and psychology to control theory. It focuses on the mechanisms that may be relevant for studying cognition by hypothesizing that information is distributed in the activity of the neurons in the system and that the timing helps in maintaining this information to lastly form decisions or actions. The system responds at best to the constraints of the outside world and learning strategies tune this internal dynamics to achieve optimal performance.\n","date":1174348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1174348800,"objectID":"d68be22e260f48e64153239a5d523d2e","permalink":"https://laurentperrinet.github.io/publication/perrinet-07/","publishdate":"2007-03-20T00:00:00Z","relpermalink":"/publication/perrinet-07/","section":"publication","summary":"The machinery behind the visual perception of motion and the subsequent sensori-motor transformation, such as in ocular following response (OFR), is confronted to uncertainties which are efficiently resolved in the primate's visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theory [Weiss, Y., Simoncelli, E.P., Adelson, E.H., 2002. Motion illusions as optimal percepts. Nature Neuroscience, 5(6), 598-604, doi:10.1038/nn858] which we previously proved to be successfully adapted to model the OFR for different levels of noise with full field gratings. More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the spatial integration of motion: (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuli [Barthélemy, F.V., Vanzetta, I., Masson, G.S., 2006. Behavioral receptive field for ocular following in humans: dynamics of spatial summation and center-surround interactions. Journal of Neurophysiology, (95), 3712-3726, doi:10.1152/jn.00112.2006]. Herein, we extended the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We present analytical results which show that the hypothesis of independence of local measures can describe the spatial integration of the motion signal. Within this framework, we successfully accounted for the contrast gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another inhibitory mechanism had to be added to account for suppressive effects of the surround.","tags":["Bayesian model","center-surround interactions","coding decoding","eye movements","lateral connections","matching pursuit","motion detection","motion-clouds","sparse coding","sparse hebbian learning","spike","visual perception"],"title":"Dynamical Neural Networks: modeling low-level vision at short latencies","type":"publication"},{"authors":["Bruno Cessac","Emmanuel Daucé","Laurent U Perrinet","Manuel Samuelides"],"categories":null,"content":"","date":1174348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1174348800,"objectID":"a0724d654fb2979e097e35470b7361ee","permalink":"https://laurentperrinet.github.io/publication/cessac-07-a/","publishdate":"2007-03-20T00:00:00Z","relpermalink":"/publication/cessac-07-a/","section":"publication","summary":"","tags":["computational neuroscience"],"title":"Introduction to Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision","type":"publication"},{"authors":["Bruno Cessac","Emmanuel Daucé","Laurent U Perrinet","Manuel Samuelides"],"categories":null,"content":"","date":1174348800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1174348800,"objectID":"988efe32fe93ebd9b5c58007fdb306e1","permalink":"https://laurentperrinet.github.io/publication/cessac-07/","publishdate":"2007-03-20T00:00:00Z","relpermalink":"/publication/cessac-07/","section":"publication","summary":"","tags":["computational neuroscience"],"title":"Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision","type":"publication"},{"authors":["Anna Montagnini","Pascal Mamassian","Laurent U Perrinet","Eric Castet","Guillaume S Masson"],"categories":null,"content":"   ","date":1169251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1169251200,"objectID":"9202f899972ade188baee0686389a3e4","permalink":"https://laurentperrinet.github.io/publication/montagnini-07/","publishdate":"2007-01-20T00:00:00Z","relpermalink":"/publication/montagnini-07/","section":"publication","summary":"The quality of the representation of an object's motion is limited by the noise in the sensory input as well as by an intrinsic ambiguity due to the spatial limitation of the visual motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that motion processing of extended ob jects is initially dominated by the local 1D motion cues orthogonal to the ob ject's edges, whereas 2D information takes progressively over and leads to the final correct representation of global motion. A Bayesian framework accounting for the sensory noise and general expectancies for ob ject velocities has proven successful in explaining several experimental findings concerning early motion processing [1, 2, 3]. However, a complete functional model, encompassing the dynamical evolution of object motion perception is still lacking. Here we outline several experimental observations concerning human smooth pursuit of moving ob jects and more particularly the time course of its initiation phase. In addition, we propose a recursive extension of the Bayesian model, motivated and constrained by our oculomotor data, to describe the dynamical integration of 1D and 2D motion information.","tags":["Bayesian model"],"title":"Bayesian modeling of dynamic motion integration","type":"publication"},{"authors":["Sylvain Fischer","Filip Šroubek","Laurent U Perrinet","Rafael Redondo","Gabriel Cristóbal"],"categories":null,"content":"This library defines the set of LogGabor kernels. These are generic edge-like filters at different scales, phases and orientations. The library develops a simple method to construct a simple multi-scale linear transform.\n Web-site Source code logGabor filters are used in numerous computer vision applications and reaches 177 citations on Google Scholar (last updated 22/10/2021).   Figure 1 Multiresolution schemes. (a) Schematic contours of the log-Gabor filters in the Fourier domain with 5 scales and 8 orientations (only the contours at 78% of the filter maximum are drawn). (b) The real part of the corresponding filters is drawn in the spatial domain. The two first scales are drawn at the bottom magnified by a factor of 4 for a better visualization. The different scales are arranged in rows and the orientations in columns. The low-pass filter is drawn in the upper-left part. (c) The corresponding imaginary parts of the filters are shown in the same arrangement. Note that the low-pass filter does not have imaginary part. Insets (b) and (c) show the final filters built through all the processes described in Section 2. (d) In the proposed scheme the elongation of log-Gabor wavelets increases with the number of orientations nt . Here the real parts (left column) and imaginary parts (right column) are drawn for the 3, 4, 6, 8, 10, 12 and 16 orientation schemes. (e) As a comparison orthogonal wavelet filters ‘Db4’ are shown. Horizontal, vertical and diagonal wavelets are arranged on columns (low-pass on top). (f) As a second comparison, steerable pyramid filters (Portilla et al., 2003) are shown. The arrangement over scales and orientations is the same as for the log-Gabor scheme.   ","date":1168646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1168646400,"objectID":"ad34d34463f51bb7a10c3ed814e22946","permalink":"https://laurentperrinet.github.io/publication/fischer-07-cv/","publishdate":"2007-01-13T00:00:00Z","relpermalink":"/publication/fischer-07-cv/","section":"publication","summary":"Meanwhile biorthogonal wavelets got a very popular image processing tool, alternative multiresolution transforms have been proposed for solving some of their drawbacks, namely the poor selectivity in orientation and the lack of translation invariance due to the aliasing between subbands. These transforms are generally overcomplete and consequently offer huge degrees of freedom in their design. At the same time their optimization get a challenging task. We proposed here a log-Gabor wavelet transform gathering the excellent mathematical properties of the Gabor functions with a carefully construction to maintain the properties of the filters and to permit exact reconstruction. Two major improvements are proposed: first the highest frequency bands are covered by narrowly localized oriented filters. And second, all the frequency bands including the highest and lowest frequencies are uniformly covered so as exact reconstruction is achieved using the same filters in both the direct and the inverse transforms (which means that the transform is self-invertible). The transform is optimized not only mathematically but it also follows as much as possible the knowledge on the receptive field of the simple cells of the Primary Visual Cortex (V1) of primates and on the statistics of natural images. Compared to the state of the art, the log-Gabor wavelets show excellent behavior in their ability to segregate the image information (e.g. the contrast edges) from incoherent Gaussian noise by hard thresholding and to code the image features through a reduced set of coefficients with large magnitude. Such characteristics make the transform a promising tool for general image processing tasks.","tags":["Biologically Inspired Computer vision","log-gabor","visual perception"],"title":"Self-Invertible 2D Log-Gabor Wavelets","type":"publication"},{"authors":["Anna Montagnini","Pascal Mamassian","Laurent U Perrinet","Eric Castet","Guillaume S Masson"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"2e0b6df605f6630767de8592454967cf","permalink":"https://laurentperrinet.github.io/publication/montagnini-07-a/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/montagnini-07-a/","section":"publication","summary":"When the visual information about an object's motion differs at the local level, the visuomotor system needs to integrate information across time to solve this ambiguity and converge to the final motion solution. For an oblique line moving horizontally, edge-related motion cues differ from terminator-related information, the latter being coherent with the line's global motion. We have previously shown that ocular tracking of this kind of stimuli is transiently biased toward the edge-orthogonal direction, before converging to the global motion direction. Here, we model the dynamic convergence to the global-motion solution as a recursive update of inferential knowledge in the velocity space. We assume that motion estimation is based on a prior distribution and two independent likelihood functions representing edge-related and terminator-related information. Importantly, the shape of the Bayesian functions is constrained by smooth-pursuit eye-movement data. Model predictions about the dynamic convergence to the correct motion solution are compared to human smooth-pursuit recordings when varying different stimulus parameters (speed, contrast).","tags":["motion detection"],"title":"Dynamic inference for motion tracking","type":"publication"},{"authors":["Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"   ","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"a9548d6acdf8e0a39c6e20b0a6ccf240","permalink":"https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/perrinet-07-neurocomp/","section":"publication","summary":"The machinery behind the visual perception of motion and the subsequent sensori-motor transformation, such as in Ocular Following Response (OFR), is confronted to uncertainties which are efficiently resolved in the primate's visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theory (Weiss et al., 2002) which we previously proved to be successfully adapted to model the OFR for different levels of noise with full field gratings (Perrinet et al., 2005). More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the spatial integration of motion : (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuli (Barthélemy et al., 2006). Herein, we extended the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We present analytical results which show that the hypothesis of independence of local measures can describe the integration of the spatial motion signal. Within this framework, we successfully accounted for the contrast gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another inhibitory mechanism had to be added to account for suppressive effects of the surround.","tags":["Bayesian model","center-surround interactions","eye movements","lateral connections","motion detection","visual perception"],"title":"Modeling spatial integration in the ocular following response using a probabilistic framework","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"c7c7a3a0103eb1b3ec0df07cfb696159","permalink":"https://laurentperrinet.github.io/publication/perrinet-07-mipm/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/perrinet-07-mipm/","section":"publication","summary":"I will illustrate in this talk how computational neuroscience may inspire and be inspired by mathematical image processing. Focusing on efficiently representing natural images in the primary visual cortex, we derive an event-based adaptive algorithm inspired by statistical inference, Matching Pursuit and Hebbian learning. This algorithm allows to learn efficient ëdge-likë̊eceptive fields similarly to Independent Components Analysis. The correlation-based inhibition has been shown to be a necessary condition for the fomation of this type of receptive fields and shows the putative functional role of lateral propagation of information in cortical layers. I'll first present state-of-the-art neural algorithms for this task, the results of a detailed analysis of this Sparse Hebbian Learning algorithm and finally draw a comparison with similar strategies.","tags":["unsupervised learning"],"title":"Neural Codes for Adaptive Sparse Representations of Natural Images","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"e02c19bb1e02aeed72895e7d6e442286","permalink":"https://laurentperrinet.github.io/publication/perrinet-07-cns/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/perrinet-07-cns/","section":"publication","summary":"We describe the theoretical formulation of a learning algorithm in a model of the primary visual cortex (V1) and present results of the efficiency of this algorithm by comparing it to the SparseNet algorithm [1]. As the SparseNet algorithm, it is based on a model of signal synthesis as a Linear Generative Model but differs in the efficiency criteria for the representation. This learning algorithm is in fact based on an efficiency criteria based on the Occam razor: for a similar quality, the shortest representation should be privileged. This inverse problem is NP-complete and we propose here a greedy solution which is based on the architecture and nature of neural computations [2]). It proposes that the supra-threshold neural activity progressively removes redundancies in the representation based on a correlation-based inhibition and provides a dynamical implementation close to the concept of neural assemblies from Hebb [3]). We present here results of simulation of this network with small natural images and compare it to the Sparsenet solution. Extending it to realistic images and to the NEST simulator http://www.nest-initiative.org/, we show that this learning algorithm based on the properties of neural computations produces adaptive and efficient representations in V1. 1. Olshausen B, Field DJ: Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Res 1997, 37:3311-3325. 2. Perrinet L: Feature detection using spikes: the greedy approach. J Physiol Paris 2004, 98(4--6):530-539. 3. Hebb DO: The organization of behavior. Wiley, New York; 1949.","tags":["area-v1"],"title":"On efficient sparse spike coding schemes for learning natural scenes in the primary visual cortex","type":"publication"},{"authors":["Andrew P Davison","Pierre Yger","Jens Kremkow","Laurent U Perrinet","Eilif Muller"],"categories":null,"content":" see a follow-up:  Andrew P Davison, Daniel Bruderle, Jochen Eppler, Jens Kremkow, Eilif Muller, Dejan Pecevski, Laurent U Perrinet, Pierre Yger  (2008). PyNN: A Common Interface for Neuronal Network Simulators. Frontiers in Neuroinformatics.  Preprint  PDF  Cite  Project  DOI     ","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"28067d09bdd855577ac84bab8489cd78","permalink":"https://laurentperrinet.github.io/publication/davison-07-cns/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/davison-07-cns/","section":"publication","summary":"Trends in programming language development and adoption point to Python as the high-level systems integration language of choice. Python leverages a vast developer-base external to the neuroscience community, and promises leaps in simulation complexity and maintainability to any neural simulator that adopts it. PyNN http://neuralensemble.org/PyNN strives to provide a uniform application programming interface (API) across neural simulators. Presently NEURON and NEST are supported, and support for other simulators and neuromorphic VLSI hardware is under development. With PyNN it is possible to write a simulation script once and run it without modification on any supported simulator. It is also possible to write a script that uses capabilities specific to a single simulator. While this sacrifices simulator-independence, it adds flexibility, and can be a useful step in porting models between simulators. The design goals of PyNN include allowing access to low-level details of a simulation where necessary, while providing the capability to model at a high level of abstraction, with concomitant gains in development speed and simulation maintainability. Another of our aims with PyNN is to increase the productivity of neuroscience modeling, by making it faster to develop models de novo, by promoting code sharing and reuse across simulator communities, and by making it much easier to debug, test and validate simulations by running them on more than one simulator. Modelers would then become free to devote more software development effort to innovation, building on the simulator core with new tools such as network topology databases, stimulus programming, analysis and visualization tools, and simulation accounting. The resulting, community-developed 'meta-simulator' system would then represent a powerful tool for overcoming the so-called complexity bottleneck that is presently a major roadblock for neural modeling.","tags":["pynn"],"title":"PyNN: towards a universal neural simulator API in Python","type":"publication"},{"authors":["Sylvain Fischer","Rafael Redondo","Laurent U Perrinet","Gabriel Cristóbal"],"categories":null,"content":" relies on log-Gabor filters:  Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal  (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision.  PDF  Cite  Code  DOI     Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).   ","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"417b4362d9153ab18454a4ab8dd41a0c","permalink":"https://laurentperrinet.github.io/publication/fischer-07/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/fischer-07/","section":"publication","summary":"relies on log-Gabor filters:  Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal  (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision.  PDF  Cite  Code  DOI     Schematic structure of the primary visual cortex implemented in the present study.","tags":["association field","Biologically Inspired Computer vision","log-gabor","motion-clouds"],"title":"Sparse Approximation of Images Inspired from the Functional Architecture of the Primary Visual Areas","type":"publication"},{"authors":["Anna Montagnini","Pascal Mamassian","Laurent U Perrinet","Guillaume S Masson"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"4d8ab5deb2e13721270514248d5beea7","permalink":"https://laurentperrinet.github.io/publication/montagnini-07-b/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/montagnini-07-b/","section":"publication","summary":"Perceptual and oculomotor data demonstrate that, when the visual information about an object's motion differs on the local (edge-related) and global levels, the local 1D motion cues dominate initially, whereas 2D information takes progressively over and leads to the final correct representation of global motion. Previous models have explained the initial errors (deviations from the global motion) in terms of best perceptual guess in the Bayesian sense. These models accounted for the intrinsic sensory noise of the image and general expectancies for object velocities. Here we propose a recursive extension of the Bayesian model, with the purpose of encompassing the whole dynamical evolution of motion processing, from the 1D cues to the correct global motion. Our model is motivated and constrained by smooth pursuit oculomotor data. Eye movements were recorded in 3 participants using the scleral search coil technique. Participants were asked to track either a single line (vertical or oblique) or a Gaussian blob moving horizontally. In our model, oculomotor data obtained with non ambiguous stimuli (e.g. with coherent local and global information, such as a Gaussian blob or a vertical line moving horizontally) are combined to constrain the initial likelihood and prior functions for the general, ambiguous case (e.g. a tilted line moving horizontally). The prior knowledge is then recursively updated by using the previous posterior probability as the current prior. The idea is that the recursive injection of posterior distribution boosts the spread of information about the object's shape, favoring the integration of 1D and 2D cues. In addition, a simple model of the sensory-oculomotor loop is taken into account, including transmission delays and the evolution of the retinal motion during pursuit. Preliminary results show substantial agreement between the model prediction and the oculomotor data.","tags":["Bayesian model"],"title":"Visual tracking of ambiguous moving objects: A recursive Bayesian model","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"6c266edfd0bb17e3549b6ce62756f687","permalink":"https://laurentperrinet.github.io/publication/perrinet-06-cns/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/perrinet-06-cns/","section":"publication","summary":"We describe the theoretical formulation of a learning algorithm in a model of the primary visual cortex (V1) and present results of the efficiency of this algorithm by comparing it to the SparseNet algorithm (Olshausen, 1996). As the SparseNet algorithm, it is based on a model of signal synthesis as a Linear Generative Model but differs in the efficiency criteria for the representation. This learning algorithm is in fact based on an efficiency criteria based on the Occam razor: for a similar quality, the shortest representation should be privilegied. This inverse problem is NP-complete and we propose here a greedy solution which is based on the architecture and nature of neural computations (Perrinet, 2006). We present here results of a simulation of this network of small natural images (available at https://github.com/bicv/SparseHebbianLearning) and compare it to the SparseNet solution. We show that this solution based on neural computations produces an adaptive algorithm for efficient representations in V1.","tags":["unsupervised learning"],"title":"An efficiency razor for model selection and adaptation in the primary visual cortex","type":"publication"},{"authors":["Anna Montagnini","Pascal Mamassian","Laurent U Perrinet","Eric Castet","Guillaume S Masson"],"categories":null,"content":" See a followup in Perrinet et al, 2012  ","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"53413d7ee2ebadfd6a679995d5787936","permalink":"https://laurentperrinet.github.io/publication/montagnini-06-neurocomp/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/montagnini-06-neurocomp/","section":"publication","summary":"The quality of the representation of an object's motion is limited by the noise in the sensory input as well as by an intrinsic ambiguity due to the spatial limitation of the visual motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that motion processing of extended ob jects is initially dominated by the local 1D motion cues orthogonal to the ob ject's edges, whereas 2D information takes progressively over and leads to the final correct represen- tation of global motion. A Bayesian framework accounting for the sensory noise and general expectancies for ob ject velocities has proven successful in explaining several experimental findings concerning early motion processing [1, 2, 3]. However, a complete functional model, encompassing the dynamical evolution of object motion perception is still lacking. Here we outline several experimental observations concerning human smooth pursuit of moving ob jects and more particu- larly the time course of its initiation phase. In addition, we propose a recursive extension of the Bayesian model, motivated and constrained by our oculomotor data, to describe the dynamical integration of 1D and 2D motion information.","tags":["Aperture problem","Bayesian model","Object motion","recursive inference","Smooth pursuit eye movement","Temporal evolution"],"title":"Bayesian modeling of dynamic motion integration","type":"publication"},{"authors":["Adrien Wohrer","Guillaume S Masson","Laurent U Perrinet","Pierre Kornprobst","Thierry Vieville"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"eaeb3213e228b72d987e02191a28d896","permalink":"https://laurentperrinet.github.io/publication/wohrer-06/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/wohrer-06/","section":"publication","summary":"","tags":["retina"],"title":"Contrast sensitivity adaptation in a virtual spiking retina and its adequation with mammalians retinas","type":"publication"},{"authors":["Laurent U Perrinet","Jens Kremkow"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"49ef7e009235b9bdc08560fc2baf3d19","permalink":"https://laurentperrinet.github.io/publication/perrinet-06-ciotat/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/perrinet-06-ciotat/","section":"publication","summary":"","tags":["gain control"],"title":"Dynamical contrast gain control mechanisms in a layer 2/3 model of the primary visual cortex","type":"publication"},{"authors":["Laurent U Perrinet","Jens Kremkow"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"cc3cf7fb2087f8e4e0f9a22608f06ce3","permalink":"https://laurentperrinet.github.io/publication/perrinet-06-fab/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/perrinet-06-fab/","section":"publication","summary":"Computations in a cortical column are characterized by the dynamical, event-based nature of neuronal signals and are structured by the layered and parallel structure of cortical areas. But they are also characterized by their efficiency in terms of rapidity and robustness. We propose and study here a model of information integration in the primary visual cortex (V1) thanks to the parallel and interconnected network of similar cortical columns. In particular, we focus on the dynamics of contrast gain control mechanisms as a function of the distribution of information relevance in a small population of cortical columns. This cortical area is modeled as a collection of similar cortical columns which receive input and are linked according to a specific connectivity pattern which is relevant to this area. These columns are simulated using the sc Nest simulator Morrison04 using conductance-based Integrate-and-Fire neurons and consist vertically in 3 different layers. The architecture was inspired by neuro-physiological observations on the influence of neighboring activities on pyramidal cells activity and correlates with the lateral flow of information observed in the primary visual cortex, notably in optical imaging experiments Jancke04, and is similar in its final implementation to local micro-circuitry of the cortical column presented by Grossberg05.  They show prototypical spontaneous dynamical behavior to different levels of noise which are relevant to the generic modeling of biological cortical columns Kremkow05. In the future, the connectivity will be derived from an algorithm that was used for modeling the transient spiking response of a layer of neurons to a flashed image and which was based on the Matching Pursuit algorithm Perrinet04. The visual input is first transmitted from the Lateral Geniculate Nucleus (LGN) using the model of Gazeres98. It transforms the image flow into a stream of spikes with contrast gain control mechanisms specific to the retina and the LGN. This spiking activity converges to the pyramidal cells of layer 2/3 thanks to the specification of receptive fields in layer 4 providing a preference for oriented local contrasts in the spatio-temporal visual flow. In particular, we use in these experiments visual input organized in a center-surround spatial pattern which was optimized in size to maximize the response of a column in the center and to the modulation of this response by the surround (bipartite stimulus). This class of stimuli provide different levels of input activation and of visual ambiguity in the visual space which were present in the spatio-temporal correlations in the input spike flow optimized to the resolution of cortical columns in the visual space. It thus provides a method to reveal the dynamics of information integration and particularly of contrast gain control which are characteristic to the function of V1.","tags":["gain control"],"title":"Dynamical contrast gain control mechanisms in a layer 2/3 model of the primary visual cortex","type":"publication"},{"authors":["Laurent U Perrinet","Jens Kremkow","Frédéric V Barthélemy","Guillaume S Masson","Frédéric Chavane"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"fed23a448e1ceae6d28530c9793cbb59","permalink":"https://laurentperrinet.github.io/publication/perrinet-06-fens/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/perrinet-06-fens/","section":"publication","summary":"","tags":["motion detection"],"title":"Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework","type":"publication"},{"authors":["Laurent U Perrinet","Frédéric V Barthélemy","Guillaume S Masson"],"categories":null,"content":" See a followup in Perrinet et al, 2012  ","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"4bcd11caab6d06c3eebc10ad68208f6d","permalink":"https://laurentperrinet.github.io/publication/perrinet-06-neurocomp/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/publication/perrinet-06-neurocomp/","section":"publication","summary":"The quality of the representation of an object's motion is limited by the noise in the sensory input as well as by an intrinsic ambiguity due to the spatial limitation of the visual motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that motion processing of extended ob jects is initially dominated by the local 1D motion cues orthogonal to the ob ject's edges, whereas 2D information takes progressively over and leads to the final correct representation of global motion. A Bayesian framework accounting for the sensory noise and general expectancies for ob ject velocities has proven successful in explaining several experimental findings concerning early motion processing [1, 2, 3]. However, a complete functional model, encompassing the dynamical evolution of object motion perception is still lacking. Here we outline several experimental observations concerning human smooth pursuit of moving ob jects and more particularly the time course of its initiation phase. In addition, we propose a recursive extension of the Bayesian model, motivated and constrained by our oculomotor data, to describe the dynamical integration of 1D and 2D motion information.","tags":["aperture problem","Bayesian model","eye movements","matching pursuit","motion detection"],"title":"Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework","type":"publication"},{"authors":["Laurent U Perrinet","Frédéric V Barthélemy","Guillaume S Masson"],"categories":null,"content":" related publication @ SPIE 2008 See a followup in Perrinet et al, 2012  ","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"cc3beaa71ce39c6ac9c5f55cc7f3458c","permalink":"https://laurentperrinet.github.io/talk/2006-01-01-neurocomp/","publishdate":"2006-01-01T00:00:00Z","relpermalink":"/talk/2006-01-01-neurocomp/","section":"talk","summary":"The quality of the representation of an object's motion is limited by the noise in the sensory input as well as by an intrinsic ambiguity due to the spatial limitation of the visual motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that motion processing of extended ob jects is initially dominated by the local 1D motion cues orthogonal to the ob ject's edges, whereas 2D information takes progressively over and leads to the final correct representation of global motion. A Bayesian framework accounting for the sensory noise and general expectancies for ob ject velocities has proven successful in explaining several experimental findings concerning early motion processing [1, 2, 3]. However, a complete functional model, encompassing the dynamical evolution of object motion perception is still lacking. Here we outline several experimental observations concerning human smooth pursuit of moving ob jects and more particularly the time course of its initiation phase. In addition, we propose a recursive extension of the Bayesian model, motivated and constrained by our oculomotor data, to describe the dynamical integration of 1D and 2D motion information.","tags":["Aperture problem","Bayesian model","Object motion","recursive inference","Smooth pursuit eye movement","Temporal evolution."],"title":"Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework","type":"talk"},{"authors":["Rafael Redondo","Sylvain Fischer","Laurent U Perrinet","Gabriel Cristóbal"],"categories":null,"content":" relies on log-Gabor filters:  Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal  (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision.  PDF  Cite  Code  DOI     Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).   ","date":1124496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1124496000,"objectID":"d8f4f222ced21e4f5119f22711160535","permalink":"https://laurentperrinet.github.io/publication/redondo-05/","publishdate":"2005-08-20T00:00:00Z","relpermalink":"/publication/redondo-05/","section":"publication","summary":"We present a biologically plausible model of simple cortical cells as 1) a linear transform representing edges and 2) a non-linear iterative stage of inhibition and facilitation between neighboring coefficients. The linear transform is a complex log-Gabor wavelet transform which is overcomplete (i.e. there are more coefficients than pixels in the image) and has exact reconstruction. The inhibition consists in diminishing down the coefficients which are not at a local-maxima along the direction normal to the edge filter orientation, whereas the facilitation enhances the collinear and co-aligned local-maximum coefficients. At each iteration and after the inhibition and facilitation stages, the reconstructed error is subtracted in the transform domain for keeping an exact reconstruction. Such process concentrates the signal energy on a few coefficients situated along the edges of the objects, yielding a sparse representation. The rationale for such procedure is: (1) th e overcompleteness offers flexibility for activity reassignment; (2) images can be coded by sparse Gabor coefficients located on object edges; (3) image contours produce aligned and collinear local-maxima in the transform domain; (4) the inhibition/facilitation processes are able to extract the contours. The sparse Gabor coefficients are mostly connected each other and located along object contours. Such layout makes chain coding suitable for compression purposes. Specially adapted to Gabor wavelets features, our chain coding represents every chain by its end-points (head and tail) and the elementary movements necessary to walk along the chain from head to tail. Moreover it predicts the module and phase of each Gabor coefficient according to the previous chain coefficient. As a result, redundancy of the transform domain is further reduced. Used for compression, the scheme limits particularly the high-frequency artifacts. The model performs also efficiently in tasks the Human Visual System is supposed to deal with, as for instance edge extraction and image denoising.","tags":["log-gabor"],"title":"Modeling of simple cells through a sparse overcomplete gabor wavelet representation based on local inhibition and facilitation","type":"publication"},{"authors":["Sylvain Fischer","Rafael Redondo","Laurent U Perrinet","Gabriel Cristóbal"],"categories":null,"content":" relies on log-Gabor filters:  Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal  (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision.  PDF  Cite  Code  DOI     Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).   ","date":1120003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1120003200,"objectID":"9c1e343bed181e9a2364a439e968a9ec","permalink":"https://laurentperrinet.github.io/publication/fischer-05-a/","publishdate":"2005-06-29T00:00:00Z","relpermalink":"/publication/fischer-05-a/","section":"publication","summary":"Efficient sparse coding of overcomplete transforms remains still anopen problem. Different methods have been proposed in theliterature, but most of them are limited by a heavy computationalcost and by difficulties to find the optimal solutions. We proposehere an algorithm suitable for Gabor wavelets and based onbiological models. It is composed by local operations betweenneighboring transform coefficients and achieves a sparserepresentation with a relatively low computational cost. Used with achain coder, this sparse Gabor wavelet transform is suitable forimage compression but is also of interest also for otherapplications, in particular for edge and contour extraction andimage denoising.","tags":["area-v1","Biologically Inspired Computer vision","log-gabor","receptive field","sparse coding"],"title":"Sparse Gabor wavelets by local operations","type":"publication"},{"authors":["Laurent U Perrinet","Frédéric V Barthélemy","Eric Castet","Guillaume S Masson"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"7074c21e8e21d730f2b0e246352f73ff","permalink":"https://laurentperrinet.github.io/publication/perrinet-05-a/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/perrinet-05-a/","section":"publication","summary":"The integration of information is essential to measure the exact 2D motion of a surface from both local ambiguous 1D motion produced by elongated edges and local non-ambiguous 2D motion from features such as corners, end-points or texture elements. The dynamics of this motion integration shows a complex time course which can be read from tracking eye movements: local 1D motion signals are extracted first and then pooled to initiate the ocular responses before that 2D motion signals are taken into account to refine the tracking direction until it matches the surface motion direction. The nature of these 1D and 2D motion computations is still unclear. Previously, we have shown that the late, 2D-driven response components to either plaids or barber-poles have very similar latencies over a large range of contrast, suggesting a shared mechanism. However, they showed different contrast response functions with these different motion stimuli, suggesting different motion processing. We designed a two-pathways Bayesian model of motion integration and showed that this family of contrast response functions can be predicted from the probability distributions of 1D and 2D motion signals for each type of stimulus. Indeed, this formulation may explain contrast response functions that could not be explained by a simple bayesian model (Weiss et al., 2002 em Nature Neuroscience bf 5 , 598--604) and gives a quantitative argument to study how local information with different relative ambiguities values may be pooled to provide an integrated response of the system. Finally, we formulate how different spatial information may be pooled and we draw the analogy of this method with methods using the partial derivative equations. This simple model correctly explains some non-linear interactions between neighboring neurons selective to motion direction which are observed in short-latency ocular following and neuro-physiological data.","tags":["Bayesian model","motion detection"],"title":"Dynamics of motion representation in short-latency ocular following: A two-pathways Bayesian model","type":"publication"},{"authors":["Sylvain Fischer","Rafael Redondo","Laurent U Perrinet","Gabriel Cristóbal"],"categories":null,"content":" relies on log-Gabor filters:  Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal  (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision.  PDF  Cite  Code  DOI     Schematic structure of the primary visual cortex implemented in the present study. Simple cortical cells are modeled through log-Gabor functions. They are organized in pairs in quadrature of phase (dark-gray circles). For each position the set of different orientations compose a pinwheel (large light-gray circles). The retinotopic organization induces that adjacent spatial positions are arranged in adjacent pinwheels. Inhibition interactions occur towards the closest adjacent positions which are in the direc-tions perpendicular to the cell preferred orientation and toward adjacent orientations (light-red connections). Facilitation occurs to-wards co-aligned cells up to a larger distance (dark-blue connections).   ","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"b2177ba9dc3caceb3228de1684b1d6c2","permalink":"https://laurentperrinet.github.io/publication/fischer-05/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/fischer-05/","section":"publication","summary":"Low-level perceptual computations may be understood in terms of efficient codes (Simoncelli and Olshausen, 2001, Annual Review of Neuroscience 24 1193-216). Following this argument, we explore models of representation for natural static images as a way to understand the processing of information in the primary visual cortex. This representation is here based on a generative linear model of the synthesis of images using an over-complete multi-resolution dictionary of edges. This transform is implemented using log-Gabor filters and permits an exact reconstruction of any image. However, this linear representation is redundant and since to any image may correspond different representations, we explore more efficient representations of the image. The problem is stated as an ill-posed inverse problem and we compare first different known strategies by computing the efficiency of the solutions given by Matching Pursuit (Perrinet, 2004, IEEE Trans. Neural Networks 15 1164-75) and sparse edge coding (Fischer, in press, Trans. Image Processing) with classical representation methods such as JPEG. This comparison allows us to provide a synthesized approach using a probabilistic representation which would progressively construct the neural representation by using lateral cooperations. We propose an algorithm which dynamically diffuses information to correlated filters so as to yield a progressively disambiguated representation. This approach takes advantage of the computational properties of spiking neurons such as Integrate-and-Fire neurons and provides an efficient yet simple model for the representation of natural images. This representation is directly linked with the edge content of natural images and we show applications of this method to edge extraction, denoising and compression. We also show that this dynamical approach fits with neuro-physiological observations and may explain the non-linear interactions between neighboring neurons which may be observed in the cortex.","tags":["Biologically Inspired Computer vision","matching pursuit"],"title":"Efficient representation of natural images using local cooperation","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"badedf0ad438e8142475d55b696d1afa","permalink":"https://laurentperrinet.github.io/publication/perrinet-05/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/publication/perrinet-05/","section":"publication","summary":"","tags":["spike"],"title":"Efficient Source Detection Using Integrate-and-Fire Neurons","type":"publication"},{"authors":["Laurent U Perrinet","Manuel Samuelides","Simon Thorpe"],"categories":null,"content":"     Progressive reconstruction of a static image using spikes in a multi-scale oriented representation. \n","date":1095638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1095638400,"objectID":"0d6343369a92cd4495623b7f2fc131bf","permalink":"https://laurentperrinet.github.io/publication/perrinet-03-ieee/","publishdate":"2004-09-20T00:00:00Z","relpermalink":"/publication/perrinet-03-ieee/","section":"publication","summary":"To understand possible strategies of temporal spike coding in the central nervous system, we study functional neuromimetic models of visual processing for static images. We will first present the retinal model which was introduced by Van Rullen and Thorpe [1] and which represents the multiscale contrast values of the image using an orthonormal wavelet transform. These analog values activate a set of spiking neurons which each fire once to produce an asynchronous wave of spikes. According to this model, the image may be progressively reconstructed from this spike wave thanks to regularities in the statistics of the coefficients determined with natural images. Here, we study mathematically how the quality of information transmission carried by this temporal representation varies over time. In particular, we study how these regularities can be used to optimize information transmission by using a form of temporal cooperation of neurons to code analog values. The original model used wavelet transforms that are close to orthogonal. However, the selectivity of realistic neurons overlap, and we propose an extension of the previous model by adding a spatial cooperation between filters. This model extends the previous scheme for arbitrary -and possibly non-orthogonal- representations of features in the images. In particular, we compared the performance of increasingly over-complete representations in the retina. Results show that this algorithm provides an efficient spike coding strategy for low-level visual processing which may adapt to the complexity of the visual input.","tags":["association field","Biologically Inspired Computer vision","matching pursuit","sparse coding","sparse hebbian learning","spike","statistics of natural images"],"title":"Coding static natural images using spiking event times: do neurons cooperate?","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1090281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1090281600,"objectID":"47b86c7011ae792a854ccb80949c7b57","permalink":"https://laurentperrinet.github.io/publication/perrinet-04-tauc/","publishdate":"2004-07-20T00:00:00Z","relpermalink":"/publication/perrinet-04-tauc/","section":"publication","summary":"A goal of low-level neural processes is to build an efficient code extracting the relevant information from the sensory input. It is believed that this is implemented in cortical areas by elementary inferential computations dynamically extracting the most likely parameters corresponding to the sensory signal. We explore here a neuro-mimetic feed-forward model of the primary visual area (V1) solving this problem in the case where the signal may be described by a robust linear generative model. This model uses an over-complete dictionary of primitives which provides a distributed probabilistic representation of input features. Relying on an efficiency criterion, we derive an algorithm as an approximate solution which uses incremental greedy inference processes. This algorithm is similar to 'Matching Pursuit' and mimics the parallel architecture of neural computations. We propose here a simple implementation using a network of spiking integrate-and-fire neurons which communicate using lateral interactions. Numerical simulations show that this Sparse Spike Coding strategy provides an efficient model for representing visual data from a set of natural images. Even though it is simplistic, this transformation of spatial data into a spatio-temporal pattern of binary events provides an accurate description of some complex neural patterns observed in the spiking activity of biological neural networks.","tags":["Bayesian model","coding decoding","matching pursuit","sparse coding","sparse hebbian learning","spike"],"title":"Feature detection using spikes : the greedy approach","type":"publication"},{"authors":["Laurent U Perrinet","Manuel Samuelides","Simon Thorpe"],"categories":null,"content":"","date":1079740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1079740800,"objectID":"05e3afe1ae15d35c01d9337299e21efd","permalink":"https://laurentperrinet.github.io/publication/perrinet-02-sparse/","publishdate":"2004-03-20T00:00:00Z","relpermalink":"/publication/perrinet-02-sparse/","section":"publication","summary":"","tags":["association field","matching pursuit","sparse coding"],"title":"Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":1074556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1074556800,"objectID":"ed4b6c8e5fb952d54a4735a2c8ecd8dd","permalink":"https://laurentperrinet.github.io/publication/perrinet-04/","publishdate":"2004-01-20T00:00:00Z","relpermalink":"/publication/perrinet-04/","section":"publication","summary":"To understand possible strategies of temporal spike coding in the central nervous system, we study functional neuromimetic models of visual processing for static images. We will first present the retinal model which was introduced by Van Rullen and Thorpe [1] and which represents the multiscale contrast values of the image using an orthonormal wavelet transform. These analog values activate a set of spiking neurons which each fire once to produce an asynchronous wave of spikes. According to this model, the image may be progressively reconstructed from this spike wave thanks to regularities in the statistics of the coefficients determined with natural images. Here, we study mathematically how the quality of information transmission carried by this temporal representation varies over time. In particular, we study how these regularities can be used to optimize information transmission by using a form of temporal cooperation of neurons to code analog values. The original model used wavelet transforms that are close to orthogonal. However, the selectivity of realistic neurons overlap, and we propose an extension of the previous model by adding a spatial cooperation between filters. This model extends the previous scheme for arbitrary -and possibly non-orthogonal- representations of features in the images. In particular, we compared the performance of increasingly over-complete representations in the retina. Results show that this algorithm provides an efficient spike coding strategy for low-level visual processing which may adapt to the complexity of the visual input.","tags":["area-v1","sparselets","unsupervised learning"],"title":"Finding Independent Components using spikes : a natural result of Hebbian learning in a sparse spike coding scheme","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"   Le jury était consistué (de gauche à droite) de Jeanny Hérault (Rapporteur), Michel Imbert (Président), Yves Burnod (Rapporteur, absent de la photo), Manuel Samuelides (Directeur de thèse) et Simon Thorpe (Co-directeur de thèse).  ","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"38073671c0b6396eeaa07e932fb80d62","permalink":"https://laurentperrinet.github.io/publication/perrinet-03-these/","publishdate":"2003-01-01T00:00:00Z","relpermalink":"/publication/perrinet-03-these/","section":"publication","summary":"Le jury était consistué (de gauche à droite) de Jeanny Hérault (Rapporteur), Michel Imbert (Président), Yves Burnod (Rapporteur, absent de la photo), Manuel Samuelides (Directeur de thèse) et Simon Thorpe (Co-directeur de thèse).","tags":["lateral connections","rank-order-coding","sparse coding","spike","stdp"],"title":"Comment déchiffrer le code impulsionnel de la vision ? Étude du flux parallèle, asynchrone et épars dans le traitement visuel ultra-rapide","type":"publication"},{"authors":["Laurent U Perrinet","Manuel Samuelides","Simon Thorpe"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"f0c0f14c99caa54516b7d24415fc5830","permalink":"https://laurentperrinet.github.io/publication/perrinet-03/","publishdate":"2003-01-01T00:00:00Z","relpermalink":"/publication/perrinet-03/","section":"publication","summary":"","tags":["area-v1","receptive field","sparse coding"],"title":"Emergence of filters from natural scenes in a sparse spike coding scheme","type":"publication"},{"authors":["Laurent U Perrinet","Manuel Samuelides"],"categories":null,"content":"   ","date":1024531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1024531200,"objectID":"3998d8ab2ad06ed206380d8b63294663","permalink":"https://laurentperrinet.github.io/publication/perrinet-02-stdp/","publishdate":"2002-06-20T00:00:00Z","relpermalink":"/publication/perrinet-02-stdp/","section":"publication","summary":"It is generally assumed that neurons in the central nervous system communicate through temporal firing patterns. As a first step, we will study the learning of a layer of realistic neurons in the particular case where the relevant messages are formed by temporally correlated patterns, or synfire patterns. The model is a layer of Integrate-and-Fire (IF) neurons with synaptic current dynamics that adapts by minimizing a cost according to a gradient descent scheme. This leads to a rule similar to Spike-Time Dependent Hebbian Plasticity (STDHP). Our results show that the rule that we derive is biologically plausible and leads to the detection of the coherence in the input in an unsupervised way. An application to shape recognition is shown as an illustration.","tags":["coding decoding","rank-order-coding","sparse hebbian learning","spike","stdp"],"title":"Coherence detection in a spiking neuron via Hebbian learning","type":"publication"},{"authors":["Laurent U Perrinet","Manuel Samuelides"],"categories":null,"content":"   Progressive reconstruction of a static image using spikes in a Laplacian pyramid.  ","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"c5358013414af2ac33add3a3ea23dfb9","permalink":"https://laurentperrinet.github.io/publication/perrinet-02-esann/","publishdate":"2002-01-01T00:00:00Z","relpermalink":"/publication/perrinet-02-esann/","section":"publication","summary":"   Progressive reconstruction of a static image using spikes in a Laplacian pyramid.  ","tags":["area-v1","receptive field","sparse coding"],"title":"Sparse Image Coding Using an Asynchronous Spiking Neural Network","type":"publication"},{"authors":["Laurent U Perrinet","Manuel Samuelides"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"919a9bd338518f23a5c65cdf941c15b9","permalink":"https://laurentperrinet.github.io/publication/perrinet-02-nsi/","publishdate":"2002-01-01T00:00:00Z","relpermalink":"/publication/perrinet-02-nsi/","section":"publication","summary":"","tags":["sparse hebbian learning","stdp"],"title":"Visual Strategies for Sparse Spike Coding","type":"publication"},{"authors":["Laurent U Perrinet","Arnaud Delorme","Simon Thorpe","Manuel Samuelides"],"categories":null,"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978307200,"objectID":"2040df7e258d8ed71d0b292a7d20cce5","permalink":"https://laurentperrinet.github.io/publication/perrinet-01/","publishdate":"2001-01-01T00:00:00Z","relpermalink":"/publication/perrinet-01/","section":"publication","summary":"","tags":["center-surround interactions"],"title":"Network of integrate-and-fire neurons using Rank Order Coding A: how to implement spike timing dependant plasticity","type":"publication"},{"authors":["Arnaud Delorme","Laurent U Perrinet","Simon Thorpe","Manuel Samuelides"],"categories":null,"content":"","date":978307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":978307200,"objectID":"e80135590d9e771b012e7c67519f17bb","permalink":"https://laurentperrinet.github.io/publication/delorme-01/","publishdate":"2001-01-01T00:00:00Z","relpermalink":"/publication/delorme-01/","section":"publication","summary":"Rank Order Coding is an alternative to conventional rate coding schemes that uses the order in which a neuron's inputs fire to encode information. In a visual system framework, we simulated the asynchronous waves of retinal spikes produced in response to natural scenes and used them to stimulate integrate-and-fire V1 neurons that implemented a standard learning rule based on spike timing. After propagating thousands of images, orientation like receptive fields arise in these neurons despite the ...","tags":["stdp"],"title":"Network of integrate-and-fire neurons using Rank Order Coding B: spike timing dependant plasticity and emergence of orientation selectivity","type":"publication"},{"authors":["Laurent U Perrinet","Manuel Samuelides"],"categories":null,"content":"","date":946684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":946684800,"objectID":"616a15db88d66df195590ab2279add09","permalink":"https://laurentperrinet.github.io/publication/perrinet-00/","publishdate":"2000-01-01T00:00:00Z","relpermalink":"/publication/perrinet-00/","section":"publication","summary":"","tags":["sparse hebbian learning","stdp","unsupervised learning"],"title":"A generative model for Spike Time Dependent Hebbian Plasticity","type":"publication"},{"authors":["Laurent U Perrinet"],"categories":null,"content":"","date":915148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":915148800,"objectID":"7fc44d4acf93dbedc536964998776b95","permalink":"https://laurentperrinet.github.io/publication/perrinet-99/","publishdate":"1999-01-01T00:00:00Z","relpermalink":"/publication/perrinet-99/","section":"publication","summary":"","tags":["rank-order-coding","unsupervised learning"],"title":"Apprentissage hebbien d'un reseau de neurones asynchrone a codage par rang","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://laurentperrinet.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"   Présentation du projet - L. Perrinet -- -- [2020-12-10] Réunion de lancement        AgileNeuRobot: Fiche d’identité  Titre : Robots aériens agiles bio-mimetiques pour le vol en conditions réelles Title : Bio-mimetic agile aerial robots flying in real-life conditions CES : CE23 - Intelligence Artificielle (ANR-20-CE23-0021) Durée: 3 ans, à partir du 1er mars 2021 Budget total: 435 k€   Spiking Neural Networks  From frame-based to event-based cameras.   Recurrent processing  Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.   Consortium:           Stéphane Viollet Ryad Benosman Laurent Perrinet   Julien Diperi Sio-Hoï Ieng Emmanuel Daucé   Inst Sciences Mouvement Inst de la Vision Inst Neurosci de la Timone      Gantt Chart of project organization.   Questions? Ask info @ laurent.perrinet@univ-amu.fr\nMore info @ web-site\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"50a80312ee8809a92b2d31e016cff4d1","permalink":"https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/2020-12-10_agileneurobot_anr/","section":"slides","summary":"Présentation du projet - L. Perrinet -- -- [2020-12-10] Réunion de lancement        AgileNeuRobot: Fiche d’identité  Titre : Robots aériens agiles bio-mimetiques pour le vol en conditions réelles Title : Bio-mimetic agile aerial robots flying in real-life conditions CES : CE23 - Intelligence Artificielle (ANR-20-CE23-0021) Durée: 3 ans, à partir du 1er mars 2021 Budget total: 435 k€   Spiking Neural Networks  From frame-based to event-based cameras.","tags":null,"title":"2020-12-10_agileneurobot_anr","type":"slides"},{"authors":null,"categories":null,"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   sub-slide  Efficiently write sub-slide Efficiently write sub-slide   sub-slide 2 \n Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/hulk.png\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"https://laurentperrinet.github.io/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   sub-slide  Efficiently write sub-slide Efficiently write sub-slide   sub-slide 2","tags":null,"title":"Slides","type":"slides"}]