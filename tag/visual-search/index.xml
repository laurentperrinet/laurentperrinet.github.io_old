<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Visual search | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/tag/visual-search/</link>
      <atom:link href="https://laurentperrinet.github.io/tag/visual-search/index.xml" rel="self" type="application/rss+xml" />
    <description>Visual search</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Thu, 17 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/media/hulk.png</url>
      <title>Visual search</title>
      <link>https://laurentperrinet.github.io/tag/visual-search/</link>
    </image>
    
    <item>
      <title>Visual search as active inference</title>
      <link>https://laurentperrinet.github.io/publication/dauce-20-iwai/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dauce-20-iwai/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;a follow-up of: 






  
    &lt;div class=&#34;pub-list-item&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  
  &lt;span &gt;&lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel Daucé&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://laurentperrinet.github.io/author/pierre-albiges/&#34;&gt;Pierre Albigès&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;&lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2020).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34;&gt;A dual foveal-peripheral visual processing model implements efficient saccade selection&lt;/a&gt;.
  &lt;em&gt;Journal of Vision&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/725879v3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  
    
  



&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;button type=&#34;button&#34; class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/dauce-20/cite.bib&#34;&gt;
  Cite
&lt;/button&gt;


  
  
    
  
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://github.com/laurentperrinet/WhereIsMyMNIST&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Code
&lt;/a&gt;












&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.1167/jov.20.8.22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;

  

&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;the mathematical details are described as a talk the 1st International WS on &lt;a href=&#34;https://twitter.com/hashtag/ActiveInference?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ActiveInference&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/IWAI2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#IWAI2020&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/ECMLPKDD?ref_src=twsrc%5Etfw&#34;&gt;@ECMLPKDD&lt;/a&gt; &lt;a href=&#34;https://t.co/4s7gHbMxiT&#34;&gt;https://t.co/4s7gHbMxiT&lt;/a&gt; and paper &amp;quot;Visual search as active inference&amp;quot; &lt;a href=&#34;https://t.co/yNCOFHf7FS&#34;&gt;https://t.co/yNCOFHf7FS&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1305488089989754883?ref_src=twsrc%5Etfw&#34;&gt;September 14, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; &gt;


  &lt;img src=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What:: talk @ &lt;a href=&#34;https://iwaiworkshop.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1st International Workshop on Active Inference (IWAI*2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Who:: Emmanuel Daucé and Laurent Perrinet&lt;/li&gt;
&lt;li&gt;Where: Ghent (Belgium), gone virtual, see &lt;a href=&#34;https://laurentperrinet.github.io/talk/2020-09-14-iwai&#34;&gt;https://laurentperrinet.github.io/talk/2020-09-14-iwai&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;When: 14/09/2020, time: 12:20:00-12:40:00&lt;/li&gt;
&lt;li&gt;What:
&lt;ul&gt;
&lt;li&gt;Slides @ &lt;a href=&#34;https://laurentperrinet.github.io/2020-09-14_IWAI&#34;&gt;https://laurentperrinet.github.io/2020-09-14_IWAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code for slides @ &lt;a href=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/&#34;&gt;https://github.com/laurentperrinet/2020-09-14_IWAI/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A dual foveal-peripheral visual processing model implements efficient saccade selection</title>
      <link>https://laurentperrinet.github.io/publication/dauce-20/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/dauce-20/</guid>
      <description>&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New &lt;a href=&#34;https://twitter.com/ARVOJOV?ref_src=twsrc%5Etfw&#34;&gt;@ARVOJOV&lt;/a&gt; : &amp;quot;A dual foveal-peripheral visual processing model implements efficient saccade selection&amp;quot; &lt;a href=&#34;https://t.co/JqnpBM5bcd&#34;&gt;https://t.co/JqnpBM5bcd&lt;/a&gt; comes with code @ &lt;a href=&#34;https://t.co/5MoIh00Bb8&#34;&gt;https://t.co/5MoIh00Bb8&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/hashtag/OpenAccess?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#OpenAccess&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/visionscience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#visionscience&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1305488088412688385?ref_src=twsrc%5Etfw&#34;&gt;September 14, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; &gt;


  &lt;img src=&#34;https://github.com/laurentperrinet/2020-09-14_IWAI/blob/master/2020-09-10_video-abstract.gif?raw=true&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning where to look: a foveated visuomotor control model</title>
      <link>https://laurentperrinet.github.io/talk/2019-07-15-cns/</link>
      <pubDate>Mon, 15 Jul 2019 12:20:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2019-07-15-cns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;download a &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-07-15-cns/2019-07-15-cns.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preliminary PDF&lt;/a&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Emmanuel Daucé @ &lt;a href=&#34;https://twitter.com/hashtag/CNS2019Barcelona?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#CNS2019Barcelona&lt;/a&gt; speaks about our joint work on « Learning where to look: a foveated visuomotor control model » more info @ &lt;a href=&#34;https://t.co/HREjuIgNCn&#34;&gt;https://t.co/HREjuIgNCn&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNSorg?ref_src=twsrc%5Etfw&#34;&gt;@CNSorg&lt;/a&gt; &lt;a href=&#34;https://t.co/GbbXhWL1k1&#34;&gt;pic.twitter.com/GbbXhWL1k1&lt;/a&gt;&lt;/p&gt;&amp;mdash; laurentperrinet (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1150713758643380226?ref_src=twsrc%5Etfw&#34;&gt;July 15, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;



















&lt;figure id=&#34;figure-problem-setting-in-generic-ecological-settings-the-visual-system-faces-a-tricky-problem-when-searching-for-one-target-from-a-class-of-targets-in-a-cluttered-environment-a-it-is-synthesized-in-the-following-experiment-after-a-fixation-period-of-200-ms-an-observer-is-presented-with-a-luminous-display--showing-a-single-target-from-a-known-class-here-digits-and-at-a-random-position-the-display-is-presented-for-a-short-period-of-500-ms-light-shaded-area-in-b-that-is-enough-to-perform-at-most-one-saccade-here-successful-on-the-potential-target-finally-the-observer-has-to-identify-the-digit-by-a-keypress-b-prototypical-trace-of-a-saccadic-eye-movement-to-the-target-position-in-particular-we-show-the-fixation-window-and-the-temporal-window-during-which-a-saccade-is-possible-green-shaded-area-c-simulated-reconstruction-of-the-visual-information-from-the-interoceptive-retinotopic-map-at-the-onset-of-the-display-and-after-a-saccade-the-dashed-red-box-indicating-the-visual-area-of-the-what-pathway-in-contrast-to-an-exteroceptive-representation-see-a-this-demonstrates-that-the-position-of-the-target-has-to-be-inferred-from-a-degraded-sampled-image-in-particular-the-configuration-of-the-display-is-such-that-by-adding-clutter-and-reducing-the-size-of-the-digit-it-may-become-necessary-to-perform-a-saccade-to-be-able-to-identify-the-digit-the-computational-pathway-mediating-the-action-has-to-infer-the-location-of-the-target-emphbefore-seeing-it-that-is-before-being-able-to-actually-identify-the-targets-category-from-a-central-fixation&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://raw.githubusercontent.com/SpikeAI/2019-07-15_CNS/master/figures/fig_intro.jpg&#34; data-caption=&#34;Problem setting: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. &amp;lt;strong&amp;gt;A)&amp;lt;/strong&amp;gt; It is synthesized in the following experiment: After a fixation period of 200 ms, an observer is presented with a luminous display  showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of 500 ms (light shaded area in B), that is enough to perform at most one saccade (here, successful) on the potential target. Finally, the observer has to identify the digit by a keypress. &amp;lt;strong&amp;gt;B)&amp;lt;/strong&amp;gt; Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window and the temporal window during which a saccade is possible (green shaded area). &amp;lt;strong&amp;gt;C)&amp;lt;/strong&amp;gt; Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display and after a saccade, the dashed red box indicating the visual area of the ``what&amp;#39;&amp;#39; pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target&amp;amp;rsquo;s category from a central fixation.&#34;&gt;


  &lt;img src=&#34;https://raw.githubusercontent.com/SpikeAI/2019-07-15_CNS/master/figures/fig_intro.jpg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Problem setting: In generic, ecological settings, the visual system faces a tricky problem when searching for one target (from a class of targets) in a cluttered environment. &lt;strong&gt;A)&lt;/strong&gt; It is synthesized in the following experiment: After a fixation period of 200 ms, an observer is presented with a luminous display  showing a single target from a known class (here digits) and at a random position. The display is presented for a short period of 500 ms (light shaded area in B), that is enough to perform at most one saccade (here, successful) on the potential target. Finally, the observer has to identify the digit by a keypress. &lt;strong&gt;B)&lt;/strong&gt; Prototypical trace of a saccadic eye movement to the target position. In particular, we show the fixation window and the temporal window during which a saccade is possible (green shaded area). &lt;strong&gt;C)&lt;/strong&gt; Simulated reconstruction of the visual information from the (interoceptive) retinotopic map at the onset of the display and after a saccade, the dashed red box indicating the visual area of the ``what&#39;&#39; pathway. In contrast to an exteroceptive representation (see A), this demonstrates that the position of the target has to be inferred from a degraded (sampled) image. In particular, the configuration of the display is such that by adding clutter and reducing the size of the digit, it may become necessary to perform a saccade to be able to identify the digit. The computational pathway mediating the action has to infer the location of the target \emph{before seeing it}, that is, before being able to actually identify the target&amp;rsquo;s category from a central fixation.
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-results-success&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-20.png&#34; data-caption=&#34;Results: success&#34;&gt;


  &lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-20.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results: success
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-results-failure-to-classify&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-32.png&#34; data-caption=&#34;Results: failure to classify&#34;&gt;


  &lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-32.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results: failure to classify
  &lt;/figcaption&gt;


&lt;/figure&gt;



















&lt;figure id=&#34;figure-results-failure-to-locate&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-47.png&#34; data-caption=&#34;Results: failure to locate&#34;&gt;


  &lt;img src=&#34;https://spikeai.github.io/2019-07-15_CNS/figures/CNS-saccade-47.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results: failure to locate
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
