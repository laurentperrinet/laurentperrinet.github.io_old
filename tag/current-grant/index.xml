<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>current-grant | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/tag/current-grant/</link>
      <atom:link href="https://laurentperrinet.github.io/tag/current-grant/index.xml" rel="self" type="application/rss+xml" />
    <description>current-grant</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Thu, 05 Oct 2023 14:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_512x512_fill_lanczos_center_3.png</url>
      <title>current-grant</title>
      <link>https://laurentperrinet.github.io/tag/current-grant/</link>
    </image>
    
    <item>
      <title>Emergences (2023 / 2027)</title>
      <link>https://laurentperrinet.github.io/grant/emergences/</link>
      <pubDate>Thu, 05 Oct 2023 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/emergences/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    TL;DR: Conventional deep learning models consume too much energy. Inspired by biology, we will explore new models that are more energy efficient.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;description-of-the-emergences-project&#34;&gt;Description of the &amp;ldquo;Emergences&amp;rdquo; project&lt;/h2&gt;
&lt;p&gt;Contemporary machine learning (ML) has incurred profound changes in the scientific, societal and economic landscapes alike. After a decade of sustained progress AI as a discipline is still making regular breakthroughs on many fronts, at the expense of an ever-increasing amount of consumption of compute resources. Modern language models feature hundreds of billion parameters and training energy consumption alone likely falls in the GWh range, with a logical forecast worsening the already prohibitive carbon footprint of AI.&lt;/p&gt;
&lt;p&gt;Besides the flourishing initiatives aimed at defining AI-friendly digital compute stack, the next logical breakthrough on the horizon is undoubtedly the emergence of disruptive AI compute technologies having improved energy efficiency. This development will likely involve the utilization of models that differ from those traditionally used in ML and exhibit properties that resemble the behavior of physical components, thereby facilitating implementation.&lt;/p&gt;
&lt;p&gt;The Emergences project aims at advancing the state-of-the art on near-physics emerging models by collaboratively exploring various computation models leveraging physical devices properties. Efforts will be put on 3 distinct fronts: Event-based models, Physics-inspired models (from physical systems dynamics) and innovative near-physics ML solutions (exploiting device properties). The investigations will be focused on embedded systems for Edge AI that call for increased energy efficiency for inference and learning, which could be incremental. They will apply to several application domains ranging for instance from the monitoring of the environment to health. Other important tasks such as common tools, performance metrics definition and model scalability analysis and will be conducted through as a collaborative transverse initiative.&lt;/p&gt;
&lt;p&gt;Emergences further intends to extend the collaborative research activities beyond the fence of the consortium by means of connecting with other projects of the PEPR IA and other research institutes, some of which are listed in this proposal. Finally, because of the unavoidable societal and philosophical implications of AI as a whole, Emergences will concurrently to the research activities run a track aimed at analyzing and anticipating the impact of its upcoming contributions.&lt;/p&gt;
&lt;h2 id=&#34;description-of-the-phd-project-wp1-focus-of-attention-a-sensory-motor-task-for-energy-reduction-in-unsupervised-spiking-neural-networks&#34;&gt;Description of the PhD project (WP1): &lt;em&gt;Focus of attention: a sensory-motor task for energy reduction in unsupervised spiking neural networks&lt;/em&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;attention mechanisms based on our cognitive architecture using a dual pathway: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel Daucé&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/pierre-albiges/&#34;&gt;Pierre Albigès&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2020).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34;&gt;A dual foveal-peripheral visual processing model implements efficient saccade selection&lt;/a&gt;.
   &lt;em&gt;Journal of Vision&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://www.biorxiv.org/content/10.1101/725879&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/dauce-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/dauce-20/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/WhereIsMyMNIST&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1167/jov.20.8.22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;implementation in a spiking neural network based: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2023).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34;&gt;Learning heterogeneous delays in a layer of spiking neurons for fast motion detection&lt;/a&gt;.
   &lt;em&gt;Biological Cybernetics&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/grimaldi-23-bc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-23-bc/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1007/s00422-023-00975-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;key-figures&#34;&gt;Key figures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;starting date: September 1, 2023&lt;/li&gt;
&lt;li&gt;Duration: 48 months&lt;/li&gt;
&lt;li&gt;14 partners&lt;/li&gt;
&lt;li&gt;Nb of PhD: 19&lt;/li&gt;
&lt;li&gt;Nb of Post doc: 13&lt;/li&gt;
&lt;li&gt;TRL: basic research&lt;/li&gt;
&lt;li&gt;Total grant requested: 6.8 M€&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;grant number XXX-23-XXX-XXX:&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&amp;quot; Ce travail a bénéficié d’une aide du gouvernement français au titre de France 2030, dans le cadre de l’Initiative d’Excellence d’Aix-Marseille Université – A*MIDEX, projet numero AMX-21-RID-025 &amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot; This work received support from the french government under the France 2030 investment plan, as part of the Initiative d’Excellence d’Aix-Marseille Université – A*MIDEX, under grant number  AMX-21-RID-025 ”&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;latest-news&#34;&gt;Latest news&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2023-10-05: Kick-off meeting!&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Polychronies (2022 / 2025)</title>
      <link>https://laurentperrinet.github.io/grant/polychronies/</link>
      <pubDate>Mon, 18 Jul 2022 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/polychronies/</guid>
      <description>&lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;Why do neurons communicate through action potentials, or spikes? An action potential is a binary event —it can occur or not, without further details— and asynchronous, i.e. it can occur at any time. In the living world, neurons almost systematically use this so-called event-based representation, though we do not yet have a clear idea why. A better understanding of this phenomenon remains a fundamental challenge in neurobiology in order to better interpret the masses of recorded data. It is also an emerging challenge in computer science to allow the efficient exploitation of a new class of sensors and impulse computers, called neuromorphic, which could allow significant gains in computing time and energy consumption —a major societal challenge in the age of the digital economy and of global warming.&lt;/p&gt;
&lt;p&gt;The goal of this project is to bring an interdisciplinary perspective on the computational advantage of time series representations for the brain and for information processing machines. In particular, we will formalize mathematically a representation in an assembly of neurons based on a set of patterns of different relative spike times called polychronous groups. This hypothesis is directly inspired by neurobiological observations in the hippocampus, and the innovative aspect is to expand the capabilities of analog representations based on the firing rate by considering a representation based on repetitions of these polychronous groups at precise times of occurrence. This formalization is particularly well suited to neuromorphic computing, and allows for supervised or self-supervised learning of polychronous groups in any event-driven data.
By extending this paradigm to a hierarchy, we envision practical applications of this approach in audio, video or neurobiological signal processing. The cross-fertilization of neuroscience and neuromimetic approaches will be instrumental in understanding the typical or pathological development of such spiking neural networks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grant number AMX-21-RID-025:&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&amp;quot; Ce travail a bénéficié d’une aide du gouvernement français au titre de France 2030, dans le cadre de l’Initiative d’Excellence d’Aix-Marseille Université – A*MIDEX, projet numero AMX-21-RID-025 &amp;quot;&lt;/li&gt;
&lt;li&gt;&amp;quot; This work received support from the french government under the France 2030 investment plan, as part of the Initiative d’Excellence d’Aix-Marseille Université – A*MIDEX, under grant number  AMX-21-RID-025 ”&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;latest-news&#34;&gt;Latest news&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;2023-05-01: &lt;a href=&#34;https://laurentperrinet.github.io/post/2023-05-01_postdoc-position_polychronies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Opening of post-doc position&lt;/a&gt; (THE POSITION HAS BEEN FILLED!)&lt;/li&gt;
&lt;li&gt;2022-12-29: check out our review paper: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/amelie-gruel/&#34;&gt;Amélie Gruel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/camille-besnainou/&#34;&gt;Camille Besnainou&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/jean-nicolas-jeremie/&#34;&gt;Jean-Nicolas Jérémie&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/jean-martinet/&#34;&gt;Jean Martinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34;&gt;Precise spiking motifs in neurobiological and neuromorphic data&lt;/a&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/grimaldi-22-polychronies.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-22-polychronies/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3390/brainsci13010068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;2022-11-28: &lt;a href=&#34;https://conect-int.github.io/talk/2022-11-28-conect-at-the-int-brainhack/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pilot project at the INT brainhack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2022-07-18: Le projet Polychronies est &lt;a href=&#34;https://www.univ-amu.fr/fr/public/lancement-de-lappel-projets-interdisciplinarite-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lauréat de l&amp;rsquo;appel à projets « Interdisciplinarité »&lt;/a&gt; !&lt;/li&gt;
&lt;li&gt;2022-02-27: read our &lt;a href=&#34;2022-02-27_AMIDEX_PerrinetCossartSchatz_Applicationform-AAP-Interdisciplinarite-2021.pdf&#34;&gt;complete proposal&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ANR ACES (2022/2026)</title>
      <link>https://laurentperrinet.github.io/grant/anr-aces/</link>
      <pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-aces/</guid>
      <description>&lt;p&gt;Contextual motor adaptation is the ability to produce different motor responses depending on different contingencies signaled by specific cues or contexts. This requires to learn the relation between antecedent stimuli, that signal the future state of the environment, motor responses, and outcomes. A wealth of research have demonstrated that motor systems such as the saccadic or the pursuit eye movement system may simultaneously adapt in two opposite directions (for instance increasing and decreasing the saccade amplitudes) when a context, such as the orbital position of the eye before the movement, signals different contingencies for each response.&lt;/p&gt;
&lt;p&gt;However, it has also been repeatedly reported that some cues, such as the target color or its shape, do not come to control the adaptation of the motor response. These observations remain unexplained and we lack adequate theoretical concepts to account for them: any stimulus, or context, that is perfectly correlated with the experimental manipulation should, in theory, induce contextual adaptation as it is conventionally thought that outcome predictability is the main factor controlling contextual learning. This has been a particularly vexing problem for the past 25 years as motor adaptation has become one of the main experimental model to study learning in humans.&lt;/p&gt;
&lt;p&gt;To solve this problem, the ACEs project relies on a general conceptual framework that elaborates on the active-inference view as well as recent proposals regarding the relation between value-based decision making and attention. Our conceptual model is grounded on the notion that, at each moment, several hypotheses regarding credit assignment (what causes what?) are competing to produce a behavioral policy. The inputs are categorized, somehow arbitrarily, as internal status, prior knowledge and sensory inputs. Sensory inputs might be viewed as affecting the hypothesis space while prior knowledge and internal status would provide bias in favor of various credit assignment hypothesis. Competition in the hypothesis space, relying on Bayesian inference, determines a unique motor response. Because out of all the different credit assignment hypotheses only one will prevail and determine the actual behavioral policy, the influence of the inputs on behavior are limited by their specific contribution to the dominating hypothesis, i.e. their weight.&lt;/p&gt;
&lt;h2 id=&#34;fiche-didentité&#34;&gt;Fiche d&amp;rsquo;identité&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Acronyme : ACES (ANR-21-CE28-0013)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Title : Assignment of credit and constraints on eye movement learning&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Coordinateur Scientifique : Laurent Madelain (ScaLab)&lt;/li&gt;
&lt;li&gt;Responsable Scientifique local : Anna Montagnini (UMR7289)&lt;/li&gt;
&lt;li&gt;Durée: 4 ans, à partir du 1er mars 2022 - 1er mars 2026&lt;/li&gt;
&lt;li&gt;Budget total: 435 k€&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://anr.fr/Projet-ANR-21-CE28-0013&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://anr.fr/Projet-ANR-21-CE28-0013&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This work was supported by ANR project ANR-21-CE28-0013 &amp;ldquo;ANR ACES&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>ANR RubinVase (2021/2024)</title>
      <link>https://laurentperrinet.github.io/grant/anr-rubinvase/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-rubinvase/</guid>
      <description>&lt;p&gt;Le but de RUBIN-VASE est de concevoir et valider de nouveaux modèles variationnels pour l&amp;rsquo;évolution des activations neuronales dans les systèmes visuel et auditif, codant naturellement le principe neurobiologique de représentation efficace. En nous concentrant sur des &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-02316989/file/InvitedJMIV_WCeq.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modifications des équations de Wilson-Cowan pour la dynamique neuronale&lt;/a&gt;, nous visons à i) valider cette approche pour le cortex visuel primaire, à travers l&amp;rsquo;étude des patterns hallucinatoires ; ii) développer un cadre neuro-inspiré pour le traitement sonore et la reconstruction vocale, à partir des mêmes principes ; iii) comparer les modèles variationnels proposés à des modèles data-driven. Pour atteindre nos objectifs nous couplerons le développement de théories mathématiques rigoureuses avec leur validation numérique et expérimentale. Cela se fera à travers une interaction originale entre des techniques variationnelles ou issues de la théorie du contrôle et des expériences psycho-physiques.&lt;/p&gt;
&lt;h2 id=&#34;carte-didentité-du-projet&#34;&gt;carte d&amp;rsquo;identité du projet&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Durée: 4 ans, à partir du 1er avril 2021&lt;/li&gt;
&lt;li&gt;Budget total (partenaire français): 665 k€&lt;/li&gt;
&lt;li&gt;Coordinateur Scientifique : &lt;a href=&#34;https://dprn.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dario PRANDI&lt;/a&gt; (Laboratoire des Signaux et Systèmes)&lt;/li&gt;
&lt;li&gt;Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE, CE48 - Fondements du numérique: informatique, automatique, traitement du signal&lt;/li&gt;
&lt;li&gt;Responsable Scientifique INT : Laurent PERRINET (UMR7289)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://anr.fr/Projet-ANR-20-CE48-0003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://anr.fr/Projet-ANR-20-CE48-0003&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work was supported by ANR project &amp;ldquo;RubinVase&amp;rdquo; N° ANR-20-CE48-0003.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ANR AgileNeuRobot (2021/2024)</title>
      <link>https://laurentperrinet.github.io/grant/anr-anr/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-anr/</guid>
      <description>&lt;!-- youtube-dl https://www.youtube.com/watch\?v\=36CTDiJjQ8I --&gt;














&lt;figure  id=&#34;figure-an-unmanned-aerial-vehicle-uav-flying-autonomously-in-a-cluttered-environment-would-require-the-agility-to-navigate-rapidly-by-detecting-as-fast-as-possible-potential-obstacles-as-represented-here-by-the-collision-zone-given-a-cruising-speed-associated-to-slow-or-fast-latencies-respectively-red-and-blue-shaded-areas-this-project-will-provide-with-a-novel-neuromorphic-architecture-designed-to-meet-these-requirements-thanks-to-an-event-based-two-way-processing&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An Unmanned aerial vehicle (UAV) flying autonomously in a cluttered environment would require the agility to navigate rapidly by detecting as fast as possible potential obstacles, as represented here by the collision zone, given a cruising speed, associated to slow or fast latencies (respectively red and blue shaded areas). This project will provide with a novel neuromorphic architecture designed to meet these requirements thanks to an event-based, two-way processing.&#34; srcset=&#34;
               /grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_11989bf1509db296ce4e56c0a5512eaf.webp 400w,
               /grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_bd10ea7c74a93e97c1d5129c215dafeb.webp 760w,
               /grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/grant/anr-anr/agile_UAV_huaf778c76111bcc2e47c9f894afc4b558_318949_11989bf1509db296ce4e56c0a5512eaf.webp&#34;
               width=&#34;760&#34;
               height=&#34;243&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      An Unmanned aerial vehicle (UAV) flying autonomously in a cluttered environment would require the agility to navigate rapidly by detecting as fast as possible potential obstacles, as represented here by the collision zone, given a cruising speed, associated to slow or fast latencies (respectively red and blue shaded areas). This project will provide with a novel neuromorphic architecture designed to meet these requirements thanks to an event-based, two-way processing.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;fiche-didentité&#34;&gt;Fiche d&amp;rsquo;identité&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Acronyme : AgileNeuRobot (ANR-20-CE23-0021)&lt;/li&gt;
&lt;li&gt;Titre : Robots aériens agiles bio-mimetiques pour le vol en conditions réelles&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Title : Bio-mimetic agile aerial robots flying in real-life conditions&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;CES : CE23 - Intelligence Artificielle / Instrument de financement : Projet de recherche collaborative (PRC) / Catégorie R&amp;amp;D : Recherche fondamentale&lt;/li&gt;
&lt;li&gt;Coordinateur Scientifique : PERRINET Laurent (UMR7289)&lt;/li&gt;
&lt;li&gt;Durée: 3 ans, à partir du 1er mars 2021 - 1er décembre 2024&lt;/li&gt;
&lt;li&gt;Budget total: 435 k€&lt;/li&gt;
&lt;li&gt;Responsables Scientifiques : Stéphane Viollet (BioRobotique, Inst Sciences Mouvement), Ryad Benosman (Inst de la Vision ) | Laurent Perrinet (NeOpTo, Inst Neurosciences de la Timone, coordinateur)&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-a-miniature-event-based-atis-sensor-contrary-to-a-classical-frame-based-camera-for-which-a-full-dense-image-representation-is-given-at-discrete-regularly-spaced-timings-the-event-based-camera-provides-with-events-at-the-micro-second-resolution-these-are-sparse-as-they-represent-luminance-increments-or-decrements-on-and-off-events-respectively&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively).&#34; srcset=&#34;
               /grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_72244f8a28585f1ccab85885fec7300e.webp 400w,
               /grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_18ec5d453856fc5dda035b77ef2a226c.webp 760w,
               /grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations_hue310d82cb803b114c601148df459c0f0_1644315_72244f8a28585f1ccab85885fec7300e.webp&#34;
               width=&#34;760&#34;
               height=&#34;217&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A miniature, event-based ATIS sensor. Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides with events at the micro-second resolution. These are sparse as they represent luminance increments or decrements (ON and OFF events, respectively).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;résumé&#34;&gt;Résumé&lt;/h2&gt;
&lt;p&gt;Des robots aériens autonomes seraient des outils essentiels dans les opérations de recherche et de sauvetage. Toutefois, voler dans des environnements complexes exige un haut niveau d&amp;rsquo;agilité, ce qui implique par exemple la capacité de déclencher des manœuvres agressives pour esquiver les obstacles: Les caméras et algorithmes d&amp;rsquo;intelligence artificielle conventionnels n&amp;rsquo;ont pas ces capacités. Dans ce projet, nous proposerons une solution associant de manière bio-inspirée une dynamique rapide de détection visuelle et de stabilisation. Nous intégrerons ces différents aspects dans un système neuromorphique événementiel de bout en bout. La clé de cette approche est l&amp;rsquo;optimisation des délais du système par traitement prédictif. Ceci permettra de voler indépendamment, sans aucune intervention de l&amp;rsquo;utilisateur. Notre objectif à plus long terme est de satisfaire ces besoins avec un minimum d&amp;rsquo;énergie et de fournir des solutions novatrices aux défis des algorithmes traditionnels d&amp;rsquo;IA.&lt;/p&gt;














&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-atis-until-the-rotors-the-camera-processor-and-motor-units-each-represents-respectively-multi-channel-feature-maps-c_i-an-estimate-of-the-depth-of-field-p-and-a-navigation-map-for-instance-time-of-contacts-on-a-polar-map-m-compared-to-a-discrete-time-pipeline-we-will-design-an-integrated-back-to-back-event-driven-system-based-on-a-fast-two-way-processing-between-the-c-p-and-m-units-event-driven-feed-forward-and-feed-back-communications-are-denoted-respectively-in-yellow-black-and-red-notice-the-attention-module-a-from-p-to-c-and-the-feed-back-of-navigation-information-from-m-and-the-imu-to-p&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Our system is divided into 3 units to process visual inputs (ATIS) until the rotors: the ***C***amera, ***P***rocessor and ***M***otor units. Each represents respectively multi-channel feature maps ($C_i$), an estimate of the depth-of-field ($P$) and a navigation map, for instance time-of-contacts on a polar map ($M$). Compared to a discrete-time pipeline, we will design an integrated, back-to-back event-driven system based on a fast, two-way processing between the ***C***, ***P*** and ***M*** units. Event-driven, feed-forward and feed-back communications are denoted respectively in yellow, black and red. Notice the attention module $A$ from $P$ to $C$ and the feed-back of navigation information from $M$ and the IMU to $P$.&#34; srcset=&#34;
               /grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_f200ed0437603f140c29fa06aa052552.webp 400w,
               /grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_b8c0f7df6c48c1940e9a5c9a53765cfa.webp 760w,
               /grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/grant/anr-anr/principe_agile_hu39db61294422958fdb81f905ce563f91_457308_f200ed0437603f140c29fa06aa052552.webp&#34;
               width=&#34;760&#34;
               height=&#34;226&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Our system is divided into 3 units to process visual inputs (ATIS) until the rotors: the &lt;em&gt;&lt;strong&gt;C&lt;/strong&gt;&lt;/em&gt;amera, &lt;em&gt;&lt;strong&gt;P&lt;/strong&gt;&lt;/em&gt;rocessor and &lt;em&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/em&gt;otor units. Each represents respectively multi-channel feature maps ($C_i$), an estimate of the depth-of-field ($P$) and a navigation map, for instance time-of-contacts on a polar map ($M$). Compared to a discrete-time pipeline, we will design an integrated, back-to-back event-driven system based on a fast, two-way processing between the &lt;em&gt;&lt;strong&gt;C&lt;/strong&gt;&lt;/em&gt;, &lt;em&gt;&lt;strong&gt;P&lt;/strong&gt;&lt;/em&gt; and &lt;em&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/em&gt; units. Event-driven, feed-forward and feed-back communications are denoted respectively in yellow, black and red. Notice the attention module $A$ from $P$ to $C$ and the feed-back of navigation information from $M$ and the IMU to $P$.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Autonomous aerial robots would be essential tools in search and rescue operations. But flying in complex environments requires a high level of agility, which implies the ability to initiate aggressive maneuvers to avoid obstacles: Conventional AI cameras and algorithms do not have these capabilities. In this project, we propose a solution that will integrate bio-inspired rapid visual detection and stabilization dynamics into an end-to-end event based neuromorphic system. The key to this approach will be the optimization of delays through predictive processing. This will allow these robots to fly independently, without any user intervention. Our longer-term goal is to meet the requirements with very little power and provide innovative solutions to the challenges of traditional AI algorithms.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work was supported by ANR project &amp;ldquo;AgileNeuRobot&amp;rdquo; N° ANR-20-CE23-0021.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ANR PRIOSENS (2021/2024)</title>
      <link>https://laurentperrinet.github.io/grant/anr-priosens/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/anr-priosens/</guid>
      <description>&lt;p&gt;A fundamental goal of systems neuroscience is to describe how sensory inputs are integrated and guide an animal&amp;rsquo;s behavior. To be able to integrate these inputs, early sensory systems have developed selectivities for specific stimulus features that allow them to analyze the inputs using these features as basis. We aim to uncover how disparate motion signals are integrated to produce a global percept of motion, and to understand the conditions in which such integration fails. Our proposal reflects the fact that adaptive behaviors in complex environments face numerous challenges, from processing noisy and uncertain visual motion information to predict future events on target trajectory contingencies and its interactions with a dynamic, cluttered environment.
We propose to use dynamic inference as an efficient theoretical framework to understand how the brain integrates Prior knowledges elaborated from statistical regularities of natural environments with different sources of information across different time scales in order to extract relevant motion information from the sensory flow and predict future events or actions. The smooth pursuit system is an excellent probe of such hierarchical dynamical inferences from target motion computation to target trajectory prediction. In marmosets, we have access to populations of neurons in pivotal cortical areas along the occipito-parieto- frontal network that have been identified in non-human and human primates. We seek to uncover a unifying empirical and theoretical framework to capture inference across different time scales.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;With Guilhem Ibos, Guillaume Masson &amp;amp; Nicholas Priebe.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;aim-3-modelling-behavioural-and-neuronal-data-within-the-active-inference-framework&#34;&gt;Aim 3, modelling behavioural and neuronal data within the active inference framework&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Type de contrat : CRCNS &lt;a href=&#34;https://anr.fr/Project-ANR-20-NEUC-0002&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;US-French Research Proposal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Durée: 4 ans, à partir du 1er novembre 2020&lt;/li&gt;
&lt;li&gt;Budget total (partenaire français): 341 k€&lt;/li&gt;
&lt;li&gt;to be recruited: Post-doctoral fellow: A post-post-doctoral fellow in computational neuroscience will be recruited. With a 2-5 years experience, salary cost is of 52K€/year, for 2 years (total: 104K€).&lt;/li&gt;
&lt;li&gt;Coordinateur Scientifique : MONTAGNINI, Anna &amp;amp; PERRINET Laurent (UMR7289)&lt;/li&gt;
&lt;li&gt;Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE&lt;/li&gt;
&lt;li&gt;Responsable Scientifique INT : MASSON Guillaume (UMR7289)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;This work was supported by ANR project &amp;ldquo;PRIOSENS&amp;rdquo; N° ANR-20-NEUC-0002.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>APROVIS3D (2019/2023)</title>
      <link>https://laurentperrinet.github.io/grant/aprovis-3-d/</link>
      <pubDate>Tue, 10 Sep 2019 10:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/grant/aprovis-3-d/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Le projet APROVIS3D est lauréat de l&amp;rsquo;&lt;a href=&#34;http://www.chistera.eu/projects/aprovis3d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;appel à projets 2018 &lt;em&gt;CHIST-ERA&lt;/em&gt;&lt;/a&gt; :&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/H1_dDB3t8lI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;APROVIS3D project targets analog computing for artificial intelligence in the form of Spiking Neural Networks (SNNs) on a mixed analog and digital architecture. The project includes including field programmable analog array (FPAA) and SpiNNaker applied to a stereopsis system dedicated to coastal surveillance using an aerial robot. Computer vision systems widely rely on artificial intelligence and especially neural network based machine learning, which recently gained huge visibility. The training stage for deep convolutional neural networks is both time and energy consuming. In contrast, the human brain has the ability to perform visual tasks with unrivalled computational and energy efficiency. It is believed that one major factor of this efficiency is the fact that information is vastly represented by short pulses (spikes) at analog – not discrete – times. However, computer vision algorithms using such representation still lack in practice, and its high potential is largely underexploited. Inspired from biology, the project addresses the scientific question of developing a low-power, end-to-end analog sensing and processing architecture of 3D visual scenes, running on analog devices, without a central clock and aims to validate them in real-life situations. More specifically, the project will develop new paradigms for biologically inspired vision, from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes. The ambitious long-term vision of the project is to develop the next generation AI paradigm that will eventually compete with deep learning. We believe that neuromorphic computing, mainly studied in EU countries, will be a key technology in the next decade. It is therefore both a scientific and strategic challenge for the EU to foster this technological breakthrough. The consortium from four EU countries offers a unique combination of expertise that the project requires. SNNs specialists from various fields, such as visual sensors (IMSE, Spain), neural network architecture and computer vision (Uni. of Lille, France) and computational neuroscience (INT, France) will team up with robotics and automatic control specialists (NTUA, Greece), and low power integrated systems designers (ETHZ, Switzerland) to help geoinformatics researchers (UNIWA, Greece) build a demonstrator UAV for coastal surveillance (TRL5). Adding up to the shared interest regarding analog based computing and computer vision, all team members have a lot to offer given their different and complementary points of view and expertise. Key challenges of this project will be end-to-end analog system design (from sensing to AI-based control of the UAV and 3D coastal volumetric reconstruction), energy efficiency, and practical usability in real conditions. We aim to show that such a bioinspired analog design will bring large benefits in terms of power efficiency, adaptability and efficiency needed to make coastal surveillance with UAVs practical and more efficient than digital approaches.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type de contrat : Subvention / Aide&lt;/li&gt;
&lt;li&gt;Durée: 3 ans, à partir du 1er avril 2020 (prolongation demandée)&lt;/li&gt;
&lt;li&gt;Budget total: 867 k€ , bugdget INT: 150 k€&lt;/li&gt;
&lt;li&gt;Partenaire(s) : AGENCE NATIONALE DE LA RECHERCHE&lt;/li&gt;
&lt;li&gt;Objet : AAP 2019 - CHIST-ERA &amp;ldquo;Analog PROcessing of bioinspired VIsion Sensors for 3D reconstruction&amp;rdquo; ANR-19-CHR3-0008-03&lt;/li&gt;
&lt;li&gt;Responsable Scientifique INT : PERRINET Laurent (UMR7289)&lt;/li&gt;
&lt;li&gt;“This project has received funding from the European Union’s ERA-NET CHIST-ERA 2018 research and innovation programme under grant agreement No ANR-19-CHR3-0008-03”&lt;/li&gt;
&lt;li&gt;Find more on the &lt;a href=&#34;http://aprovis3d.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
