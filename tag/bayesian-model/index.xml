<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian model | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/tag/bayesian-model/</link>
      <atom:link href="https://laurentperrinet.github.io/tag/bayesian-model/index.xml" rel="self" type="application/rss+xml" />
    <description>Bayesian model</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Mon, 23 Jan 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_512x512_fill_lanczos_center_3.png</url>
      <title>Bayesian model</title>
      <link>https://laurentperrinet.github.io/tag/bayesian-model/</link>
    </image>
    
    <item>
      <title>2023-01-23_game-theory-and-the-brain</title>
      <link>https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain/</guid>
      <description>&lt;h1 id=&#34;game-theory-and-brain-strategies&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;50%&#34; &gt;
&lt;p&gt;&lt;strong&gt;[2023-01-23] Atelier jeu et cerveau&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain&#34;&gt;https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;Photo by Naser Tamimi on Unsplash &lt;a href=&#34;https://unsplash.com/fr/photos/yG9pCqSOrAg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://unsplash.com/fr/photos/yG9pCqSOrAg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-1&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;80%&#34; &gt;
&lt;p&gt;&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;Le jeu du cerveau et du hasard, &lt;i&gt;The Conversation&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;What is noise? the uncertainty due to noise is symbolized by dices: a throw of fair dices, even if they are optimally simulated can not be predicted: the outcome is uniformly one facet from 1 to 6&lt;/li&gt;
&lt;li&gt;I am interested in vision, and uncertainty exists in different forms&lt;/li&gt;
&lt;li&gt;If we consider the image, can be noise at low contrast, complexity of the object, pose of the dice&lt;/li&gt;
&lt;li&gt;in this presentation, we will see different facets of noise and uncertainty, and illustrate how our brains may play with it - and delineate a theory for this game. we will also see how it may harness the noise by explicitly representing it in the neural activity&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;aleatoric-noise&#34;&gt;Aleatoric noise&lt;/h1&gt;
&lt;hr&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-random-points--a&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://a5huynh.github.io/img/2019/rng-example.png&#34; alt=&#34;Random points  (A).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;49%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Random points  (A).
    &lt;/figcaption&gt;&lt;/figure&gt;













&lt;figure  id=&#34;figure-random-points--b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://a5huynh.github.io/img/2019/poisson-disk-example.png&#34; alt=&#34;Random points  (B).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;49%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Random points  (B).
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;img src=&#34;https://a5huynh.github.io/img/2019/rng-example.png&#34; width=&#34;70%&#34; &gt;
&lt;img src=&#34;https://a5huynh.github.io/img/2019/poisson-disk-example.png&#34; width=&#34;70%&#34; &gt;
&lt;p&gt;&lt;a href=&#34;https://a5huynh.github.io/posts/2019/poisson-disk-sampling/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Huynh, generating Poisson disk noise&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;what is noise? it exists at quantum level, but if I were to ask you to draw random points how would it look like?&lt;/li&gt;
&lt;li&gt;Aleatoric comes from alea, the Latin word for â€œdice.â€ Aleatoric uncertainty is the uncertainty introduced by the randomness of an event. For example, the result of flipping a coin is an aleatoric event.&lt;/li&gt;
&lt;li&gt;In your opinion, which of the two is the most random pattern?&lt;/li&gt;
&lt;li&gt;from your responses &amp;hellip;&lt;/li&gt;
&lt;li&gt;the answer is that &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-instabilitÃ©-etienne-reyhttpslaurentperrinetgithubiopost2018-09-09_artorama&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/post/2018-09-09_artorama/featured.png&#34; alt=&#34;[InstabilitÃ©, Etienne Rey.](https://laurentperrinet.github.io/post/2018-09-09_artorama/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/post/2018-09-09_artorama/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InstabilitÃ©, Etienne Rey.&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;this was for instance used by the artist Etienne Rey to generate large panels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;our perception will generate objects out of nowhere: surfaces, groups, holes&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;this explains many cognitive biases, for instance that we expect noise to have some regularity and that we wish to explain any cluster of events by some god-like divinity&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;going further &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;when going to the same place a few years later &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;the face was gone &amp;hellip;&lt;/li&gt;
&lt;li&gt;conclusion 1: information pops out from noise&lt;/li&gt;
&lt;li&gt;conclusion 2: further information may change the interpretation&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction&#34;&gt;Sequence prediction&lt;/h1&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://github.com/chloepasturel/AnticipatorySPEM/raw/master/2020-03_video-abstract/Bet_eyeMvt/eyeMvt.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;to test this in the lab, we analyzed the response of observers to a sequences of left / right moving dots&lt;/li&gt;
&lt;li&gt;These were presented in multiple blocks of 50 trials for which we recorded eye movements and, on a subsequent day, asked them&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction-1&#34;&gt;Sequence prediction&lt;/h1&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
A: ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ ?
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
B: ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ ?
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
C: ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ ?
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
D: ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ ?
&lt;/code&gt;&lt;/pre&gt;&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;to simplify the problem, let&amp;rsquo;s show these sequences as the sequence of these 2 emojis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In sequence A, what do you think the next&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the same question could be asked in an online fashion&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence B, it&amp;rsquo;s certainly the same answer, yet with lower certitude&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence C, you go metal ğŸ¤˜&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence D, it&amp;rsquo;s different there is a clearly a tendance for ğŸ¤˜but that it switches to ğŸ‘&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;is it possible that the brain may detect such switches?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction-2&#34;&gt;Sequence prediction&lt;/h1&gt;














&lt;figure  id=&#34;figure-pasturel-et-al-2020httpslaurentperrinetgithubiopublicationpasturel-montagnini-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis.png&#34; alt=&#34;([Pasturel *et al*, 2020](https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      (&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pasturel &lt;em&gt;et al&lt;/em&gt;, 2020&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;to synthesize, we have a generative model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we found the mathematically optimal problem - and found that both eye movements + bets follow the model with switches&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The aleatoric noise is transformed into a measure of knowledge = epistemic noise&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;epistemic-noise&#34;&gt;Epistemic noise&lt;/h1&gt;
&lt;!-- 
---

# Playing with noise
















&lt;figure  id=&#34;figure-nash-equilibrium-rock-paper-scissorshttpsenwikipediaorgwikirock_paper_scissors&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/67/Rock-paper-scissors.svg&#34; alt=&#34;Nash equilibrium ([Rock paper scissors](https://en.wikipedia.org/wiki/Rock_paper_scissors)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Nash equilibrium (&lt;a href=&#34;https://en.wikipedia.org/wiki/Rock_paper_scissors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rock paper scissors&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;let&amp;rsquo;s go back to game theory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rock paper scissors: Its French name, &amp;ldquo;Chi-fou-mi&amp;rdquo;, is based on the Old Japanese words for &amp;ldquo;one, two, three&amp;rdquo; (&amp;ldquo;hi, fu, mi&amp;rdquo;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nash Equilibrium is a game theory concept that determines the optimal solution in a non-cooperative game in which each player lacks any incentive to change his/her initial strategy. Under the Nash equilibrium, a player does not gain anything from deviating from their initially chosen strategy, assuming the other players also keep their strategies unchanged.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quantamagazine.org/the-game-theory-math-behind-rock-paper-scissors-20180402/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.quantamagazine.org/the-game-theory-math-behind-rock-paper-scissors-20180402/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;

---















&lt;figure  id=&#34;figure-prisoners-dilemma-salem-marafihttpwwwsalemmaraficombusinessprisoners-dilemma&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.salemmarafi.com/wp-content/uploads/2011/10/prisoners_dilemma.jpg&#34; alt=&#34;Prisonerâ€™s Dilemma ([Salem Marafi](http://www.salemmarafi.com/business/prisoners-dilemma/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;60%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Prisonerâ€™s Dilemma (&lt;a href=&#34;http://www.salemmarafi.com/business/prisoners-dilemma/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Salem Marafi&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;


&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;uncertainty comes not from aleatoric noise but from not knowing: epistemic uncertainty&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt; --&gt;
&lt;hr&gt;
&lt;h1 id=&#34;representing-uncertainty&#34;&gt;Representing uncertainty&lt;/h1&gt;














&lt;figure  id=&#34;figure-visual-epistemic-uncertainty-hugo-ladrethttpstheconversationcomle-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.theconversation.com/files/407867/original/file-20210623-17-ai1gc3.png&#34; alt=&#34;Visual epistemic uncertainty ([Hugo Ladret](https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual epistemic uncertainty (&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;in the case of images, a local patch may have the same most likely orientation, yet with different bandwidth (textures)&lt;/li&gt;
&lt;li&gt;the primary visual cortex of mammals like humans is to detect orientations&lt;/li&gt;
&lt;li&gt;will the response be the same for both cases?&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;representing-uncertainty-1&#34;&gt;Representing uncertainty&lt;/h1&gt;














&lt;figure  id=&#34;figure-visual-epistemic-uncertainty-hugo-ladrethttpslaurentperrinetgithubiopublicationladret-22&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/ladret-22/featured_hubbf63d8d7d25b21f139c2f10354080fc_466086_720x2500_fit_q75_h2_lanczos_3.webp&#34; alt=&#34;Visual epistemic uncertainty ([Hugo Ladret](https://laurentperrinet.github.io/publication/ladret-22/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual epistemic uncertainty (&lt;a href=&#34;https://laurentperrinet.github.io/publication/ladret-22/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-2&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;80%&#34; &gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;In face of noise, the brain plays a game&lt;/li&gt;
&lt;li&gt;Evolution favors not fitness but adaptability&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-3&#34;&gt;Game theory and brain strategies&lt;/h1&gt;














&lt;figure  id=&#34;figure-aleatoric-uncertainty-pasturel-et-al-2020httpslaurentperrinetgithubiopublicationpasturel-montagnini-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis.png&#34; alt=&#34;Aleatoric uncertainty ([Pasturel *et al*, 2020](https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Aleatoric uncertainty (&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pasturel &lt;em&gt;et al&lt;/em&gt;, 2020&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;The brain uses predictive coding, for instance for sequence learning&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-4&#34;&gt;Game theory and brain strategies&lt;/h1&gt;














&lt;figure  id=&#34;figure-epistemic-uncertainty-hugo-ladrethttpstheconversationcomle-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.theconversation.com/files/407867/original/file-20210623-17-ai1gc3.png&#34; alt=&#34;Epistemic uncertainty ([Hugo Ladret](https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Epistemic uncertainty (&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;For this, it represents explictly uncertainty (epistemic noise)&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022-11-21_flash-lag-effect</title>
      <link>https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/</link>
      <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/</guid>
      <description>&lt;table width=&#34;100%&#34;&gt; 
&lt;tr&gt;
	&lt;th width=&#34;80%&#34;&gt;
	&lt;img src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/header.png&#34; width=&#34;100%&#34; &gt;
	&lt;th width=&#34;20%&#34;&gt;
	&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; width=&#34;100%&#34; &gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;[2022-11-21] Alex Reynaud&amp;rsquo;s lab meeting&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34;&gt;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;!--
---


&lt;table width=&#34;100%&#34;&gt;
&lt;tr&gt;
	&lt;th width=&#34;80%&#34;&gt;
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;figure  &gt;
	  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
	    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/khoei-masson-perrinet-17/header.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
	  &lt;/div&gt;&lt;/figure&gt;	&lt;/th&gt;
	&lt;th width=&#34;20%&#34;&gt;
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;figure  &gt;
	  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
	    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
	  &lt;/div&gt;&lt;/figure&gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;

__[2022-11-21] Alex Reynaud&#39;s lab meeting__

&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34;&gt;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&lt;/a&gt;&lt;/p&gt; --&gt;
&lt;!-- ---

|













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/khoei-masson-perrinet-17/header.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;29%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;|

__[2022-11-21] Alex Reynaud&#39;s lab meeting__
https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;timing-in-the-visual-pathways&#34;&gt;Timing in the visual pathways&lt;/h2&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-ultra-rapid-visual-processing-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-polychronies/featured.jpg&#34; alt=&#34;Ultra-rapid visual processing ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ultra-rapid visual processing (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-compensating-visual-delays-perrinet-adams--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/tsonga.jpg&#34; alt=&#34;Compensating visual delays ([Perrinet, Adams &amp;amp; Friston 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Compensating visual delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet, Adams &amp;amp; Friston 2014&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-compensating-visual-delays-perrinet-adams--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/figure-tsonga.jpg&#34; alt=&#34;Compensating visual delays ([Perrinet Adams &amp;amp; Friston, 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Compensating visual delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet Adams &amp;amp; Friston, 2014&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;travelling-waves&#34;&gt;Travelling waves?&lt;/h2&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/line_motion.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/phi_motion.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-suppressive-travelling-waves-chemla-et-al-2019httpslaurentperrinetgithubiopublicationchemla-19&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-18_JNLF/master/figures/Chemla_etal2019.png&#34; alt=&#34;Suppressive travelling waves ([Chemla *et al*, 2019](https://laurentperrinet.github.io/publication/chemla-19/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Suppressive travelling waves (&lt;a href=&#34;https://laurentperrinet.github.io/publication/chemla-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chemla &lt;em&gt;et al&lt;/em&gt;, 2019&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;predictive-coding&#34;&gt;Predictive coding&lt;/h2&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_aperture.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;!--
---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_box.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt; --&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_cube.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-based-prediction-perrinet-et-al-2012httpslaurentperrinetgithubiopublicationperrinet-12-pred&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/navier.svg&#34; alt=&#34;Motion-based prediction ([Perrinet *et al*, 2012](https://laurentperrinet.github.io/publication/perrinet-12-pred/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-based-prediction-perrinet-et-al-2012httpslaurentperrinetgithubiopublicationperrinet-12-pred&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/perrinet12pred_figure2.png&#34; alt=&#34;Motion-based prediction ([Perrinet *et al*, 2012](https://laurentperrinet.github.io/publication/perrinet-12-pred/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;61%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- ---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line-nopred_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
 --&gt;
&lt;hr&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;flash-lag-effect&#34;&gt;Flash-lag effect&lt;/h2&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-flash-lag-effect-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_cartoon.jpg&#34; alt=&#34;Flash-lag effect ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Flash-lag effect (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!--
---















&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=10.1371/journal.pcbi.1005068.g002&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;id=10.1371/journal.pcbi.1005068.g002
---















&lt;figure  id=&#34;figure-diagonal-markov&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov
    &lt;/figcaption&gt;&lt;/figure&gt;

---















&lt;figure  id=&#34;figure-diagonal-markov&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_simple.jpg&#34; alt=&#34;Diagonal markov&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov
    &lt;/figcaption&gt;&lt;/figure&gt;

---















&lt;figure  id=&#34;figure-diagonal-markov-pull&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_pull.jpg&#34; alt=&#34;Diagonal markov (pull)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov (pull)
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;!--
---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/PBP_flash_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;


---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/PBP_dot_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;


---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_flash_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;

---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt; --&gt;
&lt;!-- ---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

Flash-lag effect: MBP ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).

---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

Flash-lag effect: MBP ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)). --&gt;
&lt;hr&gt;
&lt;p&gt;








  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;








  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;Flash-lag effect: MBP (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/positional-delay.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-flash-lag-effect-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg&#34; alt=&#34;Flash-lag effect ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Flash-lag effect (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!--


---















&lt;figure  id=&#34;figure-qauntitative-result&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg&#34; alt=&#34;Qauntitative result&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Qauntitative result
    &lt;/figcaption&gt;&lt;/figure&gt;


https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg


https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true
MBP_dot_spatial_readout.mp4
MBP_flash_spatial_readout.mp4
MBP_spatial_readout.mp4
PBP_dot_spatial_readout.mp4
PBP_flash_spatial_readout.mp4

https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true

PBP_spatial_readout.mp4


src=&#34;../../publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; khoei-masson-perrinet-17


 create mode 100644 publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4
 create mode 100644 publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4

 --&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram_comp.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-reversal-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_MotionReversal_MBP.jpg&#34; alt=&#34;Motion reversal ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion reversal (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-reversal-smoothed-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_MotionReversal.jpg&#34; alt=&#34;Motion reversal (smoothed) ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion reversal (smoothed) (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag_stop.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-limit-cycles-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_limit_cycles.jpg&#34; alt=&#34;Limit cycles ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Limit cycles (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!--
---















&lt;figure  id=&#34;figure-diagonal-neural&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_neural.jpg&#34; alt=&#34;Diagonal neural&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal neural
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-application-to-the-pulfrich-phenomenonhttpseyewikiaaoorgpulfrich_phenomenon&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://eyewiki.aao.org/w/images/1/e/eb/Pulfrich.png&#34; alt=&#34;Application to the [Pulfrich phenomenon](https://eyewiki.aao.org/Pulfrich_Phenomenon)?&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Application to the &lt;a href=&#34;https://eyewiki.aao.org/Pulfrich_Phenomenon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pulfrich phenomenon&lt;/a&gt;?
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt; + &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From the retina to action: Dynamics of predictive processing in the visual system</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-20/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-20/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Find the text at &lt;a href=&#34;https://laurentperrinet.github.io/Perrinet20PredictiveProcessing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/Perrinet20PredictiveProcessing/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The source code of the text is available at &lt;a href=&#34;https://github.com/laurentperrinet/Perrinet20PredictiveProcessing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/laurentperrinet/Perrinet20PredictiveProcessing&lt;/a&gt;
This chapter is available as part of the book &amp;ldquo;&lt;a href=&#34;https://www.bloomsbury.com/uk/the-philosophy-and-science-of-predictive-processing-9781350099753/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Philosophy and Science of Predictive Processing&lt;/a&gt;&amp;rdquo; :
List of Contributors :&lt;/li&gt;
&lt;li&gt;Preface: The Brain as a Prediction Machine, Anil Seth&lt;/li&gt;
&lt;li&gt;Introduction, Dina MendonÃ§a, Manuel Curado &amp;amp; Steven S. Gouveia&lt;/li&gt;
&lt;li&gt;Part I: Predictive Processing: Philosophical Approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Predictive Processing and Representation: How Less Can Be More, Erik Myin and Thomas van Es&lt;/li&gt;
&lt;li&gt;A Humean Challenge to Predictive Coding, Colin Klein&lt;/li&gt;
&lt;li&gt;Are Markov Blankets Real and Does it Matter?, Richard Menary and Alexander J. Gillett&lt;/li&gt;
&lt;li&gt;Predictive Processing and Metaphysical Views of the Self, Robert Clowes and Klaus GÃ¤rtner&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Part II: Predictive Processing: Cognitive Science and Neuroscientific Approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;From the Retina to Action: Dynamics of Predictive Processing in the Visual System, Laurent Perrinet&lt;/li&gt;
&lt;li&gt;Predictive Processing and Consciousness: Prediction Fallacy and its Spatiotemporal Resolution, Steven S. Gouveia&lt;/li&gt;
&lt;li&gt;The Many Faces of Attention: Why Precision Optimization is not Attention, Sina Fazelpour and Madeleine Ransom&lt;/li&gt;
&lt;li&gt;Predictive Processing: Does it Compute?, Chris Thornton&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Part III: Predictive Processing: Mental Health&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;The Predictive Brain, Conscious Experience and Brain-related Conditions, Lisa Feldman Barrett and Lorena Chanes&lt;/li&gt;
&lt;li&gt;Disconnection and Diaschisis: Active Inference in Neuropsychology, Thomas Parr and Karl Friston&lt;/li&gt;
&lt;li&gt;The Phenomenology and Predictive Processing of Time in Depression, Zachariah Neemeh and Shaun Gallagher&lt;/li&gt;
&lt;li&gt;Why Use Predictive Processing to Explain Psychopathology? The Case of Anorexia Nervosa, Jakob Hohwy and Stephen Gadsby&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Afterword, Manuel Curado&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures</title>
      <link>https://laurentperrinet.github.io/publication/vacher-16/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/vacher-16/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The flash-lag effect as a motion-based predictive shift</title>
      <link>https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/</link>
      <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnrs.fr/insb/recherche/parutions/articles2017/l-perrinet.html%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Press release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;visual-illusions-their-origin-lies-in-prediction&#34;&gt;Visual illusions: their origin lies in prediction&lt;/h1&gt;
&lt;p&gt;













&lt;figure  id=&#34;figure-flash-lag-effect-when-a-visual-stimulus-moves-along-a-continuous-trajectory-it-may-be-seen-ahead-of-its-veridical-position-with-respect-to-an-unpredictable-event-such-as-a-punctuate-flash-this-illusion-tells-us-something-important-about-the-visual-system-contrary-to-classical-computers-neural-activity-travels-at-a-relatively-slow-speed-it-is-largely-accepted-that-the-resulting-delays-cause-this-perceived-spatial-lag-of-the-flash-still-after-several-decades-of-debates-there-is-no-consensus-regarding-the-underlying-mechanisms&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;*Flash-Lag Effect.* When a visual stimulus moves along a continuous trajectory, it may be seen ahead of its veridical position with respect to an unpredictable event such as a punctuate flash. This illusion tells us something important about the visual system: contrary to classical computers, neural activity travels at a relatively slow speed. It is largely accepted that the resulting delays cause this perceived spatial lag of the flash. Still, after several decades of debates, there is no consensus regarding the underlying mechanisms.&#34;
           src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/flash_lag.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;Flash-Lag Effect.&lt;/em&gt; When a visual stimulus moves along a continuous trajectory, it may be seen ahead of its veridical position with respect to an unpredictable event such as a punctuate flash. This illusion tells us something important about the visual system: contrary to classical computers, neural activity travels at a relatively slow speed. It is largely accepted that the resulting delays cause this perceived spatial lag of the flash. Still, after several decades of debates, there is no consensus regarding the underlying mechanisms.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;strong&gt;Researchers from the Timone Institute of Neurosciences bring a new theoretical hypothesis on a visual illusion discovered at the beginning of the 20th century. This illusion remained misunderstood while it poses fundamental questions about how our brains represent events in space and time. This study published on January 26, 2017 in the journal PLOS Computational Biology, shows that the solution lies in the predictive mechanisms intrinsic to the neural processing of information.&lt;/strong&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New Research: The Flash-Lag Effect as a Motion-Based Predictive Shift &lt;a href=&#34;https://t.co/K3KWPO8l4a&#34;&gt;https://t.co/K3KWPO8l4a&lt;/a&gt; Khoei et al. &lt;a href=&#34;https://twitter.com/hashtag/vision?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#vision&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/motion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#motion&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuralnetworks?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuralnetworks&lt;/a&gt; &lt;a href=&#34;https://t.co/RElm4Qqo58&#34;&gt;pic.twitter.com/RElm4Qqo58&lt;/a&gt;&lt;/p&gt;&amp;mdash; PLOS Comp Biol (@PLOSCompBiol) &lt;a href=&#34;https://twitter.com/PLOSCompBiol/status/829354100273745920?ref_src=twsrc%5Etfw&#34;&gt;February 8, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

Visual illusions are still popular: in a quasi-magical way, they can make objects appear where they are not expected&amp;hellip; They are also excellent opportunities to question the constraints of our perceptual system. Many illusions are based on motion, such as the flash-lag effect. Observe a luminous dot that moves along a rectilinear trajectory. If a second light dot is flashed very briefly just above the first, the moving point will always be perceived in front of the flash while they are vertically aligned.














&lt;figure  id=&#34;figure-fig-2-diagonal-markov-chain-in-the-current-study-the-estimated-state-vector-z--x-y-u-v-is-composed-of-the-2d-position-x-and-y-and-velocity-u-and-v-of-a-moving-stimulus-a-first-we-extend-a-classical-markov-chain-using-nijhawans-diagonal-model-in-order-to-take-into-account-the-known-neural-delay-Ï„-at-time-t-information-is-integrated-until-time-t--Ï„-using-a-markov-chain-and-a-model-of-state-transitions-pztztÎ´t-such-that-one-can-infer-the-state-until-the-last-accessible-information-pztÏ„i0tÏ„-this-information-can-then-be-pushed-forward-in-time-by-predicting-its-trajectory-from-t--Ï„-to-t-in-particular-pzti0tÏ„-can-be-predicted-by-the-same-internal-model-by-using-the-state-transition-at-the-time-scale-of-the-delay-that-is-pztztÏ„-this-is-virtually-equivalent-to-a-motion-extrapolation-model-but-without-sensory-measurements-during-the-time-window-between-t--Ï„-and-t-note-that-both-predictions-in-this-model-are-based-on-the-same-model-of-state-transitions-b-one-can-write-a-second-equivalent-pull-mode-for-the-diagonal-model-now-the-current-state-is-directly-estimated-based-on-a-markov-chain-on-the-sequence-of-delayed-estimations-while-being-equivalent-to-the-push-mode-described-above-such-a-direct-computation-allows-to-more-easily-combine-information-from-areas-with-different-delays-such-a-model-implements-nijhawans-diagonal-model-but-now-motion-information-is-probabilistic-and-therefore-inferred-motion-may-be-modulated-by-the-respective-precisions-of-the-sensory-and-internal-representations-c-such-a-diagonal-delay-compensation-can-be-demonstrated-in-a-two-layered-neural-network-including-a-source-input-and-a-target-predictive-layer-44-the-source-layer-receives-the-delayed-sensory-information-and-encodes-both-position-and-velocity-topographically-within-the-different-retinotopic-maps-of-each-layer-for-the-sake-of-simplicity-we-illustrate-only-one-2d-map-of-the-motions-x-v-the-integration-of-coherent-information-can-either-be-done-in-the-source-layer-push-mode-or-in-the-target-layer-pull-mode-crucially-to-implement-a-delay-compensation-in-this-motion-based-prediction-model-one-may-simply-connect-each-source-neuron-to-a-predictive-neuron-corresponding-to-the-corrected-position-of-stimulus-x--v--Ï„-v-in-the-target-layer-the-precision-of-this-anisotropic-connectivity-map-can-be-tuned-by-the-width-of-convergence-from-the-source-to-the-target-populations-using-such-a-simple-mapping-we-have-previously-shown-that-the-neuronal-population-activity-can-infer-the-current-position-along-the-trajectory-despite-the-existence-of-neural-delays&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=info:doi/10.1371/journal.pcbi.1005068.g002&#34; alt=&#34; Fig 2. *Diagonal Markov chain.* In the current study, the estimated state vector z = {x, y, u, v} is composed of the 2D position (x and y) and velocity (u and v) of a (moving) stimulus. (A) First, we extend a classical Markov chain using Nijhawanâ€™s diagonal model in order to take into account the known neural delay Ï„: At time t, information is integrated until time t âˆ’ Ï„, using a Markov chain and a model of state transitions p(zt|ztâˆ’Î´t) such that one can infer the state until the last accessible information p(ztâˆ’Ï„|I0:tâˆ’Ï„). This information can then be â€œpushedâ€ forward in time by predicting its trajectory from t âˆ’ Ï„ to t. In particular p(zt|I0:tâˆ’Ï„) can be predicted by the same internal model by using the state transition at the time scale of the delay, that is, p(zt|ztâˆ’Ï„). This is virtually equivalent to a motion extrapolation model but without sensory measurements during the time window between t âˆ’ Ï„ and t. Note that both predictions in this model are based on the same model of state transitions. (B) One can write a second, equivalent â€œpullâ€ mode for the diagonal model. Now, the current state is directly estimated based on a Markov chain on the sequence of delayed estimations. While being equivalent to the push-mode described above, such a direct computation allows to more easily combine information from areas with different delays. Such a model implements Nijhawanâ€™s â€œdiagonal modelâ€, but now motion information is probabilistic and therefore, inferred motion may be modulated by the respective precisions of the sensory and internal representations. (C) Such a diagonal delay compensation can be demonstrated in a two-layered neural network including a source (input) and a target (predictive) layer [44]. The source layer receives the delayed sensory information and encodes both position and velocity topographically within the different retinotopic maps of each layer. For the sake of simplicity, we illustrate only one 2D map of the motions (x, v). The integration of coherent information can either be done in the source layer (push mode) or in the target layer (pull mode). Crucially, to implement a delay compensation in this motion-based prediction model, one may simply connect each source neuron to a predictive neuron corresponding to the corrected position of stimulus (x &amp;#43; v â‹… Ï„, v) in the target layer. The precision of this anisotropic connectivity map can be tuned by the width of convergence from the source to the target populations. Using such a simple mapping, we have previously shown that the neuronal population activity can infer the current position along the trajectory despite the existence of neural delays. &#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 2. &lt;em&gt;Diagonal Markov chain.&lt;/em&gt; In the current study, the estimated state vector z = {x, y, u, v} is composed of the 2D position (x and y) and velocity (u and v) of a (moving) stimulus. (A) First, we extend a classical Markov chain using Nijhawanâ€™s diagonal model in order to take into account the known neural delay Ï„: At time t, information is integrated until time t âˆ’ Ï„, using a Markov chain and a model of state transitions p(zt|ztâˆ’Î´t) such that one can infer the state until the last accessible information p(ztâˆ’Ï„|I0:tâˆ’Ï„). This information can then be â€œpushedâ€ forward in time by predicting its trajectory from t âˆ’ Ï„ to t. In particular p(zt|I0:tâˆ’Ï„) can be predicted by the same internal model by using the state transition at the time scale of the delay, that is, p(zt|ztâˆ’Ï„). This is virtually equivalent to a motion extrapolation model but without sensory measurements during the time window between t âˆ’ Ï„ and t. Note that both predictions in this model are based on the same model of state transitions. (B) One can write a second, equivalent â€œpullâ€ mode for the diagonal model. Now, the current state is directly estimated based on a Markov chain on the sequence of delayed estimations. While being equivalent to the push-mode described above, such a direct computation allows to more easily combine information from areas with different delays. Such a model implements Nijhawanâ€™s â€œdiagonal modelâ€, but now motion information is probabilistic and therefore, inferred motion may be modulated by the respective precisions of the sensory and internal representations. (C) Such a diagonal delay compensation can be demonstrated in a two-layered neural network including a source (input) and a target (predictive) layer [44]. The source layer receives the delayed sensory information and encodes both position and velocity topographically within the different retinotopic maps of each layer. For the sake of simplicity, we illustrate only one 2D map of the motions (x, v). The integration of coherent information can either be done in the source layer (push mode) or in the target layer (pull mode). Crucially, to implement a delay compensation in this motion-based prediction model, one may simply connect each source neuron to a predictive neuron corresponding to the corrected position of stimulus (x + v â‹… Ï„, v) in the target layer. The precision of this anisotropic connectivity map can be tuned by the width of convergence from the source to the target populations. Using such a simple mapping, we have previously shown that the neuronal population activity can infer the current position along the trajectory despite the existence of neural delays.
    &lt;/figcaption&gt;&lt;/figure&gt;
Processing visual information takes time and even if these delays are remarkably short, they are not negligible and the nervous system must compensate them. For an object that moves predictably, the neural network can infer its most probable position taking into account this processing time. For the flash, however, this prediction can not be established because its appearance is unpredictable. Thus, while the two targets are aligned on the retina at the time of the flash, the position of the moving object is anticipated by the brain to compensate for the processing time: it is this differentiated treatment that causes the flash-lag effect.
The researchers show that this hypothesis also makes it possible to explain the cases where this illusion does not work: for example if the flash appears at the end of the moving dot&amp;rsquo;s trajectory or if the target reverses its path in an unexpected way. In this work, the major innovation is to use the accuracy of information in the dynamics of the model. Thus, the corrected position of the moving target is calculated by combining the sensory flux with the internal representation of the trajectory, both of which exist in the form of probability distributions. To manipulate the trajectory is to change the precision and therefore the relative weight of these two information when they are optimally combined in order to know where an object is at the present time. The researchers propose to call parodiction (from the ancient Greek paron, the present) this new theory that joins Bayesian inference with taking into account neuronal delays.














&lt;figure  id=&#34;figure-fig-5-histogram-of-the-estimated-positions-as-a-function-of-time-for-the-dmbp-model-histograms-of-the-inferred-horizontal-positions-blueish-bottom-panel-and-horizontal-velocity-reddish-top-panel-as-a-function-of-time-frame-from-the-dmbp-model-darker-levels-correspond-to-higher-probabilities-while-a-light-color-corresponds-to-an-unlikely-estimation-we-highlight-three-successive-epochs-along-the-trajectory-corresponding-to-the-flash-initiated-standard-mid-point-and-flash-terminated-cycles-the-timing-of-the-flashes-are-respectively-indicated-by-the-dashed-vertical-lines-in-dark-the-physical-time-and-in-green-the-delayed-input-knowing-Ï„--100-ms-histograms-are-plotted-at-two-different-levels-of-our-model-in-the-push-mode-the-left-hand-column-illustrates-the-source-layer-that-corresponds-to-the-integration-of-delayed-sensory-information-including-the-prior-on-motion-the-right-hand-illustrates-the-target-layer-corresponding-to-the-same-information-but-after-the-occurrence-of-some-motion-extrapolation-compensating-for-the-known-neural-delay-Ï„&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=10.1371/journal.pcbi.1005068.g005&#34; alt=&#34;Fig 5. *Histogram of the estimated positions as a function of time for the dMBP model.* Histograms of the inferred horizontal positions (blueish bottom panel) and horizontal velocity (reddish top panel), as a function of time frame, from the dMBP model. Darker levels correspond to higher probabilities, while a light color corresponds to an unlikely estimation. We highlight three successive epochs along the trajectory, corresponding to the flash initiated, standard (mid-point) and flash terminated cycles. The timing of the flashes are respectively indicated by the dashed vertical lines. In dark, the physical time and in green the delayed input knowing Ï„ = 100 ms. Histograms are plotted at two different levels of our model in the push mode. The left-hand column illustrates the source layer that corresponds to the integration of delayed sensory information, including the prior on motion. The right-hand illustrates the target layer corresponding to the same information but after the occurrence of some motion extrapolation compensating for the known neural delay Ï„.  &#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fig 5. &lt;em&gt;Histogram of the estimated positions as a function of time for the dMBP model.&lt;/em&gt; Histograms of the inferred horizontal positions (blueish bottom panel) and horizontal velocity (reddish top panel), as a function of time frame, from the dMBP model. Darker levels correspond to higher probabilities, while a light color corresponds to an unlikely estimation. We highlight three successive epochs along the trajectory, corresponding to the flash initiated, standard (mid-point) and flash terminated cycles. The timing of the flashes are respectively indicated by the dashed vertical lines. In dark, the physical time and in green the delayed input knowing Ï„ = 100 ms. Histograms are plotted at two different levels of our model in the push mode. The left-hand column illustrates the source layer that corresponds to the integration of delayed sensory information, including the prior on motion. The right-hand illustrates the target layer corresponding to the same information but after the occurrence of some motion extrapolation compensating for the known neural delay Ï„.
    &lt;/figcaption&gt;&lt;/figure&gt;
Despite the simplicity of this solution, parodiction has elements that may seem counter-intuitive. Indeed, in this model, the physical world is considered &amp;ldquo;hidden&amp;rdquo;, that is to say, it can only be guessed by our sensations and our experience. The role of visual perception is then to deliver to our central nervous system the most likely information despite the different sources of noise, ambiguity and time delays. According to the authors of this publication, the visual treatment would consist in a &amp;ldquo;simulation&amp;rdquo; of the visual world projected at the present time, even before the visual information can actually modulate, confirm or cancel this simulation. This hypothesis, which seems to belong to &amp;ldquo;science fiction&amp;rdquo;, is being tested with more detailed and biologically plausible hierarchical neural network models that should allow us to better understand the mysteries underlying our perception. Visual illusions have still the power to amaze us!
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;New from Khoei et al. The Flash-Lag Effect as a &lt;a href=&#34;https://twitter.com/hashtag/Motion?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Motion&lt;/a&gt;-Based Predictive Shift &lt;a href=&#34;https://t.co/K3KWPO8l4a&#34;&gt;https://t.co/K3KWPO8l4a&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/neuralnetworks?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neuralnetworks&lt;/a&gt; &lt;a href=&#34;https://t.co/iWsd9nK5qp&#34;&gt;pic.twitter.com/iWsd9nK5qp&lt;/a&gt;&lt;/p&gt;&amp;mdash; PLOS Comp Biol (@PLOSCompBiol) &lt;a href=&#34;https://twitter.com/PLOSCompBiol/status/829474896023474176?ref_src=twsrc%5Etfw&#34;&gt;February 8, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Estimating and anticipating a dynamic probabilistic bias in visual motion direction</title>
      <link>https://laurentperrinet.github.io/publication/pasturel-17-gdr/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/pasturel-17-gdr/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see a write-up in &amp;ldquo;&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&lt;/a&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Anticipating a moving target: role of vision and reinforcement</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-15-sfn/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-15-sfn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Eye tracking a self-moved target with complex hand-target dynamics</title>
      <link>https://laurentperrinet.github.io/publication/danion-15-sfn/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/danion-15-sfn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active inference, eye movements and oculomotor delays</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</link>
      <pubDate>Tue, 16 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/</guid>
      <description>&lt;h1 id=&#34;active-inference-tracking-eye-movements-and-oculomotor-delays&#34;&gt;Active Inference, tracking eye movements and oculomotor delays&lt;/h1&gt;
&lt;p&gt;Tracking eye movements face a difficult task: they have to be fast while they suffer inevitable delays. If we focus on area MT of humans for instance as it is crucial for detecting the motion of visual objects, sensory information coming to this area is already lagging some 35 milliseconds behind operational time â€“ that is, it reflects some past information. Still the fastest action that may be done there is only able to reach the effector muscles of the eyes some 40 milliseconds later â€“ that is, in the future. The tracking eye movement system is however able to respond swiftly and even to anticipate repetitive movements (e.g. Barnes et al, 2000 â€“ refs in manuscript). In that case, it means that information in a cortical area is both predicted from the past sensory information but also anticipated to give an optimal response in the future. Even if numerous models have been described to model different mechanisms to account for delays, no theoretical approach has tackled the whole problem explicitly. In several areas of vision research, authors have proposed models at different levels of abstractions from biomechanical models, to neurobiological implementations (e.g. Robinson, 1986) or Bayesian models. This study is both novel and important because â€“ using a neurobiologically plausible hierarchical Bayesian model â€“ it demonstrates that using generalized coordinates to finesse the prediction of a target&amp;rsquo;s motion, the model can reproduce characteristic properties of tracking eye movements in the presence of delays. Crucially, the different refinements to the model that we propose â€“ pursuit initiation, smooth pursuit eye movements, and anticipatory response â€“ are consistent with the different types of tracking eye movements that may be observed experimentally.














&lt;figure  id=&#34;figure-a-this-figure-reports-the-response-of-predictive-processing-during-the-simulation-of-pursuit-initiation-using-a-single-sweep-of-a-visual-target-while-compensating-for-sensory-motor-delays-here-we-see-horizontal-excursions-of-oculomotor-angle-red-line-one-can-see-clearly-the-initial-displacement-of-the-target-that-is-suppressed-by-action-after-a-few-hundred-milliseconds-additionally-we-illustrate-the-effects-of-assuming-wrong-sensorimotor-delays-on-pursuit-initiation-under-pure-sensory-delays-blue-dotted-line-one-can-see-clearly-the-delay-in-sensory-predictions-in-relation-to-the-true-inputs-with-pure-motor-delays-blue-dashed-line-and-with-combined-sensorimotor-delays-blue-line-there-is-a-failure-of-optimal-control-with-oscillatory-fluctuations-in-oculomotor-trajectories-which-may-become-unstable-b-this-figure-reports-the-simulation-of-smooth-pursuit-when-the-target-motion-is-hemi-sinusoidal-as-would-happen-for-a-pendulum-that-would-be-stopped-at-each-half-cycle-left-of-the-vertical-broken-black-lines-in-the-lower-right-panel-we-report-the-horizontal-excursions-of-oculomotor-angle-the-generative-model-used-here-has-been-equipped-with-a-second-hierarchical-level-that-contains-hidden-states-modeling-latent-periodic-behavior-of-the-hidden-causes-of-target-motion-with-this-addition-the-improvement-in-pursuit-accuracy-apparent-at-the-onset-of-the-second-cycle-of-motion-is-observed-pink-shaded-area-similar-to-psychophysical-experimentss&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;**(A)** This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. **(B)** This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.&#34; srcset=&#34;
               /publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_27ce3f4d3d3663df7ec7c64b71098f53.webp 400w,
               /publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_77a63aed175a4835e5038faf72152492.webp 760w,
               /publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_27ce3f4d3d3663df7ec7c64b71098f53.webp&#34;
               width=&#34;760&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;(A)&lt;/strong&gt; This figure reports the response of predictive processing during the simulation of pursuit initiation, using a single sweep of a visual target, while compensating for sensory motor delays. Here, we see horizontal excursions of oculomotor angle (red line). One can see clearly the initial displacement of the target that is suppressed by action after a few hundred milliseconds. Additionally, we illustrate the effects of assuming wrong sensorimotor delays on pursuit initiation. Under pure sensory delays (blue dotted line), one can see clearly the delay in sensory predictions, in relation to the true inputs. With pure motor delays (blue dashed line) and with combined sensorimotor delays (blue line) there is a failure of optimal control with oscillatory fluctuations in oculomotor trajectories, which may become unstable. &lt;strong&gt;(B)&lt;/strong&gt; This figure reports the simulation of smooth pursuit when the target motion is hemi-sinusoidal, as would happen for a pendulum that would be stopped at each half cycle left of the vertical (broken black lines in the lower-right panel). We report the horizontal excursions of oculomotor angle. The generative model used here has been equipped with a second hierarchical level that contains hidden states, modeling latent periodic behavior of the (hidden) causes of target motion. With this addition, the improvement in pursuit accuracy apparent at the onset of the second cycle of motion is observed (pink shaded area), similar to psychophysical experimentss.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</link>
      <pubDate>Fri, 25 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-04-25-kaplan-beijing/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see &lt;a href=&#34;https://laurentperrinet.github.io/publication/kaplan-khoei-14/&#34;&gt;Kaplan and al, 2014&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Axonal delays and on-time control of eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2014-01-10-int-fest/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</title>
      <link>https://laurentperrinet.github.io/publication/kaplan-khoei-14/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kaplan-khoei-14/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1208.6471&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;see  follow-up on motion extrapolation: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2013).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Motion-based prediction explains the role of tracking in motion extrapolation&lt;/a&gt;.
   &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/khoei-13-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-13-jpp/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2013.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2017).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/&#34; target=&#34;_blank&#34;&gt;
     Slides
   &lt;/a&gt;
   
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 















&lt;figure  id=&#34;figure-figure-4-rasterplot-of-input-and-output-spikes-the-raster-plot-from-excitatory-neurons-is-ordered-according-to-their-position-each-input-spike-is-a-blue-dot-and-each-output-spike-is-a-black-dot-while-input-is-scattered-during-blanking-periods-figure-1-the-network-output-shows-shows-some-tuned-activity-during-the-blank-compare-with-the-activity-before-visual-stimulation-to-decode-such-patterns-of-activity-we-used-a-maximum-likelihood-estimation-technique-based-on-the-tuning-curve-of-the-neurons&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.frontiersin.org/files/Articles/53894/fncom-07-00112-r2/image_m/fncom-07-00112-g003.jpg&#34; alt=&#34;Figure 4: *Rasterplot of input and output spikes.* The raster plot from excitatory neurons is ordered according to their position. Each input spike is a blue dot and each output spike is a black dot. While input is scattered during blanking periods (Figure 1), the network output shows shows some tuned activity during the blank (compare with the activity before visual stimulation). To decode such patterns of activity we used a maximum-likelihood estimation technique based on the tuning curve of the neurons.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 4: &lt;em&gt;Rasterplot of input and output spikes.&lt;/em&gt; The raster plot from excitatory neurons is ordered according to their position. Each input spike is a blue dot and each output spike is a black dot. While input is scattered during blanking periods (Figure 1), the network output shows shows some tuned activity during the blank (compare with the activity before visual stimulation). To decode such patterns of activity we used a maximum-likelihood estimation technique based on the tuning curve of the neurons.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Anisotropic connectivity implements motion-based prediction in a spiking neural network</title>
      <link>https://laurentperrinet.github.io/publication/kaplan-13/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/kaplan-13/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1208.6471&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;see  follow-up on motion extrapolation: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2013).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Motion-based prediction explains the role of tracking in motion extrapolation&lt;/a&gt;.
   &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/khoei-13-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-13-jpp/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2013.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2017).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/&#34; target=&#34;_blank&#34;&gt;
     Slides
   &lt;/a&gt;
   
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 

lication/khoei-13-jpp&amp;quot; view=&amp;ldquo;4&amp;rdquo; &amp;gt;}}&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction and development of the response to an &#39;on the way&#39; stimulus</title>
      <link>https://laurentperrinet.github.io/publication/khoei-13-cns/</link>
      <pubDate>Tue, 01 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-13-cns/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1208.6471&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;see  follow-up on motion extrapolation: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2013).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Motion-based prediction explains the role of tracking in motion extrapolation&lt;/a&gt;.
   &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/khoei-13-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-13-jpp/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2013.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2017).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/&#34; target=&#34;_blank&#34;&gt;
     Slides
   &lt;/a&gt;
   
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Smooth Pursuit and Visual Occlusion: Active Inference and Oculomotor Control in Schizophrenia</title>
      <link>https://laurentperrinet.github.io/publication/adams-12/</link>
      <pubDate>Fri, 26 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/adams-12/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;header&#34; srcset=&#34;
               /publication/adams-12/adams-12_hu29c32926b1b95830ea26f1d656d475c2_81970_f0e20b887dac8e35bf119b319a408395.webp 400w,
               /publication/adams-12/adams-12_hu29c32926b1b95830ea26f1d656d475c2_81970_10c5d0165bce8f157a46b44af988de65.webp 760w,
               /publication/adams-12/adams-12_hu29c32926b1b95830ea26f1d656d475c2_81970_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/adams-12/adams-12_hu29c32926b1b95830ea26f1d656d475c2_81970_f0e20b887dac8e35bf119b319a408395.webp&#34;
               width=&#34;760&#34;
               height=&#34;188&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grabbing, tracking and sniffing as models for motion detection and eye movements</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-27-fil/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-27-fil/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</link>
      <pubDate>Thu, 12 Jan 2012 17:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/publication/masson-12-areadne/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/masson-12-areadne/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Motion-based prediction is sufficient to solve the aperture problem</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-12-pred/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-12-pred/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;header&#34; srcset=&#34;
               /publication/perrinet-12-pred/perrinet-12-pred_hud71ae606cf067d7c8c576f18c76b5351_58874_f17d8c274c1e3263d901592e3d3f6c88.webp 400w,
               /publication/perrinet-12-pred/perrinet-12-pred_hud71ae606cf067d7c8c576f18c76b5351_58874_1a6d6d92bba387f3e75424f76c95dfc4.webp 760w,
               /publication/perrinet-12-pred/perrinet-12-pred_hud71ae606cf067d7c8c576f18c76b5351_58874_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred_hud71ae606cf067d7c8c576f18c76b5351_58874_f17d8c274c1e3263d901592e3d3f6c88.webp&#34;
               width=&#34;661&#34;
               height=&#34;301&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;















&lt;figure  id=&#34;figure-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-receptive-field-of-a-neuron-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-introducing-predictive-coding-resolves-the-aperture-problem&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: itâ€™s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.&#34;
           src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/line_particles.gif&#34;
           loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the receptive field of a neuron) leads to ambiguous velocity measurements compared to physical motion: itâ€™s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Introducing predictive coding resolves the aperture problem.
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-1-a-the-estimation-of-the-motion-of-an-elongated-slanted-segment-here-moving-horizontally-to-the-right-on-a-limited-area-such-as-the-dotted-circle-leads-to-ambiguous-velocity-measurements-compared-to-physical-motion-its-the-aperture-problem-we-represent-as-arrows-the-velocity-vectors-that-are-most-likely-detected-by-a-motion-energy-model-hue-indicates-direction-angle-due-to-the-limited-size-of-receptive-fields-in-sensory-cortical-areas-such-as-shown-by-the-dotted-white-circle-such-problem-is-faced-by-local-populations-of-neurons-that-visually-estimate-the-motion-of-objects-a-inset-on-a-polar-representation-of-possible-velocity-vectors-the-cross-in-the-center-corresponds-to-the-null-velocity-the-outer-circle-corresponding-to-twice-the-amplitude-of-physical-speed-we-plot-the-empirical-histogram-of-detected-velocity-vectors-this-representation-gives-a-quantification-of-the-aperture-problem-in-the-velocity-domain-at-the-onset-of-motion-detection-information-is-concentrated-along-an-elongated-constraint-line-whitehigh-probability-blackzero-probability-b-we-use-the-prior-knowledge-that-in-natural-scenes-motion-as-defined-by-its-position-and-velocity-is-following-smooth-trajectories-quantitatively-it-means-that-velocity-is-approximately-conserved-and-that-position-is-transported-according-to-the-known-velocity-we-show-here-such-a-transition-on-position-and-velocity-respectively-x_t-and-v_t-from-time-t-to-t--dt-with-the-perturbation-modeling-the-smoothness-of-prediction-in-position-and-velocity-respectively-n_x-and-n_v-c-applying-such-a-prior-on-a-dynamical-system-detecting-motion-we-show-that-motion-converges-to-the-physical-motion-after-approximately-one-spatial-period-the-line-moved-by-twice-its-height-c-inset-the-read-out-of-the-system-converged-to-the-physical-motion-motion-based-prediction-is-sufficient-to-resolve-the-aperture-problem-d-as-observed-at-the-perceptual-level-castet-et-al-1993-pei-et-al-2010-size-and-duration-of-the-tracking-angle-bias-decreased-with-respect-to-the-height-of-the-line-height-was-measured-relative-to-a-spatial-period-respectively-60-40-and-20-here-we-show-the-average-tracking-angle-red-out-from-the-probabilistic-representation-as-a-function-of-time-averaged-over-20-trials-error-bars-show-one-standard-deviation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 1: *(A)* The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: itâ€™s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. *(A-inset)* On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). *(B)* We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t &amp;#43; dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). *(C)* Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). *(C-Inset)* The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. *(D)* As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_dc88887ede72c5299d48bb77570e20e9.webp 400w,
               /publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_8892f9cba58fa93a66654c36d23d62d4.webp 760w,
               /publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure1_hu50c6c0e86a1c3fa8f91372ed4700c982_114936_dc88887ede72c5299d48bb77570e20e9.webp&#34;
               width=&#34;80%&#34;
               height=&#34;717&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: &lt;em&gt;(A)&lt;/em&gt; The estimation of the motion of an elongated, slanted segment (here moving horizontally to the right) on a limited area (such as the dotted circle) leads to ambiguous velocity measurements compared to physical motion: itâ€™s the aperture problem. We represent as arrows the velocity vectors that are most likely detected by a motion energy model; hue indicates direction angle. Due to the limited size of receptive fields in sensory cortical areas (such as shown by the dotted white circle), such problem is faced by local populations of neurons that visually estimate the motion of objects. &lt;em&gt;(A-inset)&lt;/em&gt; On a polar representation of possible velocity vectors (the cross in the center corresponds to the null velocity, the outer circle corresponding to twice the amplitude of physical speed), we plot the empirical histogram of detected velocity vectors. This representation gives a quantification of the aperture problem in the velocity domain: At the onset of motion detection, information is concentrated along an elongated constraint line (white=high probability, black=zero probability). &lt;em&gt;(B)&lt;/em&gt; We use the prior knowledge that in natural scenes, motion as defined by its position and velocity is following smooth trajectories. Quantitatively, it means that velocity is approximately conserved and that position is transported according to the known velocity. We show here such a transition on position and velocity (respectively $x_t$ and $V_t$) from time t to t + dt with the perturbation modeling the smoothness of prediction in position and velocity (respectively $N_x$ and $N_V$). &lt;em&gt;(C)&lt;/em&gt; Applying such a prior on a dynamical system detecting motion, we show that motion converges to the physical motion after approximately one spatial period (the line moved by twice its height). &lt;em&gt;(C-Inset)&lt;/em&gt; The read-out of the system converged to the physical motion: Motion-based prediction is sufficient to resolve the aperture problem. &lt;em&gt;(D)&lt;/em&gt; As observed at the perceptual level [Castet et al., 1993, Pei et al., 2010], size and duration of the tracking angle bias decreased with respect to the height of the line. Height was measured relative to a spatial period (respectively 60%, 40% and 20%). Here we show the average tracking angle red-out from the probabilistic representation as a function of time, averaged over 20 trials (error bars show one standard deviation).
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-2-architecture-of-the-model-the-model-is-constituted-by-a-classical-measurement-stage-and-of-a-predictive-coding-layer-the-measurement-stage-consists-of-a-inferring-from-two-consecutive-frames-of-the-input-flow-b-a-likelihood-distribution-of-motion-this-layer-interacts-with-the-predictive-layer-which-consists-of-c-a-prediction-stage-that-infers-from-the-current-estimate-and-the-transition-prior-the-upcoming-state-estimate-and-d-an-estimation-stage-that-merges-the-current-prediction-of-motion-with-the-likelihood-measured-at-the-same-instant-in-the-previous-layer-b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_5b7bd70e19958712cea8e1ce73ffaf07.webp 400w,
               /publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_7826c1e83d833e0ac9c83616aefcb452.webp 760w,
               /publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure2_hua58f2c4d512bd134e26bfd0c1b490549_114366_5b7bd70e19958712cea8e1ce73ffaf07.webp&#34;
               width=&#34;80%&#34;
               height=&#34;695&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 2: Architecture of the model. The model is constituted by a classical measurement stage and of a predictive coding layer. The measurement stage consists of (A) inferring from two consecutive frames of the input flow, (B) a likelihood distribution of motion. This layer interacts with the predictive layer which consists of (C) a prediction stage that infers from the current estimate and the transition prior the upcoming state estimate and (D) an estimation stage that merges the current prediction of motion with the likelihood measured at the same instant in the previous layer (B).
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-3-to-explore-the-state-space-of-the-dynamical-system-we-simulated-motion-based-prediction-for-a-simple-small-dot-size-25-of-a-spatial-period-moving-horizontally-from-the-left-to-the-right-of-the-screen-we-tested-different-levels-of-sensory-noise-with-respect-to-different-levels-of-internal-noise-that-is-to-different-values-of-the-strength-of-prediction-right-results-show-the-emergence-of-different-states-for-different-prediction-precisions-a-regime-when-prediction-is-weak-and-which-shows-high-tracking-error-and-variability-no-tracking---nt-a-phase-for-intermediate-values-of-prediction-strength-as-in-figure-1-exhibiting-a-low-tracking-error-and-low-variability-in-the-tracking-phase-true-tracking---tt-and-finally-a-phase-corresponding-to-higher-precisions-with-relatively-efficient-mean-detection-but-high-variability-false-tracking---ft-we-give-3-representative-examples-of-the-emerging-states-at-one-contrast-level-c--01-with-starting-red-and-ending-blue-points-and-respectively-nt-tt-and-ft-by-showing-inferred-trajectories-for-each-trial-left-we-define-tracking-error-as-the-ratio-between-detected-speed-and-target-speed-and-we-plot-it-with-respect-to-the-stimulus-contrast-as-given-by-the-inverse-of-sensory-noise-error-bars-give-the-variability-in-tracking-error-as-averaged-over-20-trials-as-prediction-strength-increases-there-is-a-transition-from-smooth-contrast-response-function-nt-to-more-binary-responses-tt-and-ft&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. *(Right)* Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. *(Left)* We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_be53ee557e3c5e579dcc942d70c78694.webp 400w,
               /publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_f9cacd172659838b060d2a93f526799a.webp 760w,
               /publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure3_hub57ab3bf4b1a67c5e3a894ed7253439e_82715_be53ee557e3c5e579dcc942d70c78694.webp&#34;
               width=&#34;80%&#34;
               height=&#34;483&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 3: To explore the state-space of the dynamical system, we simulated motion-based prediction for a simple small dot (size 2.5% of a spatial period) moving horizontally from the left to the right of the screen. We tested different levels of sensory noise with respect to different levels of internal noise, that is, to different values of the strength of prediction. &lt;em&gt;(Right)&lt;/em&gt; Results show the emergence of different states for different prediction precisions: a regime when prediction is weak and which shows high tracking error and variability (No Tracking - NT), a phase for intermediate values of prediction strength (as in Figure 1) exhibiting a low tracking error and low variability in the tracking phase (True Tracking - TT) and finally a phase corresponding to higher precisions with relatively efficient mean detection but high variability (False Tracking - FT). We give 3 representative examples of the emerging states at one contrast level (C = 0.1) with starting (red) and ending (blue) points and respectively NT, TT and FT by showing inferred trajectories for each trial. &lt;em&gt;(Left)&lt;/em&gt; We define tracking error as the ratio between detected speed and target speed and we plot it with respect to the stimulus contrast as given by the inverse of sensory noise. Error bars give the variability in tracking error as averaged over 20 trials. As prediction strength increases, there is a transition from smooth contrast response function (NT) to more binary responses (TT and FT).
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-figure-4-top-prediction-implements-a-competition-between-different-trajectories-here-we-focus-on-one-step-of-the-algorithm-by-testing-different-trajectories-at-three-key-positions-of-the-segment-stimulus-the-two-edges-and-the-center-dashed-circles-compared-to-the-pure-sensory-velocity-likelihood-left-insets-in-grayscale-prediction-modulates-response-as-shown-by-the-velocity-vectors-direction-coded-as-hue-as-in-figure-1-and-by-the-ratio-of-velocity-probabilities-log-ratio-in-bits-right-insets-there-is-no-change-for-the-middle-of-the-segment-yellow-tone-but-trajectories-that-are-predicted-out-of-the-line-are-explained-away-navy-tone-while-others-may-be-amplified-orange-tone-notice-the-asymmetry-between-both-edges-the-upper-edge-carrying-a-suppressive-predictive-information-while-the-bottom-edge-diffuses-coherent-motion-bottom-finally-the-aperture-problem-is-solved-due-to-the-repeated-application-of-this-spatio-temporal-contextual-information-modulation-to-highlight-the-anisotropic-diffusion-of-information-over-the-rest-of-the-line-we-plot-as-a-function-of-time-horizontal-axis-the-histogram-of-the-detected-motion-marginalized-over-horizontal-positions-vertical-axis-while-detected-direction-of-velocity-is-given-by-the-distribution-of-hues-blueish-colors-correspond-to-the-direction-perpendicular-to-the-diagonal-while-a-green-color-represents-a-disambiguated-motion-to-the-right-as-in-figure-1-the-plot-shows-that-motion-is-disambiguated-by-progressively-explaining-away-incoherent-motion-note-the-asymmetry-in-the-propagation-of-coherent-information&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 4: *(Top)* Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are â€œexplained awayâ€ (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. *(Bottom)* Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.&#34; srcset=&#34;
               /publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_17a1fffaec91d838e8b3896507d9f848.webp 400w,
               /publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_c3205db28a64702af8391f5826aa922c.webp 760w,
               /publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/figure4_hu0046952bdf6ba50e9f2bcf443e1786e1_145753_17a1fffaec91d838e8b3896507d9f848.webp&#34;
               width=&#34;80%&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 4: &lt;em&gt;(Top)&lt;/em&gt; Prediction implements a competition between different trajectories. Here, we focus on one step of the algorithm by testing different trajectories at three key positions of the segment stimulus: the two edges and the center (dashed circles). Compared to the pure sensory velocity likelihood (left insets in grayscale), prediction modulates response as shown by the velocity vectors (direction coded as hue as in Figure 1) and by the ratio of velocity probabilities (log ratio in bits, right insets). There is no change for the middle of the segment (yellow tone), but trajectories that are predicted out of the line are â€œexplained awayâ€ (navy tone) while others may be amplified (orange tone). Notice the asymmetry between both edges, the upper edge carrying a suppressive predictive information while the bottom edge diffuses coherent motion. &lt;em&gt;(Bottom)&lt;/em&gt; Finally, the aperture problem is solved due to the repeated application of this spatio-temporal contextual information modulation. To highlight the anisotropic diffusion of information over the rest of the line, we plot as a function of time (horizontal axis) the histogram of the detected motion marginalized over horizontal positions (vertical axis), while detected direction of velocity is given by the distribution of hues. Blueish colors correspond to the direction perpendicular to the diagonal while a green color represents a disambiguated motion to the right (as in Figure 1). The plot shows that motion is disambiguated by progressively explaining away incoherent motion. Note the asymmetry in the propagation of coherent information.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Perceptions as Hypotheses: Saccades as Experiments</title>
      <link>https://laurentperrinet.github.io/publication/friston-12/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/friston-12/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;header&#34; srcset=&#34;
               /publication/friston-12/friston-12_hu2d7533dc5725a34acf278029fda9d270_136523_b1b06c764894f0e47a856b49a62ca45a.webp 400w,
               /publication/friston-12/friston-12_hu2d7533dc5725a34acf278029fda9d270_136523_1576e7d2c89800dc30e346d6608b38d5.webp 760w,
               /publication/friston-12/friston-12_hu2d7533dc5725a34acf278029fda9d270_136523_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/friston-12/friston-12_hu2d7533dc5725a34acf278029fda9d270_136523_b1b06c764894f0e47a856b49a62ca45a.webp&#34;
               width=&#34;760&#34;
               height=&#34;196&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;















&lt;figure  id=&#34;figure-this-schematic-shows-the-dependencies-among-various-quantities-that-are-assumed-when-modeling-the-exchanges-of-a-self-organizing-system-like-the-brain-with-the-environment-the-top-panel-describes-the-states-of-the-environment-and-the-system-or-agent-in-terms-of-a-probabilistic-dependency-graph-where-connections-denote-directed-dependencies-the-quantities-are-described-within-the-nodes-of-this-graph-with-exemplar-forms-for-their-dependencies-on-other-variables-see-main-text-here-hidden-and-internal-states-are-separated-by-action-and-sensory-states-both-action-and-internal-states-encoding-a-conditional-density-minimize-free-energy-while-internal-states-encoding-prior-beliefs-maximize-salience-both-free-energy-and-salience-are-defined-in-terms-of-a-generative-model-that-is-shown-as-fictive-dependency-graph-in-the-lower-panel-note-that-the-variables-in-the-real-world-and-the-form-of-their-dynamics-are-different-from-that-assumed-by-the-generative-model-this-is-why-external-states-are-in-bold-furthermore-note-that-action-is-a-state-in-the-model-of-the-brain-but-is-replaced-by-hidden-controls-in-the-brains-model-of-its-world-this-means-that-the-agent-is-not-aware-of-action-but-has-beliefs-about-hidden-causes-in-the-world-that-action-can-fulfill-through-minimizing-free-energy-these-beliefs-correspond-to-prior-expectations-that-sensory-states-will-be-sampled-in-a-way-that-optimizes-conditional-confidence-or-salience&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.frontiersin.org/files/Articles/21922/fpsyg-03-00151-r4/image_m/fpsyg-03-00151-g001.jpg&#34; alt=&#34;**This schematic shows the dependencies among various quantities that are assumed when modeling the exchanges of a self organizing system like the brain with the environment.** The top panel describes the states of the environment and the system or agent in terms of a probabilistic dependency graph, where connections denote directed dependencies. The quantities are described within the nodes of this graph with exemplar forms for their dependencies on other variables (see main text). Here, hidden and internal states are separated by action and sensory states. Both action and internal states encoding a conditional density minimize free energy, while internal states encoding prior beliefs maximize salience. Both free energy and salience are defined in terms of a generative model that is shown as fictive dependency graph in the lower panel. Note that the variables in the real world and the form of their dynamics are different from that assumed by the generative model; this is why external states are in bold. Furthermore, note that action is a state in the model of the brain but is replaced by hidden controls in the brainâ€™s model of its world. This means that the agent is not aware of action but has beliefs about hidden causes in the world that action can fulfill through minimizing free energy. These beliefs correspond to prior expectations that sensory states will be sampled in a way that optimizes conditional confidence or salience.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;This schematic shows the dependencies among various quantities that are assumed when modeling the exchanges of a self organizing system like the brain with the environment.&lt;/strong&gt; The top panel describes the states of the environment and the system or agent in terms of a probabilistic dependency graph, where connections denote directed dependencies. The quantities are described within the nodes of this graph with exemplar forms for their dependencies on other variables (see main text). Here, hidden and internal states are separated by action and sensory states. Both action and internal states encoding a conditional density minimize free energy, while internal states encoding prior beliefs maximize salience. Both free energy and salience are defined in terms of a generative model that is shown as fictive dependency graph in the lower panel. Note that the variables in the real world and the form of their dynamics are different from that assumed by the generative model; this is why external states are in bold. Furthermore, note that action is a state in the model of the brain but is replaced by hidden controls in the brainâ€™s model of its world. This means that the agent is not aware of action but has beliefs about hidden causes in the world that action can fulfill through minimizing free energy. These beliefs correspond to prior expectations that sensory states will be sampled in a way that optimizes conditional confidence or salience.
    &lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Role of motion-based prediction in motion extrapolation</title>
      <link>https://laurentperrinet.github.io/publication/khoei-12-sfn/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-12-sfn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1208.6471&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;see  follow-up on motion extrapolation: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2013).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Motion-based prediction explains the role of tracking in motion extrapolation&lt;/a&gt;.
   &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/khoei-13-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-13-jpp/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2013.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2017).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/&#34; target=&#34;_blank&#34;&gt;
     Slides
   &lt;/a&gt;
   
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PropriÃ©tÃ©s Ã©mergentes d&#39;un modÃ¨le de prÃ©diction probabiliste utilisant un champ neural</title>
      <link>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</link>
      <pubDate>Sat, 02 Jul 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2011-07-02-neuro-med-talk/</guid>
      <description>&lt;p&gt;La finalitÃ© de cette manifestation est de permettre Ã  nos chercheurs de se rÃ©unir en groupes de travail et en ateliers afin de dÃ©couvrir la thÃ©matique des neurosciences et son interdisciplinaritÃ©. La manifestation se tient dans le cadre des activitÃ©s du laboratoire LAMS, de ABC MATHINFO, du GDRI NeurO et du rÃ©seau mÃ©diterranÃ©en &lt;a href=&#34;http://www.neuromedproject.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroMed&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;SPIE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Pursuing motion illusions: a realistic oculomotor framework for Bayesian inference</title>
      <link>https://laurentperrinet.github.io/publication/bogadhi-11/</link>
      <pubDate>Fri, 22 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/bogadhi-11/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;header&#34; srcset=&#34;
               /publication/bogadhi-11/bogadhi-11_hu80486606c0922d9e2f48872699e8bedf_125662_125b92f100a4d5ba05ac94ef24eb6b42.webp 400w,
               /publication/bogadhi-11/bogadhi-11_hu80486606c0922d9e2f48872699e8bedf_125662_f35ae281c0551bb3bb9e57feffd61aa1.webp 760w,
               /publication/bogadhi-11/bogadhi-11_hu80486606c0922d9e2f48872699e8bedf_125662_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/bogadhi-11/bogadhi-11_hu80486606c0922d9e2f48872699e8bedf_125662_125b92f100a4d5ba05ac94ef24eb6b42.webp&#34;
               width=&#34;760&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;See a followup in 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1208.6471&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Role of motion inertia in dynamic motion integration for smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/khoei-11-ecvp/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-11-ecvp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1208.6471&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;see  follow-up on motion extrapolation: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2013).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Motion-based prediction explains the role of tracking in motion extrapolation&lt;/a&gt;.
   &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/khoei-13-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-13-jpp/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2013.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2017).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/&#34; target=&#34;_blank&#34;&gt;
     Slides
   &lt;/a&gt;
   
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic models of the low-level visual system: the role of prediction in detecting motion</title>
      <link>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</link>
      <pubDate>Fri, 17 Dec 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-12-17-tauc-talk/</guid>
      <description>&lt;p&gt;An event ranging &amp;ldquo;From Mathematical Image Analysis to Neurogeometry of the Brain&amp;rdquo; &lt;a href=&#34;http://www.conftauc.cnrs-gif.fr/programme.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LADISLAV TAUC &amp;amp; GDR MSPC NEUROSCIENCES CONFERENCE&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;related publication from Mina Khoei @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-10-tauc/&#34;&gt;TAUC 2012&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Models of low-level vision: linking probabilistic models and neural masses</title>
      <link>https://laurentperrinet.github.io/talk/2010-01-08-facets/</link>
      <pubDate>Fri, 08 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2010-01-08-facets/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;see this more recent talk @ &lt;a href=&#34;https://laurentperrinet.github.io/talk/2012-01-12-vision-at-ucl/&#34;&gt;UCL, London&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A recurrent Bayesian model of dynamic motion integration for smooth pursuit</title>
      <link>https://laurentperrinet.github.io/publication/bogadhi-10-vss/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/bogadhi-10-vss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical emergence of a neural solution for motion integration</title>
      <link>https://laurentperrinet.github.io/publication/khoei-10-tauc/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/khoei-10-tauc/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Based on 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2012).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Motion-based prediction is sufficient to solve the aperture problem&lt;/a&gt;.
  &lt;em&gt;Neural Computation&lt;/em&gt;.
  
  &lt;p&gt;





  
  
    
  
&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://arxiv.org/abs/1208.6471&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  Preprint
&lt;/a&gt;




  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/perrinet-12-pred.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-12-pred/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;see  follow-up on motion extrapolation: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2013).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/&#34;&gt;Motion-based prediction explains the role of tracking in motion extrapolation&lt;/a&gt;.
   &lt;em&gt;Journal of Physiology-Paris&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-13-jpp/khoei-13-jpp.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-13-jpp/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1016/j.jphysparis.2013.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;li&gt;see  follow-up on the flash-lag effect: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/mina-a-khoei/&#34;&gt;Mina A Khoei&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/guillaume-s-masson/&#34;&gt;Guillaume S Masson&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2017).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34;&gt;The flash-lag effect as a motion-based predictive shift&lt;/a&gt;.
   &lt;em&gt;PLoS Computational Biology&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://hal-amu.archives-ouvertes.fr/hal-01771125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Preprint
 &lt;/a&gt;
 
 
 
 
   
     
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/khoei-masson-perrinet-17/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
   
   
     
   
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   Code
 &lt;/a&gt;
 
 
 
 
 
 
 
   
   &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/&#34; target=&#34;_blank&#34;&gt;
     Slides
   &lt;/a&gt;
   
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1371/journal.pcbi.1005068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dynamical emergence of a neural solution for motion integration</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-areadne/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probabilistic models of the low-level visual system: the role of prediction in detecting motion</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-10-tauc/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-10-tauc/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Decoding center-surround interactions in population of neurons for the ocular following response</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-09-cosyne/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-09-cosyne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamics of distributed 1D and 2D motion representations for short-latency ocular following</title>
      <link>https://laurentperrinet.github.io/publication/barthelemy-08/</link>
      <pubDate>Fri, 01 Feb 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/barthelemy-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Decoding the population dynamics underlying ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-areadne/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-areadne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Modeling spatial integration in the ocular following response to center-surround stimulation using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08-a/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What adaptive code for efficient spiking representations? A model for the formation of receptive fields of simple cells</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-08/</link>
      <pubDate>Tue, 01 Jan 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-08/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamical Neural Networks: modeling low-level vision at short latencies</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07/</link>
      <pubDate>Thu, 01 Mar 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07/</guid>
      <description>&lt;p&gt;Dynamical Neural Networks (DyNNs) are a class of models for networks of neurons where particular focus is put on the role of time in the emergence of functional computational properties. The definition and study of these models involves the cooperation of a large range of scientific fields from statistical physics, probabilistic modelling, neuroscience and psychology to control theory. It focuses on the mechanisms that may be relevant for studying cognition by hypothesizing that information is distributed in the activity of the neurons in the system and that the timing helps in maintaining this information to lastly form decisions or actions. The system responds at best to the constraints of the outside world and learning strategies tune this internal dynamics to achieve optimal performance.
This chapter introduces the book. See also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/bruno-cessac/&#34;&gt;Bruno Cessac&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel DaucÃ©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/manuel-samuelides/&#34;&gt;Manuel Samuelides&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2007).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/cessac-07/&#34;&gt;Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision&lt;/a&gt;.
  &lt;em&gt;Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision&lt;/em&gt;.
  
  &lt;p&gt;








  





&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/cessac-07/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/bruno-cessac/&#34;&gt;Bruno Cessac&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/emmanuel-dauce/&#34;&gt;Emmanuel DaucÃ©&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/manuel-samuelides/&#34;&gt;Manuel Samuelides&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2007).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/cessac-07-a/&#34;&gt;Introduction to Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision&lt;/a&gt;.
  &lt;em&gt;Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision&lt;/em&gt;.
  
  &lt;p&gt;








  
    
  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;http://www.springerlink.com/index/10.1140/epjst/e2007-00057-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/cessac-07-a/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;













&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1140/epjst/e2007-00057-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  DOI
&lt;/a&gt;


&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian modeling of dynamic motion integration</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-07/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-07/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;header&#34; srcset=&#34;
               /publication/montagnini-07/montagnini-07_hubcd9364771dda868695d1ad41f1e39ed_88325_aadcac45c71b46d21f6fcfd7d6deb611.webp 400w,
               /publication/montagnini-07/montagnini-07_hubcd9364771dda868695d1ad41f1e39ed_88325_893e01406038387bd44b2604bb86c508.webp 760w,
               /publication/montagnini-07/montagnini-07_hubcd9364771dda868695d1ad41f1e39ed_88325_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/montagnini-07/montagnini-07_hubcd9364771dda868695d1ad41f1e39ed_88325_aadcac45c71b46d21f6fcfd7d6deb611.webp&#34;
               width=&#34;760&#34;
               height=&#34;248&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modeling spatial integration in the ocular following response using a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;header&#34; srcset=&#34;
               /publication/perrinet-07-neurocomp/perrinet-07-neurocomp_huc8016d502ed90e8bf472bc36070fe734_103963_2b56d05c06b4cdd68ce3afd406a3d763.webp 400w,
               /publication/perrinet-07-neurocomp/perrinet-07-neurocomp_huc8016d502ed90e8bf472bc36070fe734_103963_5f446c37cb0267214f942abc6b210255.webp 760w,
               /publication/perrinet-07-neurocomp/perrinet-07-neurocomp_huc8016d502ed90e8bf472bc36070fe734_103963_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/publication/perrinet-07-neurocomp/perrinet-07-neurocomp_huc8016d502ed90e8bf472bc36070fe734_103963_2b56d05c06b4cdd68ce3afd406a3d763.webp&#34;
               width=&#34;760&#34;
               height=&#34;275&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual tracking of ambiguous moving objects: A recursive Bayesian model</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-07-b/</link>
      <pubDate>Mon, 01 Jan 2007 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-07-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian modeling of dynamic motion integration</title>
      <link>https://laurentperrinet.github.io/publication/montagnini-06-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/montagnini-06-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-06-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-06-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</title>
      <link>https://laurentperrinet.github.io/talk/2006-01-01-neurocomp/</link>
      <pubDate>Sun, 01 Jan 2006 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/talk/2006-01-01-neurocomp/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;related publication @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-08-spie/&#34;&gt;SPIE 2008&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See a followup in &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34;&gt;Perrinet et al, 2012&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Dynamics of motion representation in short-latency ocular following: A two-pathways Bayesian model</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-05-a/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-05-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Feature detection using spikes : the greedy approach</title>
      <link>https://laurentperrinet.github.io/publication/perrinet-04-tauc/</link>
      <pubDate>Thu, 01 Jul 2004 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/publication/perrinet-04-tauc/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
