<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Events | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/post/</link>
      <atom:link href="https://laurentperrinet.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Events</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Mon, 01 May 2023 09:00:00 +0000</lastBuildDate>
    <image>
      <url>https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_512x512_fill_lanczos_center_3.png</url>
      <title>Events</title>
      <link>https://laurentperrinet.github.io/post/</link>
    </image>
    
    <item>
      <title>Postdoc position &#34;Accurate detection of precise spiking motifs in neurobiological data&#34;</title>
      <link>https://laurentperrinet.github.io/post/2023-05-01_postdoc-position_polychronies/</link>
      <pubDate>Mon, 01 May 2023 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2023-05-01_postdoc-position_polychronies/</guid>
      <description>&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    THE POSITION HAS BEEN FILLED.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Dear colleagues,&lt;/p&gt;
&lt;p&gt;Applications are welcome for a fully funded 18-month postdoctoral position for the development of an algorithm for the &lt;strong&gt;accurate detection of precise spiking motifs in neurobiological data&lt;/strong&gt;. The position will be located at the &lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;INT&lt;/a&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, France. The project is funded by the &lt;a href=&#34;https://laurentperrinet.github.io/grant/polychronies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;polychronies&lt;/a&gt; grant (AMX-21-RID-025) and coordinated by &lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt; together with &lt;a href=&#34;https://thomas.schatz.cogserver.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Schatz&lt;/a&gt; (theory) and &lt;a href=&#34;https://www.inmed.fr/developpement-des-microcircuits-gabaergiques-corticaux-fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosa Cossart&lt;/a&gt; (neurobiology).&lt;/p&gt;
&lt;p&gt;Candidates should have experience in computational neuroscience, physics, engineering, or related fields, and a strong background in machine learning. The candidate must have good computer science skills (programming skills, git versioning, &amp;hellip;) and Python programming experience is required. A multidisciplinary background would be highly appreciated, especially an advanced knowledge of mathematics. The candidate must have a strong interest in neuroscience. The candidate must be fluent in English and willing to proactively interact with partners in different communities, including theoretical neuroscience, machine learning, or neurobiology. The preferred candidate should have the ability to work independently and be flexible to adapt to the working methods of the supervisors.&lt;/p&gt;
&lt;p&gt;For more information, please visit &lt;a href=&#34;https://laurentperrinet.github.io/post/2023-05-01_postdoc-position_polychronies&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/post/2023-05-01_postdoc-position_polychronies&lt;/a&gt;. To apply, please contact me at &lt;a href=&#34;mailto:Laurent.Perrinet@univ-amu.fr&#34;&gt;Laurent.Perrinet@univ-amu.fr&lt;/a&gt; and provide a synthetic cover letter and resume. I always respond, usually in less than a week, so please contact me again if you have not heard from me. The ideal start date is October 1, 2023, but can be arranged flexibly to suit the candidate. The appointment is for 18 months and applications are welcome immediately.&lt;/p&gt;
&lt;p&gt;Thank you for forwarding this announcement to potential candidates!&lt;/p&gt;
&lt;h2 id=&#34;related-references&#34;&gt;Related references&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;More details on the &amp;ldquo;polychronies&amp;rdquo; grant: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/rosa-cossart/&#34;&gt;Rosa Cossart&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/thomas-schatz/&#34;&gt;Thomas Schatz&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/grant/polychronies/&#34;&gt;Polychronies (2022 / 2025)&lt;/a&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Our recent review on Precise spiking motifs in neurobiological and neuromorphic data: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/amelie-gruel/&#34;&gt;Am√©lie Gruel&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/camille-besnainou/&#34;&gt;Camille Besnainou&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/jean-nicolas-jeremie/&#34;&gt;Jean-Nicolas J√©r√©mie&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/jean-martinet/&#34;&gt;Jean Martinet&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2022).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34;&gt;Precise spiking motifs in neurobiological and neuromorphic data&lt;/a&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/grimaldi-22-polychronies.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-22-polychronies/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.3390/brainsci13010068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Application of detecting spiking motifs in neuromorphic data: 
 
 
 
 
 
 
 
 
 
   
 
 
 &lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
   &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;
 
   
   
 
   &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
     
 
   &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/antoine-grimaldi/&#34;&gt;Antoine Grimaldi&lt;/a&gt;&lt;/span&gt;, &lt;span &gt;
       &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
   &lt;/span&gt;
   (2023).
   &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34;&gt;Learning heterogeneous delays in a layer of spiking neurons for fast motion detection&lt;/a&gt;.
   &lt;em&gt;Biological Cybernetics&lt;/em&gt;.
   
   &lt;p&gt;
 
 
 
 
 
 
 
 
   
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/grimaldi-23-bc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   PDF
 &lt;/a&gt;
 
 
 
 &lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
         data-filename=&#34;/publication/grimaldi-23-bc/cite.bib&#34;&gt;
   Cite
 &lt;/a&gt;
 
 
 
 
 
 
 
 
 
 
 
 
 
 &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://doi.org/10.1007/s00422-023-00975-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
   DOI
 &lt;/a&gt;
 
 
 &lt;/p&gt;
 
   
   
 &lt;/div&gt;
 
 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A theoretical framework on the accurate (supervised) detection of spiking motifs in (synthetic) multi-unit raster plots 









  


&lt;div class=&#34;pub-list-item view-citation&#34; style=&#34;margin-bottom: 1rem&#34;&gt;
  &lt;i class=&#34;far fa-file-alt pub-icon&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;

  
  

  &lt;span class=&#34;article-metadata li-cite-author&#34;&gt;
    

  &lt;span &gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/&#34;&gt;Laurent U Perrinet&lt;/a&gt;&lt;/span&gt;
  &lt;/span&gt;
  (2023).
  &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-23-icann/&#34;&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/a&gt;.
  &lt;em&gt;ICANN Special Session on Recent Advances in Spiking Neural Networks&lt;/em&gt;.
  
  &lt;p&gt;








  



&lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-23-icann/perrinet-23-icann.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
  PDF
&lt;/a&gt;



&lt;a href=&#34;#&#34; class=&#34;btn btn-outline-primary btn-page-header btn-sm js-cite-modal&#34;
        data-filename=&#34;/publication/perrinet-23-icann/cite.bib&#34;&gt;
  Cite
&lt;/a&gt;














  
  
  
    
  
  
  
  
  
    
  
  &lt;a class=&#34;btn btn-outline-primary btn-page-header btn-sm&#34; href=&#34;https://laurentperrinet.github.io/publication/perrinet-23-icann/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;
    URL&lt;/a&gt;

&lt;/p&gt;

  
  
&lt;/div&gt;


&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;job offer posted on: &lt;a href=&#34;https://euraxess.ec.europa.eu/jobs/112647&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Euraxess&lt;/a&gt; - &lt;a href=&#34;https://jobrxiv.org/job/cnrs-aix-marseille-univ-27778-accurate-detection-of-precise-spiking-motifs-in-neurobiological-data/?feed_id=45012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jobrXiv&lt;/a&gt; - &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1654155062388637699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt; - &lt;a href=&#34;https://euraxess.ec.europa.eu/jobs/115351&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;academic positions&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;research-context&#34;&gt;Research context&lt;/h2&gt;
&lt;p&gt;The thesis will be carried out in the team &amp;ldquo;NEuronal OPerations in visual TOpographic maps&amp;rdquo; (NeOpTo) within the &lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, a welcoming and lively town by the Mediterranean Sea in the south of France. The research team is led by F. Chavane (DR, CNRS) and currently hosts 4 permanent staff, 3 post-docs and 4 PhD students. The research themes of the team are focused on neuronal operations within visual cortical maps. Indeed, along the cortical hierarchy, low-level features such as the position and orientation of the visual stimulus (but also auditory tone, somatosensory touch, etc&amp;hellip;) but also higher-level features (such as faces, viewpoints of objects, etc&amp;hellip;) are represented topographically on the cortical surface.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cristal N¬∞2</title>
      <link>https://laurentperrinet.github.io/post/2022-09-30_cristal-no2/</link>
      <pubDate>Fri, 30 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2022-09-30_cristal-no2/</guid>
      <description>&lt;h1 id=&#34;cristal-n2---arbre-th√©orique--2014&#34;&gt;Cristal N¬∞2 - Arbre th√©orique / 2014&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dimensions: 134 X 91 X 21 CM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Miroirs / Bois&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Don de l&amp;rsquo;artiste √† l&amp;rsquo;Institut de Neurosciences de la Timone&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://ondesparalleles.org/projets/cristal-n2-2/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ondesparalleles.org/projets/cristal-n2-2/?lang=en&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Horizon Faille @ interstices</title>
      <link>https://laurentperrinet.github.io/post/2021-10-04_interstices/</link>
      <pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2021-10-04_interstices/</guid>
      <description>&lt;h1 id=&#34;horizon-faille&#34;&gt;Horizon Faille&lt;/h1&gt;
&lt;h2 id=&#34;festival-interstices-du-5-au-17-octobre-du-mercredi-au-dimanche-de-14h-√†-18h&#34;&gt;Festival interstices, du 5 au 17 octobre Du mercredi au dimanche de 14h √† 18h&lt;/h2&gt;
&lt;p&gt;Du 5 au 17 octobre, le festival &lt;a href=&#34;http://festival-interstice.net/2021/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;interstices&lt;/a&gt; pr√©sente des expositions et performances qui proposent √† travers des cr√©ations spectaculaires.&lt;/p&gt;











&lt;div class=&#34;gallery&#34;&gt;

  
  
  
    
    
    
    
    
      
        
      
    
    &lt;a data-fancybox=&#34;gallery-2021-10-04_interstices&#34; href=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115239_695.JPG&#34; data-caption=&#34;Close up on TRAMES. Image credit: Etienne Rey&#34;&gt;
      &lt;img src=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115239_695_hu42ca9c91f4eba789ff360281689d6ce7_1214269_0x190_resize_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;Close up on TRAMES. Image credit: Etienne Rey&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
      
        
      
    
    &lt;a data-fancybox=&#34;gallery-2021-10-04_interstices&#34; href=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115354_945.JPG&#34; data-caption=&#34;Vague (d√©tail). Image credit: Etienne Rey&#34;&gt;
      &lt;img src=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115354_945_hu4f8c6d80b578128a3b4d17128bad33c4_716121_0x190_resize_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;Vague (d√©tail). Image credit: Etienne Rey&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
      
        
      
    
    &lt;a data-fancybox=&#34;gallery-2021-10-04_interstices&#34; href=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115401_307.JPG&#34; data-caption=&#34;Vague (d√©tail). Image credit: Etienne Rey&#34;&gt;
      &lt;img src=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115401_307_hu4f8c6d80b578128a3b4d17128bad33c4_677167_0x190_resize_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;Vague (d√©tail). Image credit: Etienne Rey&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
      
        
      
    
    &lt;a data-fancybox=&#34;gallery-2021-10-04_interstices&#34; href=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115905_633.JPG&#34; data-caption=&#34;Vague. Image credit: Etienne Rey&#34;&gt;
      &lt;img src=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_115905_633_hua85530dd0fd011b0bdc10fa86d87edf9_1172612_0x190_resize_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;Vague. Image credit: Etienne Rey&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
      
        
      
    
    &lt;a data-fancybox=&#34;gallery-2021-10-04_interstices&#34; href=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_122124_962.JPG&#34; data-caption=&#34;Trames (macro). Image credit: Etienne Rey&#34;&gt;
      &lt;img src=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_122124_962_hu42ca9c91f4eba789ff360281689d6ce7_1066667_0x190_resize_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;Trames (macro). Image credit: Etienne Rey&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
      
        
      
    
    &lt;a data-fancybox=&#34;gallery-2021-10-04_interstices&#34; href=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_122134_038.JPG&#34; data-caption=&#34;Trames (meso). Image credit: Etienne Rey&#34;&gt;
      &lt;img src=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_122134_038_hua85530dd0fd011b0bdc10fa86d87edf9_1747490_0x190_resize_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;Trames (meso). Image credit: Etienne Rey&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  
    
    
    
    
    
      
        
      
    
    &lt;a data-fancybox=&#34;gallery-2021-10-04_interstices&#34; href=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_122145_287.JPG&#34; data-caption=&#34;Trames (micro). Image credit: Etienne Rey&#34;&gt;
      &lt;img src=&#34;https://laurentperrinet.github.io/media/albums/2021-10-04_interstices/DJI_20211005_122145_287_hua85530dd0fd011b0bdc10fa86d87edf9_2080069_0x190_resize_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;Trames (micro). Image credit: Etienne Rey&#34; width=&#34;253&#34; height=&#34;190&#34;&gt;
    &lt;/a&gt;
  

&lt;/div&gt;
&lt;p&gt;Horizon Faille est une installation globale qui cherche √† d√©fier la gravit√© de la nature. Prenant appui sur deux notions dont l‚Äôartiste en a fait ses motifs principaux ‚Äì les failles du paysage et l‚Äôimmat√©rielle ligne d‚Äôhorizon ‚Äì elle rel√®ve autant de la po√©sie que de l‚Äôexp√©rience sensorielle. La notion de faille exprime la fracture, au sens g√©ologique.
Elle est √† consid√©rer comme un interstice, une zone de transformation, un passage d‚Äôun √©tat √† un autre. De m√™me, l‚Äôhorizon scinde la terre du ciel dans une tentative de g√©om√©trisation de l‚Äôunivers, de mise en espace des √©l√©ments naturels. Intouchable ligne de partage, ce filin tendu d√©signe aussi le seuil de vision du paysage. C‚Äôest la ligne imaginaire qui se forme √† partir de notre position dans l‚Äôespace. Elle est ce qui √©chappe √† la vue ou √† la repr√©sentation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Soutenance Angelo Franciosini</title>
      <link>https://laurentperrinet.github.io/post/2021-09-28_soutenance-angelo-franciosini/</link>
      <pubDate>Thu, 09 Sep 2021 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2021-09-28_soutenance-angelo-franciosini/</guid>
      <description>&lt;h1 id=&#34;sdpc--a-sparse-and-predictive-model-of-the-early-visual-system-soutenance-de-th√®se-angelo-franciosini&#34;&gt;&amp;ldquo;SDPC : a sparse and predictive model of the early visual system&amp;rdquo; Soutenance de th√®se Angelo Franciosini&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Date : Mardi 28 septembre 2021 √† 13h (CEST)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lieu: en &lt;a href=&#34;https://univ-amu-fr.zoom.us/j/93571492344?pwd=NTlTbjhvM1pxR2ZUY3ZYKzhURTRmUT09&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;virtuel&lt;/a&gt; et salle &lt;a href=&#34;http://patrimoinemedical.univmed.fr/rues/rues_gastaut.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Henri Gastaut&lt;/a&gt;, au rez de chauss√©e de l&amp;rsquo;INT (how to &lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;). La th√®se √©tait suivie d‚Äôun pot au R+4 de l‚Äô&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; (how to &lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quoi: le manuscrit sera disponible apr√®s la soutenance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;jury&#34;&gt;Jury&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://findanexpert.unimelb.edu.au/profile/5669-anthony-burkitt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anthony Burkitt&lt;/a&gt;, University of Melbourne, Rapporteur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.brown.edu/academics/cognitive-linguistic-psychological-sciences/people/faculty/thomas-serre&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas Serre&lt;/a&gt;, Brown University, Rapporteur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://incc-paris.fr/people/laura-dugue/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laura Dugu√©&lt;/a&gt;, Integrative Neuroscience &amp;amp; Cognition Center, Examinateur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://emmanuel.dauce.free.fr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ism.univ-amu.fr/viollet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;St√©phane Viollet&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;, CNRS, Directeur de th√®se&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;One goal of visual neuroscience is to understand how the brain interprets sensory information and to describe cortical representations according to a specific computational model. In this thesis, we describe how a successful model for visual perception, Predictive Coding (PC), can be extended to account for highly nonlinear operations in the primary visual cortex of mammals (V1). In this thesis, we generalize PC in a convolutional network and propose an algorithm called Sparse Deep Predictive Coding (SDPC), which models the properties of the early visual cortex. We present the SDPC framework in two scientific articles: in the first, we use our network to model local interactions in the early visual system (V1/V2) and we show how feedback connectivity allows the visual system to adapt to the statistics of natural images. In a second article, we show that the SDPC can predict the emergence of nonlinear responses in V1 (complex cells) and explain the link between complex cells and higher-level structures like cortical orientation maps, across species. Finally, we will propose some extensions that will allow the SDPC to serve as a general model of the visual system.&lt;/p&gt;
&lt;h2 id=&#34;r√©sum√©&#34;&gt;R√©sum√©&lt;/h2&gt;
&lt;p&gt;Un des objectifs des neurosciences visuelles est de comprendre comment le cerveau interpr√®te les informations sensorielles et de d√©crire les repr√©sentations corticales gr√¢ce √† un mod√®le computationnel. Dans cette th√®se, nous d√©crivons comment un mod√®le de perception visuelle, le Codage Pr√©dictif, peut √™tre √©tendu pour rendre compte des op√©rations non lin√©aires dans le cortex visuel primaire des mammif√®res (V1). Dans cette th√®se, nous g√©n√©ralisons le Codage Pr√©dictif dans un r√©seau convolutif pour cr√©er un mod√®le appel√© Sparse Deep Predictive Coding (SDPC). Nous pr√©sentons le SDPC dans deux articles scientifiques : dans le premier, nous utilisons notre r√©seau pour mod√©liser les interactions locales dans le syst√®me visuel pr√©coce (V1/V2) et nous montrons comment la connectivit√© de r√©troaction permet au syst√®me visuel de s‚Äôadapter aux statistiques des images naturelles. Dans un second article, nous montrons que le SDPC peut pr√©dire l‚Äô√©mergence de r√©ponses non lin√©aires dans V1 (cellules complexes) et expliquer le lien entre cellules complexes et des structures de plus haut niveau comme les cartes d‚Äôorientation corticales, et ceci pour diff√©rentes esp√®ces. Enfin, nous proposerons quelques extensions qui permettront au SDPC de servir de mod√®le g√©n√©ral du syst√®me visuel.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Turing Patterns</title>
      <link>https://laurentperrinet.github.io/post/2021-06-15_neural-turing/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2021-06-15_neural-turing/</guid>
      <description>&lt;h1 id=&#34;neural-turing-patterns&#34;&gt;Neural Turing Patterns&lt;/h1&gt;
&lt;p&gt;Il y a des structures dans la nature qui √©mergent spontan√©ment. Ils ont √©t√© √©tudi√©s pour la premi√®re fois par le math√©maticien anglais Alan Turing (1912 ‚Äì 1954), qui a √©galement introduit le concept de machine informatique contemporaine. Des exemples de ces formes sont les rayures, les taches, les grilles, les pavages, les bulles, les spirales, les mousses et les vagues. Dans le cas sp√©cifique de cette ≈ìuvre, il est montr√© une collection de mod√®les de Turing g√©n√©r√©s via des simulations neuronales. Chaque carr√© est un r√©seau neuronal de cellules qui se d√©clenchent avec des intensit√©s faibles (bleues), moyennes (blanches) et √©lev√©es (rouge). Beaucoup d&amp;rsquo;entre eux se ressemblent beaucoup, tandis que d&amp;rsquo;autres sont tr√®s diff√©rents. Regroup√©es comme elles le sont, les images semblent avoir une continuit√© graphique avec leurs voisins plus proches. Mais, cette fonctionnalit√© visuelle est bien une illusion de la Gestalt, car ce sont des r√©sultats de simulation compl√®tement ind√©pendants.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;There are structures in nature that spontaneously emerge. They were studied for the first time by the English mathematician Alan Turing (1912 ‚Äì 1954), who also introduced the concept of the contemporary computational machine. Examples of those shapes are stripes, spots, grids, tessellations, bubbles, spirals, foams and waves. In the specific case of this artwork, a collection of Turing patterns are generated via brain simulations.  Each square is a network of neural cells that fire with low (blue), medium (white) and high (red) intensities. Many of them look very similar, while others are very different. Grouped as they are, pictures seem to have a graphical continuity with their closest. But, this visual feature is indeed a Gestalt illusion, because they are completely independent simulation results.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Les illusions s√®ment le trouble dans les esprits</title>
      <link>https://laurentperrinet.github.io/post/2021-04-06-larecherche/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2021-04-06-larecherche/</guid>
      <description>&lt;p&gt;Publication d&amp;rsquo;un nouvel article g√©n√©raliste autour des illusions visuelles, &amp;ldquo;&lt;em&gt;Les illusions s√®ment le trouble dans les esprits&lt;/em&gt;&amp;rdquo; √† d√©couvrir dans lee dossier &lt;a href=&#34;https://www.larecherche.fr/les-illusions-s%C3%A8ment-le-trouble-dans-les-esprits&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;La Recherche n¬∞565&lt;/a&gt; (trimestriel N¬∞565 dat√© avril-juin 2021):&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Nouvel article &amp;quot;Les illusions s√®ment le trouble dans les esprits&amp;quot; avec Herv√© Ratel &lt;a href=&#34;https://t.co/4YIEGdsrby&#34;&gt;https://t.co/4YIEGdsrby&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/maglarecherche?ref_src=twsrc%5Etfw&#34;&gt;@maglarecherche&lt;/a&gt; &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/INSB_CNRS?ref_src=twsrc%5Etfw&#34;&gt;@INSB_CNRS&lt;/a&gt; - et √©mu de publier dans le mag de mes ann√©es ado qui m&amp;#39;a permis de d√©couvrir le monde de la rechercheüßë‚Äçüî¨ juste retour des choses!&lt;/p&gt;&amp;mdash; @laurentperrinet@neuromatch.social (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1381305529553813504?ref_src=twsrc%5Etfw&#34;&gt;April 11, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.larecherche.fr/sites/larecherche.fr/files/parution_parution_image/LaRechercheTrim_13412_565_2104_2106_210408_Conscience_Couverture.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Les objectifs sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mieux comprendre la fonction de la perception visuelle en explorant certaines limites;&lt;/li&gt;
&lt;li&gt;mieux comprendre l‚Äôimportance de l‚Äôaspect dynamique de la perception;&lt;/li&gt;
&lt;li&gt;mieux comprendre le r√¥le de l‚Äôaction dans la perception.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Une version pr√©c√©dente est accessible sur le &lt;a href=&#34;https://laurentperrinet.github.io/2019-05_illusions-visuelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo GitHub&lt;/a&gt;, ainsi que les &lt;a href=&#34;https://github.com/laurentperrinet/2019-05_illusions-visuelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sources&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD offer &#34;Ultra-fast vision using Spiking Neural Networks&#34;</title>
      <link>https://laurentperrinet.github.io/post/2020-06-30_phd-position/</link>
      <pubDate>Tue, 30 Jun 2020 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2020-06-30_phd-position/</guid>
      <description>&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    THE POSITION HAS BEEN FILLED.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Dear colleagues,&lt;/p&gt;
&lt;p&gt;Applications are welcome for a fully funded doctoral position at &lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;INT&lt;/a&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, France. Your mission will be to build ultra-fast vision algorithms using event-based cameras and spiking neural networks. The project is funded by the &lt;a href=&#34;https://laurentperrinet.github.io/grant/aprovis-3-d/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;APROVIS3D&lt;/a&gt; grant (ANR-19-CHR3-0008-03) and will be coordinated by &lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;. The work will be carried out in collaboration with a leading computer science institute at Universit√© C√¥te d‚ÄôAzur (Sophia Antipolis, France), the Laboratoire d&amp;rsquo;Informatique, Signaux et Syst√®mes de Sophia-Antipolis (I3S, UMR7271 - UNS CNRS), that will be part of the supervision team. We are seeking candidates with a strong background in machine learning, computer vision and computational neuroscience.&lt;/p&gt;
&lt;p&gt;To obtain further information, please visit &lt;a href=&#34;https://laurentperrinet.github.io/post/2020-06-30_phd-position&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/post/2020-06-30_phd-position&lt;/a&gt; or contact me @ &lt;a href=&#34;mailto:Laurent.Perrinet@univ-amu.fr&#34;&gt;Laurent.Perrinet@univ-amu.fr&lt;/a&gt;. To candidate, follow instructions on the dedicated &lt;a href=&#34;https://bit.ly/3igRji4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;server from the CNRS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The starting date is set to October 1st, 2020 and the appointment is for 36 month. Applications are welcome immediately.&lt;/p&gt;
&lt;p&gt;Thanks for distributing this announcement to potential candidates!&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;CD Doctorant &amp;quot;Vision ultra-rapide utilisant des R√©seaux de neurones impulsionnels&amp;quot; H/F (MARSEILLE) (MARSEILLE 05) &lt;a href=&#34;https://t.co/I5CXWxR3zi&#34;&gt;https://t.co/I5CXWxR3zi&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Emploi?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Emploi&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/OffreEmploi?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#OffreEmploi&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/Recrutement?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Recrutement&lt;/a&gt;&lt;/p&gt;&amp;mdash; EmploiCNRS (@EmploiCNRS) &lt;a href=&#34;https://twitter.com/EmploiCNRS/status/1277872035700539392?ref_src=twsrc%5Etfw&#34;&gt;June 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;detailed-description-ultra-fast-vision-using-spiking-neural-networks&#34;&gt;Detailed description: &amp;ldquo;Ultra-fast vision using Spiking Neural Networks&amp;rdquo;&lt;/h2&gt;
&lt;p&gt;Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. Crucially, given an equal constraint on energy consumption, these algorithms are relatively slow compared to biological vision. It is believed that one major factor of this rapidity is the fact that visual information is represented by short pulses (spikes) at analog ‚Äì not discrete ‚Äì times (&lt;a href=&#34;#Paugam12&#34;&gt;Paugam and Bohte, 2012&lt;/a&gt;). However, most classical computer vision algorithms rely on such frame-based approaches. One solution to overcome their limitations is to use event-based representations, but these still lack in practice, and their high potential is largely underexploited. Inspired by biology, the project addresses the scientific question of developing a low-power sensing architecture for the processing of visual scenes, able to function on analog devices without a central clock and aimed at being validated in real-life situations. More specifically, the project will develop new paradigms for biologically inspired computer vision (&lt;a href=&#34;#Cristobal15&#34;&gt;Cristobal, Keil and Perrinet, 2015&lt;/a&gt;), from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this doctoral project, we propose to address major limitations of classical computer vision by implementing specific dynamical features of cortical circuits: &lt;em&gt;spiking neural networks&lt;/em&gt; (&lt;a href=&#34;#Perrinet04&#34;&gt;Perrinet, Thorpe and Samuelides, 2004&lt;/a&gt;; &lt;a href=&#34;#Lagorce16&#34;&gt;Lagorce et al., 2018&lt;/a&gt;), &lt;em&gt;lateral diffusion of neural information&lt;/em&gt; (&lt;a href=&#34;#Chavane2000&#34;&gt;Chavane et al., 2011&lt;/a&gt;; &lt;a href=&#34;#muller2018cortical&#34;&gt;Muller et al., 2018&lt;/a&gt;) and &lt;em&gt;dynamic neuronal association fields&lt;/em&gt; (&lt;a href=&#34;#Fr%c3%a9gnac2012&#34;&gt;Fr√©gnac et al., 2012&lt;/a&gt;; &lt;a href=&#34;#Fr%c3%a9gnac2016&#34;&gt;Fr√©gnac et al., 2016&lt;/a&gt;; &lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;)&lt;/strong&gt;. One starting point is to use event-based cameras &lt;a href=&#34;#Dupeyroux18&#34;&gt;(Dupeyroux et al., 2018)&lt;/a&gt; and to extend results of self-supervised learning that we have obtained on static, natural images (&lt;a href=&#34;#BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt;Boutin et al., 2020&lt;/a&gt;) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the &amp;ldquo;association field&amp;rdquo; described at the psychophysical (&lt;a href=&#34;#Field1993&#34;&gt;Field et al., 1993&lt;/a&gt;), spiking (&lt;a href=&#34;#Li2002&#34;&gt;Li and Gilbert, 2002&lt;/a&gt;) and synaptic (&lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;) levels. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (&lt;a href=&#34;#Voges12&#34;&gt;Voges and Perrinet, 2012&lt;/a&gt;). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). It is not well understood, but probably decisive for ultra-fast vision, how recurrent cortico-cortical loops add a level of distributed top-down complexity in the feed-forward stream of information which participates to the ultra-fast integration of sensory input and perceptual context (&lt;a href=&#34;#Keller2019&#34;&gt;Keller et al., 2019&lt;/a&gt;). Coupled with the dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for defining ultra-fast vision algorithms.&lt;/p&gt;
&lt;h2 id=&#34;expected-profile-of-the-candidate&#34;&gt;Expected profile of the candidate&lt;/h2&gt;
&lt;p&gt;Candidates should have experience in the domain of computational neuroscience, physics, engineering or related, and a solid training in machine learning and computer vision.&lt;/p&gt;
&lt;p&gt;The candidate has to show good skills in computer science (programming skills, architecture understanding, git versioning, &amp;hellip;), and in image processing methods. Good command of programming tools (Python scripting) is required. Multidisciplinary background would be strongly appreciated and in particular an advanced knowledge in mathematics, for a deep understanding of signal processing methods, along with strong computational skills. The candidate needs to show a keen interest in neuroscience. It is a bonus if the candidate is curious about neuroscience and visual perception.&lt;/p&gt;
&lt;p&gt;The candidate has to fluently speak English to understand publications and to attend international conferences and workshops and pro-actively interact with partners in France, Switzerland, Spain and Greece. The preferred candidate will have the ability to work autonomously, and needs to be flexible to comply with the working method of the supervisors.&lt;/p&gt;
&lt;h2 id=&#34;research-context&#34;&gt;Research context&lt;/h2&gt;
&lt;p&gt;The thesis will be carried out in the team &amp;ldquo;NEuronal OPerations in visual TOpographic maps&amp;rdquo; (NeOpTo) within the &lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, a welcoming and lively town by the Mediterranean sea in the south of France. The research team is led by F. Chavane (DR2, CNRS) and currently hosts 4 permanent staff, 3 post-docs and 4 PhD students. The research themes of the team are focused on neuronal operations within visual cortical maps. Indeed, along the cortical hierarchy, low-level features such as the position and orientation of the visual stimulus (but also auditory tone, somatosensory touch, etc&amp;hellip;) but also higher-level features (such as faces, viewpoints of objects, etc&amp;hellip;) are represented topographically on the cortical surface.&lt;/p&gt;
&lt;p&gt;This work will be conducted in direct collaboration with &lt;a href=&#34;http://i3s.unice.fr/jmartinet/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jean Martinet&lt;/a&gt; who will co-supervise the thesis. We will develop these algorithms in collaboration with &lt;a href=&#34;https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;amp;hl=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryad Benosman&lt;/a&gt; (Universit√© Pierre et Marie Curie) and &lt;a href=&#34;https://scholar.google.com/citations?user=iIGoymcAAAAJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;St√©phane Viollet&lt;/a&gt; (√©quipe biorobotique, Institut des Sciences du Mouvement).&lt;/p&gt;
&lt;h2 id=&#34;fr-description-du-sujet-de-th√®se&#34;&gt;FR: Description du sujet de th√®se&lt;/h2&gt;
&lt;p&gt;La vision biologique est √©tonnamment efficace. Pour tirer parti de cette efficacit√©, l&amp;rsquo;apprentissage profond et les r√©seaux neuronaux convolutionnels (CNN) ont r√©cemment permis de r√©aliser de grandes avanc√©es en mati√®re de vision artificielle par ordinateur. Cependant, ces algorithmes sont aujourd&amp;rsquo;hui confront√©s √† de multiples d√©fis : les architectures apprises sont souvent peu interpr√©tables, sont d√©mesur√©ment gourmandes en √©nergie, n&amp;rsquo;int√®grent g√©n√©ralement pas les informations contextuelles qui semblent parfaitement adapt√©es √† la vision biologique et √† la perception humaine. Aussi ces algorithmes sont relativement lents -√† consommation √©nerg√©tique √©gale- par rapport √† la vision biologique. On pense qu&amp;rsquo;un facteur majeur de cette rapidit√© est le fait que l&amp;rsquo;information est repr√©sent√©e par de courtes impulsions √† des moments analogiques - et non discrets. Toutefois, les algorithmes de vision par ordinateur utilisant une telle repr√©sentation dans des r√©seaux de neurones impulsionnels font encore d√©faut dans la pratique, et son important potentiel est largement sous-exploit√©. Ce projet, qui est inspir√© de la biologie, aborde la question scientifique du d√©veloppement d&amp;rsquo;une architecture ultra-rapide de d√©tection et de traitement de sc√®nes visuelles, fonctionnant sur des appareils sans horloge centrale, et visant √† valider ce genre d&amp;rsquo;algorithmes √©v√©nementiels dans des situations r√©elles. Plus sp√©cifiquement, le projet d√©veloppera de nouveaux paradigmes pour une vision d&amp;rsquo;inspiration biologique, de la d√©tection au traitement, afin d&amp;rsquo;aider des machines telles que les robots a√©riens autonomes (UAV), les v√©hicules autonomes ou les robots √† acqu√©rir une compr√©hension de haut niveau des sc√®nes visuelles.&lt;/p&gt;
&lt;h2 id=&#34;fr-contexte-de-travail&#34;&gt;FR: Contexte de travail&lt;/h2&gt;
&lt;p&gt;La th√®se sera effectu√©e dans l&amp;rsquo;√©quipe &amp;ldquo;NEuronal OPerations in visual TOpographic maps&amp;rdquo; (NeOpTo) au sein de l&amp;rsquo;Institut de Neurosciences de la Timone (INT). L&amp;rsquo;√©quipe de recherche est dirig√©e par F. Chavane (DR2, CNRS) et accueille actuellement 4 personnels permanents, 3 post-doctorants et 4 doctorants. Les th√©matiques de recherche de l&amp;rsquo;√©quipe sont centr√©es sur les op√©rations neuronales au sein de cartes corticales visuelles. En effet, le long de la hi√©rarchie corticale, les caract√©ristiques de bas niveau telles que la position, l‚Äôorientation du stimulus visuel (mais aussi la tonalit√© auditive, le toucher somatosensoriel, etc&amp;hellip;) mais aussi les caract√©ristiques de niveau sup√©rieur (telles que les visages, les points de vue d‚Äôobjets, etc&amp;hellip;) sont repr√©sent√©es topographiquement sur la surface corticale.&lt;/p&gt;
&lt;p&gt;Cette th√®se sera men√©e en collaboration directe avec &lt;a href=&#34;http://i3s.unice.fr/jmartinet/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jean Martinet&lt;/a&gt; qui co-supervisera cette th√®se. Nous d√©velopperons ces algorithmes en collaboration avec &lt;a href=&#34;https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;amp;hl=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryad Benosman&lt;/a&gt; (Universit√© Pierre et Marie Curie) et &lt;a href=&#34;https://scholar.google.com/citations?user=iIGoymcAAAAJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;St√©phane Viollet&lt;/a&gt; (√©quipe biorobotique, Institut des Sciences du Mouvement).&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt;Boutin, Victor, Angelo Franciosini, Fr√©d√©ric Chavane, Franck Ruffier, and Laurent U Perrinet. (2019). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://arxiv.org/abs/1902.07651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.&lt;/a&gt;&amp;rdquo; &lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Dupeyroux18&#34;&gt;Julien Dupeyroux, Victor Boutin, Julien R Serres, Laurent U Perrinet, St√©phane Viollet. (2018). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation.&lt;/a&gt;&amp;rdquo; &lt;em&gt;ISCAS&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chavane2011&#34;&gt;Chavane, F., Sharon, D., Jancke, D., Marre, O., Fr√©gnac, Y. and Grinvald, A. (2011). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://doi.org/10.1016/S0928-4257%2800%2901096-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lateral spread of orientation selectivity in V1 is controlled by intracortical cooperativity.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Physiology Paris&lt;/em&gt; 94 (5-6): 333&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Cristobal15&#34;&gt;Gabriel Crist√≥bal, Laurent U Perrinet, Matthias S Keil (2015). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biologically Inspired Computer Vision.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Wiley&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Field1993&#34;&gt;Field, D.J., Hayes, A. and Hess, R.F. (1993). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://doi.org/10.1016/0042-6989%2893%2990156-Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour integration by the human visual system: Evidence for a local ‚Äúassociation field‚Äù.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Vision Research&lt;/em&gt; 33 (2), pp. 173-193.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;gerard2016synaptic&#34;&gt;Gerard-Mercier, Florian, Pedro V Carelli, Marc Pananceau, Xoana G Troncoso, and Yves Fr√©gnac. (2016). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://www.jneurosci.org/content/36/14/3925&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synaptic Correlates of Low-Level Perception in V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 36 (14): 3925&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Keller2019&#34;&gt;Keller, A., Roth, M.M. and Scanziani, M. (2019). &lt;/a&gt; 2019. &amp;ldquo;&lt;a href=&#34;https://www.abstractsonline.com/pp8/#!/7883/presentation/65856&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The feedback receptive field of neurons in the mammalian primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;American Society for Neuroscience Abstracts&lt;/em&gt;, 403.13. Chicago.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Lagorce16&#34;&gt;Lagorce, X., Orchard, G., Galluppi, F., Shi, B. E., &amp;amp; Benosman, R. B.&lt;/a&gt; (2016). &amp;ldquo;&lt;a href=&#34;https://www.neuromorphic-vision.com/public/publications/1/publication.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HOTS: a hierarchy of event-based time-surfaces for pattern recognition.&lt;/a&gt;&amp;rdquo; &lt;em&gt;IEEE transactions on pattern analysis and machine intelligence&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Li2002&#34;&gt;Li W, Pi√´ch V, Gilbert CD&lt;/a&gt; (2006). &amp;ldquo;&lt;a href=&#34;http://www.paper.edu.cn/scholar/showpdf/MUz2UN2INTA0eQxeQh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour saliency in primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Neuron&lt;/em&gt;, 50(6):951‚Äì962.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;muller2018cortical&#34;&gt;Muller, Lyle, Fr√©d√©ric Chavane, John Reynolds, and Terrence J Sejnowski. &lt;/a&gt; (2018). &amp;ldquo;&lt;a href=&#34;https://papers.cnl.salk.edu/PDFs/Cortical%20travelling%20waves_%20mechanisms%20and%20computational%20principles.%202018-4515.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cortical Travelling Waves: Mechanisms and Computational Principles.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Nature Reviews Neuroscience&lt;/em&gt; 19 (5): 255.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Paugam12&#34;&gt;H√©l√®ne Paugam-Moisy, Sander M. Bohte. &lt;/a&gt; (2012). &amp;ldquo;Computing with Spiking Neuron Networks.&amp;rdquo; &lt;em&gt;Handbook of Natural Computing&lt;/em&gt;, Springer-Verlag, pp.335-376, 2012&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Perrinet04&#34;&gt;Laurent U Perrinet, Manuel Samuelides, Simon J Thorpe. &lt;/a&gt; (2004). &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-03-ieee/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Coding static natural images using spiking event times: do neurons cooperate?&amp;rdquo;&lt;/a&gt; &lt;em&gt;IEEE Transactions on Neural Networks&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Tang18&#34;&gt;Tang, Hanlin, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. &lt;/a&gt; (2018). &amp;ldquo;&lt;a href=&#34;https://doi.org/10.1073/pnas.1719397115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recurrent computations for visual pattern completion.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 115 (35) 8835-8840.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Voges12&#34;&gt;Voges, Nicole, and Laurent U Perrinet.&lt;/a&gt; (2012). &amp;ldquo;&lt;a href=&#34;https://doi.org/10.3389/fncom.2012.00041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Complex Dynamics in Recurrent Cortical Networks Based on Spatially Realistic Connectivities.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt; 6.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2020-03-13: Soutenance Victor Boutin</title>
      <link>https://laurentperrinet.github.io/post/2020-03-13_soutenance-victor-boutin/</link>
      <pubDate>Wed, 04 Mar 2020 14:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2020-03-13_soutenance-victor-boutin/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Int√©ress√©s par le &amp;quot;Sparse deep predictive coding&amp;quot; / &amp;quot;codage hi√©rarchique, √©pars et pr√©dictif&amp;quot; ? Victor Boutin (Equipe NeOpTo) soutiendra sa th√®se de doctorat intitul√©e Vendredi 13 mars √† 14h  &lt;a href=&#34;https://t.co/BIrciyiRUf&#34;&gt;https://t.co/BIrciyiRUf&lt;/a&gt;&lt;br&gt;ü§ù &lt;a href=&#34;https://twitter.com/univamu?ref_src=twsrc%5Etfw&#34;&gt;@univamu&lt;/a&gt;  &lt;a href=&#34;https://twitter.com/CNRS?ref_src=twsrc%5Etfw&#34;&gt;@CNRS&lt;/a&gt; &lt;a href=&#34;https://twitter.com/regionpaca?ref_src=twsrc%5Etfw&#34;&gt;@regionpaca&lt;/a&gt; &lt;a href=&#34;https://twitter.com/CNRS_dr12?ref_src=twsrc%5Etfw&#34;&gt;@CNRS_dr12&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/INT?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#INT&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://twitter.com/FranckRUFFIER?ref_src=twsrc%5Etfw&#34;&gt;@FranckRUFFIER&lt;/a&gt;&lt;/p&gt;&amp;mdash; @laurentperrinet@neuromatch.social (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1235128290458951680?ref_src=twsrc%5Etfw&#34;&gt;March 4, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Date : Vendredi 13 mars √† 14h&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lieu:  salle Henri Gastaut, au rez de chauss√©e de l&amp;rsquo;INT  (how to &lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;). La th√®se √©tait suivie d‚Äôun pot au R+4 de l‚Äô&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; (how to &lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quoi: le manuscrit est disponible sur  &lt;a href=&#34;http://www.theses.fr/2020AIXM0028&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.theses.fr/2020AIXM0028&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;jury&#34;&gt;Jury&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;amp;hl=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryad Benosman&lt;/a&gt;, Universit√© Pierre et Marie Curie, Rapporteur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.google.fr/citations?hl=fr&amp;amp;user=uR-7ex4AAAAJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simon Thorpe&lt;/a&gt;, CNRS, Rapporteur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.i2m.univ-amu.fr/perso/sandrine.anthoine/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sandrine Anthoine&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://neuro-psi.cnrs.fr/spip.php?article934&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yves Fr√©gnac&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sidkouider.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sid Kouider&lt;/a&gt;, CNRS, Examinateur&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;, CNRS, Directeur de th√®se&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ism.univ-amu.fr/ruffier/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Franck Ruffier&lt;/a&gt;, CNRS, Co-directeur de th√®se&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Mossadek_Talby&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mossadek Talby&lt;/a&gt;, AMU, Jury invit√©&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Building models to efficiently represent images is a central and difficult problem in the machine learning community. The neuroscientific study of the early visual cortical areas is a great source of inspiration to find economical and robust solutions. For instance, Sparse Coding (SC) is one of the most successful frameworks to model neural computation at the local scale in the visual cortex. It directly derives from the efficient coding hypothesis and could be thought of as a competitive mechanism that describes visual stimulus using the activity of a small fraction of neurons. At the structural scale of the ventral visual pathways, feedforward models of vision have accounted for neurophysiological evidence and provide the most successful frameworks for object recognition tasks. Nevertheless, these models do not leverage the high density of feedback and lateral interactions observed in the visual cortex. In particular, these connections are known to integrate contextual and attentional modulations to feedforward signals. The Predictive Coding (PC) theory has been proposed to model top-down and bottom-up interaction between cortical regions. The presented thesis introduces a model combining Sparse Coding and Predictive Coding in a hierarchical and convolutional architecture. Our model, called  Sparse Deep Predictive Coding (SDPC), was trained on several different databases including faces and natural images. We analyze the SPDC from a computational and a biological perspective. In terms of computation, the recurrent connectivity introduced by the PC framework allows the SDPC to converge to lower prediction errors with a higher convergence rate. In addition, we combine neuroscientific evidence with machine learning methods to analyze the impact of recurrent processing at both the neural organization and representational level. At the neural organization level, the feedback signal of the model accounted for a reorganization of the V1 association fields that promotes contour integration. At the representational level, the SDPC exhibited significant denoising ability which is highly correlated with the strength of the feedback from V2 to V1. These results from the SDPC model demonstrate that neuro-inspiration might be the right methodology to design more powerful and more robust computer vision algorithms.&lt;/p&gt;
&lt;h2 id=&#34;r√©sum√©&#34;&gt;R√©sum√©&lt;/h2&gt;
&lt;p&gt;La repr√©sentation concise et efficace de l&amp;rsquo;information est un probl√®me qui occupe une place centrale dans l&amp;rsquo;apprentissage machine. Le cerveau, et plus particuli√®rement le cortex visuel, ont depuis longtemps trouv√© des solutions performantes et robustes afin de r√©soudre un tel probl√®me. A l&amp;rsquo;√©chelle locale, le codage √©pars est l&amp;rsquo;un des m√©canismes les plus prometteurs pour mod√©liser le traitement de l&amp;rsquo;information au sein des populations de neurones dans le cortex visuel. Le codage √©pars introduit une comp√©tition entre les neurones afin de d√©crire un stimulus visuel en limitant le nombre de neurones actifs. A l&amp;rsquo;√©chelle structurelle, les mod√®les dits ascendants d√©crivent le cortex visuel comme une succession d&amp;rsquo;unit√©s de traitement dans lesquelles l&amp;rsquo;information se propage de la r√©tine vers les couches profondes du cortex. Ces mod√®les ont expliqu√© avec succ√®s un grand nombre de ph√©nom√®nes neuro-physiologiques et ont servi d&amp;rsquo;inspiration afin de construire des algorithmes de reconnaissance d&amp;rsquo;objets extr√™mement performants. N√©anmoins, les mod√®les ascendants n&amp;rsquo;expliquent pas le grand nombre de connections r√©currentes et descendantes que l&amp;rsquo;on trouve dans le cortex visuel. Ces connections sont connues pour moduler l&amp;rsquo;activit√© des neurones en incluant des details contextuels au flux d&amp;rsquo;information ascendant. La th√©orie du codage pr√©dictif a √©t√© sugg√©r√©e pour mod√©liser les connections ascendantes, r√©currentes, et descendantes que l&amp;rsquo;on retrouve entre les diff√©rentes r√©gions corticales. Cette th√®se propose de combiner codage √©pars et codage pr√©dictif au sein d&amp;rsquo;un mod√®le hi√©rarchique et convolutif. Nous avons entrain√© ce mod√®le sur diff√©rentes bases de donn√©es afin de l&amp;rsquo;analyser avec une perspective √† la fois computationnelle et biologique. D&amp;rsquo;un point de vue computationnel, nous d√©montrons que les connections descendantes, introduites par le codage pr√©dictif, permettent une convergence meilleure et plus rapide du mod√®le. De plus, nous analysons les effets des connections descendantes sur l&amp;rsquo;organisation des populations de neurones, ainsi que leurs cons√©quences sur la mani√®re dont notre algorithme se repr√©sente les images. Nous montrons que les connections descendantes r√©organisent les champs d&amp;rsquo;association de neurones dans V1 afin de permettre une meilleure int√©gration des contours. En outre, nous observons que ces connections permettent une meilleure reconstruction des images bruit√©es. Nos r√©sultats sugg√®rent que l&amp;rsquo;inspiration des neurosciences fournit un cadre prometteur afin de d√©velopper des algorithmes de vision artificielles plus performants et plus robustes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Postdoc position on Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves</title>
      <link>https://laurentperrinet.github.io/post/2019-10-28_postdoc-position/</link>
      <pubDate>Mon, 21 Oct 2019 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-28_postdoc-position/</guid>
      <description>&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    THE POSITION HAS BEEN FILLED.
  &lt;/div&gt;
&lt;/div&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Applications are welcome for a post-doctoral position at INT-Marseille, France exploring novel visual computations using spatio-temporal diffusion kernels and travelling waves. More info @ &lt;a href=&#34;https://t.co/f6tUR8XW6y&#34;&gt;https://t.co/f6tUR8XW6y&lt;/a&gt; &lt;a href=&#34;https://t.co/odzckjtloa&#34;&gt;pic.twitter.com/odzckjtloa&lt;/a&gt;&lt;/p&gt;&amp;mdash; @laurentperrinet@neuromatch.social (@laurentperrinet) &lt;a href=&#34;https://twitter.com/laurentperrinet/status/1188940039293751297?ref_src=twsrc%5Etfw&#34;&gt;October 28, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Dear colleagues,&lt;/p&gt;
&lt;p&gt;Applications are welcome for a post-doctoral position at &lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;INT&lt;/a&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, France. Your mission will be to explore novel visual computations using spatio-temporal diffusion kernels and traveling waves. The project is funded by the &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-horizontal-v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANR Horizontal V1&lt;/a&gt; grant (ANR-17-CE37-0006) from the French National Research Agency (ANR) and will be coordinated by &lt;a href=&#34;https://laurentperrinet.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;, in collaboration with &lt;a href=&#34;https://www.mullerlab.ca&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lyle Muller&lt;/a&gt; and &lt;a href=&#34;http://www.int.univ-amu.fr/spip.php?page=equipe&amp;amp;equipe=NeOpTo&amp;amp;lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fr√©d√©ric Chavane&lt;/a&gt; at INT and &lt;a href=&#34;http://neuro-psi.cnrs.fr/spip.php?article934&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yves Fr√©gnac&lt;/a&gt; and Jan Antolik at UNIC-NeuroPSI, Gif. We are seeking candidates with a strong background in machine learning, computer vision and computational neuroscience.&lt;/p&gt;
&lt;p&gt;For more information, visit &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-28_postdoc-position&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/post/2019-10-28_postdoc-position&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The starting date is set to January 6th, 2020 but can be flexibly extended. To obtain further information or send applications (including a full CV, a letter of motivation, 2 reference names), please contact: &lt;a href=&#34;mailto:Laurent.Perrinet@univ-amu.fr&#34;&gt;Laurent.Perrinet@univ-amu.fr&lt;/a&gt;. The appointment is for 18 month. Applications are welcome immediately and until the end of year 2019.&lt;/p&gt;
&lt;p&gt;Thanks for distributing this announcement to potential candidates!&lt;/p&gt;
&lt;h1 id=&#34;detailed-description-visual-computations-using-spatio-temporal-diffusion-kernels-and-traveling-waves&#34;&gt;Detailed description: Visual computations using Spatio-temporal Diffusion Kernels and Traveling Waves&lt;/h1&gt;
&lt;p&gt;Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. It is clear from recent advances in system and computational neuroscience that nonlinear, recurrent interactions in visual cortical networks are key to this efficiency¬†(&lt;a href=&#34;#Tang18&#34;&gt;Tang et al., 2018&lt;/a&gt;; &lt;a href=&#34;#Kietzmann19&#34;&gt;Kietzmann et al., 2019&lt;/a&gt;). We will use inspiration from neurophysiology and brain imaging to resolve this apparent gap between traditional CNNs and biological visual systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this post-doctoral project, we propose to address these major limitations by focusing on specific dynamical features of cortical circuits: &lt;em&gt;lateral diffusion of sensory-evoked traveling waves&lt;/em&gt; (&lt;a href=&#34;#Chavane2000&#34;&gt;Chavane et al., 2011&lt;/a&gt;; &lt;a href=&#34;#muller2018cortical&#34;&gt;Muller et al., 2018&lt;/a&gt;) and &lt;em&gt;dynamic neuronal association fields&lt;/em&gt; (&lt;a href=&#34;#Fr%c3%a9gnac2012&#34;&gt;Fr√©gnac et al., 2012&lt;/a&gt;; &lt;a href=&#34;#Fr%c3%a9gnac2016&#34;&gt;Fr√©gnac et al., 2016&lt;/a&gt;; &lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;)&lt;/strong&gt;. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (&lt;a href=&#34;#Voges12&#34;&gt;Voges and Perrinet, 2012&lt;/a&gt;). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). Less studied, but probably decisive in active vision, recurrent cortico-cortical loops add a level of distributed top-down complexity which participates to the lateral integration of sensory input and perceptual context (&lt;a href=&#34;#Keller2019&#34;&gt;Keller et al., 2019&lt;/a&gt;). Coupled with the continuous time dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for generating information diffusion through traveling waves. Inspired by recent work in neuroscience uncovering the ubiquity of these waves during visual processing, we aim to design a self-supervised CNN that will exploit these dynamics for new applications in computer vision.&lt;/p&gt;
&lt;p&gt;The proposed work will be organized as a collaboration between two labs (INT, Marseille and UNIC, Gif) along three tasks to be integrated in a unified model:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The starting point will be to extend results of self-supervised learning that we have obtained on static, natural images (&lt;a href=&#34;#BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt;Boutin et al., 2019&lt;/a&gt;) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the &amp;ldquo;association field&amp;rdquo; described at the psychophysical (&lt;a href=&#34;#Field1993&#34;&gt;Field et al., 1993&lt;/a&gt;), spiking (&lt;a href=&#34;#Li2002&#34;&gt;Li and Gilbert, 2002&lt;/a&gt;) and synaptic (&lt;a href=&#34;#gerard2016synaptic&#34;&gt;Gerard-Mercier et al., 2016&lt;/a&gt;) levels.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The central aim will be to develop a dynamical version of this feedback/lateral kernel in the context of the &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-horizontal-v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANR Horizontal-V1&lt;/a&gt; project, linking the two labs and confronted to their recent electrophysiological data pointing to different classes of spatio-temporal diffusion and different degree of anisotropies during apparent and continuous motion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The implementation of this kernel inspired by CNN theory will be compared with a biologically realistic models of the early visual system (&lt;a href=&#34;#Antolik2019&#34;&gt;Antolik et al., 2019&lt;/a&gt;), and simulations of the lateral diffusion kernel will be developed in collaboration with &lt;a href=&#34;http://antolik.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jan Antolik&lt;/a&gt;, external collaborator to the ANR grant.  In parallel, using tools linking neural activity to VSD imaging¬†(&lt;a href=&#34;#muller2014stimulus&#34;&gt;Muller et al., 2014&lt;/a&gt;; &lt;a href=&#34;#Chemla2018&#34;&gt;Chemla et al., 2019&lt;/a&gt;), we will analyze at a more mesocopic level the role of observed traveling waves in forming efficient representations of the visual world.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;expected-profile-of-the-candidate&#34;&gt;Expected profile of the candidate&lt;/h2&gt;
&lt;p&gt;Candidates should have at least a PhD degree in the domain of computational neuroscience, physics, engineering or related, and a solid training in machine learning and computer vision.&lt;/p&gt;
&lt;p&gt;The candidate has to show good skills in computer science (programming skills, architecture understanding, git versioning, &amp;hellip;), and in image processing methods. Good command of programming tools (Python scripting) is required. Multidisciplinary background would be strongly appreciated and in particular an advanced knowledge in mathematics, for a deep understanding of signal processing methods, along with strong computational skills. The candidate needs to show a keen interest in neuroscience. It is a bonus if the candidate is curious about neuroscience and visual perception.&lt;/p&gt;
&lt;p&gt;The candidate has to fluently speak English to understand publications and to attend international conferences and workshops. The preferred candidate will have the ability to work autonomously, and needs to be flexible to comply with the working method of the supervisors.&lt;/p&gt;
&lt;h2 id=&#34;research-context&#34;&gt;Research context&lt;/h2&gt;
&lt;p&gt;This project is funded by the French National Research Agency (ANR) under the &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-horizontal-v1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ANR Horizontal V1&lt;/a&gt; grant (coordinator Y. Fr√©gnac) which aims at understanding the emergence of sensory predictions linking local shape attributes (orientation, contour) to global indices of movement (direction, speed, trajectory) at the earliest stage of cortical processing (primary visual cortex, i.e. V1). The cross-talk between physiological and theoretical approaches will be fostered by the close collaboration with the teams of Fr√©d√©ric Chavane at INT and Yves Fr√©gnac at UNIC. The theoretical work will be performed in close collaboration with &lt;a href=&#34;https://www.mullerlab.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lyle Muller&lt;/a&gt; (Western U) and Jan Antolik (Prague). The project will be primarily hosted at the &lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; in &lt;a href=&#34;https://en.wikipedia.org/wiki/Marseille&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marseille&lt;/a&gt;, a lively town by the Mediterranean sea in the south of France, but the applicant will be asked also to show mobility to visit the other partner lab when needed.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Antolik2019&#34;&gt; Antolik, J, C Monier, Y Fr√©gnac, AP Davison. (2019). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/416156v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A comprehensive data-driven model of cat primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;BioRxiv&lt;/em&gt;, 416156.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;BoutinFranciosiniChavaneRuffierPerrinet20&#34;&gt; Boutin, Victor, Angelo Franciosini, Fr√©d√©ric Chavane, Franck Ruffier, and Laurent U Perrinet. (2019). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://arxiv.org/abs/1902.07651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.&lt;/a&gt;&amp;rdquo; &lt;em&gt;arXiv&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chavane2000&#34;&gt; Chavane, F., C. Monier, V. Bringuier, P. Baudot, L. Borg-Graham, J. Lorenceau, and Y. Fr√©gnac. 2000. &lt;/a&gt; &amp;ldquo;The Visual Cortical Association Field: A Gestalt Concept or a Psychophysiological Entity?&amp;rdquo; &lt;em&gt;Frontiers in System Neuroscience&lt;/em&gt; 4(5): 1-26.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chavane2011&#34;&gt; Chavane, F., Sharon, D., Jancke, D., Marre, O., Fr√©gnac, Y. and Grinvald, A.  (2011). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://doi.org/10.1016/S0928-4257%2800%2901096-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lateral spread of orientation selectivity in V1 is controlled by intracortical cooperativity.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Physiology Paris&lt;/em&gt; 94 (5-6): 333&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Chemla2018&#34;&gt; Chemla, Sandrine, Alexandre Reynaud, Matteo diVolo, Yann Zerlaut, Laurent Perrinet, Alain Destexhe, and Fr√©d√©ric Chavane. &lt;/a&gt; (2018). &amp;ldquo;&lt;a href=&#34;https://doi.org/10.1523/JNEUROSCI.2792-18.2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Suppressive Waves Disambiguate the Representation of Long-Range Apparent Motion in Awake Monkey V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 39 (22) 4282-4298.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Field1993&#34;&gt; Field, D.J., Hayes, A. and Hess, R.F. (1993). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://doi.org/10.1016/0042-6989%2893%2990156-Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour integration by the human visual system: Evidence for a local ‚Äúassociation field‚Äù.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Vision Research&lt;/em&gt; 33 (2), pp. 173-193.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Fr√©gnac2012&#34;&gt; Fr√©gnac, Y. (2012)  &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01685152/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reading out the synaptic echoes of low-level perception in V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;European Conference in Computer Vision&lt;/em&gt; 486-495. Springer, Berlin, Heidelberg.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Fr√©gnac2016&#34;&gt; Fr√©gnac, Y., Fournier, J., Gerard-Mercier, F., Monier, C., Carelli, P., , M., Troncoso, X. (2016).  &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://link-springer-com.insb.bib.cnrs.fr/content/pdf/10.1007%2F978-3-319-28802-4_4.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Visual Brain: Computing Through Multiscale Complexity.&lt;/a&gt;&amp;rdquo; In &lt;em&gt;Micro-, Meso- and Macro-Dynamics of the Brain&lt;/em&gt; pp 43-57.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;gerard2016synaptic&#34;&gt; Gerard-Mercier, Florian, Pedro V Carelli, Marc Pananceau, Xoana G Troncoso, and Yves Fr√©gnac. (2016). &lt;/a&gt; &amp;ldquo;&lt;a href=&#34;https://www.jneurosci.org/content/36/14/3925&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synaptic Correlates of Low-Level Perception in V1.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 36 (14): 3925&amp;ndash;42.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Keller2019&#34;&gt;Keller, A., Roth, M.M. and Scanziani, M. (2019).  &lt;/a&gt; 2019. &amp;ldquo;&lt;a href=&#34;https://www.abstractsonline.com/pp8/#!/7883/presentation/65856&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The feedback receptive field of neurons in the mammalian primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;American Society for Neuroscience Abstracts&lt;/em&gt;,  403.13. Chicago.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Kietzmann19&#34;&gt;Kietzmann, Tim C., Courtney J. Spoerer, Lynn K. A. S√∂rensen, Radoslaw M. Cichy, Olaf Hauk, and Nikolaus Kriegeskorte. &lt;/a&gt; (2019). &amp;ldquo;&lt;a href=&#34;https://doi.org/10/gf9j2t&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recurrence Is Required to Capture the Representational Dynamics of the Human Visual System.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;, October, 201905544.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Li2002&#34;&gt;Li W, Pi√´ch V, Gilbert CD&lt;/a&gt;  (2006). &amp;ldquo;&lt;a href=&#34;http://www.paper.edu.cn/scholar/showpdf/MUz2UN2INTA0eQxeQh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contour saliency in primary visual cortex.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Neuron&lt;/em&gt;, 50(6):951‚Äì962.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;muller2014stimulus&#34;&gt;Muller, Lyle, Alexandre Reynaud, Fr√©d√©ric Chavane, and Alain Destexhe. &lt;/a&gt; (2014). &amp;ldquo;&lt;a href=&#34;http://www.int.univ-amu.fr/IMG/pdf/Muller_Nature_Communications2014.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Stimulus-Evoked Population Response in Visual Cortex of Awake Monkey Is a Propagating Wave.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Nature Communications&lt;/em&gt; 5: 3675.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;muller2018cortical&#34;&gt; Muller, Lyle, Fr√©d√©ric Chavane, John Reynolds, and Terrence J Sejnowski. &lt;/a&gt; (2018). &amp;ldquo;&lt;a href=&#34;https://papers.cnl.salk.edu/PDFs/Cortical%20travelling%20waves_%20mechanisms%20and%20computational%20principles.%202018-4515.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cortical Travelling Waves: Mechanisms and Computational Principles.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Nature Reviews Neuroscience&lt;/em&gt; 19 (5): 255.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Tang18&#34;&gt;Tang, Hanlin, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. &lt;/a&gt; (2018). &amp;ldquo;&lt;a href=&#34;https://doi.org/10.1073/pnas.1719397115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recurrent computations for visual pattern completion.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt; 115 (35) 8835-8840.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a name=&#34;Voges12&#34;&gt; Voges, Nicole, and Laurent U Perrinet.&lt;/a&gt; (2012). &amp;ldquo;&lt;a href=&#34;https://doi.org/10.3389/fncom.2012.00041&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Complex Dynamics in Recurrent Cortical Networks Based on Spatially Realistic Connectivities.&lt;/a&gt;&amp;rdquo; &lt;em&gt;Frontiers in Computational Neuroscience&lt;/em&gt; 6.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2019-10-10: GDR vision 2019</title>
      <link>https://laurentperrinet.github.io/post/2019-10-10_gdrvision/</link>
      <pubDate>Thu, 10 Oct 2019 12:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-10_gdrvision/</guid>
      <description>&lt;p&gt;Avec Anna Montagnini, Manuel Vidal et Fran√ßoise Vitu, nous organisons cette ann√©e le GDR Vision √† Marseille les journ√©es du 10 et 11 octobre.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;plus d&amp;rsquo;infos sur &lt;a href=&#34;https://gdrvision2019.sciencesconf.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gdrvision2019.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;nous aurons un atelier m√©thodologique le jeudi matin sur les apports possibles du Deep Learning pour les sciences de la vision: &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Utiliser l&amp;rsquo;apprentissage profond en vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;la session sp√©ciale du jeudi est sponsoris√©e par la &lt;a href=&#34;https://laurentperrinet.github.io/grant/spikeai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;projet SpikeAI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;R√©unions pass√©es:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lille: &lt;a href=&#34;https://gdrvision2017.sciencesconf.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gdrvision2017.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Paris: &lt;a href=&#34;https://gdrvision2018.sciencesconf.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gdrvision2018.sciencesconf.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2019-10-10: Atelier Utiliser l&#39;apprentissage profond en vision</title>
      <link>https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/</link>
      <pubDate>Thu, 10 Oct 2019 09:30:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-10_gdrvision-atelier/</guid>
      <description>&lt;p&gt;Date : jeudi 10 octobre de 9h30 √† 12h30&lt;/p&gt;
&lt;p&gt;Intervenants : Laurent Perrinet et Chlo√© Pasturel&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://gdrvision2019.sciencesconf.org/resource/page/id/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;programme&lt;/a&gt;: Nous proposons dans cet atelier pratique de pr√©senter les nouveaux enjeux apport√©s par l&amp;rsquo;apprentissage profond et plus g√©n√©ralement par l&amp;rsquo;apprentissage machine. L&amp;rsquo;objectif est de montrer sous forme de simples exercises pratiques comment ces nouveaux outils permettent 1) de cat√©goriser des images 2) d&amp;rsquo;apprendre un tel mod√®les 3) de g√©n√©rer de nouvelles images √† partir d&amp;rsquo;une base existante.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SpikeAI/2019-10-10_ML-tutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/SpikeAI/2019-10-10_ML-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Atelier concoct√© en collaboration avec &lt;a href=&#34;https://github.com/chloepasturel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chlo√© Pasturel&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cet atelier fait partie du &lt;a href=&#34;https://laurentperrinet.github.io/post/2019-10-10_gdrvision/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GDR vision 2019&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2019-10-07: Le temps des sens</title>
      <link>https://laurentperrinet.github.io/post/2019-10-07_neurostories/</link>
      <pubDate>Mon, 07 Oct 2019 18:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-10-07_neurostories/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Cette pr√©sentation lors des &lt;a href=&#34;http://neuroschool-stories.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroStories&lt;/a&gt; vise √† aborder la notion de temps dans le cerveau.&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/jJKTdlChefc&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Chaque ann√©e, NeuroSchool nous raconte des histoires sur un th√®me √† la fois philosophique et scientifique. L‚Äôobjectif est de faire conna√Ætre, d‚Äôune mani√®re inventive, les recherches de pointe men√©es √† Marseille et ailleurs, dans le domaine des neurosciences. Le format inventif associe des NeuroStories et des causeries scientifiques.&amp;rdquo; &lt;a href=&#34;http://neuroschool-stories.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuroschool-stories.com/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Le texte de cette pr√©sentation est repris dans &lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Conversation&lt;/a&gt; (&lt;a href=&#34;https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lien direct&lt;/a&gt;) ainsi que dans &lt;a href=&#34;https://www.science-et-vie.com/paroles-d-experts/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-53387&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Science &amp;amp; Vie&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2019-10-07-neurostories-videos-of-my-talk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neurostories: d&amp;rsquo;autres figures anim√©es du flash-lag effect&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Illusions et hallucinations visuelles : une porte sur la perception</title>
      <link>https://laurentperrinet.github.io/post/2019-06-06-theconversation/</link>
      <pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-06-06-theconversation/</guid>
      <description>&lt;p&gt;Publication d&amp;rsquo;un nouvel article g√©n√©raliste autour des &amp;ldquo;Illusions et hallucinations visuelles&amp;rdquo; √† d√©couvrir sur le site &lt;a href=&#34;https://theconversation.com/illusions-et-hallucinations-visuelles-une-porte-sur-la-perception-117389&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TheConversation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Illusions et hallucinations visuelles : une porte sur la perception &lt;a href=&#34;https://t.co/nOJFVRYDz7&#34;&gt;https://t.co/nOJFVRYDz7&lt;/a&gt; &lt;a href=&#34;https://t.co/HiBPHbKdSA&#34;&gt;pic.twitter.com/HiBPHbKdSA&lt;/a&gt;&lt;/p&gt;&amp;mdash; The Conversation France (@FR_Conversation) &lt;a href=&#34;https://twitter.com/FR_Conversation/status/1136743272024612886?ref_src=twsrc%5Etfw&#34;&gt;June 6, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Les objectifs sont :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mieux comprendre la fonction de la perception visuelle en explorant certaines limites ;&lt;/li&gt;
&lt;li&gt;mieux comprendre l‚Äôimportance de l‚Äôaspect dynamique de la perception ;&lt;/li&gt;
&lt;li&gt;mieux comprendre le r√¥le de l‚Äôaction dans la perception.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;fr&#34; dir=&#34;ltr&#34;&gt;Les &lt;a href=&#34;https://twitter.com/hashtag/illusions?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#illusions&lt;/a&gt; et &lt;a href=&#34;https://twitter.com/hashtag/hallucinations?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#hallucinations&lt;/a&gt; nous informent sur les &lt;a href=&#34;https://twitter.com/hashtag/perceptions?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#perceptions&lt;/a&gt; mais aussi sur la compr√©hension des m√©canismes c√©r√©braux.&lt;br&gt;&lt;br&gt;Laurent Perrinet, &lt;a href=&#34;https://twitter.com/imera_amu?ref_src=twsrc%5Etfw&#34;&gt;@imera_amu&lt;/a&gt;, nous explique comment et pourquoi une image peut tromper nos sens.&lt;br&gt;&lt;br&gt;üå™Ô∏è&lt;a href=&#34;https://t.co/5QSaezZonz&#34;&gt;https://t.co/5QSaezZonz&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/neurosciences?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#neurosciences&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/cerveau?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#cerveau&lt;/a&gt;üß† &lt;a href=&#34;https://t.co/t8ehsyoGt8&#34;&gt;pic.twitter.com/t8ehsyoGt8&lt;/a&gt;&lt;/p&gt;&amp;mdash; The Conversation France (@FR_Conversation) &lt;a href=&#34;https://twitter.com/FR_Conversation/status/1136989689867620353?ref_src=twsrc%5Etfw&#34;&gt;June 7, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Une version √©tendue est accessible sur le &lt;a href=&#34;https://laurentperrinet.github.io/2019-05_illusions-visuelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo GitHub&lt;/a&gt;, ainsi que les &lt;a href=&#34;https://github.com/laurentperrinet/2019-05_illusions-visuelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sources&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2019-05-20: Symposium on Active Inference at NeuroFrance 2019</title>
      <link>https://laurentperrinet.github.io/post/2019-05-23-neurofrance/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-05-23-neurofrance/</guid>
      <description>&lt;h2 id=&#34;active-inference-bridging-theoretical-and-experimental-neurosciences--inference-active-un-pont-entre-neurosciences-th√©oriques-et-exp√©rimentales&#34;&gt;Active Inference: Bridging theoretical and experimental neurosciences. / Inference Active: Un pont entre neurosciences th√©oriques et exp√©rimentales.&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.neurosciences.asso.fr/V2/colloques/SN19/index_en.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://neuro-marseille.org/wp-content/uploads/2018/07/capture-decran-2018-07-06-a-190423.png&#34; alt=&#34;Site NeuroFrance&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SYMPOSIUM S17&lt;/li&gt;
&lt;li&gt;When: 23.05.2019 11:00-13:00h&lt;/li&gt;
&lt;li&gt;When: Endoume 1+2&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;s171httpswwwprofessionalabstractscomnf2019iplannerpresentation1397-active-inference-and-brain-computer-interfaces--inf√©rence-active-et-interfaces-cerveau-machine&#34;&gt;&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1397&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.1&lt;/a&gt; 	Active inference and Brain-Computer Interfaces / Inf√©rence active et interfaces cerveau-machine&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mattout J. (Lyon, France), Mladenovic J. (Lyon, France), Frey J. (Bordeaux, France)3, Joffily M. (Lyon, France), Maby E. (Lyon, France), Lotte F. (Lyon, France)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Brain-Computer Interfaces (BCIs) devices bypass natural pathways to connect the brain with a machine, directly. They may rely on invasive or non-invasive measures of brain activity and applications cover a large domain, mostly but not restricted to clinical ones. A major objective is to restore communication and autonomy in heavily motor impaired patients.
However, no BCI has made its way to a routinely used clinical application yet. One lead for improvement is to endow the machine with learning abilities so that it can optimize its decisions and adapt to changes in the user signals over time1. Several approaches have been proposed but a generic framework is still lacking to foster the development of efficient adaptive BCIs2.
Initially proposed to model perception, learning and action by the brain, the Active Inference (AI) framework offers great promises in that aim3. It rests on an explicit generative model of the environment. In BCI, from the machine&amp;rsquo;s point of view, brain signals play the role of sensory inputs on which the machine&amp;rsquo;s perception of mental states will be based. Furthermore, the machine builds up decisions and trades between different actions such as: go on observing, deciding to decide, correcting its previous action or moving on.
In this talk, I will present an instantiation of AI in the context of the EEG-based P300-speller BCI for communication, showing it can flexibly combine complementary adaptive features pertaining to both perception and action, and yield significant improvements as shown on realistic simulations. We will discuss perspectives to further extend the current model and performance as well as the challenges ahead to implement this framework online.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Mattout, J. Brain-Computer Interfaces: A Neuroscience Paradigm of Social Interaction? A Matter of Perspective. Frontiers in Human Neuroscience 6, (2012).&lt;/li&gt;
&lt;li&gt;Mladenovic, J., Mattout, J. &amp;amp; Lotte, F. A Generic Framework for Adaptive EEG-Based BCI Training and Operation. in Brain-computer interfaces handbook: technological and theoretical advances (eds. Nam, C. S., Nijholt, A. &amp;amp; Lotte, F.) Chapter 31 (Taylor &amp;amp; Francis, CRC Press, 2018).&lt;/li&gt;
&lt;li&gt;Friston, K., Mattout, J. &amp;amp; Kilner, J. Action understanding and active inference. Biological Cybernetics 104, 137-160 (2011).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;s172httpswwwprofessionalabstractscomnf2019iplannerpresentation1398-comparing-active-inference-and-reinforcement-learning-models-of-a-go-nogo-task-and-their-relationships-to-striatal-dopamine-2-receptors-assessed-using-pet--comparaison-des-mod√®les-dinf√©rence-active-et-dapprentissage-par-renforcement-dans-une-t√¢che-go--nogo--relation-avec-les-r√©cepteurs-dopaminergiques-d2-striataux-√©valu√©s-par-tep&#34;&gt;&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.2&lt;/a&gt; 	Comparing active inference and reinforcement learning models of a Go NoGo task and their relationships to striatal dopamine 2 receptors assessed using PET / Comparaison des mod√®les d&amp;rsquo;inf√©rence active et d&amp;rsquo;apprentissage par renforcement dans une t√¢che Go / NoGo : relation avec les r√©cepteurs dopaminergiques D2 striataux √©valu√©s par TEP&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;R. Adams (London)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1398&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1398&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Adaptive behaviour includes the ability to choose actions that result in advantageous outcomes. It is key to survival and a fundamental function of nervous systems. Active inference (AI) and reinforcement learning (RL) are two influential models of how the brain might achieve this. A key AI parameter is the precision of beliefs about policies. Precision controls the stochasticity of action selection - similar to decision temperature in RL - and is thought to be encoded by striatal dopamine. 75 healthy subjects performed a &amp;lsquo;go/no-go&amp;rsquo; task, and we measured striatal dopamine 2/3 receptor (D2/3R) availability in a subset of 25 using [11C]-(+)-PHNO positron emission tomography. In behavioural model comparison, RL performed best across the whole group but AI performed best in accurate subjects. D2/3R availability in the limbic striatum correlated with AI policy precision and also with RL irreducible decision &amp;rsquo;noise&amp;rsquo;. Limbic striatal D2/3R availability also correlated with AI Pavlovian prior beliefs - i.e. the respective probabilities of making or withholding actions in rewarding or loss-avoiding contexts - and the RL learning rate. These findings are consistent with the notion that occupancy of inhibitory striatal D2/3Rs controls the variability of action selection.&lt;/p&gt;
&lt;h3 id=&#34;s173httpswwwprofessionalabstractscomnf2019iplannerpresentation1399-principles-and-psychophysics-of-active-inference-in-anticipating-a-dynamic-switching-probabilistic-bias--principes-et-psychophysique-de-linf√©rence-active-dans-lestimation-dun-biais-dynamique-et-volatile-de-probabilit√©&#34;&gt;&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1399&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.3&lt;/a&gt; 	Principles and psychophysics of active inference in anticipating a dynamic, switching probabilistic bias / Principes et psychophysique de l&amp;rsquo;inf√©rence active dans l¬¥estimation d&amp;rsquo;un biais dynamique et volatile de probabilit√©&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;L. Perrinet (Marseille)&lt;/li&gt;
&lt;li&gt;see more info on this &lt;a href=&#34;https://laurentperrinet.github.io/talk/2019-05-23-neurofrance/&#34;&gt;talk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;s174httpswwwprofessionalabstractscomnf2019iplannerpresentation1400-is-laziness-contagious-a-computational-approach-to-attitude-alignment--la-fain√©antise-est-elle-contagieuse-une-approche-computationnelle-de-lalignement-des-attitudes&#34;&gt;&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/1400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.4&lt;/a&gt; 	Is laziness contagious? A computational approach to attitude alignment / La fain√©antise est-elle contagieuse? Une approche computationnelle de l¬¥alignement des attitudes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;J. Daunizeau (Paris)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What do people learn from observing others¬¥ attitudes, such as prudence, impatience or laziness? Rather than viewing these attitudes as examples of subjective and biologically entrenched personality traits, we assume that they derive from uncertain (and mostly implicit) beliefs about how to best weigh risks, delays and efforts in ensuing cost-benefit trade-offs. In this view, it is adaptive to update one¬¥s belief after having observed others¬¥ attitude, which provides valuable information regarding how to best behave in related difficult decision contexts. This is the starting point of our bayesian model of attitude alignment, which we derive in the light of recent neuroimaging findings. First, we disclose a few non-trivial predictions from this model. Second, we validate these predictions experimentally by profiling people¬¥s prudence, impatience and laziness both before and after guessing a series of cost-benefit arbitrages performed by calibrated artificial agents (which are impersonating human individuals). Third, we extend these findings and assess attitude alignment in autistic individuals. Finally, we discuss the relevance and implications of this work, with a particular emphasis on the assessment of biases of social cognition.&lt;/p&gt;
&lt;h3 id=&#34;s175httpswwwprofessionalabstractscomnf2019iplannerpresentation223-generative-bayesian-modeling-for-causal-inference-between-neural-activity-and-behavior-in-drosophila-larva&#34;&gt;&lt;a href=&#34;https://www.professionalabstracts.com/nf2019/iplanner/#/presentation/223&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;S17.5&lt;/a&gt; 	Generative Bayesian modeling for causal inference between neural activity and behavior in Drosophila larva&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;C. Barre (Paris) (TBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A fundamental property of the central nervous system is its ability to select appropriate behavioral patterns or sequences of behavioral patterns in response to sensory cues, but what are the biological mechanisms underlying decision making? The Drosophila larva is an ideal animal model for reverse-engineering the neural processes underlying behavior. The full connectome of the larva brain has been imaged at the individual-synapse level using electron microscopy.
The host of genetic techniques available for Drosophila allows us to optogenetically manipulate over 1,500 of its roughly 12,000 neurons individually in freely behaving larvae.
This enables us to establish causal relationships between neural activity, and behavior at the fundamental level of individual neurons and neural connections.
We have access to video record of the individual behavior of ~3,000,000 larvae. We have identified 6 stereotypical behavioral patterns using a combination of supervised and unsupervised machine learning. The behavioral identified for the larva: crawl, turn, stop, crawl backward, hunch (retract the head), and roll (lateral slide). Each realization of a behavioral pattern is characterized by a different duration, amplitude, and velocity.
Here we present a generative model that extracts the behavior of wildtype larvae using Bayesian inference, and interprets behavioral changes following neuron activation or inactivation from large-scale experimental screens. Fig. shows the average behavior of 10,000 larvae over time in a screen where a single neuron is activated at t=30s. A clear change in behavior is seen following activation is seen which is well captured by the model, illustrating its accuracy.
The generative model enables us to robustly detect behavioral modifications as significant deviations of the patterns in the larvae&amp;rsquo;s sequence of activities from their equilibrium behavior.&lt;/p&gt;
&lt;h3 id=&#34;neurofrance-marseille-capitale-des-neurosciences&#34;&gt;NeuroFrance: Marseille, capitale des neurosciences&lt;/h3&gt;
&lt;p&gt;Du  22 au 24 mai 2019 au Palais des congr√®s de Marseille (Parc Chanot), pr√®s de 1300 chercheurs, cliniciens et √©tudiants venus du monde entier partageront leurs travaux lors de NeuroFrance 2019, colloque international organis√© par la Soci√©t√© des Neurosciences.Au total, 8 conf√©rences pl√©ni√®res, 42 symposiums, 6 sessions sp√©cialis√©es, 525 communications affich√©es, ainsi qu‚Äôune exposition avec 42 entreprises et soci√©t√©s de biotechnologies, feront de ce colloque un moment exceptionnel pour mettre en lumi√®re les avanc√©es majeures scientifiques et technologiques sur le fonctionnement du cerveau. Vous pourrez aussi d√©couvrir le &amp;ldquo;Neurovillage&amp;rdquo; qui permettra de vous immerger au c≈ìur des innovations neuroscientifiques marseillaises, ainsi que  l‚Äôexposition  ¬´ L‚ÄôArt en t√™te ¬ª, compos√©e de cinq ≈ìuvres originales cr√©√©es par des artistes et des scientifiques. Plusieurs √©v√©nements seront √©galement propos√©s autour du colloque pour le grand public comme pour les chercheurs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018-11-09 : Retinal computations</title>
      <link>https://laurentperrinet.github.io/post/2018-11-09_seminaire-escobar/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-11-09_seminaire-escobar/</guid>
      <description>&lt;h1 id=&#34;2018-11-09--retinal-computations-by-maria-jos√©-escobar-chile&#34;&gt;2018-11-09 : &amp;ldquo;Retinal computations&amp;rdquo; by Maria Jos√© Escobar (Chile)&lt;/h1&gt;
&lt;p&gt;During a seminar at the Institute of Neurosciences Timone in Marseille, &lt;a href=&#34;http://profesores.elo.utfsm.cl/~mjescobar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mar√≠a Jos√© Escobar, Ph.D.&lt;/a&gt; :&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Retinal computations&amp;rdquo; : The retina is part of the nervous system and consists in well-organized layers of different cell types and functions. Those cells have been vastly studied in various animal models, and also the circuits conveying to different functional categories. All these different types of either physiological properties or computation equivalents revealed the retina as not a single light to electricity encoder but a pre-processing layer, which is in charge to extract relevant visual signals from the environment that are critical for animal survival. During this seminar, we describe some of the computations performed by the retina, and how this knowledge can be applied to solve engineering problems, such as image processing and robot controllers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.int.univ-amu.fr/IMG/200x130xsiteon0.png,q1331299836.pagespeed.ic.IKYGzK4Zu8.png&#34; alt=&#34;Sponsored by&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018-04-05 : *Probabilities and Optimal Inference to understand the Brain* Workshop</title>
      <link>https://laurentperrinet.github.io/post/2018-04-05_optimal-inference-brain-workshop/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-04-05_optimal-inference-brain-workshop/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;participants&#34; srcset=&#34;
               /post/2018-04-05_optimal-inference-brain-workshop/IMG_20180406_164630_huae698e854cd8a9de2cb29f84e673de52_1153827_4ca93f9633bacb96bea5b50a67d8d39d.webp 400w,
               /post/2018-04-05_optimal-inference-brain-workshop/IMG_20180406_164630_huae698e854cd8a9de2cb29f84e673de52_1153827_92595f1182896aec7e68c0c0d5949134.webp 760w,
               /post/2018-04-05_optimal-inference-brain-workshop/IMG_20180406_164630_huae698e854cd8a9de2cb29f84e673de52_1153827_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2018-04-05_optimal-inference-brain-workshop/IMG_20180406_164630_huae698e854cd8a9de2cb29f84e673de52_1153827_4ca93f9633bacb96bea5b50a67d8d39d.webp&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;probabilities-and-optimal-inference-to-understand-the-brain&#34;&gt;Probabilities and Optimal Inference to understand the Brain&lt;/h1&gt;
&lt;h2 id=&#34;a-2-day-workshop-at-the-institute-of-neurosciences-timone-in-marseille&#34;&gt;a 2-day workshop at the Institute of Neurosciences Timone in Marseille&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;affiche&#34; srcset=&#34;
               /post/2018-04-05_optimal-inference-brain-workshop/featured_hua1f3e7f4c0c9e87c9b8193a67c44f8ab_3107732_12d275936498515eae8406ca7e5a39e3.webp 400w,
               /post/2018-04-05_optimal-inference-brain-workshop/featured_hua1f3e7f4c0c9e87c9b8193a67c44f8ab_3107732_2cfe37c28267763f50140880b24750de.webp 760w,
               /post/2018-04-05_optimal-inference-brain-workshop/featured_hua1f3e7f4c0c9e87c9b8193a67c44f8ab_3107732_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2018-04-05_optimal-inference-brain-workshop/featured_hua1f3e7f4c0c9e87c9b8193a67c44f8ab_3107732_12d275936498515eae8406ca7e5a39e3.webp&#34;
               width=&#34;552&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Date&lt;/dt&gt;
&lt;dd&gt;April 5-6th 2018&lt;/dd&gt;
&lt;dt&gt;Location&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute of Neurosciences Timone in Marseille in the south of
France&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Main site&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;https://opt-infer-brain.sciencesconf.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://opt-infer-brain.sciencesconf.org/&lt;/a&gt;&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Full program&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;&lt;a href=&#34;https://opt-infer-brain.sciencesconf.org/program/details&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://opt-infer-brain.sciencesconf.org/program/details&lt;/a&gt;.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt&gt;Organizing committee&lt;/dt&gt;
&lt;dd&gt;Paul Apicella, Frederic Danion, Nicole Malfait, Anna Montagnini and
Laurent Perrinet&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.int.univ-amu.fr/IMG/200x130xsiteon0.png,q1331299836.pagespeed.ic.IKYGzK4Zu8.png&#34; alt=&#34;Sponsored by&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2018-03-26 : PhD Program: course in Computational Neuroscience</title>
      <link>https://laurentperrinet.github.io/post/2018-03-26-cours-neuro-comp-fep/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-03-26-cours-neuro-comp-fep/</guid>
      <description>&lt;h1 id=&#34;phd-program-course-in-computational-neuroscience&#34;&gt;PhD Program: course in Computational Neuroscience&lt;/h1&gt;
&lt;p&gt;Context&lt;/p&gt;
&lt;p&gt;Computational neuroscience is an expending field that is proving to be essential in neurosciences. The aim of this course will be to provide a common solid background in computational neurosciences. The course will comprise historical recall of the field and a description of the different modelling approaches that are currently developed, including details about their specificities, limits and advantages.&lt;/p&gt;
&lt;p&gt;Objective&lt;/p&gt;
&lt;p&gt;The course aims at introducing students with the major tools that will be necessary during their thesis to model or analyze their neuroscientific results. While it will start by a short, generic introduction, we will then explore different systems at different scales. On the first day, we will study the different possible regimes in which a single neuron can behave, while progressively introducing the theory of dynamical systems to understand these more globally. Then, during the second day, we will introduce methods to analyze neuroscientific data in general, such as Bayesian methods and information theory. This will be implemented by simple practical examples.&lt;/p&gt;
&lt;p&gt;Language of intervention&lt;/p&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;p&gt;Number of hours&lt;/p&gt;
&lt;p&gt;~20 hours (session 1=7 + session 2=7 + session 3=4)&lt;/p&gt;
&lt;p&gt;Max participants&lt;/p&gt;
&lt;p&gt;15 for the practical sessions (afternoon Day 2 and Day 3), unlimited for theoretical courses&lt;/p&gt;
&lt;p&gt;Public priority&lt;/p&gt;
&lt;p&gt;PhD students&lt;/p&gt;
&lt;p&gt;Public concerned&lt;/p&gt;
&lt;p&gt;PhD students, interested M2 students and postdocs&lt;/p&gt;
&lt;p&gt;Location&lt;/p&gt;
&lt;p&gt;Institut des Neurosciences de la Timone (INT)&lt;/p&gt;
&lt;p&gt;Keywords&lt;/p&gt;
&lt;p&gt;neuronal modelling, neural circuit modelling, information theory, decoding and encoding&lt;/p&gt;
&lt;p&gt;Targets&lt;/p&gt;
&lt;p&gt;Understanding how computational modelling can be used to formulate and solve neuroscience problems at different spatial and temporal scales; learning the formal notions of information, encoding and decoding and experimenting their use on toy datasets&lt;/p&gt;
&lt;p&gt;Program&lt;/p&gt;
&lt;p&gt;&lt;em&gt;First session:&lt;/em&gt; Introduction to modeling single neurons (morning); An introduction to neural masses: modeling assemblies of neurons up to capturing collective oscillations and resting state dynamics in a mean-field model - presentation of the Virtual Brain software (afternoon) - &lt;em&gt;Second session:&lt;/em&gt; An overview on &amp;ldquo;What is encoding?&amp;rdquo; &amp;ldquo;What is decoding?&amp;rdquo;: formalization of the notion of information in neural activity; shared and transferred information; integration, segregation and complexity (morning). Bayesian probabilities, the Free-energy principle and Active Inference, with practical demonstrations in python (afternoon). &lt;em&gt;Third session:&lt;/em&gt; the problem of information estimation in practice. Practical exercices in Matlab: estimating entropy and stimulus decodability from spike trains; comparing coding hypotheses (morning).&lt;/p&gt;
&lt;p&gt;Pre-required&lt;/p&gt;
&lt;p&gt;Basic knowledge of statistics and probability and calculus (differential equations,&amp;hellip;) is useful, but steps will be explained and complex math avoided as much as possible. Practical exercises are in python and/or MATLAB, so basic knowledge of these environments is a plus.&lt;/p&gt;
&lt;h2 id=&#34;program&#34;&gt;program&lt;/h2&gt;
&lt;h3 id=&#34;day-1--2018-03-26--an-introduction-to-computational-neuroscience&#34;&gt;day 1 : 2018-03-26 : an introduction to Computational Neuroscience&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;09:30-12:30 = &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2015-12-08_cours_neurocomp/2017-03-06_LaurentPezard.pdf&#34; title=&#34;Introduction to modeling single neurons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to modeling single neurons&lt;/a&gt; (LaP)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14:00-17:00 = An introduction to neural masses: modeling assemblies of neurons up to capturing resting state dynamics in a mean-field model - presentation of the Virtual Brain software (DaB)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-2--2018-03-27--information-theory--bayesian-models&#34;&gt;day 2 : 2018-03-27 : Information theory / bayesian models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;09:15-10:30 = An overview on &amp;ldquo;What is encoding?&amp;rdquo; &amp;ldquo;What is decoding?&amp;rdquo;: formalization of the notion of information in neural activity (DaB)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;11:00-12:15 = (&amp;hellip;continued after the coffee break: ) Live information! From sharing information to transferring information (and a glimpse into the zoo of higher-order friends) (DaB)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14:00-17:10 = &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/files/2018-03-26_cours-NeuroComp_FEP.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probabilities, the Free-energy principle and Active Inference&lt;/a&gt; (LuP).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-3--2018-03-28--practical-course-on-information-theory&#34;&gt;day 3 : 2018-03-28 : Practical course on Information theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;09:30-12:30 = Practical course on Information theory (DaB)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-material-related-to-the-course&#34;&gt;More material related to the course&lt;/h2&gt;
&lt;h3 id=&#34;day-1---morning--the-single-neuron&#34;&gt;day 1 - morning : the single neuron&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;site du livre de Gerstner et al &amp;ldquo;Neuronal Dynamics&amp;rdquo;: &lt;a href=&#34;http://neuronaldynamics.epfl.ch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuronaldynamics.epfl.ch/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A (longer) introduction to the Hodgkin-Huxley model in three steps by Dr Stefano Luccioli&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez1.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuro.fi.isc.cnr.it/uploads/TALKS/lez3.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An interactive course with Wulfram Gerstner &lt;a href=&#34;https://www.edx.org/course/neuronal-dynamics-computational-epflx-bio465-1x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.edx.org/course/neuronal-dynamics-computational-epflx-bio465-1x&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;His book ONLINE &lt;a href=&#34;http://cn.epfl.ch/~gerstner/NeuronalDynamics-MOOC1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://cn.epfl.ch/~gerstner/NeuronalDynamics-MOOC1.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-1---afternoon--neural-mass-models&#34;&gt;day 1 - afternoon : neural mass models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Another interactive course @ Washington University &lt;a href=&#34;https://www.coursera.org/course/compneuro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.coursera.org/course/compneuro&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Collection of didactic material for the EU FP7 ITN Neural Engineering Transformative Technology &lt;a href=&#34;http://www.neural-engineering.eu/training/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.neural-engineering.eu/training/index.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Didactic material from Lab in Computational Neuroscience &lt;a href=&#34;http://neuro.fi.isc.cnr.it/index.php?page=didactic-material&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuro.fi.isc.cnr.it/index.php?page=didactic-material&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A open source simulator of a whole brain which runs on your laptop, &amp;ldquo;The Virtual Brain&amp;rdquo;: &lt;a href=&#34;http://thevirtualbrain.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://thevirtualbrain.org&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-2---morning--information-theory&#34;&gt;day 2 - morning : information theory&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The best book on information theory and decoding, freely available directly from the author: &lt;a href=&#34;http://www.inference.phy.cam.ac.uk/itprnn/book.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.inference.phy.cam.ac.uk/itprnn/book.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;a gentle introduction to bayesian methods : &lt;a href=&#34;https://homepages.inf.ed.ac.uk/pseries/Peg_files/Chapter9_SotiropoulosSeries.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://homepages.inf.ed.ac.uk/pseries/Peg_files/Chapter9_SotiropoulosSeries.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;day-2---afternoon--bayesian-models&#34;&gt;day 2 - afternoon : bayesian models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;an interesting read : &lt;a href=&#34;http://cognitrn.psych.indiana.edu/busey/q551/PDFs/PredictivCodingRaoBallard.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://cognitrn.psych.indiana.edu/busey/q551/PDFs/PredictivCodingRaoBallard.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;a tutorial on free-energy : some exercises : &lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0022249615000759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.sciencedirect.com/science/article/pii/S0022249615000759&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;solutions to the tutorial : &lt;a href=&#34;https://laurentperrinet.github.io/sciblog/posts/2017-01-15-bogacz-2017-a-tutorial-on-free-energy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/sciblog/posts/2017-01-15-bogacz-2017-a-tutorial-on-free-energy.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contacts&#34;&gt;contacts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LaP: Laurent Pezard &amp;laquo;&lt;a href=&#34;mailto:Laurent.Pezard@univ-amu.fr&#34;&gt;Laurent.Pezard@univ-amu.fr&lt;/a&gt;&amp;raquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DaB: Demian Battaglia &amp;laquo;&lt;a href=&#34;mailto:demian.battaglia@univ-amu.fr&#34;&gt;demian.battaglia@univ-amu.fr&lt;/a&gt;&amp;raquo;, INS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LuP: Laurent Udo Perrinet &amp;laquo;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&amp;raquo;, INT&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PhD program: Nicole Malfait &amp;laquo;&lt;a href=&#34;mailto:Nicole.Malfait@univ-amu.fr&#34;&gt;Nicole.Malfait@univ-amu.fr&lt;/a&gt;&amp;raquo;, Anna Montagnini &amp;laquo;&lt;a href=&#34;mailto:anna.montagnini@univ-amu.fr&#34;&gt;anna.montagnini@univ-amu.fr&lt;/a&gt;&amp;raquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.int.univ-amu.fr/IMG/200x130xsiteon0.png,q1331299836.pagespeed.ic.IKYGzK4Zu8.png&#34; alt=&#34;Sponsored by&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2016-10-26 : EUVIP BICV</title>
      <link>https://laurentperrinet.github.io/post/2016-10-26_euvip-bicv/</link>
      <pubDate>Wed, 26 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2016-10-26_euvip-bicv/</guid>
      <description>&lt;h1 id=&#34;2016-10-26--euvip-special-session-on-biologically-inspired-computer-vision&#34;&gt;2016-10-26 : EUVIP Special Session on &lt;em&gt;Biologically Inspired Computer Vision&lt;/em&gt;&lt;/h1&gt;
&lt;h2 id=&#34;description-of-the-session&#34;&gt;description of the session&lt;/h2&gt;
&lt;p&gt;Recent advances in imaging technologies have yielded scientific data at
unprecedented detail and volume, leading to the need of a shift of
paradigm in image processing and computer vision. Beyond the usual
classical von Neumann architecture, one strategy that is emerging in
order to process and interpret this amount of data follows from the
architecture of biological organisms and shows for instance
computational paradigms implementing asynchronous communication with a
high degree of local connectivity in sensors or brain tissues. This
session aims at bringing together researchers from different fields of
Biologically Inspired Computer Vision to present latest results in the
field, from fundamental to more specialized topics, including visual
analysis based on a computational level, hardware implementation, and
the design of new more advanced vision sensors. It is expected to
provide a comprehensive overview in the computer area of biologically
motivated vision. On the one hand, biological organisms can provide a
source of inspiration for new computationally efficient and robust
vision models and on the other hand machine vision approaches can
provide new insights for understanding biological visual systems. This
session covers a wide range of topics from fundamental to more
specialized topics, including visual analysis based on a computational
level, hardware implementation, and the design of new more advanced
vision sensors. In particular, we expect to provide an overview of a few
representative applications and current state of the art of the research
in this area.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;URL
&lt;a href=&#34;http://www-l2ti.univ-paris13.fr/euvip2016/index.php/86-euvip2016/129-tentative-technical-program-in-detail&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www-l2ti.univ-paris13.fr/euvip2016/index.php/86-euvip2016/129-tentative-technical-program-in-detail&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;date
October 26th, 2016&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Location
Ecole Centrale Marseille&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Address
&lt;a href=&#34;https://www.centrale-marseille.fr/fr/acces-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;38 rue Fr√©d√©ric Joliot-Curie 13013 Marseille,
France&lt;/a&gt; Phone : +33
(0)4 91 05 45 45&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Programme&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;13.50  &lt;a href=&#34;http://ieeexplore.ieee.org/document/7764586/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual System Inspired Algorithm For Contours, Corner And T Junction Detection&lt;/a&gt;, Antoni Buades, &lt;em&gt;Rafael Grompone Von Gioi&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;13.50  &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-26-perrinet-16-euvip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biologically-inspired characterization of sparseness in natural images&lt;/a&gt;, &lt;em&gt;Laurent Perrinet&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.10 &lt;a href=&#34;http://david.alleysson.free.fr/Publications/JIST0224reprint.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Color filter array imitating the random nature of color arrangement in the human cone mosaic&lt;/a&gt;, Prakhar Amba, &lt;em&gt;David Alleysson&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.30 &lt;a href=&#34;http://ieeexplore.ieee.org/document/7764601/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Illuminant-Independent Analysis Of Reflectance As Sensed By Humans, And Its Applicability To Computer Vision&lt;/a&gt;, Alban Flachot, Phelma, J.Kevin O&amp;rsquo;Regan, &lt;em&gt;Edoardo Provenzi&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;14.50 &lt;a href=&#34;https://laurentperrinet.github.io/talk/2016-10-26-fillatre-barlaud-perrinet-16-euvip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Categorization of microscopy images using a biologically inspired edge co-occurrences descriptor&lt;/a&gt;, Lionel Fillatre, Michel Barlaud, &lt;em&gt;Laurent Perrinet&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Elasticit√© dynamique</title>
      <link>https://laurentperrinet.github.io/post/2016-06-02_elasticite/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2016-06-02_elasticite/</guid>
      <description>&lt;h1 id=&#34;trame-√©lasticit√©---fondation-vasarely-√†-aix-en-provence&#34;&gt;Trame √âlasticit√© @  Fondation Vasarely √† Aix-en-Provence&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Dans ¬´ Trame √âlasticit√© ¬ª, pr√©sent√©e en 2016 dans le cadre d‚Äôun hommage √† Victor Vasarely √† la Fondation d‚ÄôAix-en-Provence, 25 monolithes de 3‚Äâm de hauteur et 40‚Äâcm de largeur √©taient plac√©s sur un socle rectiligne de 5‚Äâm de long et pouvaient tourner ind√©pendamment suivant leur axe vertical. Cette chor√©graphie produisait des moments de calme cristallin qui rapidement se transformaient en instants de chaos. Ce proc√©d√© permettait de projeter son propre reflet tout en le fragmentant dans l‚Äôenvironnent de l‚Äô≈ìuvre, notamment les rythmes color√©s de Vasarely, afin de produire un va-et-vient entre les mondes r√©els et per√ßus. Les observateurs devaient alors changer de perspective pour r√©soudre cette incertitude et explorer le lien entre le monde r√©el et le monde per√ßu  (voir aussi &lt;a href=&#34;https://laurentperrinet.github.io/2023-01-31_formes-et-perception/#fig:%c3%89lasticit%c3%a9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cet article&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/198189587&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;2016, &lt;a href=&#34;https://github.com/NaturalPatterns/elasticite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/NaturalPatterns/elasticite&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;L&amp;rsquo;installation ¬´ Trame √âlasticit√© ¬ª agit comme un filtre et g√©n√®re de nouveaux espaces d√©multipli√©s, comme un empilement quasi infini d&amp;rsquo;horizons.
Par principe de r√©flexion, la pi√®ce absorbe l&amp;rsquo;image de l&amp;rsquo;environnement et accumule les points de vue ; le mouvement permanent requalifie continuellement ce qui est regard√© et entendu.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DIMENSIONS : 3 m de haut 5 m de large, INOX POLI MIROIR / ALUMINIUM / ACIER / MOTEURS / PROGRAMME TEMPS R√âEL&lt;/li&gt;
&lt;li&gt;LIEU : Fondation Vasarely&lt;/li&gt;
&lt;li&gt;EXPOSITION : &lt;a href=&#34;http://ondesparalleles.org/projets/trame-elasticite-vasarely/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multiplicit√©, Fondation Vasarely dans le cadre de l‚ÄôHommage √† Victor Vasarely&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EXPOSITION : DU 2 juin au 2 octobre 2016&lt;/li&gt;
&lt;li&gt;VIDEOS : &lt;a href=&#34;http://vimeo.com/198189587&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://vimeo.com/198189587&lt;/a&gt; Cr√©dits :  ¬© Etienne Rey&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TRAME_Elasticit√©.jpg&lt;/p&gt;














&lt;figure  id=&#34;figure-trame-√©lasticit√©httpsondesparallelesorgprojetstrame-elasticite-vasarely-2016-un-hommage-√†-victor-vasarely-dans-le-cadre-de-la-fondation-daix-en-provence-copyright-√©tienne-rey-adagp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;[*Trame √âlasticit√©*](https://ondesparalleles.org/projets/trame-elasticite-vasarely/), 2016, un hommage √† Victor Vasarely dans le cadre de la Fondation d&amp;#39;Aix-en-Provence. Copyright √âtienne Rey (ADAGP).&#34; srcset=&#34;
               /post/2016-06-02_elasticite/TRAME_Elasticit%C3%A9_hu2e6867f663211fe3eba714f17ca98efc_3577116_df28fde6b7b03fa172f9e352b7fa99d0.webp 400w,
               /post/2016-06-02_elasticite/TRAME_Elasticit%C3%A9_hu2e6867f663211fe3eba714f17ca98efc_3577116_4f2c1e2c5ea3f1e8329194c38db88401.webp 760w,
               /post/2016-06-02_elasticite/TRAME_Elasticit%C3%A9_hu2e6867f663211fe3eba714f17ca98efc_3577116_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2016-06-02_elasticite/TRAME_Elasticit%C3%A9_hu2e6867f663211fe3eba714f17ca98efc_3577116_df28fde6b7b03fa172f9e352b7fa99d0.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      &lt;a href=&#34;https://ondesparalleles.org/projets/trame-elasticite-vasarely/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Trame √âlasticit√©&lt;/em&gt;&lt;/a&gt;, 2016, un hommage √† Victor Vasarely dans le cadre de la Fondation d&amp;rsquo;Aix-en-Provence. Copyright √âtienne Rey (ADAGP).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Compos√© d‚Äôune succession de lames de miroirs, verticales et rotatives, l‚Äôinstallation Trame se joue des reflets et de la d√©multiplication de l‚Äôespace, offrant au spectateur une multiplicit√© de points de vue dans lesquels il peut se perdre √† loisir. Par un effet de ¬´ porosit√© ¬ª recherch√© par l‚Äôartiste, le dispositif dialogue intens√©ment avec les Int√©grations.&lt;/p&gt;
&lt;p&gt;Devant l‚Äô≈ìuvre en constante m√©tamorphose, l‚Äôalphabet plastique de Vasarely se recompose ainsi √† l‚Äôinfini comme un jeu de construction renouvelable. Dans cette ≈ìuvre, Etienne Rey explore en profondeur les possibilit√©s offertes par le mouvement, la lumi√®re, et surtout l‚Äôinteraction entre l‚Äô≈ìuvre, le public et l‚Äôespace, ouvrant sur de nouveaux rapports sensibles et sensoriels au monde.&lt;/p&gt;
&lt;p&gt;(Texte : V√©ronique Baton)&lt;/p&gt;
&lt;h1 id=&#34;trame-√©lasticit√©---104-paris&#34;&gt;Trame √âlasticit√© @  104 (Paris)&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/150813922&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;LIEU:: NEMO, BIENNALE INTERNATIONALE DES ARTS NUMERIQUES - CENTQUATRE - 104&lt;/li&gt;
&lt;li&gt;EXPOSITION : &lt;a href=&#34;http://www.104.fr/programmation/evenement.html?evenement=518&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prosopop√©es : Quand les objets prennent vie&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;VERNISSAGE : SAMEDI 5 D√âCEMBRE 14h &amp;gt; 23h30&lt;/li&gt;
&lt;li&gt;EXPOSITION : DU 6 D√âCEMBRE 2015 AU 18 JANVIER 2016&lt;/li&gt;
&lt;li&gt;VIDEOS : &lt;a href=&#34;https://vimeo.com/150654250&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vimeo.com/150654250&lt;/a&gt; : installation;  &lt;a href=&#34;https://vimeo.com/146242233&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vimeo.com/146242233&lt;/a&gt; : simulation, Cr√©dits : ¬´ Trame √âlasticit√© ¬ª ¬© Etienne Rey, Adagp Paris 2015&lt;/li&gt;
&lt;/ul&gt;

 &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
   &lt;iframe src=&#34;https://www.youtube.com/embed/UvE3ysXieSk&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.lafriche.org/public_data/diapo/resident/1454686884/desk/2._elasticite_dynamique-etienne_rey-photoquentin_chevrier_pour_art2m_et_arcadi_ile_de_france.jpg&#34; alt=&#34;arcadi&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Elasticit√© dynamique est compos√©e des pi√®ces Expansion, Trame √âlasticit√© et Lignes sonores. Volume hexagonal en miroir de 7 m√®tres de diam√®tre, Expansion fonctionne comme une chambre d&#39;√©cho. A l&#39;int√©rieur de ce volume se situe ¬´ Trame √âlasticit√© ¬ª. Constitu√©e de 25 lames de miroir en rotation, cette pi√®ce r√©oriente continuellement le regard. Quant √† Lignes sonores, elle est form√©e de quatre monolithes orient√©s vers Expansion et √©met des sons qui se r√©orientent en fonction du mouvement des lames. (¬© Etienne Rey, Adagp Paris 2015)&#39;&#39;|width=&amp;quot;100%&amp;quot;}}Elasticit√© dynamique est compos√©e des pi√®ces Expansion, Trame et Lignes sonores. &amp;lt;&amp;lt;BR&amp;gt;&amp;gt; Volume hexagonal en miroir de 7 m√®tres de diam√®tre, Expansion fonctionne comme une chambre d&#39;√©cho. A l&#39;int√©rieur de ce volume se situe Trame. Constitu√©e de 25 lames de miroir en rotation, cette pi√®ce r√©oriente continuellement le regard. Quant √† Lignes sonores, elle est form√©e de quatre monolithes orient√©s vers Expansion et √©met des sons qui se r√©orientent en fonction du mouvement des lames. &amp;lt;&amp;lt;BR&amp;gt;&amp;gt;(¬© Etienne Rey, Adagp Paris 2015)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;equipe&#34;&gt;EQUIPE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Etienne Rey : Artiste plasticien&lt;/li&gt;
&lt;li&gt;Wilfried Wendling : Compositeur&lt;/li&gt;
&lt;li&gt;Laurent Perrinet : Chercheur en Neurosciences √† l‚ÄôINT / CNRS-AMU&lt;/li&gt;
&lt;li&gt;Atelier Ni : Accompagnement conception&lt;/li&gt;
&lt;li&gt;Gauthier Le Rouzic : √âlectronique&lt;/li&gt;
&lt;li&gt;Lucie Evans : Assistante&lt;/li&gt;
&lt;li&gt;Remerciements : Guillaume Stagnaro&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;production-d√©l√©gu√©e&#34;&gt;PRODUCTION D√âL√âGU√âE&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Seconde Nature&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;soutiens&#34;&gt;SOUTIENS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;DRAC PACA _aide individuelle √† la cr√©ation&lt;/li&gt;
&lt;li&gt;REGION PACA _CAC art visuel&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;co-production&#34;&gt;CO-PRODUCTION&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ARCADI&lt;/li&gt;
&lt;li&gt;La Muse en Circuit. Centre National de Cr√©ation Musicale&lt;/li&gt;
&lt;li&gt;CNRS-AMU / INT, Institut de Neurosciences de la Timone&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Instabilit√© (series) @ Art-O-Rama</title>
      <link>https://laurentperrinet.github.io/post/2018-09-09_artorama/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-09-09_artorama/</guid>
      <description>&lt;h1 id=&#34;instabilit√©-series&#34;&gt;Instabilit√© (series)&lt;/h1&gt;
&lt;h2 id=&#34;installation-s√©rigraphie-dessin-mural-lumi√®re-2018--guest-artist-m√©c√®nes-du-sud--art-o-rama-fair-marseille-i-2018&#34;&gt;Installation (s√©rigraphie, dessin mural, lumi√®re), 2018;  Guest artist, M√©c√®nes du Sud / Art-O-Rama (Fair), Marseille I 2018&lt;/h2&gt;
&lt;p&gt;M√©c√®nes du Sud invite chaque ann√©e un artiste laur√©at pour concevoir un stand-projet au sein du salon d‚Äôart contemporain ART-O-RAMA  : &amp;quot; Le travail d&amp;rsquo;&lt;a href=&#34;https://laurentperrinet.github.io/author/etienne-rey/&#34;&gt;Etienne Rey&lt;/a&gt;, laur√©at 2011, explore la notion m√™me d&amp;rsquo;espace. Il d√©tourne des ph√©nom√®nes physiques pour faire surgir par des biais perceptifs une conscience &amp;ldquo;d&amp;rsquo;√™tre l√†&amp;rdquo;. Ses installations sont exp√©rientielles par nature. Elles ne proposent pas une exp√©rience elles la contiennent. Ce sont des oeuvres actives qui impliquent pr√©sence et espace. Instabilit√©s, l&amp;rsquo;ensemble pr√©sent√© ici, issu de recherches en cours de d√©veloppement, met en tension espace et temps dans une exp√©rience vibratoire. Chaos et cristal attisent des forces instables dans un rapport de contraintes qui d√©termine la complexit√© des oeuvres pr√©sent√©es.&amp;quot; B√©n√©dicte Chevallier&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En collaboration avec le chercheur Laurent Perrinet, CNRS-AMU / Institut de Neurosciences de la Timone&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;links--liens&#34;&gt;Links / Liens:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ART-O-RAMA - &lt;a href=&#34;http://art-o-rama.fr/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://art-o-rama.fr/en&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lemonde.fr/argent/article/2018/08/19/avec-art-o-rama-marseille-se-demarque-sur-le-marche-de-l-art_5343904_1657007.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lemonde.fr/argent/article/2018/08/19/avec-art-o-rama-marseille-se-demarque-sur-le-marche-de-l-art_5343904_1657007.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.apollo-magazine.com/why-manifesta-makes-sense-in-marseille/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.apollo-magazine.com/why-manifesta-makes-sense-in-marseille/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Retour par &lt;a href=&#34;https://www.enrevenantdelexpo.com/2018/09/07/retour-sur-art-o-rama-2018-j1-marseille-1er-partie/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;En revenant de l&amp;rsquo;expo !&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i0.wp.com/www.enrevenantdelexpo.com/wp-content/uploads/2018/09/Art-O-Rama-2018-M%C3%A9c%C3%A8nes-du-Sud-%C3%89ienne-Rey-01.jpg&#34; alt=&#34;Sortie mod√®le OptimalPacking&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SALON INTERNATIONAL D&amp;rsquo;ART CONTEMPORAIN ART-O-RAMA&lt;/li&gt;
&lt;li&gt;SALON : 31 AO√õT &amp;gt; 2 SEPTEMBRE 2018&lt;/li&gt;
&lt;li&gt;EXPOSITION &amp;gt; 9 SEPTEMBRE 2018&lt;/li&gt;
&lt;li&gt;ESPACE PARTENAIRES - J1, QUAI DE LA JOLIETTE, MARSEILLE 2e&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Proverbes Et Citations</title>
      <link>https://laurentperrinet.github.io/post/proverbes-et-citations/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/proverbes-et-citations/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;I thought of the slowing down or the speeding up of motion as a sort of temporal equivalent: slow motion as an enlargement, a microscopy of time, and speeded-up motion as a foreshortening, a telescopy of time‚Äù (Oliver Sacks, ‚ÄúThe River of Consciousness‚Äù)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Pas besoin d‚Äô√™tre dans une b√¢tisse pour se sentir hant√©, le cerveau a suffisamment de couloirs.&amp;rdquo; (√âmilie Dickinson, fin du 19e si√®cle)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚ÄùLearn from nature: that is where our future lies‚Äù (Leonardo da Vinci)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;We should all do what, in the long run, gives us joy, even if it is only picking grapes or sorting spikes.&amp;rdquo; (E. B. White, 1989)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;‚ÄúAll things originate from one another, and vanish into one another/ According to necessity; They give each other justice and recompense for injustice / In conformity with the order of Time.‚Äù &lt;a href=&#34;https://www.theguardian.com/books/2023/feb/13/anaximander-and-the-nature-of-science-by-carlo-rovelli-review-the-ancient-master-of-the-universe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anaximender&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;There are two kinds of people: those who accept that things can be divided into two distinct categories, and those who choose to live in denial.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;I strongly believe that ‚Äòif you are the smartest person in the room, you‚Äôre in the wrong room‚Äô. Unless you are the only person in the room.&amp;rdquo; &lt;a href=&#34;https://nin.nl/about-us/the-organisation/team/evgenia-salta/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evgenia Salta&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.&amp;rdquo; Edsger Dijkstra&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Si les cochons pouvaient regarder en l&amp;rsquo;air, on en ferait des marins&amp;hellip;&amp;rdquo; (anonyme)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;L&amp;rsquo;art optique, c&amp;rsquo;est : ¬´ ce qui se passe dans l&amp;rsquo;esprit du spectateur quand son ≈ìil est oblig√© d&amp;rsquo;organiser un champ perceptif tel qu&amp;rsquo;il est n√©cessairement instable¬ª Viktor Vasarely&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;To us, probability is the very guide of life.&amp;rdquo; &lt;a href=&#34;https://www.causeweb.org/cause/resources/fun/quotes/cicero-probability?id=290&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marcus Tullius Cicero (106 BCE - 43 BCE)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Notre m√®re st√©rile r√©clame un enfant. Mon ami, mon amour d&amp;rsquo;ami, Que cela soit terrible ou sublime, Ce n&amp;rsquo;est pas moi qui clame, c&amp;rsquo;est la terre qui tonne&amp;rdquo; Attila J√≥zsef (1924, traduit dans la chanson √©ponyme de &lt;a href=&#34;https://genius.com/Noir-desir-ce-nest-pas-moi-qui-clame-lyrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Noir D√©sir&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Savoir marcher sur le fil tendu entre la fronti√®re des densit√©s humaines sauve de l&amp;rsquo;isolement.&amp;rdquo; Babouillec&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;¬´¬†Ne pense pas mais regarde plut√¥t¬†!¬†¬ª Ludwig Wittgenstein (Remarques philosophiques, fragment 66)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Free will, we&amp;rsquo;re determined to have it&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Scientist would rather borrow the toothbrush of other scientists than their words&amp;rdquo; - G. Edelman&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚ÄúSlow is smooth, smooth is fast.‚Äù  &lt;a href=&#34;https://www.nytimes.com/2020/11/09/sports/emily-harrington-free-climb-yosemite.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emily Harrington on El Cap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;The question of whether a computer can think is no more interesting than the question of whether a submarine can swim.&amp;rdquo; - Edsger W. Dijkstra&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;We are all prodigious Olympians in perceptual and motor areas, so good that we make the difficult look easy.&amp;rdquo; (Hans Moravec)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Science is like sex: sometimes something useful comes out, but that is not the reason we are doing it.&amp;rdquo; (Richard Feynman)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Mathematics is no more computation than typing is literature.&amp;rdquo; (John Allen Paulos)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;C&amp;rsquo;est ce que je fais qui m&amp;rsquo;apprend ce que je cherche.&amp;rdquo; (Pierre Soulages)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚Äú&lt;strong&gt;Harry Potter:&lt;/strong&gt; Is this real? Or has this been happening inside my head?  &lt;strong&gt;Professor Albus Dumbledore:&lt;/strong&gt; Of course it is happening inside your head, Harry, but why on earth should that mean that it is not real?‚Äù ‚Äï (J.K. Rowling, &lt;a href=&#34;https://www.goodreads.com/work/quotes/2963218&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Harry Potter and the Deathly Hallows&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;I do know what time is,&amp;rdquo; Tubby declared. He paused. &amp;ldquo;Time,&amp;rdquo; he added slowly &amp;ndash; &amp;ldquo;time is what keeps everything from happening at once. I know that&amp;ndash;I seen it in print too.&amp;rdquo; (Ray Cummings)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;You don‚Äôt see it because it‚Äôs there, it‚Äôs there because you see it.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Look for the bare necessities / The simple bare necessities / Forget about your worries and your strife / I mean the bare necessities / Old Mother Nature‚Äôs recipes / That bring the bare necessities of life&amp;rdquo; ‚Äì Baloo‚Äôs song [The Jungle Book]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Ni rire, ni pleurer, ni haiÃàr, mais comprendre&amp;rdquo; (Baruch Spinoza)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;For years there has been a theory that millions of monkeys typing at random on millions of typewriters would reproduce the entire works of Shakespeare. The Internet has proven this theory to be untrue.&amp;rdquo; (???, ???)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Je me suis endormie, ce matin/en pensant/sur tes l√®vres.&amp;rdquo;  (Anonyme, sur les murs, Marseille, 2017)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;If you can look into the seeds of time, And say which grain will grow and which will not; Speak&amp;hellip;&amp;rdquo; (Shakespeare, Macbeth, Act I, Scene 3)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Here, too, the honorable finds its due, and there are tears for passing things; here, too, things mortal touch the mind.&amp;rdquo; (Virgil, Aeneid, 29-19 BC)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Man&amp;rsquo;s maturity is to have regained the seriousness that he had as a child at play.&amp;rdquo; (Friedrich Nietzsche)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;There are 10 types of people in the world. Those who understand binary, those who don&amp;rsquo;t, those who weren&amp;rsquo;t expecting a base 8 joke, and 5 other types of people.&amp;rdquo; &lt;a href=&#34;http://unix.stackexchange.com/questions/178162/why-does-bash-think-016-1-15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Story&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Ce qui fut se refait; tout coule comme une eau / Et rien dessous le Ciel ne se voit de nouveau/ Mais la forme se change en une autre nouvelle/ Et ce changement-l√†. Vivre au monde s&amp;rsquo;appelle&amp;rdquo; - Ronsard, Hymnes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Hanlon&amp;rsquo;s Razor: Never attribute to malice what is adequately explained by stupidity.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Essentially, all models are wrong, but some are useful.&amp;rdquo;  Box, George E. P.; Norman R. Draper (1987). Empirical Model-Building and Response Surfaces, p. 424, Wiley. ISBN 0471810339&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Computers were around for 50 years before we figured out how to create the internet for example.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In sum, the physicist can never subject an isolated hypothesis to experimental test, but only a whole group of hypotheses; when the experiment is in disagreement with his predictions, what he learns is that at least one of the hypotheses constituting this group is unacceptable and ought to be modified; but the experiment does not designate which one should be changed.&amp;rdquo; (P. Duhem, The Aim and Structure of Physical Theory, 1914)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Un bon ma√Ætre a ce souci constant : enseigner √† se passer de lui.&amp;rdquo; - Andr√© Gide&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚ÄúTruth in science can be defined as the working hypothesis best suited to open the way to the next better one.‚Äù(Konrad Lorenz)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚Äúall motion is illusion‚Äù (Zeno of Elea, 490 BC )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Be humble for you are made of dung. Be noble for you are made of stars.&amp;rdquo; (Serbian proverb)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;voir des objets ne consiste pas √† en extraire des traits visuels, mais √† guider visuellement l&amp;rsquo;action dirig√©e vers eux.&amp;rdquo;  (Francisco Varela in &amp;lsquo;&amp;lsquo;L&amp;rsquo;inscription corporelle de l&amp;rsquo;esprit&amp;rsquo;&amp;rsquo;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;la m√©moire n‚Äôest pas faite pour se rappeler du pass√© mais pour pr√©dire le futur.&amp;rdquo; (&lt;a href=&#34;http://dpea-archi.philo.over-blog.com/article-interview-de-alain-berthoz-par-thierry-paquot-61601814.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alain Berthoz&lt;/a&gt; , 2010)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the days when the Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. &amp;ldquo;What are you doing?&amp;rdquo;, asked Minsky. &amp;ldquo;I am training a randomly wired neural net to play Tic-tac-toe&amp;rdquo;, Sussman replied. &amp;ldquo;Why is the net wired randomly?&amp;rdquo;, asked Minsky. &amp;ldquo;I do not want it to have any preconceptions of how to play&amp;rdquo;, Sussman said. Minsky then shut his eyes. &amp;ldquo;Why do you close your eyes?&amp;rdquo; Sussman asked his teacher. &amp;ldquo;So that the room will be empty.&amp;rdquo; At that moment, Sussman was enlightened.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;It doesn&amp;rsquo;t matter how beautiful your theory is, it doesn&amp;rsquo;t matter how smart you are. If it doesn&amp;rsquo;t agree with experiment, it&amp;rsquo;s wrong.&amp;rdquo; Richard P. Feynman&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Think of the image of the world in a convex mirror. &amp;hellip; A well-made convex mirror of moderate aperture represents the objects in front of it as apparently solid and in fixed positions behind its surface. But the images of the distant horizon and of the sun in the sky lie behind the mirror at a limited distance, equal to its focal length. Between these and the surface of the mirror are found the images of all the other objects before it, but the images are diminished and flattened in proportion to the distance of their objects from the mirror. &amp;hellip; Yet every straight line or plane in the outer world is represented by a straight line or plane in the image. The image of a man measuring with a rule a straight line from the mirror, would contract more and more the farther he went, but with his shrunken rule the man in the image would count out exactly the same results as in the outer world, all lines of sight in the mirror would be represented by straight lines of sight in the mirror. In short, I do not see how men in the mirror are to discover that their bodies are not rigid solids and their experiences good examples of the correctness of Euclidean axioms. But if they could look out upon our world as we look into theirs without overstepping the boundary, they must declare it to be a picture in a spherical mirror, and would speak of us just as we speak of them; and if two inhabitants of the different worlds could communicate with one another, neither, as far as I can see, would be able to convince the other that he had the true, the other the distorted, relation. Indeed I cannot see that such a question would have any meaning at all, so long as mechanical considerations are not mixed up with it.&amp;rdquo; ‚Äî Hermann von Helmholtz In &amp;lsquo;On the Origin and Significance of Geometrical Axioms,&amp;rdquo; Popular Scientific Lectures&amp;lt; Second Series (1881), 57-59. In Robert  Moritz, Memorabilia Mathematica (1914), 357-358.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Whoever, in the pursuit of science, seeks after immediate practical utility, may generally rest assured that he will seek in vain.&amp;rdquo; ‚Äî Hermann von Helmholtz, Edmund Atkinson (trans.), Popular Lectures on Scientific Subjects: First Series (1883), 29&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;¬´ Au d√©part, l‚Äôart du puzzle semble un art bref, un art mince, tout entier contenu dans un maigre enseignement de la Gestalttheorie : l‚Äôobjet vis√© ‚Äî qu‚Äôil s‚Äôagisse d‚Äôun acte perceptif, d‚Äôun apprentissage, d‚Äôun syst√®me physiologique ou, dans le cas qui nous occupe, d‚Äôun puzzle en bois ‚Äî n‚Äôest pas une somme d‚Äô√©l√©ments qu‚Äôil faudrait d‚Äôabord isoler et analyser, mais un ensemble, c‚Äôest √† dire une forme, une structure : l‚Äô√©l√©ment ne pr√©existe pas √† l‚Äôensemble, il n‚Äôest ni plus imm√©diat ni plus ancien, ce ne sont pas les √©l√©ments qui d√©terminent l‚Äôensemble, mais l‚Äôensemble qui d√©termine les √©l√©ments : la connaissance du tout et de ses lois, de l‚Äôensemble et de sa structure, ne saurait √™tre d√©duite de la connaissance s√©par√©e des parties qui le composent : cela veut dire qu‚Äôon peut regarder une pi√®ce d‚Äôun puzzle pendant trois jours et croire tout savoir de sa configuration et de sa couleur sans avoir le moins du monde avanc√© : seule compte la possibilit√© de relier cette pi√®ce √† d‚Äôautres pi√®ces et, en ce sens, il y a quelque chose de commun entre l‚Äôart du puzzle et l‚Äôart du go ; seules les pi√®ces rassembl√©es prendront un caract√®re lisible, prendront un sens : consid√©r√©e isol√©ment, une pi√®ce d‚Äôun puzzle ne veut rien dire ; elle est seulement question impossible, d√©fi opaque ; mais √† peine a-t-on r√©ussi, au terme de plusieurs minutes d‚Äôessais et d‚Äôerreurs, ou en une demi-seconde prodigieusement inspir√©e, √† la connecter √† l‚Äôune de ses voisines, que la pi√®ce dispara√Æt, cesse d‚Äôexister en tant que pi√®ce : l‚Äôintense difficult√© qui a pr√©c√©d√© ce rapprochement, et que le mot puzzle ‚Äî √©nigme ‚Äî d√©signe si bien en anglais, non seulement n‚Äôa plus de raison d‚Äô√™tre, mais semble n‚Äôen avoir jamais eu, tant elle est devenue √©vidence : les deux pi√®ces miraculeusement r√©unies n‚Äôen font plus qu‚Äôune, √† son tour source d‚Äôerreur, d‚Äôh√©sitation, de d√©sarroi et d‚Äôattente. ¬ª (PEREC Georges, La vie mode d‚Äôemploi, √âditions Hachette, 1978, pp. 235-236.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;there&amp;rsquo;s really a trend toward ATLs ( Acronyms of Three Letters).&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In theory there is no difference between theory and practice. In practice there is.&amp;rdquo; Yogi Berra&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Les d√©corations, c&amp;rsquo;est comme les bombes, √ßa tombe toujours sur ceux qui ne les m√©ritent pas&amp;rdquo; &lt;a href=&#34;http://www.lemonde.fr/societe/article/2010/03/22/proces-viguier-deux-hommes-en-colere_1322481_3224_1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Me Dupond-Moretti&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Science is what we understand well enough to explain to a computer. Art is everything else we do.&amp;rdquo; D. Knuth, foreword to &amp;ldquo;A=B&amp;rdquo; (1995)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;les cons √ßa ose tout et c&amp;rsquo;est m√™me √† √ßa qu&amp;rsquo;on les reconna√Æt&amp;rdquo; (Audiard)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;All generalisations are dangerous, including this one.&amp;rdquo; (Alexandre Dumas)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Il vaut mieux mobiliser son intelligence sur des conneries que mobiliser sa connerie sur des choses intelligentes.&amp;rdquo; Jacques Rouxel&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Nothing in biology makes sense except in the light of evolution&amp;rdquo;. (Theodosius Dobzhansky, 1900‚Äì1975)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Copy from one, it‚Äôs plagiarism; copy from two, it‚Äôs research.&amp;rdquo; (Wilson Mizner)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In the past the man has been first, in the future the system must be first.&amp;rdquo; (F. Taylor)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;L&amp;rsquo;autorit√© n&amp;rsquo;admet que deux r√¥les : le bourreau et la victime, transforme les gens en poup√©es qui ne connaissent plus que peur et haine, tandis que la culture plonge dans les abysses. L&amp;rsquo;autorit√© d√©forme ses enfants et change leur amour en un combat de coq&amp;hellip; L&amp;rsquo;effondrement de l&amp;rsquo;autorit√© aura des r√©percussions sur le bureau, l&amp;rsquo;√©glise et l&amp;rsquo;√©cole. Tout est li√©. L&amp;rsquo;√©galit√© et la libert√© ne sont pas des luxes que l&amp;rsquo;on √©carte impun√©ment. Sans ceux-ci, l&amp;rsquo;ordre ne peut survivre longtemps sans se rapprocher de profondeurs inimaginables.&amp;rdquo; Alan Moore, &lt;a href=&#34;http://fr.wikipedia.org/wiki/V_pour_Vendetta&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;V pour Vendetta&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;In a widely circulated joke [from the days of the first computer], a group of engineers assemble the most powerful computer that had ever been conceived and ask it the ultimate question: Is there a God? After several tense minutes of clicking and clacking and flashing of lights, a card pops out which reads: There is &amp;lsquo;&amp;rsquo;now&amp;rsquo;&amp;rsquo;.&amp;rdquo; (Alwyn Scott in &amp;lsquo;&amp;lsquo;How Smart is a Neuron?&amp;rsquo;&amp;rsquo; in A Review of Christof Kochs&amp;rsquo; &amp;lsquo;&amp;lsquo;Biophysics of Computation&amp;rsquo;&amp;rsquo; )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Que les cons le restent!&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Those who can &amp;ndash; do. Those who can&amp;rsquo;t &amp;ndash; teach. (H.L. Mencken). Those who cannot teach &amp;ndash; administrate. (Martin)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Under capitalism, man exploits man. Under communism, it&amp;rsquo;s just the opposite. (John Kenneth Galbraith)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Love is the triumph of imagination over intelligence. (H. L. Mencken)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The power of accurate observation is commonly called cynicism by those who have not got it. (George Bernard Shaw)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remember, beneath every cynic there lies a romantic, and probably an injured one. (Glenn Beck)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;If we see the light at the end of the tunnel, it&amp;rsquo;s the light of an oncoming train.&amp;rdquo; &amp;ndash; Robert Lowell&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Any intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius &amp;ndash; and a lot of courage &amp;ndash; to move in the opposite direction&amp;rdquo;  &amp;ndash; Albert Einstein&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Je sais ce que je crois. Je continuerais √† exprimer ce que je crois, et ce que je crois&amp;hellip; je crois que ce que je crois est bien.&amp;rdquo;- GWB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Soudain, je ne sais comment, le cas fut subit, je n&amp;rsquo;eus loisir de le consid√©rer, Panurge, sans autre chose dire, jette en pleine mer son mouton criant et b√™lant. Tous les autres moutons, criant et b√™lant en pareille intonation, commenc√®rent √† se jeter et √† sauter en mer apr√®s, √† la file. La foule √©tait √† qui le premier y sauterait apr√®s leur compagnon. Il n&amp;rsquo;√©tait pas possible de les en emp√™cher, comme vous savez du mouton le naturel, toujours suivre le premier, quelque part qu&amp;rsquo;il aille.&amp;rdquo; Rabelais&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Personne ne peut dire dans quel but l&amp;rsquo;homme a √©t√© amen√© en ce monde, ni quelle sera la destin√©e de l&amp;rsquo;esp√®ce. Cependant, il y a de grands esprits qui ne vivent pas pour leur bien-√™tre pr√©sent mais en vue d&amp;rsquo;une fin impersonnelle, comme un coureur qui s&amp;rsquo;√©puiserait dans une course de relais, pour un troph√©e qu&amp;rsquo;il ignore et qu&amp;rsquo;un autre remportera.&amp;rdquo; Charles Morgan, Essai sur l&amp;rsquo;unit√© de l&amp;rsquo;esprit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Science sans conscience n&amp;rsquo;est que ruine de l&amp;rsquo;√¢me.&amp;rdquo;  (Rabelais, 1532)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;‚ÄúLinux is only free if your time has no value.‚Äù (Jamie Zawinski)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Napol√©on : Monsieur de Laplace, je ne trouve pas dans votre syst√®me mention de Dieu  ?    Laplace : Sire, je n&amp;rsquo;ai pas eu besoin de cette hypoth√®se. (d&amp;rsquo;autres savants ayant d√©plor√© que Laplace fasse l&amp;rsquo;√©conomie d&amp;rsquo;une hypoth√®se qui avait justement &amp;ldquo;le m√©rite d&amp;rsquo;expliquer tout&amp;rdquo;, Laplace r√©pondit cette fois-ci √† l&amp;rsquo;Empereur :    Laplace : Cette hypoth√®se, Sire, explique en effet tout, mais ne permet de pr√©dire rien. En tant que savant, je me dois de vous fournir des travaux permettant des pr√©dictions &amp;quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;The supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience&amp;rdquo; often paraphrased as &amp;ldquo;Theories should be as simple as possible, but no simpler.&amp;rdquo; Albert Einstein (1933)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;The surest sign of the existence of extra- terrestrial intelligence is that they never bothered to come down here and visit us!&amp;rdquo; Calvin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;¬´Si tu manges le fruit d&amp;rsquo;un grand arbre, n&amp;rsquo;oublie jamais de remercier le vent!¬ª &amp;lsquo;&amp;rsquo;tradition orale bambara au Mali&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Lorsque vous avez √©limin√© l‚Äôimpossible, ce qui reste, si improbable soit-il, est n√©cessairement la v√©rit√©.&amp;rdquo; - Arthur Conan Doyle - &amp;lsquo;&amp;lsquo;Le signe des Quatre&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;I have come to believe that the whole world is an enigma, a harmless enigma that is made terrible by our own mad attempt to interpret it as though it had an underlying truth.&amp;rdquo;  &amp;ndash; Umberto Eco&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nos id√©es doivent √™tre aussi vastes que la nature pour pouvoir en rendre compte.&amp;rdquo; &lt;a href=&#34;http://www.evene.fr/citations/auteur.php?ida=813&amp;amp;celebrite=arthur-conan-doyle%7c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arthur Conan Doyle&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Tout le monde savait que ce truc l√† √©tait impossible a faire. Jusqu&amp;rsquo;au jour ou est arriv√© quelqu&amp;rsquo;un qui ne le savait pas, et qui l&amp;rsquo;a fait.&amp;rdquo; Winston Churchill&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Ce proverbe, je l&amp;rsquo;ai sur le bout de la&amp;hellip; &amp;quot; &amp;ndash; Manu (2006-01-23T19:34:16Z)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;When you can measure what you are speaking about and express it in numbers, you know something about it; but when you cannot measure it, when you cannot express it in numbers, your knowledge is of the meager and unsatisfactory kind.&amp;rdquo; &amp;lsquo;&amp;lsquo;Lord Kelvin&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I know you believe you understand what you think I said, but I am not sure you realize that what you heard is not what I meant. &amp;lsquo;&amp;lsquo;Richard Nixon&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try to understand everything, but believe nothing! &amp;lsquo;&amp;lsquo;Unknown&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;George Walker Bush a propos de ses d√©mentis sur la coca√Øne, r√©pond ¬´ I haven&amp;rsquo;t denied anything. ¬ª&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bayes maxim : &amp;ldquo;condition the joint probability on what we know and marginalize on what we don&amp;rsquo;t care&amp;rdquo; John Coughlan (in prob. models / kersten)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Monty Python&amp;rsquo;s &amp;ldquo;Life of Brian&amp;rdquo;: {{{(Brian)-&amp;ldquo;You are all individuals!&amp;rdquo; (crowd)-&amp;ldquo;We are all individuals!&amp;quot;(Brian)-&amp;ldquo;You have to be different!&amp;rdquo; (crowd)-&amp;ldquo;Yes, we are all different!&amp;rdquo; (loner)-&amp;ldquo;I&amp;rsquo;m not.&amp;rdquo;}}}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Monty Python&amp;rsquo;s &amp;ldquo;Life of Brian&amp;rdquo;: (Brian:) &amp;ldquo;You have to work it out for yourselves!&amp;rdquo; (Crowd:) &amp;ldquo;Yes, we have to work it out for ourselves&amp;hellip; (silence) Tell us more!&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Combien d&amp;rsquo;autres corps c√©lestes, outre ces com√®tes, se meuvent en secret sans jamais se montrer aux yeux des hommes. Dieu n&amp;rsquo;a pas fait toutes les choses pour l&amp;rsquo;homme.&amp;rdquo; S√©n√©que&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hawking&amp;rsquo;s principle for popularizations : &amp;ldquo;each math symbol reduces the potential readership by a factor r = 1/2&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Les ordinateurs sont trop fiables pour remplacer rellement les humains.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prediction is very difficult, especially about the future. Niels Bohr&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On peut vous le faire :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bien&lt;/li&gt;
&lt;li&gt;vite&lt;/li&gt;
&lt;li&gt;pour peu cher
Ne choisissez pas plus de deux options.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;s. v..s p..v.z c.mpr.ndr. c.l. v..s p..v.z .tr. d.v.l.pp√©&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Intelligence artificielle : art de programmer les ordinateurs de sorte qu&amp;rsquo;ils se comportent comme ils le font dans les films.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;hypoaristerolactoth√©rapie : m√©thode de d√©pannage des machines par le coup de pied en bas √† gauche.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Le peu que je sais , c&amp;rsquo;est √† mon ignorance que je le dois .GUITRY , Sacha&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TOULET , Paul-Jean &amp;ldquo;Apprends √†te conna√Ætre : tu t&amp;rsquo;aimeras moins . Et √† conna√Ætre les autres , tu ne les aimeras plus.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Occam&amp;rsquo;s razor : &amp;ldquo;Accept the simplest explanation that fits the data.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;On two occasions, I have been asked [by members of Parliament], &amp;lsquo;Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?&amp;rsquo; I am not able to rightly apprehend the kind of confusion of ideas that could provoke such a question.&amp;rdquo;&amp;ndash; Charles Babbage (1791-1871)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Reality is that which, when you stop believing in it, doesn&amp;rsquo;t go away&amp;rdquo;. Philip K. Dick&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Puisque ce d√©sordre nous √©chappe, feignons d&amp;rsquo;en √™tre l&amp;rsquo;organisateur. &amp;quot; Cocteau&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Non seulement la solution n&amp;rsquo;existe pas, mais en plus elle n&amp;rsquo;est pas unique.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Detection is, or ought to be, an exact science, and should be treated in the same cold and unemotional manner. You have attempted to tinge it with romanticism, which produces much the same effect as if you worked a love-story or an elopement into the fifth proposition of Euclid. &amp;ndash; SHERLOCK HOLMES &amp;lsquo;&amp;lsquo;Sign of The Four&amp;rsquo;&amp;rsquo;, Chapter 1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;quot; Act without doing;
work without effort.
Think of the small as large
and the few as many.
Confront the difficult
while it is still easy;
accomplish the great task
by a series of small acts. &amp;quot; Lao-Tze&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Les math√©matiques ne sont pas une moindre immensit√© que la mer&amp;rdquo; V. Hugo&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Le hasard n&amp;rsquo;est que la mesure de notre ignorance&amp;rdquo; Henri Poincar√©, La science et l&amp;rsquo;hypoth√®se&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Je r√©pugne toute religion qui ne se voit philosophe&amp;rdquo; Lolo Tseu&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Sans gravit√© ‚Äì une po√©tique de l‚Äôair</title>
      <link>https://laurentperrinet.github.io/post/2019-06-22_ardemone/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2019-06-22_ardemone/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/317504725&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;-densit√©-flou--2019&#34;&gt;¬´ Densit√© flou ¬ª (2019)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;La perception est notre capacit√© √† interpr√©ter les informations que nous recevons √† travers nos sens. Parfois, notre perception peut nous faire voir des objets qui n‚Äôexistent pas, comme un visage dans les textures d‚Äôun rocher. Ce ph√©nom√®ne a √©t√© utilis√© dans ¬´ Densit√© ‚Äì Flou ¬ª, pr√©sent√©e √† Avignon en 2019 (Figure 3), qui consistait en un ensemble de triangles accol√©s et dispos√©s al√©atoirement sur une surface. La forte densit√© des triangles induit la perception de formes imaginaires comme des voiles, des perspectives ou des visages  (voir &lt;a href=&#34;https://laurentperrinet.github.io/2023-01-31_formes-et-perception/#fig:Densit%c3%a9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cet article&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;














&lt;figure  id=&#34;figure-√©tienne-rey---horizon-faille---densit√©-flou-2019---sans-gravit√©---une-po√©tique-de-lair-√†---ardenome---avignon--httpswwwenrevenantdelexpocom&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;√âtienne Rey - Horizon faille - Densit√© flou, 2019 - Sans gravit√© - une po√©tique de l‚Äôair √† - Ardenome - Avignon ¬© https://www.enrevenantdelexpo.com&#34; srcset=&#34;
               /post/2019-06-22_ardemone/Avignon-02_hu07228ddf5546db0de787e00a1ab491cd_249119_96251068c8f61735f2b11cd1ba356fa1.webp 400w,
               /post/2019-06-22_ardemone/Avignon-02_hu07228ddf5546db0de787e00a1ab491cd_249119_d1294e94c8cf63c7d9dcbea84868e705.webp 760w,
               /post/2019-06-22_ardemone/Avignon-02_hu07228ddf5546db0de787e00a1ab491cd_249119_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2019-06-22_ardemone/Avignon-02_hu07228ddf5546db0de787e00a1ab491cd_249119_96251068c8f61735f2b11cd1ba356fa1.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      √âtienne Rey - Horizon faille - Densit√© flou, 2019 - Sans gravit√© - une po√©tique de l‚Äôair √† - Ardenome - Avignon ¬© &lt;a href=&#34;https://www.enrevenantdelexpo.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.enrevenantdelexpo.com&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- https://www.enrevenantdelexpo.com/wp-content/uploads/2019/05/%C3%89tienne-Rey-Horizon-faille-Densit%C3%A9-flou-2019-Sans-gravit%C3%A9-une-po%C3%A9tique-de-l%E2%80%99air-%C3%A0-Ardenome-Avignon-02.jpg --&gt;
&lt;h1 id=&#34;-tension-superficielle--2019&#34;&gt;¬´ Tension superficielle ¬ª (2019)&lt;/h1&gt;














&lt;figure  id=&#34;figure-√©tienne-rey--horizon-faille--tension-superficielle-2019--sans-gravit√©--une-po√©tique-de-lair-√†--ardenome--avignon--httpswwwenrevenantdelexpocom&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i1.wp.com/www.enrevenantdelexpo.com/wp-content/uploads/2019/05/%C3%89tienne-Rey-Horizon-faille-Tension-superficielle-2019-Sans-gravit%C3%A9-une-po%C3%A9tique-de-l%E2%80%99air-%C3%A0-Ardenome-Avignon-00_1.jpg&#34; alt=&#34;√âtienne Rey ‚Äì Horizon faille ‚Äì Tension superficielle, 2019 ‚Äì Sans gravit√© ‚Äì une po√©tique de l‚Äôair √† ‚Äì Ardenome ‚Äì Avignon ¬© https://www.enrevenantdelexpo.com&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      √âtienne Rey ‚Äì Horizon faille ‚Äì Tension superficielle, 2019 ‚Äì Sans gravit√© ‚Äì une po√©tique de l‚Äôair √† ‚Äì Ardenome ‚Äì Avignon ¬© &lt;a href=&#34;https://www.enrevenantdelexpo.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.enrevenantdelexpo.com&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h1 id=&#34;dans-le-cadre-de-sans-gravit√©-une-po√©tique-de-lair---etienne-rey--mathilde-lavenne--hugo-deverch√®re--edith-dekyndt---23-mars--22-juin-2019&#34;&gt;dans le cadre de &amp;ldquo;SANS GRAVIT√â, UNE PO√âTIQUE DE L&amp;rsquo;AIR - ETIENNE REY / MATHILDE LAVENNE / HUGO DEVERCH√àRE / EDITH DEKYNDT - 23 MARS &amp;gt; 22 JUIN 2019&amp;rdquo;&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Sans Gravit√© a √©t√© con√ßue par EDIS dans le cadre de ‚ÄúChroniques, Biennale des Imaginaires Num√©riques‚Äù, qui r√©unit un ensemble d‚Äôinstitutions culturelles de la R√©gion PACA.
Apr√®s Aix-Marseille, l‚ÄôArdenome √† Avignon est la seconde √©tape
de ce parcours r√©gional.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Horizon faille est une installation globale qui cherche √† d√©fier la gravit√© de la nature. Prenant appui sur deux notions dont l‚Äôartiste en a fait ses motifs principaux - les failles du paysage et l‚Äôimmat√©rielle ligne d‚Äôhorizon. La notion de faille exprime la fracture, au sens g√©ologique. Elle est √† consid√©rer comme un interstice, une zone de transformation, un passage d‚Äôun √©tat √† un autre. De m√™me, l‚Äôhorizon scinde la terre du ciel dans une tentative de g√©om√©trisation de l‚Äôunivers, de mise en espace des √©l√©ments naturels. Intouchable ligne de partage, ce filin tendu d√©signe aussi le seuil de vision du paysage. C‚Äôest la ligne imaginaire qui se forme √† partir de notre position dans l‚Äôespace. Elle est ce qui √©chappe √† la vue ou √† la repr√©sentation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;L‚Äôensemble de ces ≈ìuvres r√©sonnent en √©chos visuels les unes avec les autres. Elles d√©crivent des mouvements, des points de rupture, forment des zones de passage, explorent des √©tats de m√©tamorphose issus d‚Äôun paysage initial dont les perspectives ont √©t√© d√©pli√©s. En parcourant du regard ces diff√©rentes propositions plastiques, le visiteur prend conscience de l‚Äôespace qui l‚Äôentoure et le trouble √† la fois. Un espace √©lastique, d√©multiplicateur qui lui offre une diversit√© d‚Äôangles dans lesquels il peut se perdre √† loisir, comme en √©tat d‚Äôapesanteur. Le monde n‚Äôest pas fig√©, il est une d√©rive constante, une recomposition permanente.
V√©ronique Baton&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Accompagnement R√©alisation : Atelier Ni et Guillaume Stagnaro&lt;/li&gt;
&lt;li&gt;Variable Density et Tension Superficielle en collaboration avec Laurent Perrinet, chercheur Institut de neurosciences de la Timone, INT.&lt;/li&gt;
&lt;li&gt;Tirage s√©rigraphique Atelier Tchikebe.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;links--liens&#34;&gt;Links / Liens:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ardemone &lt;a href=&#34;https://www.ardenome.fr/sans-gravite-ardenome-chroniques&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ardenome.fr/sans-gravite-ardenome-chroniques&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Retour par &lt;a href=&#34;https://www.enrevenantdelexpo.com/2019/05/17/sans-gravite-une-poetique-de-air-ardenome-avignon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;En revenant de l&amp;rsquo;expo !&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SALON INTERNATIONAL D&amp;rsquo;ART CONTEMPORAIN ART-O-RAMA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SALON : 31 AO√õT &amp;gt; 2 SEPTEMBRE 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;EXPOSITION &amp;gt; 9 SEPTEMBRE 2018&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ESPACE PARTENAIRES - J1, QUAI DE LA JOLIETTE, MARSEILLE 2e&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TRAMES</title>
      <link>https://laurentperrinet.github.io/post/2018-04-10_trames/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-04-10_trames/</guid>
      <description>&lt;h1 id=&#34;trames&#34;&gt;TRAMES&lt;/h1&gt;

 &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
   &lt;iframe src=&#34;https://player.vimeo.com/video/191830797&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;p&gt;√Ä la Fondation Vasarely √† Aix-en-Provence, &lt;a href=&#34;https://laurentperrinet.github.io/author/etienne-rey/&#34;&gt;Etienne Rey&lt;/a&gt; a choisi d‚Äôinstaller dans la salle des Int√©grations architectoniques un ballet visuel hypnotique.&lt;/p&gt;
&lt;p&gt;¬´Trame instabilit√©¬ª est un travail en cours de recherche. Le projet est bas√© sur des principes d‚Äôoccultations partielles en couches associ√©es √† des trames qui font √©merger une dimension immat√©rielle. L‚Äôexp√©rience de perception de ces motifs produit un sentiment de basculement de la perception dans le sens o√π le motifs r√©el passe au second plan pour laisser place √† l‚Äô√©mergence d‚Äôune figure du vide, c‚Äôest dans les blancs immat√©riel que des formes apparaissent et vacillent occupant tout notre champ visuel. Ces apparitions virtuelles, purs ph√©nom√®nes optiques n‚Äôexistent pas dans notre monde ¬´physique¬ª, r√©el.&lt;/p&gt;
&lt;p&gt;Ce qui est en jeu ici c‚Äôest l‚Äô√©mergence de l‚Äôapparition de motifs virtuels r√©sultat de la relation entre une r√©alit√© physique, la grandeur et l‚Äôordonnancement de trames et notre physiologie qui conduit √† cette √©tat de perception. Lorsqu‚Äôon est fasse √† ces motifs ce qui saute au yeux plus que le motif r√©el c‚Äôest sa r√©sultante, instable et √©ph√©m√®re qui fait apparaitre une richesse de figures g√©om√©triques qui se transforment et √©voluent en fonction du temps d‚Äôobservation et du point de vue. Sur ce principe de dispositif optique, le travail de chacun des motifs, li√© √† un s√©quen√ßage de trames conduit √† faire apparaitre une composition et des √©mergences de formes sp√©cifiques. L‚Äôexp√©rience de perception de chacun des motifs explore les notions d‚Äôinstabilit√©, de flux, d‚Äô√©mergences ‚Ä¶ dont l‚Äôexp√©rience donne √† entrevoir des formes que l‚Äôon retrouve dans la nature ou les ph√©nom√®nes naturels: le dessin du pelage d‚Äôun z√®bre, une accumulation de bulles de savons, ou plus g√©n√©ralement dans les compositions chimiques issue de la th√©orie de la morphog√©n√®se de Turing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L‚Äô≈ìuvre ¬´ Trames Instabilit√© ¬ª (voir aussi &lt;a href=&#34;https://laurentperrinet.github.io/2023-01-31_formes-et-perception/#fig:Trames&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cet article&lt;/a&gt;) d‚ÄôEtienne Rey montre comment les principes anatomiques de la formation de l‚Äôimage sur la r√©tine peuvent √™tre utilis√©s dans l‚Äôart. Il dispose des motifs √©l√©mentaires sur une grille hexagonale resserr√©e et rythmique. Une seconde grille est superpos√©e en profondeur et cr√©e un effet de moir√© d‚Äôoscillation plus lente. Cette ≈ìuvre est calibr√©e pour rentrer en r√©sonance avec les limites induites par l‚Äôanatomie de la r√©tine. Les deux √©chelles entrent en r√©sonance avec l‚Äôarrangement des photor√©cepteurs de la r√©tine et cr√©ent une impression d‚Äôinstabilit√©. Les points semblent s‚Äôorganiser suivant des alignements en p√©riph√©rie, sugg√©rant une organisation en profondeur, mais cette perception dispara√Æt d√®s qu‚Äôon veut la saisir, un mouvement oculaire tel qu‚Äôune saccade, ce qui invite √† la remplacer par une autre.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Sortie mod√®le&#34; srcset=&#34;
               /post/2018-04-10_trames/featured_hu171df5d2f0a674503e2f87f770cd1304_2667379_97e53a19f0b109b4533955e721dd973f.webp 400w,
               /post/2018-04-10_trames/featured_hu171df5d2f0a674503e2f87f770cd1304_2667379_adad3d84f94081102bf3e39ab8fe5b29.webp 760w,
               /post/2018-04-10_trames/featured_hu171df5d2f0a674503e2f87f770cd1304_2667379_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2018-04-10_trames/featured_hu171df5d2f0a674503e2f87f770cd1304_2667379_97e53a19f0b109b4533955e721dd973f.webp&#34;
               width=&#34;760&#34;
               height=&#34;607&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;En collaboration avec le chercheur Laurent Perrinet, CNRS-AMU / Institut de Neurosciences de la Timone.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trame, √âlasticit√© &amp;amp; √âcran n¬∞3  √©taient aussi pr√©sent√©s au Festival Ososph√®re, Strasbourg en Avril 2017.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://maison-contemporain.com/wp-content/uploads/2020/04/INSTABILITE_ORANGE_ROUGE_F3030-scaled.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TROPIQUE</title>
      <link>https://laurentperrinet.github.io/post/2013-10-10_tropique/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2013-10-10_tropique/</guid>
      <description>&lt;h1 id=&#34;tropique&#34;&gt;TROPIQUE&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;¬´ Tropique ¬ª est une installation artistique cr√©√©e par Etienne Rey en collaboration avec Wilfried Wendling (son) et sous mon expertise scientifique. Elle a √©t√© produite pour l‚ÄôAnn√©e europ√©enne de la culture d‚ÄôAix-Marseille et pr√©sent√©e en 2013 √† la fondation Vasarely. Cette installation immersive consiste en une sculpture de lumi√®re incluse dans un espace ferm√© de 20 m√®tres de longueur sur 15 m√®tres de large. La salle est remplie de minuscules billes d‚Äôeau transparentes en suspension. Elles produisent une diffraction visible lorsqu‚Äôelles sont illumin√©es par les vid√©oprojecteurs qui sont plac√©s aux bords oppos√©s de la salle. Les sources de lumi√®re projettent des segments qui composent l‚Äôalphabet de la sculpture. Chaque segment est caract√©ris√© par sa position, sa longueur et son orientation et chacun cr√©e une lame de lumi√®re dans l‚Äôespace de la salle. Une fois les segments combin√©s, ils forment un monde propre √† la sculpture et isol√© du monde habituel (voir aussi &lt;a href=&#34;https://laurentperrinet.github.io/2023-01-31_formes-et-perception/#fig:Tropique&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cet article&lt;/a&gt;). La grammaire qui r√©git les mouvements de ces segments est inspir√©e par les forces d‚Äôattraction et de r√©pulsion observ√©es aux tailles microscopiques des cellules et macroscopique des galaxies. Cette population de segments √©voluait alors comme un syst√®me autonome, sans sc√©nario pr√©-√©crit ou enregistr√© et compl√©t√© par une synchronisation des diff√©rentes sources de lumi√®re ainsi que du syst√®me de g√©n√©ration spatiale du son. Un point crucial de l‚Äôinstallation √©tait d‚Äôintroduire une interaction intime entre ce syst√®me et chaque observateur. Un discret syst√®me de capteurs de mouvement permettait de localiser la pr√©sence des diff√©rents observateurs et de modifier la configuration de la sculpture en fonction de leurs mouvements. Le syst√®me √©voluait ainsi de fa√ßon autonome d‚Äôune sculpture de lumi√®re que l‚Äôon pouvait regarder et toucher √† une configuration dans laquelle le spectateur √©tait plong√© dans un monde propre, intime. Dans cet √©tat, les segments align√©s autour de l‚Äôobservateur formaient une ‚Äúaura‚Äù o√π tout rep√®re de perspective √©tait perdu. Ce dispositif, en manipulant visible et invisible, levait alors le voile sur des m√©canismes cach√©s de la perception&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/66161665&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/NaturalPatterns/Tropique&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/NaturalPatterns/Tropique&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.ran-dan.net/eng/wp-content/uploads/2012/01/E-REY-TropiqueS2-G-1024x576.jpg&#34; alt=&#34;Sortie mod√®le&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.ososphere.org/catalogue/wp-content/uploads/2016/05/tropique_rey_oso2012_groslier8_Web_nb.jpg&#34; alt=&#34;Tropique d&amp;amp;rsquo;Etienne Rey La Coop 2012 / Cr√©dits photo Philippe Groslier&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&amp;lsquo;&amp;lsquo;Etienne Rey investigates the invisible and mutual relationships which take place between human and his environment. &lt;a href=&#34;http://digitalperformanceculture.blog.fr/2012/02/18/tropique-d-etienne-rey-12817804/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prize-winner of the 1st call for projects of the RAN&lt;/a&gt;, its project of immersive installation Tropique puts in link the perception of the space connected to the movement, to the light and to the sound. Within the framework of a residence of creation in the Centre des arts, the object of which is ‚Äù to sculpt the light ‚Äú, he presents a work in progress of this installation.&amp;rsquo;&amp;rsquo;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_b.jpg&#34; alt=&#34;Tropique&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_a.jpg&#34; alt=&#34;Tropique&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://ondesparalleles.org/projets/tropique-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique&lt;/a&gt; est une installation environnementale, un espace vide de mati√®re, qui se densiÔ¨Åe en ondes sonores et lumineuses, activ√©es et modul√©es par la pr√©sence et l‚Äôactivit√© humaines. Ce projet met en lien la perception de l‚Äôespace articul√©e au mouvement, √† la lumi√®re et au son. Les personnes qui se situent dans l‚Äôespace sont entour√©es d‚Äôune aura lumineuse et sonore qui Ô¨Çuctue en fonction des mouvements et de la proximit√© des corps.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_c.jpg&#34; alt=&#34;Tropique&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Etienne Rey investigates the invisible and mutual relationships which take place between human and his environment. Prize-winner of the 1st call for projects of the RAN, its project of immersive installation Tropique puts in link the perception of the space connected to the movement, to the light and to the sound. Within the framework of a residence of creation in the Centre des arts, the object of which is ‚Äù to sculpt the light ‚Äú, he presents a work in progress of this installation.&#39;&#39;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/33718945&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_d.jpg&#34; alt=&#34;Tropique&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://ondesparalleles.org/wp-content/uploads/2014/02/tropique_fiche_l.jpg&#34; alt=&#34;Tropique&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Tropique plonge le visiteur au coeur d‚Äôun espace embrum√©, sculpt√© par la lumi√®re et le son. Exp√©rience sensorielle, le monde de Tropique √©volue dans un entre-deux, rendant palpables des mat√©rialit√©s d‚Äôordinaires invisibles. Exp√©rience perceptive, le dispositif irradie l‚Äôespace, l‚Äôincorpore, l‚Äôamalgame, le dilate. La lumi√®re se diffuse jusqu‚Äô√† modifier notre rapport √† l‚Äôespace, elle le redessine √† travers notre propre vision et provoque une exp√©rience personnelle, une √©motion visuelle. L‚Äôenvironnement r√©agit aux variations de l‚Äôactivit√© dans l‚Äôinstallation, √† la fa√ßon dont nous l‚Äôhabitons et le transformons. Tropique contruit un espace dynamique une architecture mobile √† l‚Äô√©tat de l‚Äôair, en miroir √† notre pr√©sence. Ce qui est r√©v√©l√© est un ensemble vivant, la plupart du temps imperceptible, comme une mise une lumi√®re de notre √©cosyst√®me et de ses interrelations. Ce projet est √©labor√© avec le concours d‚Äôune √©quipe pluridisciplinaire compos√©e d‚Äôun chercheur en Neuroscience : Laurent Perrinet, d‚Äôun compositeur : Wilfried Wendling d‚Äôun ing√©nieur : Julien Marro Dauzat.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Tropique √ü @ INT&#34; srcset=&#34;
               /post/2013-10-10_tropique/affiche_TROPIQUE-INT_hu383366ca4f0b317a45b7b59d9bf8f55c_2170203_6054dedbd206694f05a32d34e4669430.webp 400w,
               /post/2013-10-10_tropique/affiche_TROPIQUE-INT_hu383366ca4f0b317a45b7b59d9bf8f55c_2170203_12c618c0cb653461c48754dfbcccad55.webp 760w,
               /post/2013-10-10_tropique/affiche_TROPIQUE-INT_hu383366ca4f0b317a45b7b59d9bf8f55c_2170203_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2013-10-10_tropique/affiche_TROPIQUE-INT_hu383366ca4f0b317a45b7b59d9bf8f55c_2170203_6054dedbd206694f05a32d34e4669430.webp&#34;
               width=&#34;538&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Accueilli en r√©sidence dans le cadre des r√©sidences de recherche de l‚ÄôIM√©RA pendant 6 mois (3 p√©riodes de 2 mois),et soutenu dans le cadre d&amp;rsquo;un Atelier de l&amp;rsquo;!EuroM√©diterran√©e, ce projet est √©labor√© en collaboration avec des chercheurs. Nous abordons ainsi les questions de la cognition et de la perception de l‚Äôespace li√©es √† la vue, √† l‚Äôaudition, et au d√©placement, auxquelles nous lierons les questions relatives √† la diffusion de ph√©nom√®nes ondulatoires.&lt;/p&gt;
&lt;h2 id=&#34;news&#34;&gt;news&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/56198653&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;space odyss√©e&lt;/a&gt; √† &lt;a href=&#34;http://www.institutfrancais-seoul.com/portfolio-item/exposition-home-cinema/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;l&amp;rsquo;institut francais en Coree du Sud de juin a octobre 2016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Un entretien d&amp;rsquo;ER durant le &lt;a href=&#34;https://www.youtube.com/watch?v=lA2bovigzLg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mois multi 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/lA2bovigzLg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Un entretien avec &lt;a href=&#34;https://youtu.be/z8328z2WO-w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Roger Malina&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/z8328z2WO-w&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Installation &lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : Tourn√©e en Cor√©e avec Mac de Cr√©teil en 2016 / dates et lieux √† venir.&lt;/li&gt;
&lt;li&gt;Installation &lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : FESTIVAL VIA / MAUBEUGE / 12 AU 22 MARS 2015 - FESTIVAL EXIT / CR√âTEIL / 26 MARS AU 05 AVRIL 2015 - LE PRINTEMPS √Ä SAINT SAUVEUR / LILLE / 27 AVRIL 2016 AU 28 AO√õT 2016 : &lt;a href=&#34;http://www.maccreteil.com/fr/mac/event/338/Home-cinema-Festival-Exit#sthash.bEyRuaDX.dpuf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Home Cinema&lt;/a&gt;. √Ä lire, sur &lt;a href=&#34;http://www.digitalarti.com/fr/blog/digitalarti_mag/home_cinema_matiere_audiovisuelle_modulable_pour_hyper_spectateur&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;digitatarti&lt;/a&gt;.
Installation &lt;a href=&#34;http://ondesparalleles.org/projets/tropique-7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique&lt;/a&gt; : Festival international d‚Äôarts multidisciplinaires et √©lectroniques &lt;a href=&#34;http://mmrectoverso.org/fr/mois-multi/spectacles/installations-2/tropique/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Le Mois Multi 16 / Qu√©bec du 4 f√©vrier au 1 mars 2015&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/lA2bovigzLg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Installation &lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : Lille 3000, du 26 SEPT 2015 &amp;gt; 17 JAN 2016&lt;/li&gt;
&lt;li&gt;Installation &lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : FESTIVAL INTERNATIONAL EXIT 2015 Cr√©teil du 26 MARS -&amp;gt; 05 AVRIL 2015&lt;/li&gt;
&lt;li&gt;Installation &lt;a href=&#34;http://ondesparalleles.org/projets/space-odyssey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Odyssey&lt;/a&gt; : Festival Via 2015, Maubeuge du jeudi 12 mars 2015 au dimanche 22 mars 2015&lt;/li&gt;
&lt;li&gt;Du 13 au 23 Mars 2014: &lt;a href=&#34;http://www.lemanege.com/cgi?lg=fr&amp;amp;pag=1310&amp;amp;tab=108&amp;amp;rec=1188&amp;amp;frm=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TROPIQUE @ FESTIVAL VIA&lt;/a&gt; : Depuis presque 30 ans, le Festival VIA flirte avec les fronti√®res des territoires artistiques, √† la crois√©e des arts de la sc√®ne, de la cr√©ation technologique et num√©rique. Toujours plus international et interdisciplinaire, VIA traduit, √† Maubeuge et √† Mons, la vitalit√© de la sc√®ne contemporaine. Il est √©galement √† noter que cette √©dition pr√©figure la Capitale europ√©enne de la culture 2015 qui se d√©roulera dans ces deux villes l&amp;rsquo;ann√©e prochaine.  Pour cette nouvelle √©dition de VIA sera pr√©sent√©e l&amp;rsquo;installation Tropique d&amp;rsquo;√âtienne Rey, produite par Seconde Nature et r√©cemment d√©voil√©e lors du festival ¬´ Chroniques des Mondes Possibles ¬ª dans le cadre d&amp;rsquo;E-topie, Marseille-Provence 2013.&lt;/li&gt;
&lt;li&gt;16 novembre au 15 d√©cembre 2013: pr√©sentation √† Paris dans le cadre du festival &lt;a href=&#34;http://www.digitalarti.com/fr/blog/digitalarti_mag/festival_nemo_tropiques_etienne_rey&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nemo&lt;/a&gt; ; √©couter &lt;a href=&#34;http://www.franceinter.fr/emission-latac-faux-sourires-vision-cosmique-installation-tropique&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;la t√™te au carr√© sur France Inter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;10 Octobre au 10 novembre 2013: pr√©sentation finale √† la fondation Vasarely √† Aix-en-Provence dans le cadre du festival &lt;a href=&#34;http://www.fondationvasarely.org/uk/e_topie.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;e-Topie&lt;/a&gt;, cf &lt;a href=&#34;http://www.liberation.fr/culture/2013/10/15/riche-e-topie-a-aix_939736&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cahier beaux-arts de lib√©&lt;/a&gt;, &lt;a href=&#34;http://www.laprovence.com/article/economie/2628580/aix-la-ville-candidate-pour-devenir-quartier-numerique.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;la provence&lt;/a&gt;  : 10450 spectateurs sur 32 jours&lt;/li&gt;
&lt;li&gt;Juin 2013: &lt;a href=&#34;https://laurentperrinet.github.io/post/2013-10-10_tropique/featured_hu4307c572b52602b5d74387cbed9b8bcf_354150_720x0_resize_q90_lanczos.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;r√©sidence √† l&amp;rsquo;INT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D√©cembre 2012: dans le &lt;a href=&#34;http://www.mp2013.fr/evenements/2013/10/atelier-de-leuromediterranee-tropique-etienne-rey-a-limera/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;programme officiel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;D√©cembre 2012: signature de la convention avec le CNRS&lt;/li&gt;
&lt;li&gt;Tropique est pr√©sent√© &lt;a href=&#34;http://www.ososphere.org/2012/evenement/tropique/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;07 au 14 d√©cembre 2012, dans le cadre du festival &amp;ldquo;Les nuits de l‚ÄôOsosph√®re&amp;rdquo; de Strasbourg&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Juin 2012: Tropique en r√©sidence √† la fondation Vasarely&lt;/li&gt;
&lt;li&gt;Mars 2012: &lt;a href=&#34;http://www.mecenesdusud.fr/blog/index.php?post/2012/03/Tropique-%C3%A0-Nantes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tropique √† Nantes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://creative.arte.tv/en/space/Tropique/messages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arte creative&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tropique @ &lt;a href=&#34;http://www.ran-dan.net/eng/?p=22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAN (Enghien-les-Bains)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vimeo.com/33718945&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Etienne Rey Tropique Experimentation Film2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Turbulences</title>
      <link>https://laurentperrinet.github.io/post/2018-01-20_turbulences/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2018-01-20_turbulences/</guid>
      <description>&lt;h1 id=&#34;turbulences&#34;&gt;Turbulences&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://player.vimeo.com/video/303255760&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;vimeo video&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;installation-in-situ-2018-collection-of-the-fran√ßois-schneider-foundation-wattwiller-i-2018&#34;&gt;Installation in situ, 2018; Collection of the Fran√ßois Schneider Foundation, Wattwiller I 2018&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;En agissant sur les variations et les rayonnements d‚Äôordre physique, &lt;a href=&#34;https://laurentperrinet.github.io/author/etienne-rey/&#34;&gt;Etienne Rey&lt;/a&gt; fait appara√Ætre l‚Äô√©paisseur existentielle du vide.&amp;rdquo; B√©n√©dicte Chevallier, M√©c√®nes du Sud&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;L‚Äôinstallation Turbulences explore l‚Äô√©mergence de caustiques, ph√©nom√®nes caract√©ristiques de la relation entre l‚Äôeau, la lumi√®re et l‚Äôair.
Le mouvement y perturbe un √©tat optique stable. La turbulence des plis lumineux donne l‚Äôillusion d‚Äôun corps flottant.&lt;/p&gt;
&lt;p&gt;Installation immersive plastique et sonore, l‚Äôexp√©rience de l‚Äôoeuvre se joue dans l‚Äôexploration de dimensions immat√©rielles par l‚Äôobservateur.&lt;/p&gt;
&lt;p&gt;Cr√©ation originale pour la Fondation Fran√ßois Schneider
Production : Quatre 4.0 / L‚ÄôOsospheÃÄre, en partenariat avec la Fondation Fran√ßois Schneider et le soutien de la R√©gion Grand Est
Accompagnement, production d√©l√©gu√©e : bOssa&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;INAUGURATION : SAMEDI 27 OCTOBRE 2018&lt;/li&gt;
&lt;li&gt;EXPOSITION &amp;gt; 20 JANVIER 2019&lt;/li&gt;
&lt;li&gt;FONDATION FRAN√áOIS SCHNEIDER - 27 RUE DE LA PREMI√àRE ARM√âE, 68700 WATTWILLER&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2014-04-17: Soutenance d&#39;habilitation √† diriger des recherches (HDR)</title>
      <link>https://laurentperrinet.github.io/post/2014-04-17_hdr/</link>
      <pubDate>Thu, 17 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2014-04-17_hdr/</guid>
      <description>&lt;p&gt;Quand: le 17 avril 2014 de 14 H30 √† 16 H 30,&lt;/p&gt;
&lt;p&gt;Quoi: ‚ÄúCodage pr√©dictif dans les transformations visuo-motrices‚Äù.&lt;/p&gt;
&lt;p&gt;Lieu:  salle Henri Gastaut, au rez de chauss√©e de l&amp;rsquo;INT  (how to &lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;La soutenance est suivie d‚Äôun pot au R+4 de l‚Äô&lt;a href=&#34;http://www.int.univ-amu.fr/?lang=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institut de Neurosciences de la Timone&lt;/a&gt; (how to &lt;a href=&#34;http://www.int.univ-amu.fr/contact&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;get there&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;jury&#34;&gt;Jury&lt;/h2&gt;
&lt;p&gt;La soutenance est ouverte √† tous, merci d‚Äôannoncer votre pr√©sence √† &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Le jury est compos√© par::&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. Laurent Madelain, Universit√© Lille III&lt;/li&gt;
&lt;li&gt;Dr. Alain Destexhe, Universit√© Paris XI (Rapporteur)&lt;/li&gt;
&lt;li&gt;Prof. Gustavo Deco, Universitat Pompeu Fabra, Barcelona (Rapporteur)&lt;/li&gt;
&lt;li&gt;Dr. Guillaume Masson, Aix-Marseille Universit√©&lt;/li&gt;
&lt;li&gt;Dr. Viktor Jirsa,  Aix-Marseille Universit√© (Rapporteur)&lt;/li&gt;
&lt;li&gt;Prof. J.-L. Mege, Aix-Marseille Universit√©&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>2010-06-22 : CodeJamNr4</title>
      <link>https://laurentperrinet.github.io/post/2010-06-22_codejam-nr4/</link>
      <pubDate>Tue, 22 Jun 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2010-06-22_codejam-nr4/</guid>
      <description>&lt;h1 id=&#34;facets-code-jam-workshop-4&#34;&gt;FACETS Code Jam Workshop #4&lt;/h1&gt;
&lt;p&gt;We held a CodeJam 22nd-24th June 2010, in Marseille.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://neuralensemble.org/media/images/codejam4_group_photo.jpg&#34; alt=&#34;Participants&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the
&lt;a href=&#34;http://neuralensemble.org/meetings/CodeJam4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The goal of the FACETS CodeJam workshops is to catalyze open-source, collaborative software development in computational and systems neuroscience and neuroinformatics, by bringing together researchers, students and engineers to share ideas, present their work, and write code together. The general format of the workshops is to dedicate the mornings to invited and contributed talks, leaving the afternoons free for discussions and code sprints. &lt;BR&gt;
For the 4th FACETS CodeJam, the main theme of the meeting will be workflows: what are the best practices for combining different tools (simulators, analysis tools, visualization tools, databases etc.) to ensure the efficient and reproducible flow of data and information from experiment conception to publication and archiving? &lt;BR&gt;
(&amp;hellip;) &lt;BR&gt;
The meeting organizers gratefully acknowledge the support of the European Union through the FACETS Project (grant no. IST-2005-15879), and the International Neuroinformatics Co-ordinating Facility (INCF). We also wish to express our great appreciation to the DyVA team at the Institut de Neurosciences Cognitives de la M√©diterran√©e for providing us with a great location and much assistance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuralensemble.org/meetings/CodeJam4.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuralensemble.org/meetings/CodeJam4.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://neuralensemble.org/meetings/CJ4_Program_v2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://neuralensemble.org/meetings/CJ4_Program_v2.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://facets.kip.uni-heidelberg.de/internal/jss/AttendMeeting?mI=73&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FACETS code jam #4&lt;/a&gt;{.https}&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Affiche&#34; srcset=&#34;
               /post/2010-06-22_codejam-nr4/featured_huc11aae8642c4fd2e999375944dee0092_59845_2923eecd2f15094c08c145eb88c3f13c.webp 400w,
               /post/2010-06-22_codejam-nr4/featured_huc11aae8642c4fd2e999375944dee0092_59845_dcd17a1e5893000d3ff320654353e003.webp 760w,
               /post/2010-06-22_codejam-nr4/featured_huc11aae8642c4fd2e999375944dee0092_59845_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2010-06-22_codejam-nr4/featured_huc11aae8642c4fd2e999375944dee0092_59845_2923eecd2f15094c08c145eb88c3f13c.webp&#34;
               width=&#34;579&#34;
               height=&#34;591&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2010-05-27 : Neurocomp08</title>
      <link>https://laurentperrinet.github.io/post/2008-10-08_neurocomp/</link>
      <pubDate>Thu, 27 May 2010 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2008-10-08_neurocomp/</guid>
      <description>&lt;h1 id=&#34;2008-10-08--deuxi√®me-conf√©rence-fran√ßaise-de-neurosciences-computationnelles-neurocomp08&#34;&gt;2008-10-08 : Deuxi√®me conf√©rence fran√ßaise de Neurosciences Computationnelles, &amp;ldquo;Neurocomp08&amp;rdquo;&lt;/h1&gt;
&lt;p&gt;La deuxi√®me conf√©rence fran√ßaise de Neurosciences Computationnelles, &amp;ldquo;Neurocomp08&amp;rdquo;, s&amp;rsquo;est d√©roul√©e √† la Facult√© de M√©decine de Marseille du 8 au 11 octobre 2008. Cette conf√©rence, organis√©e par le groupe de travail Neurocomp, a permis de r√©unir les principaux acteurs fran√ßais du domaine (francophones ou non). Le champ des Neurosciences Computationnelles porte sur l&amp;rsquo;√©tude des m√©canismes de calcul qui sont √† l&amp;rsquo;origine de nos capacit√©s cognitives. Cette approche n√©cessite l&amp;rsquo;int√©gration constructive de nombreux domaines disciplinaires, du neurone au comportement, des sciences du vivant √† la mod√©lisation num√©rique. Avec ce colloque, nous avons offert un lieu d&amp;rsquo;√©changes afin de favoriser des collaborations interdisciplinaires entre des √©quipes relevant des neurosciences, des sciences de l&amp;rsquo;information, de la physique statistique, de la robotique. Cette √©dition a √©galement √©t√© l&amp;rsquo;occasion d&amp;rsquo;ouvrir le cadre √† de nouveaux domaines (mod√®les pour l&amp;rsquo;imagerie, interfaces cerveau-machine,&amp;hellip;) notamment gr√¢ce √† des ateliers th√©matiques (une nouveaut√© dans cette √©dition). Certains des principaux enjeux du domaine ont √©t√© pr√©sent√©s par quatre conf√©renciers invit√©s : Ad Aertsen (Freiburg, Allemagne), Gustavo Deco (Barcelone, Espagne), Gregor Sch√∂ner (Bochum, Allemagne), Andrew B. Schwartz (Pittsburgh, USA). Des interventions orale plus courtes et plus sp√©cifiques √©taient √©galement au programme, sur la base d&amp;rsquo;une s√©lection du comit√© de lecture. Une cinquantaine de posters ont √©galement √©t√© pr√©sent√©s au cours de ces journ√©es. Le premier jour √©tait consacr√© aux mod√®les de la cellule neurale, aux mod√®les des traitements visuels et corticaux, ainsi qu&amp;rsquo;aux mod√®les de r√©seaux de neurones bio-mim√©tiques. La seconde journ√©e √©tait consacr√©e aux interfaces cerveau-machine, √† la dynamique des grands ensembles de neurones, √† la plasticit√© fonctionnelle et aux interfaces neurales. Enfin, la journ√©e de samedi √©tait consacr√©e √† des ateliers th√©matiques, l&amp;rsquo;un sur les interfaces cerveau-machine, l&amp;rsquo;autre sur la vision computationnnelle. Cette conf√©rence a connu un beau succ√®s de par l&amp;rsquo;affluence (200 personnes environ) et la qualit√© des interventions. Ce succ√®s tient √©galement au fort soutien financier et organisationnel qu&amp;rsquo;elle a obtenu de ses partenaires. Les organisateurs remercient le CNRS, la Soci√©t√© des neurosciences, le conseil r√©gional de la r√©gion Provence Alpes C√¥te d&amp;rsquo;Azur, le conseil g√©n√©ral des Bouches de Rh√¥ne, la mairie de Marseille, l&amp;rsquo;universit√© de Provence, l&amp;rsquo;IFR &amp;ldquo;Sciences du cerveau et de la cognition&amp;rdquo;, l&amp;rsquo;INRIA, ainsi que la facult√© de m√©decine de Marseille et l&amp;rsquo;universit√© de la M√©diterran√©e qui ont h√©berg√© la conf√©rence.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Les actes de la conf√©rence regroupant les 68 contributions sont disponibles sur le &lt;a href=&#34;http://hal.archives-ouvertes.fr/NEUROCOMP08&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;serveur HAL d√©di√©&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Affiche&#34; srcset=&#34;
               /post/2008-10-08_neurocomp/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_d1bd98594f92c2640958730ee308e159.webp 400w,
               /post/2008-10-08_neurocomp/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_825461c5cc98d0534d047894d7e95fb8.webp 760w,
               /post/2008-10-08_neurocomp/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2008-10-08_neurocomp/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_d1bd98594f92c2640958730ee308e159.webp&#34;
               width=&#34;408&#34;
               height=&#34;135&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computational Neuroscience: From Representations to Behavior</title>
      <link>https://laurentperrinet.github.io/post/2010-05-27_neurocomp-marseille-workshop/</link>
      <pubDate>Wed, 08 Oct 2008 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/post/2010-05-27_neurocomp-marseille-workshop/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Date: 27-28 May 2010&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Location: Amphith√©√¢tre Charve at the Saint-Charles&amp;rsquo; University campus&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M√©tro :
Line 1 et 2 (St Charles), a 5 minute walk from the railway station.
&lt;a href=&#34;http://maps.google.com/maps/ms?ie=UTF8&amp;amp;hl=fr&amp;amp;t=h&amp;amp;msa=0&amp;amp;msid=104552809318940980121.0004855ba608957ac9d29&amp;amp;ll=43.297245,5.369546&amp;amp;spn=0.011978,0.027874&amp;amp;z=16&#34; class=&#34;http&#34;&gt;&lt;/li&gt;
&lt;li&gt;Map (Amphith√©√¢tre Charve, University Main Entrance, etc.)&lt;/a&gt;
&lt;a href=&#34;http://85.31.207.119/SITERTM_WEB/PagesFlash/pdf/PlanReseau.pdf&#34; class=&#34;http&#34;&gt;&lt;/li&gt;
&lt;li&gt;Metro, Bus and Tramway&lt;/a&gt;
&lt;a href=&#34;http://www.navettemarseilleaeroport.com/indexA.php&#34; class=&#34;http&#34;&gt;&lt;/li&gt;
&lt;li&gt;Getting to Marseille from Airport&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Computational Neuroscience emerges now as a major breakthrough in
exploring cognitive functions. It brings together theoretical tools that
elucidate fundamental mechanisms responsible for experimentally observed
behaviour in the applied neurosciences. This is the second Computational
Neuroscience Workshop organized by the &amp;ldquo;NeuroComp Marseille&amp;rdquo; network.&lt;/p&gt;
&lt;p&gt;It will focus on latest advances on the understanding of how information
may be represented in neural activity (1st day) and on computational
models of learning, decision-making and motor control (2nd day). The
workshop will bring together leading researchers in these areas of
theoretical neuroscience. The meeting will consist of invited speakers
with sufficient time to discuss and share ideas and data. All
conferences were in English.&lt;/p&gt;
&lt;h2 id=&#34;program&#34;&gt;Program&lt;/h2&gt;
&lt;p&gt;27 May 2010 &lt;strong&gt;Neural representations for sensory information &amp;amp; the
structure-function relation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;9h00-9h30&lt;/p&gt;
&lt;p&gt;Reception and coffee&lt;/p&gt;
&lt;p&gt;9h30-10h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io/&#34; class=&#34;http&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;
Institut de Neurosciences Cognitives de la M√©diterran√©e, CNRS and
Universit√© de la M√©diterran√©e - Marseille
&lt;strong&gt;¬´Presentation of the Workshop and Topic¬ª&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;10h00-11h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.ceremade.dauphine.fr/~peyre/&#34; class=&#34;http&#34;&gt;Gabriel Peyr√©&lt;/a&gt;&lt;/em&gt;
CNRS and Universit√© Paris-Dauphine
&lt;a href=&#34;http://www.ceremade.dauphine.fr/~peyre/talks/2010-05-20-neurosciences-marseilles.pdf&#34; class=&#34;http&#34;&gt;&lt;strong&gt;¬´Sparse Geometric Processing of Natural Images¬ª&lt;/strong&gt;&lt;/a&gt;
In this talk, I will review recent works on the sparse representations
of natural images. I will in particular focus on both the application of
these emerging models to image processing problems, and their potential
implication for the modeling of visual processing.
Natural images exhibit a wide range of geometric regularities, such as
curvilinear edges and oscillating textures. Adaptive image
representations select bases from a dictionary of orthogonal or
redundant frames that are parameterized by the geometry of the image. If
the geometry is well estimated, the image is sparsely represented by
only a few atoms in this dictionary.
On an ingeniering level, these methods can be used to enhance the
resolution of super-resolution inverse problems, and can also be used to
perform texture synthesis. On a biological level, these mathematical
representations share similarities with low level grouping processes
that operate in areas V1 and V2 of the visual brain. We believe both
processing and biological application of geometrical methods work hand
in hand to design and analyze new cortical imaging methods.&lt;/p&gt;
&lt;p&gt;11h00-12h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.crea.polytechnique.fr/JeanPetitot/home.html&#34; class=&#34;http&#34;&gt;Jean Petitot&lt;/a&gt;&lt;/em&gt;
Centre d&amp;rsquo;Analyse et de Math√©matique Sociales, Ecole des Hautes Etudes en
Sciences Sociales - Paris &lt;strong&gt;¬´Neurogeometry of visual perception¬ª&lt;/strong&gt;
In relation with experimental data, we propose a geometric model of the
functional architecture of the primary visual cortex (V1) explaining
contour integration. The aim is to better understand the type of
geometry algorithms implemented by this functional architecture. The
contact structure of the 1-jet space of the curves in the plane, with
its generalization to the roto-translation group, symplectifications,
and sub-Riemannian geometry, are all neurophysiologically realized by
long-range horizontal connections. Virtual structures, such as illusory
contours of the Kanizsa type, can then be explained by this model.&lt;/p&gt;
&lt;p&gt;12h00&lt;/p&gt;
&lt;p&gt;Lunch&lt;/p&gt;
&lt;p&gt;14h00-14h45&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/pseries/&#34; class=&#34;http&#34;&gt;Peggy Series&lt;/a&gt;&lt;/em&gt;
Institute for Adaptive and Neural Computation, Edinburgh
&lt;strong&gt;¬´Bayesian Priors in Perception and Decision Making¬ª&lt;/strong&gt;
We&amp;rsquo;ll present two recent projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first project (with M. Chalk and A. R. Seitz) is an experimental
investigation of the influence of expectations on the perception of
simple stimuli. Using a simple task involving estimation and detection
of motion random dots displays, we examined whether expectations can be
developed quickly and implicitly and how they affect perception. We find
that expectations lead to attractive biases such that stimuli appear as
being more similar to the expected one than they really are, as well as
visual hallucinations in the absence of a stimulus. We discuss our
findings in terms of Bayesian Inference.&lt;/li&gt;
&lt;li&gt;In the second project (with A. Kalra and Q. Huys), we explore the
concepts of optimism and pessimism in decision making. Optimism is
usually assessed using questionnaires, such as the LOT-R. Here, using a
very simple behavioral task, we show that optimism can be described in
terms of a prior on expected future rewards. We examine the correlation
between the shape of this prior for individual subjects and their scores
on questionnaires, as well as with other measures of personality traits.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;14h45-15h45&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.informatik.uni-ulm.de/ni/staff/HNeumann/&#34; class=&#34;http&#34;&gt;Heiko Neumann&lt;/a&gt;&lt;/em&gt; (in
collaboration with Florian Raudies)
Inst. of Neural Information Processing, Ulm University Germany
&lt;strong&gt;¬´Cortical mechanisms of transparent motion perception ‚Äì a neural
model¬ª&lt;/strong&gt;
Transparent motion is perceived when multiple motions different in
directions and/or speeds are presented in the same part of visual space.
In perceptual experiments the conditions have been studied under which
motion transparency occurs. An upper limit in the number of perceived
transparent layers has been investigated psychophysically. Attentional
signals can improve the perception of a single motion amongst several
motions. While criteria for the occurrence of transparent motion have
been identified only few potential neural mechanisms have been discussed
so far to explain the conditions and mechanisms for segregating multiple
motions.
A neurodynamical model is presented which builds upon a previously
developed neural architecture emphasizing the role of feedforward
cascade processing and feedback from higher to earlier stages for
selective feature enhancement and tuning. Results of computational
experiments are consistent with findings from physiology and
psychophysics. Finally, the model is demonstrated to cope with realistic
data from computer vision benchmark databases.
Work supported by European Union (project SEARISE), BMBF, and CELEST&lt;/p&gt;
&lt;p&gt;15h45-15h00&lt;/p&gt;
&lt;p&gt;Coffee break&lt;/p&gt;
&lt;p&gt;16h00-17h00&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CANCELED&lt;/strong&gt;
&lt;em&gt;&lt;a href=&#34;http://pauli.uni-muenster.de/tp/index.php?id=9&amp;amp;L=1&#34; class=&#34;http&#34;&gt;Rudolf Friedrich&lt;/a&gt;&lt;/em&gt;
Institute f√ºr Theoretische Physik Westf√§lische Wilhelms Universit√§t
M√ºnster
&lt;strong&gt;¬´Windows to Complexity: Disentangling Trends and Fluctuations in
Complex Systems¬ª&lt;/strong&gt;
In the present talk, we discuss how to perform an analysis of
experimental data of complex systems by disentangling the effects of
dynamical noise (fluctuations) and deterministic dynamics (trends). We
report on results obtained for various complex systems like turbulent
fields, the motion of dissipative solitons in nonequilibrium systems,
traffic flows, and biological data like human tremor data and brain
signals. Special emphasis is put on methods to predict the occurrence of
qualitative changes in systems far from equilibrium.
[1] R. Friedrich, J. Peinke, M. Reza Rahimi Tabar: Importance of
Fluctuations: Complexity in the View of stochastic Processes (in:
Springer Encyclopedia on Complexity and System Science, (2009))&lt;/p&gt;
&lt;p&gt;17h00-17h45&lt;/p&gt;
&lt;p&gt;General Discussion&lt;/p&gt;
&lt;p&gt;&lt;span id=&#34;line-39&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;28 May 2010 &lt;strong&gt;Computational models of learning and decision making&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;9h30-10h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://brovelli.free.fr/&#34; class=&#34;http&#34;&gt;Andrea Brovelli&lt;/a&gt;&lt;/em&gt;
Institut de Neurosciences Cognitives de la M√©diterran√©e, CNRS and
Universit√© de la M√©diterran√©e - Marseille
&lt;strong&gt;¬´An introduction to Motor Learning, Decision-Making and Motor
Control¬ª&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;10h00-11h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://emmanuel.dauce.free.fr&#34; class=&#34;http&#34;&gt;Emmanuel Dauc√©&lt;/a&gt;&lt;/em&gt;
Mouvement &amp;amp; Perception, UMR 6152, Facult√© des sciences du sport
&lt;strong&gt;¬´Adapting the noise to the problem : a Policy-gradient approach of
receptive fields formation¬ª&lt;/strong&gt;
In machine learning, Kernel methods are give a consistent framework for
applying the perceptron algorithm to non-linear problems. In
reinforcement learning, the analog of the perceptron delta-rule is
called the &amp;ldquo;policy-gradient&amp;rdquo; approch proposed by Williams in 1992 in the
framework of stochastic neural networks. Despite its generality and
straighforward applicability to continuous command problems, quite few
developments of the method have been proposed since. Here we present an
account of the use of a kernel transformation of the perception space
for learning a motor command, in the case of eye orientation and
multi-joint arm control. We show that such transformation allows the
system to learn non-linear transformation, like the log-like resolution
of a foveated retina, or the transformation from a cartesian perception
space to a log-polar command, by shaping appropriate receptive fields
from the perception to the command space. We also present a method for
using multivariate correlated noise for learning high-DOF control
problems, and propose some interpretations on the putative role of
correlated noise for learning in biological systems.&lt;/p&gt;
&lt;p&gt;11h00-12h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.eng.cam.ac.uk/~ml468/&#34; class=&#34;http&#34;&gt;M√°t√© Lengyel&lt;/a&gt;&lt;/em&gt;
Computational &amp;amp; Biological Learning Lab, Department of Engineering,
University of Cambridge
&lt;strong&gt;¬´Why remember? Episodic versus semantic memories for optimal decision
making¬ª&lt;/strong&gt;
Memories are only useful inasmuch as they allow us to act adaptively in
the world. Previous studies on the use of memories for decision making
have almost exclusively focussed on implicit rather than declarative
memories, and even when they did address declarative memories they dealt
only with semantic but not episodic memories. In fact, from a purely
computational point of view, it seems wasteful to have memories that are
episodic in nature: why should it be better to act on the basis of the
recollection of single happenings (episodic memory), rather than the
seemingly normative use of accumulated statistics from multiple events
(semantic memory)? Using the framework of reinforcement learning, and
Markov decision processes in particular, we analyze in depth the
performance of episodic versus semantic memory-based control in a
sequential decision task under risk and uncertainty in a class of simple
environments. We show that episodic control should be useful in a range
of cases characterized by complexity and inferential noise, and most
particularly at the very early stages of learning, long before
habitization (the use of implicit memories) has set in. We interpret
data on the transfer of control from the hippocampus to the striatum in
the light of this hypothesis.&lt;/p&gt;
&lt;p&gt;12h00-14h00&lt;/p&gt;
&lt;p&gt;Lunch&lt;/p&gt;
&lt;p&gt;14h00-15h00&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.cs.bris.ac.uk/~rafal/&#34; class=&#34;http&#34;&gt;Rafal Bogacz&lt;/a&gt;&lt;/em&gt;
Department of Computer Science, University of Bristol
&lt;strong&gt;¬´Optimal decision making and reinforcement learning in the
cortico-basal-ganglia circuit¬ª&lt;/strong&gt;
During this talk I will present a computational model describing
decision making process in the cortico-basal ganglia circuit. The model
assumes that this circuit performs statistically optimal test that
maximizes speed of decisions for any required accuracy. In the model,
this circuit computes probabilities that considered alternatives are
correct, according to Bayes‚Äô theorem. This talk will show that the
equation of Bayes‚Äô theorem can be mapped onto the functional anatomy of
a circuit involving the cortex, basal ganglia and thalamus. This theory
provides many precise and counterintuitive experimental predictions,
ranging from neurophysiology to behaviour. Some of these predictions
have been already validated in existing data and others are a subject of
ongoing experiments. During the talk I will also discuss the
relationships between the above model and current theories of
reinforcement learning in the cortico-basal-ganglia circuit.&lt;/p&gt;
&lt;p&gt;15h00-15h30&lt;/p&gt;
&lt;p&gt;Coffee break&lt;/p&gt;
&lt;p&gt;15h30-16h30&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://e.guigon.free.fr/&#34; class=&#34;http&#34;&gt;Emmanuel Guigon&lt;/a&gt;&lt;/em&gt;
Institut des Syst√®mes Intelligents et de Robotique, UPMC - CNRS / UMR
7222
&lt;strong&gt;¬´Optimal feedback control as a principle for adaptive control of
posture and movement¬ª&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;16h30-17h15&lt;/p&gt;
&lt;p&gt;General Discussion&lt;/p&gt;
&lt;p&gt;&lt;span id=&#34;line-54&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&#34;line-57&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&#34;sponsored-by&#34;&gt;Sponsored by&lt;/h2&gt;
&lt;p&gt;&lt;span id=&#34;line-59&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.incm.cnrs-mrs.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.incm.cnrs-mrs.fr/images/logo-INCM.png&#34; title=&#34;http://www.incm.cnrs-mrs.fr/&#34; class=&#34;external_image&#34; style=&#34;width:15.0%&#34; alt=&#34;http://www.incm.cnrs-mrs.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-60&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.ism.univmed.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.ism.univmed.fr/IMG/logoISM2.gif&#34; title=&#34;http://www.ism.univmed.fr/&#34; class=&#34;external_image&#34; style=&#34;width:10.0%&#34; alt=&#34;http://www.ism.univmed.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-61&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://sites.univ-provence.fr/ifrscc/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://sites.univ-provence.fr/ifrscc/plugins/kitcnrs/images/logoifr.jpg&#34; title=&#34;http://sites.univ-provence.fr/ifrscc/&#34; class=&#34;external_image&#34; style=&#34;width:5.0%&#34; alt=&#34;http://sites.univ-provence.fr/ifrscc/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-62&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.univmed.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.univmed.fr/App_Themes/Default/images/hp/logo_d.gif&#34; title=&#34;http://www.univmed.fr/&#34; class=&#34;external_image&#34; style=&#34;width:8.0%&#34; alt=&#34;http://www.univmed.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-63&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.univ-provence.fr/&#34; class=&#34;http&#34;&gt;&lt;img src=&#34;http://www.univ-provence.fr/Local/up/fr/bandeau/logo_up.gif&#34; title=&#34;http://www.univ-provence.fr/&#34; class=&#34;external_image&#34; style=&#34;width:5.0%&#34; alt=&#34;http://www.univ-provence.fr/&#34; /&gt;&lt;/a&gt;
&lt;span id=&#34;line-64&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;a href=&#34;http://www.univ-provence.fr/gsite/index.php?project=pole3c&#34; class=&#34;http&#34;&gt;Pole 3c&lt;/a&gt;
&lt;span id=&#34;line-66&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;span
id=&#34;line-68&#34; class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;span id=&#34;line-69&#34;
class=&#34;anchor&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Affiche&#34; srcset=&#34;
               /post/2010-05-27_neurocomp-marseille-workshop/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_d1bd98594f92c2640958730ee308e159.webp 400w,
               /post/2010-05-27_neurocomp-marseille-workshop/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_825461c5cc98d0534d047894d7e95fb8.webp 760w,
               /post/2010-05-27_neurocomp-marseille-workshop/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://laurentperrinet.github.io/post/2010-05-27_neurocomp-marseille-workshop/featured_hu2a73fa56d035ed370b994c75abc43e76_33187_d1bd98594f92c2640958730ee308e159.webp&#34;
               width=&#34;408&#34;
               height=&#34;135&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
