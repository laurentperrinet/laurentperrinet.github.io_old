<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Laurent U Perrinet" />

  
  
  
    
  
  <meta name="description" content="THE POSITION HAS BEEN FILLED. Offre de thèse &#34;Vision ultra-rapide par réseaux de neurones impulsionnels&#34; à Marseille" />

  
  <link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/post/2020-06-30_phd-position/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.8722fd0f5cb8e5cb38d3b73367786cc1.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-140381649-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-140381649-1', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  


  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://laurentperrinet.github.io/post/2020-06-30_phd-position/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@laurentperrinet" />
    <meta property="twitter:creator" content="@laurentperrinet" />
  
  <meta property="og:site_name" content="Novel visual computations" />
  <meta property="og:url" content="https://laurentperrinet.github.io/post/2020-06-30_phd-position/" />
  <meta property="og:title" content="PhD offer &#34;Ultra-fast vision using Spiking Neural Networks&#34; | Novel visual computations" />
  <meta property="og:description" content="THE POSITION HAS BEEN FILLED. Offre de thèse &#34;Vision ultra-rapide par réseaux de neurones impulsionnels&#34; à Marseille" /><meta property="og:image" content="https://laurentperrinet.github.io/post/2020-06-30_phd-position/featured.jpeg" />
    <meta property="twitter:image" content="https://laurentperrinet.github.io/post/2020-06-30_phd-position/featured.jpeg" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2020-06-30T09:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2020-06-30T09:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://laurentperrinet.github.io/post/2020-06-30_phd-position/"
  },
  "headline": "PhD offer \"Ultra-fast vision using Spiking Neural Networks\"",
  
  "image": [
    "https://laurentperrinet.github.io/post/2020-06-30_phd-position/featured.jpeg"
  ],
  
  "datePublished": "2020-06-30T09:00:00Z",
  "dateModified": "2020-06-30T09:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Laurent U Perrinet"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Novel visual computations",
    "logo": {
      "@type": "ImageObject",
      "url": "https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "THE POSITION HAS BEEN FILLED. Offre de thèse \"Vision ultra-rapide par réseaux de neurones impulsionnels\" à Marseille"
}
</script>

  

  

  

  





  <title>PhD offer &#34;Ultra-fast vision using Spiking Neural Networks&#34; | Novel visual computations</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="0ad2dfb60790794e7c4b5f42cdc483ed" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Featured</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Events</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#people"><span>People</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#grants"><span>Grants</span></a>
          </li>

          
          

          

          
          
          
            
              
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  





















  
  


<div class="article-container pt-3">
  <h1>PhD offer &#34;Ultra-fast vision using Spiking Neural Networks&#34;</h1>

  
  <p class="page-subtitle">THE POSITION HAS BEEN FILLED. CD Doctorant &ldquo;Vision ultra-rapide par réseaux de neurones impulsionnels&rdquo; H/F (MARSEILLE)</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jun 30, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  
  
  

  
  

</div>

  





</div>


<div class="article-header container featured-image-wrapper mt-4 mb-4" style="max-width: 800px; max-height: 570px;">
  <div style="position: relative">
    <img src="/post/2020-06-30_phd-position/featured_hue9b582315461dd5abc542b7f862bd9ba_150954_1200x2500_fit_q75_h2_lanczos.webp" width="800" height="570" alt="" class="featured-image">
    <span class="article-header-caption">Rufous Hummingbird &ldquo;Super fast little hummer on a scarlet Kunzea plant, (thanks for the plant ID, Teddy) El Chorro regional park&rdquo; photo <a href="https://www.flickr.com/photos/puliarfanita/13322040205" target="_blank" rel="noopener">Anita Ritenour</a> - Attribution 2.0 Generic (CC BY 2.0)</span>
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <div class="alert alert-warning">
  <div>
    THE POSITION HAS BEEN FILLED.
  </div>
</div>
<p>Dear colleagues,</p>
<p>Applications are welcome for a fully funded doctoral position at <a href="http://www.int.univ-amu.fr/?lang=en" target="_blank" rel="noopener">INT</a> in <a href="https://en.wikipedia.org/wiki/Marseille" target="_blank" rel="noopener">Marseille</a>, France. Your mission will be to build ultra-fast vision algorithms using event-based cameras and spiking neural networks. The project is funded by the <a href="https://laurentperrinet.github.io/grant/aprovis-3-d/" target="_blank" rel="noopener">APROVIS3D</a> grant (ANR-19-CHR3-0008-03) and will be coordinated by <a href="https://laurentperrinet.github.io/" target="_blank" rel="noopener">Laurent Perrinet</a>. The work will be carried out in collaboration with a leading computer science institute at Université Côte d’Azur (Sophia Antipolis, France), the Laboratoire d&rsquo;Informatique, Signaux et Systèmes de Sophia-Antipolis (I3S, UMR7271 - UNS CNRS), that will be part of the supervision team. We are seeking candidates with a strong background in machine learning, computer vision and computational neuroscience.</p>
<p>To obtain further information, please visit <a href="https://laurentperrinet.github.io/post/2020-06-30_phd-position" target="_blank" rel="noopener">https://laurentperrinet.github.io/post/2020-06-30_phd-position</a> or contact me @ <a href="mailto:Laurent.Perrinet@univ-amu.fr">Laurent.Perrinet@univ-amu.fr</a>. To candidate, follow instructions on the dedicated <a href="https://bit.ly/3igRji4" target="_blank" rel="noopener">server from the CNRS</a>.</p>
<p>The starting date is set to October 1st, 2020 and the appointment is for 36 month. Applications are welcome immediately.</p>
<p>Thanks for distributing this announcement to potential candidates!</p>
<blockquote class="twitter-tweet"><p lang="fr" dir="ltr">CD Doctorant &quot;Vision ultra-rapide utilisant des Réseaux de neurones impulsionnels&quot; H/F (MARSEILLE) (MARSEILLE 05) <a href="https://t.co/I5CXWxR3zi">https://t.co/I5CXWxR3zi</a> <a href="https://twitter.com/hashtag/Emploi?src=hash&amp;ref_src=twsrc%5Etfw">#Emploi</a> <a href="https://twitter.com/hashtag/OffreEmploi?src=hash&amp;ref_src=twsrc%5Etfw">#OffreEmploi</a> <a href="https://twitter.com/hashtag/Recrutement?src=hash&amp;ref_src=twsrc%5Etfw">#Recrutement</a></p>&mdash; EmploiCNRS (@EmploiCNRS) <a href="https://twitter.com/EmploiCNRS/status/1277872035700539392?ref_src=twsrc%5Etfw">June 30, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<h2 id="detailed-description-ultra-fast-vision-using-spiking-neural-networks">Detailed description: &ldquo;Ultra-fast vision using Spiking Neural Networks&rdquo;</h2>
<p>Biological vision is surprisingly efficient. To take advantage of this efficiency, Deep learning and convolutional neural networks (CNNs) have recently produced great advances in artificial computer vision. However, these algorithms now face multiple challenges: learned architectures are often not interpretable, disproportionally energy greedy, and often lack the integration of contextual information that seems optimized in biological vision and human perception. Crucially, given an equal constraint on energy consumption, these algorithms are relatively slow compared to biological vision. It is believed that one major factor of this rapidity is the fact that visual information is represented by short pulses (spikes) at analog – not discrete – times (<a href="#Paugam12">Paugam and Bohte, 2012</a>). However, most classical computer vision algorithms rely on such frame-based approaches. One solution to overcome their limitations is to use event-based representations, but these still lack in practice, and their high potential is largely underexploited. Inspired by biology, the project addresses the scientific question of developing a low-power sensing architecture for the processing of visual scenes, able to function on analog devices without a central clock and aimed at being validated in real-life situations. More specifically, the project will develop new paradigms for biologically inspired computer vision (<a href="#Cristobal15">Cristobal, Keil and Perrinet, 2015</a>), from sensing to processing, in order to help machines such as Unmanned Autonomous Vehicles (UAV), autonomous vehicles, or robots gain high-level understanding from visual scenes.</p>
<p><strong>In this doctoral project, we propose to address major limitations of classical computer vision by implementing specific dynamical features of cortical circuits: <em>spiking neural networks</em> (<a href="#Perrinet04">Perrinet, Thorpe and Samuelides, 2004</a>; <a href="#Lagorce16">Lagorce et al., 2018</a>), <em>lateral diffusion of neural information</em> (<a href="#Chavane2000">Chavane et al., 2011</a>; <a href="#muller2018cortical">Muller et al., 2018</a>) and <em>dynamic neuronal association fields</em> (<a href="#Fr%c3%a9gnac2012">Frégnac et al., 2012</a>; <a href="#Fr%c3%a9gnac2016">Frégnac et al., 2016</a>; <a href="#gerard2016synaptic">Gerard-Mercier et al., 2016</a>)</strong>. One starting point is to use event-based cameras <a href="#Dupeyroux18">(Dupeyroux et al., 2018)</a> and to extend results of self-supervised learning that we have obtained on static, natural images (<a href="#BoutinFranciosiniChavaneRuffierPerrinet20">Boutin et al., 2020</a>) showing in a recurrent cortical-like artificial CNN architecture the emergence of interactions which phenomenologically correspond to the &ldquo;association field&rdquo; described at the psychophysical (<a href="#Field1993">Field et al., 1993</a>), spiking (<a href="#Li2002">Li and Gilbert, 2002</a>) and synaptic (<a href="#gerard2016synaptic">Gerard-Mercier et al., 2016</a>) levels. Indeed, the architecture of primary visual cortex (V1), the direct target of the feedforward visual flow, contains dense local recurrent connectivity with sparse long-range connections (<a href="#Voges12">Voges and Perrinet, 2012</a>). Such connections add to the traditional convolutional kernels representing feedforward and local recurrent amplification a novel lateral interaction kernel within a single layer (across positions and channels). It is not well understood, but probably decisive for ultra-fast vision, how recurrent cortico-cortical loops add a level of distributed top-down complexity in the feed-forward stream of information which participates to the ultra-fast integration of sensory input and perceptual context (<a href="#Keller2019">Keller et al., 2019</a>). Coupled with the dynamics of cortical circuits, this elaborate multiplexed architecture provides the conditions possible for defining ultra-fast vision algorithms.</p>
<h2 id="expected-profile-of-the-candidate">Expected profile of the candidate</h2>
<p>Candidates should have experience in the domain of computational neuroscience, physics, engineering or related, and a solid training in machine learning and computer vision.</p>
<p>The candidate has to show good skills in computer science (programming skills, architecture understanding, git versioning, &hellip;), and in image processing methods. Good command of programming tools (Python scripting) is required. Multidisciplinary background would be strongly appreciated and in particular an advanced knowledge in mathematics, for a deep understanding of signal processing methods, along with strong computational skills. The candidate needs to show a keen interest in neuroscience. It is a bonus if the candidate is curious about neuroscience and visual perception.</p>
<p>The candidate has to fluently speak English to understand publications and to attend international conferences and workshops and pro-actively interact with partners in France, Switzerland, Spain and Greece. The preferred candidate will have the ability to work autonomously, and needs to be flexible to comply with the working method of the supervisors.</p>
<h2 id="research-context">Research context</h2>
<p>The thesis will be carried out in the team &ldquo;NEuronal OPerations in visual TOpographic maps&rdquo; (NeOpTo) within the <a href="http://www.int.univ-amu.fr/?lang=en" target="_blank" rel="noopener">Institut de Neurosciences de la Timone</a> in <a href="https://en.wikipedia.org/wiki/Marseille" target="_blank" rel="noopener">Marseille</a>, a welcoming and lively town by the Mediterranean sea in the south of France. The research team is led by F. Chavane (DR2, CNRS) and currently hosts 4 permanent staff, 3 post-docs and 4 PhD students. The research themes of the team are focused on neuronal operations within visual cortical maps. Indeed, along the cortical hierarchy, low-level features such as the position and orientation of the visual stimulus (but also auditory tone, somatosensory touch, etc&hellip;) but also higher-level features (such as faces, viewpoints of objects, etc&hellip;) are represented topographically on the cortical surface.</p>
<p>This work will be conducted in direct collaboration with <a href="http://i3s.unice.fr/jmartinet/en" target="_blank" rel="noopener">Jean Martinet</a> who will co-supervise the thesis. We will develop these algorithms in collaboration with <a href="https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;hl=fr" target="_blank" rel="noopener">Ryad Benosman</a> (Université Pierre et Marie Curie) and <a href="https://scholar.google.co.uk/citations?user=iIGoymcAAAAJ" target="_blank" rel="noopener">Stéphane Viollet</a> (équipe biorobotique, Institut des Sciences du Mouvement).</p>
<h2 id="fr-description-du-sujet-de-thèse">FR: Description du sujet de thèse</h2>
<p>La vision biologique est étonnamment efficace. Pour tirer parti de cette efficacité, l&rsquo;apprentissage profond et les réseaux neuronaux convolutionnels (CNN) ont récemment permis de réaliser de grandes avancées en matière de vision artificielle par ordinateur. Cependant, ces algorithmes sont aujourd&rsquo;hui confrontés à de multiples défis : les architectures apprises sont souvent peu interprétables, sont démesurément gourmandes en énergie, n&rsquo;intègrent généralement pas les informations contextuelles qui semblent parfaitement adaptées à la vision biologique et à la perception humaine. Aussi ces algorithmes sont relativement lents -à consommation énergétique égale- par rapport à la vision biologique. On pense qu&rsquo;un facteur majeur de cette rapidité est le fait que l&rsquo;information est représentée par de courtes impulsions à des moments analogiques - et non discrets. Toutefois, les algorithmes de vision par ordinateur utilisant une telle représentation dans des réseaux de neurones impulsionnels font encore défaut dans la pratique, et son important potentiel est largement sous-exploité. Ce projet, qui est inspiré de la biologie, aborde la question scientifique du développement d&rsquo;une architecture ultra-rapide de détection et de traitement de scènes visuelles, fonctionnant sur des appareils sans horloge centrale, et visant à valider ce genre d&rsquo;algorithmes événementiels dans des situations réelles. Plus spécifiquement, le projet développera de nouveaux paradigmes pour une vision d&rsquo;inspiration biologique, de la détection au traitement, afin d&rsquo;aider des machines telles que les robots aériens autonomes (UAV), les véhicules autonomes ou les robots à acquérir une compréhension de haut niveau des scènes visuelles.</p>
<h2 id="fr-contexte-de-travail">FR: Contexte de travail</h2>
<p>La thèse sera effectuée dans l&rsquo;équipe &ldquo;NEuronal OPerations in visual TOpographic maps&rdquo; (NeOpTo) au sein de l&rsquo;Institut de Neurosciences de la Timone (INT). L&rsquo;équipe de recherche est dirigée par F. Chavane (DR2, CNRS) et accueille actuellement 4 personnels permanents, 3 post-doctorants et 4 doctorants. Les thématiques de recherche de l&rsquo;équipe sont centrées sur les opérations neuronales au sein de cartes corticales visuelles. En effet, le long de la hiérarchie corticale, les caractéristiques de bas niveau telles que la position, l’orientation du stimulus visuel (mais aussi la tonalité auditive, le toucher somatosensoriel, etc&hellip;) mais aussi les caractéristiques de niveau supérieur (telles que les visages, les points de vue d’objets, etc&hellip;) sont représentées topographiquement sur la surface corticale.</p>
<p>Cette thèse sera menée en collaboration directe avec <a href="http://i3s.unice.fr/jmartinet/en" target="_blank" rel="noopener">Jean Martinet</a> qui co-supervisera cette thèse. Nous développerons ces algorithmes en collaboration avec <a href="https://scholar.google.fr/citations?user=_ZTFUooAAAAJ&amp;hl=fr" target="_blank" rel="noopener">Ryad Benosman</a> (Université Pierre et Marie Curie) et <a href="https://scholar.google.co.uk/citations?user=iIGoymcAAAAJ" target="_blank" rel="noopener">Stéphane Viollet</a> (équipe biorobotique, Institut des Sciences du Mouvement).</p>
<h1 id="references">References</h1>
<ul>
<li>
<p><a name="BoutinFranciosiniChavaneRuffierPerrinet20">Boutin, Victor, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, and Laurent U Perrinet. (2019). </a> &ldquo;<a href="https://arxiv.org/abs/1902.07651" target="_blank" rel="noopener">Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system.</a>&rdquo; <em>arXiv</em></p>
</li>
<li>
<p><a name="Dupeyroux18">Julien Dupeyroux, Victor Boutin, Julien R Serres, Laurent U Perrinet, Stéphane Viollet. (2018). </a> &ldquo;<a href="https://laurentperrinet.github.io/publication/dupeyroux-boutin-serres-perrinet-viollet-18/" target="_blank" rel="noopener">M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation.</a>&rdquo; <em>ISCAS</em></p>
</li>
<li>
<p><a name="Chavane2011">Chavane, F., Sharon, D., Jancke, D., Marre, O., Frégnac, Y. and Grinvald, A. (2011). </a> &ldquo;<a href="https://doi.org/10.1016/S0928-4257%2800%2901096-2" target="_blank" rel="noopener">Lateral spread of orientation selectivity in V1 is controlled by intracortical cooperativity.</a>&rdquo; <em>Journal of Physiology Paris</em> 94 (5-6): 333&ndash;42.</p>
</li>
<li>
<p><a name="Cristobal15">Gabriel Cristóbal, Laurent U Perrinet, Matthias S Keil (2015). </a> &ldquo;<a href="https://laurentperrinet.github.io/publication/cristobal-perrinet-keil-15-bicv/" target="_blank" rel="noopener">Biologically Inspired Computer Vision.</a>&rdquo; <em>Wiley</em>.</p>
</li>
<li>
<p><a name="Field1993">Field, D.J., Hayes, A. and Hess, R.F. (1993). </a> &ldquo;<a href="https://doi.org/10.1016/0042-6989%2893%2990156-Q" target="_blank" rel="noopener">Contour integration by the human visual system: Evidence for a local “association field”.</a>&rdquo; <em>Vision Research</em> 33 (2), pp. 173-193.</p>
</li>
<li>
<p><a name="gerard2016synaptic">Gerard-Mercier, Florian, Pedro V Carelli, Marc Pananceau, Xoana G Troncoso, and Yves Frégnac. (2016). </a> &ldquo;<a href="https://www.jneurosci.org/content/36/14/3925" target="_blank" rel="noopener">Synaptic Correlates of Low-Level Perception in V1.</a>&rdquo; <em>Journal of Neuroscience</em> 36 (14): 3925&ndash;42.</p>
</li>
<li>
<p><a name="Keller2019">Keller, A., Roth, M.M. and Scanziani, M. (2019). </a> 2019. &ldquo;<a href="https://www.abstractsonline.com/pp8/#!/7883/presentation/65856" target="_blank" rel="noopener">The feedback receptive field of neurons in the mammalian primary visual cortex.</a>&rdquo; <em>American Society for Neuroscience Abstracts</em>, 403.13. Chicago.</p>
</li>
<li>
<p><a name="Lagorce16">Lagorce, X., Orchard, G., Galluppi, F., Shi, B. E., &amp; Benosman, R. B.</a> (2016). &ldquo;<a href="https://www.neuromorphic-vision.com/public/publications/1/publication.pdf" target="_blank" rel="noopener">HOTS: a hierarchy of event-based time-surfaces for pattern recognition.</a>&rdquo; <em>IEEE transactions on pattern analysis and machine intelligence</em>.</p>
</li>
<li>
<p><a name="Li2002">Li W, Piëch V, Gilbert CD</a> (2006). &ldquo;<a href="http://www.paper.edu.cn/scholar/showpdf/MUz2UN2INTA0eQxeQh" target="_blank" rel="noopener">Contour saliency in primary visual cortex.</a>&rdquo; <em>Neuron</em>, 50(6):951–962.</p>
</li>
<li>
<p><a name="muller2018cortical">Muller, Lyle, Frédéric Chavane, John Reynolds, and Terrence J Sejnowski. </a> (2018). &ldquo;<a href="https://papers.cnl.salk.edu/PDFs/Cortical%20travelling%20waves_%20mechanisms%20and%20computational%20principles.%202018-4515.pdf" target="_blank" rel="noopener">Cortical Travelling Waves: Mechanisms and Computational Principles.</a>&rdquo; <em>Nature Reviews Neuroscience</em> 19 (5): 255.</p>
</li>
<li>
<p><a name="Paugam12">Hélène Paugam-Moisy, Sander M. Bohte. </a> (2012). &ldquo;Computing with Spiking Neuron Networks.&rdquo; <em>Handbook of Natural Computing</em>, Springer-Verlag, pp.335-376, 2012</p>
</li>
<li>
<p><a name="Perrinet04">Laurent U Perrinet, Manuel Samuelides, Simon J Thorpe. </a> (2004). <a href="https://laurentperrinet.github.io/publication/perrinet-03-ieee/" target="_blank" rel="noopener">&ldquo;Coding static natural images using spiking event times: do neurons cooperate?&rdquo;</a> <em>IEEE Transactions on Neural Networks</em>.</p>
</li>
<li>
<p><a name="Tang18">Tang, Hanlin, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. </a> (2018). &ldquo;<a href="https://doi.org/10.1073/pnas.1719397115" target="_blank" rel="noopener">Recurrent computations for visual pattern completion.</a>&rdquo; <em>Proceedings of the National Academy of Sciences</em> 115 (35) 8835-8840.</p>
</li>
<li>
<p><a name="Voges12">Voges, Nicole, and Laurent U Perrinet.</a> (2012). &ldquo;<a href="https://doi.org/10.3389/fncom.2012.00041" target="_blank" rel="noopener">Complex Dynamics in Recurrent Cortical Networks Based on Spatially Realistic Connectivities.</a>&rdquo; <em>Frontiers in Computational Neuroscience</em> 6.</p>
</li>
</ul>

    </div>

    




<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/events/">events</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://laurentperrinet.github.io/post/2020-06-30_phd-position/&amp;text=PhD%20offer%20&amp;#34;Ultra-fast%20vision%20using%20Spiking%20Neural%20Networks&amp;#34;" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://laurentperrinet.github.io/post/2020-06-30_phd-position/&amp;t=PhD%20offer%20&amp;#34;Ultra-fast%20vision%20using%20Spiking%20Neural%20Networks&amp;#34;" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=PhD%20offer%20&amp;#34;Ultra-fast%20vision%20using%20Spiking%20Neural%20Networks&amp;#34;&amp;body=https://laurentperrinet.github.io/post/2020-06-30_phd-position/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://laurentperrinet.github.io/post/2020-06-30_phd-position/&amp;title=PhD%20offer%20&amp;#34;Ultra-fast%20vision%20using%20Spiking%20Neural%20Networks&amp;#34;" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=PhD%20offer%20&amp;#34;Ultra-fast%20vision%20using%20Spiking%20Neural%20Networks&amp;#34;%20https://laurentperrinet.github.io/post/2020-06-30_phd-position/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://laurentperrinet.github.io/post/2020-06-30_phd-position/&amp;title=PhD%20offer%20&amp;#34;Ultra-fast%20vision%20using%20Spiking%20Neural%20Networks&amp;#34;" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://laurentperrinet.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/laurent-u-perrinet/avatar_hu34ad5b8db880542c60288aed1996fc62_245755_270x270_fill_q75_lanczos_center.jpg" alt="Laurent U Perrinet"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://laurentperrinet.github.io/">Laurent U Perrinet</a></h5>
      <h6 class="card-subtitle">Researcher in Computational Neuroscience</h6>
      <p class="card-text">My research interests include Machine Learning and computational neuroscience applied to Vision.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/laurentperrinet" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0002-9536-010X" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2022 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.7f632ea429b2189fa80232d9afedded1.js"></script>

    
    
    
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.ab2f2890dbe3e2e83579366d3d6e8fd9.js"></script>

    
    
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js" type="module"></script>
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
