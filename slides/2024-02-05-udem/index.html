<!DOCTYPE html>
<html lang="en-us">
<head>

  
  
  
  

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Hugo Blox Builder 5.9.6">

  
    <link rel="manifest" href="/manifest.webmanifest">
  

  <link rel="icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://laurentperrinet.github.io/slides/2024-02-05-udem/">

  <title> | Novel visual computations</title>

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/theme/white.min.css">

  
  
  
  
    
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css">
  

  
  
  
</head>
<body>

  
<div class="reveal">
  <div class="slides">
    
    
    

    
    
    
    
    

    
    

    
    
    
    <section>
    
      <section>
<h1 id="neuromorphic-models-of-visionhttpslaurentperrinetgithubioslides2024-02-05-udemtransitionfade"><a href="https://laurentperrinet.github.io/slides/2024-02-05-udem/?transition=fade" target="_blank" rel="noopener">Neuromorphic models of vision</a></h1>
<h4 id="laurent-perrinethttpslaurentperrinetgithubio"><em><a href="https://laurentperrinet.github.io" target="_blank" rel="noopener">Laurent Perrinet</a></em></h4>
<h4 id="u2024-02-05httpslaurentperrinetgithubiotalk2023-12-01-biocomp-seminar-at-udems-school-of-optometry-montréalhttpsoptoumontrealcaecoleenglishu"><u><a href="https://laurentperrinet.github.io/talk/2023-12-01-biocomp" target="_blank" rel="noopener">[2024-02-05]</a> <a href="https://opto.umontreal.ca/ecole/english/" target="_blank" rel="noopener">Seminar at UdeM’s School of Optometry, Montréal</a></u></h4>
<img src="https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg" alt="logos" height="130"/>
<p><a href="mailto:laurent.perrinet@univ-amu.fr">laurent.perrinet@univ-amu.fr</a></p>
<aside class="notes">
  <p><em>Hello</em>, can you hear me in the back?</p>
<p>I&rsquo;m Laurent Perrinet from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit, and during this seminar at the BioComp 2023 colloquium, I&rsquo;ll be presenting <em>event-driven cameras</em>, a new technology in the field of imaging, and the impact of this technology on our understanding of vision. I&rsquo;d like to <em>thank</em> organizers for this opportunity, and all of you for coming. These slides are available from my website, along with a number of references. The <em>outline</em> of the talk is as follows: first, we&rsquo;ll describe what an event-driven camera is - in particular, by comparing it to a conventional camera; then, we&rsquo;ll show some examples of applications of these cameras with dedicated algorithms; and finally, we&rsquo;ll present how our knowledge of biological mechanisms in neuroscience can enable us to improve these algorithms.</p>

</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="sensing-light">Sensing light</h1>
<aside class="notes">
  First of all, the general aim of <em>imaging</em> is to represent a visual signal, i.e. a luminous intensity, a color, distributed over the visual field, giving us a vivid impression of the visual scene before our eyes.
</aside>

    </section>
    

    
    
    
      

<section data-noprocess data-shortcode-slide
  
      
      data-background-image="http://lepassetempsderose.l.e.pic.centerblog.net/fddea7fb.gif"
  >

<aside class="notes">
  This is perfectly illustrated in this <em>galloping horse</em>. We get a <em>vivid</em> impression of movement. Thanks to a rapid sequence of still images consistent with the scene being represented. This technique clearly exploits a visual <em>illusion</em>, because we know that at each point in the visual space, the light signal is made up of a <em>continuous</em> stream of an analogous signal representing the energy of the photos.
</aside>

    </section>
    

    
    
    
      

<section data-noprocess data-shortcode-slide
  
      
      data-background-image="https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif"
  >

<aside class="notes">
  This technique is inspired by the research carried out by <a href="https://en.wikipedia.org/wiki/%c3%89tienne-Jules_Marey" target="_blank" rel="noopener">Étienne-Jules <em>Marey</em></a>, under the term <em>chronophotography</em>, litterally shooting scene with a gun-like apparatus to shoot a visual scene. It notably enabled later Muybridge to scientifically demonstrate the mechanism of a horse&rsquo;s gallop.
</aside>

    </section>
    

    
    
    
      

<section data-noprocess data-shortcode-slide
  
      
      data-background-image="https://media.giphy.com/media/4Y8PqJGFJ21CE/giphy.gif"
  >

<aside class="notes">
  The use of such dynamic <em>visualization</em> is crucial in the scientific field, whether in biology or physics, as it enables us to quantify the characteristics of the experiment being carried out - I&rsquo;m thinking, for example, of quantifying the movements and number of bacteria in a biological assay.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="representing-spatio-temporal-luminous-information">Representing spatio-temporal luminous information</h2>
<!-- 

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="http://1.bp.blogspot.com/-odG4Twu0Blc/UrN3ytufKnI/AAAAAAAACRM/dzJNcpV4JfY/s1600/Monty&#43;Python%27s&#43;1.gif" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
 -->


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/movie.gif" alt="" loading="lazy" data-zoomable width="25%" /></div>
  </div></figure>

<aside class="notes">
  To better understand the mechanism behind this technology, let&rsquo;s take a sample video.
Here, I&rsquo;ve taken a grayscale <em>video</em> from an episode from the Monty Python Flying Circus TV series.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="representing-spatio-temporal-luminous-information-1">Representing spatio-temporal luminous information</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/analog_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  <p>&hellip; and we will focus on a <em>single pixel</em> in the space of the visual field
In this way, we can represent the evolution of the <em>log intensity</em> of the light signal as a function of time.</p>
<p><a href="http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The&#43;Horse&#43;in&#43;Motion,&#43;1878.%C2%A0Eadweard&#43;Muybridge&#43;%28b.&#43;9&#43;April,&#43;1830%29The&#43;first&#43;movie&#43;ever&#43;made,&#43;from&#43;still&#43;photographs..gif" target="_blank" rel="noopener">http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The+Horse+in+Motion,+1878.%C2%A0Eadweard+Muybridge+(b.+9+April,+1830)The+first+movie+ever+made,+from+still+photographs..gif</a>
<a href="https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif" target="_blank" rel="noopener">https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif</a>
<a href="https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;h=600" target="_blank" rel="noopener">https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&h=600</a>
<a href="http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif" target="_blank" rel="noopener">http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif</a>
<a href="https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif%22" target="_blank" rel="noopener">https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif"</a></p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="frame-based-camera-temporal-discretization">Frame-Based Camera: Temporal discretization</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/frame-based_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  From this representation, expressed in continuous time, we can <em>discretize</em> time and measure the log intensity at regular time intervals. The difference between two images gives the <em>temporal resolution</em>, and its inverse gives the number of images per second. This is the representation classically used in chronophotography, but also in all conventional video stream <em>acquisition and viewing</em> technologies.
This technology is highly efficient for a wide range of signals. However, it does have certain <em>limitations</em>.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="frame-based-camera-aliasing">Frame-Based Camera: Aliasing</h2>


















<figure  id="figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://lenzgregor.com/posts/event-cameras/post-rethinking/frames.gif" alt="[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]" loading="lazy" data-zoomable width="85%" /></div>
  </div><figcaption>
      [<a href="https://lenzgregor.com/posts/event-cameras/" target="_blank" rel="noopener">Gregor Lenz, 2020</a>]
    </figcaption></figure>

<aside class="notes">
  Let&rsquo;s take the <em>example</em> of three colored cubes rotating in a frontal axis along a circle. Because of temporal resolution and the length of time the shutter is open, the images captured at each instant can produce a certain amount of <em>blur</em>, and movement can become increasingly difficult to estimate. If the movement of the cubes begins to accelerate, temporal <em>aliasing</em> can be observed.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="frame-based-camera-wagon-wheel-illusion">Frame-Based Camera: Wagon-Wheel Illusion</h2>


















<figure  id="figure-sam-brinson-2020httpswwwsambrinsoncomnature-of-perception">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://vignette.wikia.nocookie.net/revengeristsconsortium/images/2/25/Whee.gif/revision/latest/scale-to-width-down/340?cb=20141209071330" alt="[[Sam Brinson, 2020](https://www.sambrinson.com/nature-of-perception/)]" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      [<a href="https://www.sambrinson.com/nature-of-perception/" target="_blank" rel="noopener">Sam Brinson, 2020</a>]
    </figcaption></figure>

<aside class="notes">
  This phenomenon is particularly striking when we look at a spinning <em>wheel</em> at high speed, and this wheel&rsquo;s rotational speed is such that two successive images give the illusion that the movement is in the opposite direction to the real, physical moment. It&rsquo;s striking here in this car wheel, where you can perceive that the central hub appears motionless, and the wheel is perceived as turning in the <em>opposite direction</em> to the physical rolling motion on the road.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="event-based-camera">Event-Based Camera</h1>
<aside class="notes">
  Now let&rsquo;s introduce the <em>event camera</em>.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-1">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  This consists of a conventional sensor which, like most CMOS-type sensors, transforms visual energy into an electric current. However, there are two fundamental differences, inspired by our knowledge of the retina, which is the sensor of vision. Firstly, each pixel of this sensor is <em>independent</em> and is not cadenced according to a global clock. Secondly, each pixel will follow the evolution of the log intensity and signal an event when an increment or decrement exceeds a threshold. Let&rsquo;s explain this mechanism in relation to our analog signal.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-2">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_0.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  First of all, the signal will evolve over time, &hellip;
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-3">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_1.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  &hellip; and we can see here that it may cross a <em>threshold</em>. An event will then be produced by this pixel. Here, the <em>event</em> is of negative polarity, as it corresponds to a decrement.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-4">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_2.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>


    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-5">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_5.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  Then, the signal will continue its course in time and cross a threshold again, possibly once more, at which point a new event will be produced. Here, we&rsquo;re also seeing increments, ie positive polarizations.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-6">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_10.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>


    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-7">Event-Based Camera</h2>
<p>

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_20.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  And so on, this simple mechanism will produce a <em>stream</em> of events for each pixel, this <em>list</em> being made up of the times of occurrence and the corresponding polarities.
</aside></p>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-8">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_-1.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>


    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-9">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  Let&rsquo;s show it now applied to the whole analog signal.
It&rsquo;s worth noting in particular that, compared with frame-by-frame representations, this one is particularly <em>sparse</em>: in particular, a signal with very few changes can be represented by just a few events. This is a very useful feature, not only because it saves <em>bandwidth</em>, but also because it allows us to concentrate the <em>computations</em> around the few events that represent the image. It&rsquo;s also a fundamental feature of neuron function in the brain, and we&rsquo;ll come back to it later.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-10">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<!-- 

















<figure  id="figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://lenzgregor.com/posts/event-cameras/post-rethinking/events.gif" alt="[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      [<a href="https://lenzgregor.com/posts/event-cameras/" target="_blank" rel="noopener">Gregor Lenz, 2020</a>]
    </figcaption></figure>
 -->
<aside class="notes">
  <p>Finally, we obtain a list of events for each pixels  which can be <em>merged</em> for the image as a whole, forming a list of events, including pixel addresses, times of occurrence and polarities. As they are generated over time, they are naturally arranged in order of occurrence. All these events are then transmitted in <em>real time</em> to the output bus, typically by means of a USB3 connection. Note the analogy between this representation and the one made in the optic nerve that connects our retina to the rest of the brain: indeed, the million ganglion cells that make up the retina&rsquo;s output emit action potentials, which are the only source of information that leaves the retina via the <em>optic nerve</em>.</p>
<ul>
<li><a href="https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg" target="_blank" rel="noopener">https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg</a></li>
</ul>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-11">Event-Based Camera</h2>
<table>
<thead>
<tr>
<th>Sensor</th>
<th>Range</th>
<th>Framerate</th>
<th>Resolution</th>
<th>Power</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human eye</td>
<td>60 (?) dB</td>
<td>300 (?) fps</td>
<td>100 (?) Mpx</td>
<td>10 mW</td>
</tr>
<tr>
<td>DSLR</td>
<td>44.6 dB</td>
<td>120     fps</td>
<td>2&ndash;20   Mpx</td>
<td>30  W</td>
</tr>
<tr>
<td>Ultra-high speed</td>
<td>64   dB</td>
<td>10^4 fps</td>
<td>0.3&ndash;4  Mpx</td>
<td>300 W</td>
</tr>
<tr>
<td>Event-based</td>
<td>120  dB</td>
<td>10^6 fps</td>
<td>0.1&ndash;2  Mpx</td>
<td>30 mW</td>
</tr>
</tbody>
</table>
<aside class="notes">
  <p>There are several properties of event-driven cameras that make them remarkable. First of all, the <em>temporal precision</em> of events is of the order of microseconds, enabling a theoretical frame rate of the order of a million images per second to be reached. This can be compared with a conventional camera, which is of the order of a hundred images per second, or with a high-speed camera, which can reach 10,000 images per second. It is difficult to estimate the sampling frequency of human perception, because while 25 frames per second is often sufficient for movie viewing, it has been shown that the human eye can distinguish temporal details up to 300 or even 1,000 frames per second. It&rsquo;s worth noting that the <em>spatial resolution</em> of these event cameras is often relatively modest, in the order of megapixels, but this is not a technical limitation, but rather due to the technological applications in which these cameras are commonly used. Compared with conventional cameras, which will consume several watts, event cameras consume very little electrical <em>energy</em>, in the order of 10 milliwatts, a consumption equivalent to that of the human eye.  Another important feature of these cameras is their ability to detect a very wide <em>range</em> of luminosity, far exceeding that of conventional cameras at 120 dB (a factor of a million, compared with the human eye&rsquo;s factor of 1 in a thousand between full moon and full sun),</p>
<p><a href="https://en.wikipedia.org/wiki/Event_camera#Functional_description" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Event_camera#Functional_description</a></p>
<p>more in <a href="https://arxiv.org/pdf/1904.08405.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.08405.pdf</a></p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-12">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  This ability to <em>adapt</em> to changing light conditions can be illustrated by going back to our analog signal and its event representation, and imagining. A typical example would be an autonomous car driving in daylight, entering and leaving a <em>tunnel</em>, involving changes in brightness by a factor of several thousand.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-13">Event-Based Camera</h2>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_low.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

<aside class="notes">
  Here we have a division by a factor 8 of the signal in the middle section. It will be reported by a frame-based camera. In an event-based camera, this is represented here by a <em>sharp decrement</em> in log intensity space and clearly indicated by events of negative polarity, but we can see that since this is a camera that uses log intensity, dividing the light signal produces the <em>same signal</em> course over time, and therefore events that are identical.  Event-driven cameras are therefore particularly well-suited to <em>dynamic signals</em>, where the lighting context can change drastically.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="event-based-computer-vision">Event-Based Computer vision</h1>
<aside class="notes">
  <p>These cameras therefore look very promising for future applications, particularly for embedded applications, but also for applications linked to scientific experiments. However, we can see that the image <em>representation</em> is completely different, that is, we can no longer consider static images that follow one another at a regular rate, and for which we could have applied the algorithms that have been developed for decades in the field of <em>computer vision</em>. We end up with a signal that corresponds to events that are transmitted as a stream from the camera. And we have to reinvent all computer vision algorithms to make them <em>event-driven</em>.</p>
<p>TODO: the process is active driven by the signal compared to acquired</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-recognition-dvs-gesture">Always-on Object Recognition: DVS gesture</h2>
<!-- 

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>
 -->
<p><img src="https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif"  width="33%"/><img src="https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif"  width="33%"/><img src="https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif"  width="33%"/></p>
<!-- !"" width="33%" >}}

















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>


















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>
 -->

    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-recognition">Always-on Object Recognition</h2>


















<figure  id="figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23/hots.png" alt="[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-23/" target="_blank" rel="noopener">Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023</a>]
    </figcaption></figure>

<aside class="notes">
  The first algorithm we developed with Antoine Grimaldi, who is a PhD student, and in collaboration with Sio Ieng and Ryad Benosman of Sorbonne University, who are recognized researchers in the development of this type of camera, is an improvement on an existing algorithm, <em>HOTS</em>. This algorithm uses a relatively classical convolutional and hierarchical information processing architecture, which passes information &ldquo;forward&rdquo; from the camera and its event representation, and then through different processing layers to converge on a high-level representation that can be used for classification, in this case to recognize the identity of the digit presented as input, i.e. an eight digit. A fundamental feature of this algorithm is that it transforms the event representation into multiplexed, parallel channels, which analogously represent the temporal pattern of events, or &ldquo;<em>temporal surface</em>&rdquo;. These are represented in the different layers by the individual temporal surfaces. An interesting feature of this algorithm is that learning in each of the layers is <em>unsupervised</em>, which is a significant improvement over conventional deep learning algorithms that assume that a classification error signal can be back-propagated along the entire hierarchy, which is notoriously incorrect. Starting from this algorithm, we improved it by including neuro-biological knowledge, especially about the balance between different parallel communication pathways by including <em>homeostasis</em> rules.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-gesture-recognition">Always-on Object Gesture Recognition</h2>


















<figure  id="figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23/gesture_offline.png" alt="[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]" loading="lazy" data-zoomable width="90%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-23/" target="_blank" rel="noopener">Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023</a>]
    </figcaption></figure>

<aside class="notes">
  <p>To illustrate the results of our algorithm, we applied a classic camera dataset involving the classification of 10 different types of human <em>gestures</em>. These biological movements are, for example, clapping hands, saying hello or a drum movement. The chance level is therefore at 10%, and we have observed that when all events have been processed, the <em>original</em> algorithm achieves a performance of around 70%. By adding <em>homeostasis</em>, we have reached a higher level of 82%, demonstrating the usefulness of using neuroscientific knowledge to improve machine learning algorithms.</p>
<p>We also built on a fundamental characteristic of biological systems. In fact, this kind of algorithm is classically used to process the flow of events, but classification is only used as a last resort when all the events have been processed. We have modified the algorithm so that this classification can be done <em>online</em>, in real time, event by event. In this way, processing in the various layers is triggered by the arrival of each event, which is propagated from the camera through all the layers to the classification layer.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-gesture-recognition-1">Always-on Object Gesture Recognition</h2>


















<figure  id="figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23/gesture_online.png" alt="[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]" loading="lazy" data-zoomable width="90%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-23/" target="_blank" rel="noopener">Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023</a>]
    </figcaption></figure>

<aside class="notes">
  What&rsquo;s more interesting is that we were also able to show the <em>evolution</em> of the average performance obtained on a data set, and as a function of the number of events processed by the algorithm. The blue curve shows that if below 10 events, we remain at the level of chance, we then experience a gradual increase in performance that reaches the level of the original algorithm with ten thousand events, and exceeds this <em>performance</em> when we have even 10 times more. A major advantage of this algorithm is that it can be asked to classify the nature of what it sees in its event camera, not once the entire signal has been processed by the system, but at any time. This characteristic is essential in biology. For example, imagine you&rsquo;re on the savannah and a <em>lion</em> jumps out at you. You won&rsquo;t have the flexibility to wait for the video sequence to finish processing before making the right decision, which is to flee. Another variant in our algorithm consists of selecting the output classification events based on a calculation of the precision for each event. By using a <em>threshold</em> on this precision, we can achieve a very good level of performance, with just a hundred events, and so achieve a characteristic that is common in biological networks, i.e. that a decision is not taken gradually, but emerges abruptly (here after 200 events) and then improves and stabilizes.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="spiking-neural-networks">Spiking Neural Networks</h1>
<aside class="notes">
  We have therefore illustrated the use of <em>event-driven</em> cameras on a particular algorithm. This algorithm has the particularity of processing the flow of events coming from the camera event by event, so that potentially each of these events triggers a cascade of mechanisms in the different processing layers, and thus enables a classification value to be updated at any given moment. This type of operation is characteristic of the way neurons work in the brain, i.e. using an event-based representation of information processing. This is what we call <em>spiking neural networks</em>.
</aside>

    </section>
    

    
    
    
    <section>
    
      


















<figure  id="figure-tonic-manualhttpstonicreadthedocsioenlatest_imagesneuron-modelspng">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://tonic.readthedocs.io/en/latest/_images/neuron-models.png" alt="[[Tonic manual](https://tonic.readthedocs.io/en/latest/_images/neuron-models.png)]" loading="lazy" data-zoomable width="70%" /></div>
  </div><figcaption>
      [<a href="https://tonic.readthedocs.io/en/latest/_images/neuron-models.png" target="_blank" rel="noopener">Tonic manual</a>]
    </figcaption></figure>

<aside class="notes">
  Indeed, most neural networks used in deep learning use an analog representation. This is illustrated in this figure, which represents the various analog inputs to a formal neuron as they are linearly integrated by the synapses, then transformed by a non-linear function to generate an activation which is itself analog. This basic <em>perceptron</em> principle is at the foundation of all existing neural networks, and in particular enables the construction of convolutional-type networks which are currently the champions for image classification, having outperformed human performance for several years. However, while this is true for static images, it can become prohibitively expensive with videos. This is why it can be interesting to use <em>spiking</em> neurons instead, which, instead of receiving an analog input, will receive events that will trigger cascades of mechanisms in the neuronal cell, notably represented by the cell&rsquo;s membrane potential. Typically, we&rsquo;ll include a threshold for triggering action potential in this cell, which will generate new output events on the cell&rsquo;s axon.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-lif-neuron">Spiking Neural Networks: LIF Neuron</h2>


















<figure  id="figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif" alt="[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      [Grimaldi <em>et al</em>, 2023, <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/" target="_blank" rel="noopener">Precise Spiking Motifs</a>]
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>This is illustrated in this <em>animation</em>, which shows how we can transform a list of input events by giving them different weights, and then <em>integrate</em> them into the cell&rsquo;s soma to generate output events.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-neuromorphic-hardware">Spiking Neural Networks: neuromorphic hardware</h2>


















<figure  id="figure-loihi-2">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg" alt="Loihi 2" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      Loihi 2
    </figcaption></figure>

<aside class="notes">
  <p>This new type of representation represents a <em>paradigm shift</em> in computation, in the same way that event-driven cameras have brought with them a paradigm shift in image representation. The development of these two new algorithms, which use impulse neural networks, is accompanied by the development of new neuromorphic chips, such as the Loihi 2 chip developed by Intel, which replaces a central computing unit with a massively parallelized <em>array</em> of elementary event-driven computing units. As with event-driven cameras, this has the dual advantage of being very fast and consuming very little energy. Other types of <em>neuromorphic chips</em> are currently being developed and may soon be used instead of conventional CPUs or GPUs.</p>
<figure  id="figure-propheseehttpsdocspropheseeaistableconceptshtml">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg" alt="[Prophesee](https://docs.prophesee.ai/stable/concepts.html)" loading="lazy" data-zoomable width="45%" /></div>
  </div><figcaption>
      <a href="https://docs.prophesee.ai/stable/concepts.html" target="_blank" rel="noopener">Prophesee</a>
    </figcaption></figure>
<p>Loihi: <a href="https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg" target="_blank" rel="noopener">https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg</a></p>
<p><a href="https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;strip=none&amp;ssl=1" target="_blank" rel="noopener">https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&strip=none&ssl=1</a></p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-in-neurobiology">Spiking Neural Networks in neurobiology</h2>


















<figure  id="figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="http://i.stack.imgur.com/ixnrz.png" alt="[[Mainen &amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb" target="_blank" rel="noopener">Mainen &amp; Sejnowski, 1995</a>]
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>Spiking neural networks therefore seem very promising for processing the output of event-driven cameras, but the study of <em>neurophysiology</em> shows us that their operation can sometimes seem incongruous and far from the perceptron. In this first example, taken from an article by Mainen and Sejnowski from 1995, we see the response of the same neuron to several <em>repetitions</em> of a stimulation in panel A. At the top, we see the membrane potential of this neuron in response to a 200 Pico ampere <em>current step</em>, which shows that the membrane potential is not reproducible across different trials. This is illustrated by showing the spike response over time for the different trials, which shows a strong alignment at the start of stimulation, but that this diffuses little by little, so that after around 750 milliseconds there is no longer any coherence between the different trials. The situation is different in panel B, where the neuron is stimulated with <em>noise</em>. In this case, the responses are so precise for the different trials that the membrane potential traces are overlapping almost exactly. The subtlety of this paper lies in its use of a <em>frozen</em> noise, i.e. one that is repeated unchanged across trials. In this way, it demonstrates that neurons are not so much sensitive to analog values presented in the form of square pulses, but rather to dynamic signals for which they will respond with very high precision in the dynamic domain.</p>

</aside>
<!-- 
---


## Spiking Neural Networks in neurobiology



















<figure  id="figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png" alt="[[Mainen &amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb" target="_blank" rel="noopener">Mainen &amp; Sejnowski, 1995</a>]
    </figcaption></figure>



<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<ul>
<li>reproduucibility</li>
</ul>
</aside> -->

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-in-neurobiology-1">Spiking Neural Networks in neurobiology</h2>


















<figure  id="figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png" alt="[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py" target="_blank" rel="noopener">Diesmann et al. 1999</a>]
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>In this other example, I show a simulation that reproduces the 1999 paper by Diesmann and colleagues. This <em>theoretical model</em> considers ten groups of 100 neurons that are connected from group to group. An interesting property of this system is to show that for the same stimulation, i.e. for the same number of spikes, information can propagate from group to group only if it is sufficiently <em>concentrated in time</em>. For the first two groups, the information is too dispersed in the first group and spreads progressively and increasingly in subsequent groups. Above a certain threshold, the information formed by a group of relatively synchronous spikes is correctly transmitted to the various groups in the network. This <em>non-linear</em> behavior is one of the characteristics of spiking networks, giving them a certain richness, but also a certain complexity.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-in-neurobiology-2">Spiking Neural Networks in neurobiology</h2>


















<figure  id="figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg" alt="[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/" target="_blank" rel="noopener">Haimerl et al, 2019</a>]
    </figcaption></figure>

<aside class="notes">
  <strong>2 MINUTE</strong>
A third example shows an experiment conducted by Rosa Cossart&rsquo;s group at INMED and recently published by Haimerl and colleagues. It shows the results of <em>calcium fluorescence</em> imaging recordings in mice. By arranging the different neurons in <em>temporal order of activation</em>, it shows a sequential activation of these neurons, a mechanism which resembles the model mentioned earlier. These activation groups are strongly correlated with the <em>motor behavior</em> of the mouse, as described in the graph at the top. Of particular interest is the fact that these sequences of activity are stable over time and can be recorded on a <em>subsequent day</em>. This illustrates the importance of dynamics in the integration of neural computations.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="spiking-neural-networks-spiking-motifs">Spiking Neural Networks: Spiking motifs</h1>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>These observations have led us to <em>review</em> neurobiological evidence around the existence of a neural representation that would use the relative time of spikes as a means of representing information. In particular, it is possible to use the conduction <em>delays</em> that exist in the transmission of spikes from one neuron to another. It may seem paradoxical, but these delays are not simply a constraint, but can help to improve our ability to represent information by way of <em>spiking motifs</em>.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-spiking-motifs-1">Spiking Neural Networks: Spiking motifs</h2>


















<figure  id="figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png" alt="[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      [Grimaldi <em>et al</em>, 2023, <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/" target="_blank" rel="noopener">Precise Spiking Motifs</a>]
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>If we consider, for example, this ultra-simplified network consisting of three presynaptic neurons and two output neurons connected by <em>heterogeneous</em> delays, then we can see that a <em>synchronous</em> input will generate membrane activity in the two output neurons at different times, so the threshold will never be reached, and these neurons will not produce an output impulse. On the other hand, if these delays are such that the action potentials converge on the neuron at the same instant, then these contributions will be able to sum up at the <em>same instant</em> and produce an output spike, as denoted here by the red bar.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-spiking-motifs-2">Spiking Neural Networks: Spiking motifs</h2>


















<figure  id="figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif" alt="Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)." loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      Review on <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/" target="_blank" rel="noopener">Precise Spiking Motifs</a>.
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>To better understand this mechanism, let&rsquo;s return to our animation of a spiking neuron. Action potentials arrive at the neuron and are <em>immediately</em> transmitted to the neuron&rsquo;s cell body to be integrated and potentially generate a spike.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-spiking-motifs-3">Spiking Neural Networks: Spiking motifs</h2>


















<figure  id="figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif" alt="Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)." loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      Review on <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/" target="_blank" rel="noopener">Precise Spiking Motifs</a>.
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>When using <em>heterogeneous</em> delays, the situation is different, as the information will take a differential time to arrive or not at the neuron&rsquo;s cell body. Note that if we include a particular <em>spiking motif</em>, which we have here highlighted by green action potentials, then these converge at the same instant thanks to the delay. We will therefore have a detection in the neuron in the form of a new impulse.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn">Spiking Neural Networks: HD-SNN</h2>











  





<video autoplay loop  >
  <source src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4" type="video/mp4">
</video>

<aside class="notes">
  <strong>2 MINUTE</strong>
We used this theoretical principle in an algorithm for detecting movement in an image. To do this, we first generated event data using natural images that are set in motion along trajectories that resemble those produced by free exploration of the visual scene. You&rsquo;ll notice several features of the event-driven output, such as the fact that faster motion generates more spikes, or that edges oriented parallel to one direction produce few changes, and therefore little spike output - the so-called aperture problem.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-1">Spiking Neural Networks: HD-SNN</h2>


















<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/laurentperrinet/figures/7f382a8074552de1a6a0c5728c60d48788b5a9f8/animated_neurons/conv_HDSNN.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/" target="_blank" rel="noopener">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>We then used a neural network with a classical architecture, which we enhanced by using an impulse representation that takes into account different possible synaptic delays. In this figure, we have represented the input in the left grid, which represents the occurrence of spikes of positive or negative polarity. Then we have represented different processing channels denoted by the colors green and orange, which are applied to this input to produce membrane activity. As illustrated above, this activity will produce output pulses, notably in synaptic connection nuclei, with heterogeneous delays corresponding to the detection of precise spatio-temporal patterns.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-2">Spiking Neural Networks: HD-SNN</h2>


















<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="90%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/" target="_blank" rel="noopener">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>One advantage of this network is that it is differentiable, enabling us to apply classical machine learning methods, notably supervised learning. We then see the emergence of different convolution kernels, and here I represent a subset of its kernels for different directions, as denoted by the red arrows on the left of the graph. It shows the kernels obtained on the spatial representation according to the different columns, and each row represents the different delays from a delay of one on the right to a delay of 12 time steps on the left. Detectors that follow the motion emerge. For example, for the top line from top to bottom. These kernels integrate both positive neurons in red and negative polarity inputs in blue.
Such spatio-temporal filtering is observed in neurobiology, but to my knowledge had never been observed in a model of spiking neurons trained under natural conditions.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-3">Spiking Neural Networks: HD-SNN</h2>


















<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_raw.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/" target="_blank" rel="noopener">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>We will now study the performance of this network in detecting motion in the flow of events entering the network. When we use all the weights of the convolution kernel, we get a very good performance of the order of 99%, represented by the black dot in the top right-hand corner. Note that in the kernels we&rsquo;ve seen emerge, most of the synaptic weights are close to zero, so we might consider removing some of these weights, as this can be shown to reduce the number of event calculations required.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-4">Spiking Neural Networks: HD-SNN</h2>


















<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_shortening.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/" target="_blank" rel="noopener">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>

<aside class="notes">
  <strong>2 MINUTE</strong>
This is what we&rsquo;ve done, by first removing the parts of the core corresponding to the longest delays. This &ldquo;shortens&rdquo; the kernel. We quickly observed a degradation in performance, which reached half-saturation when we reduced the number of weights by around 50%. This demonstrates the importance of integrating information that is quite distant and structured over time.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-5">Spiking Neural Networks: HD-SNN</h2>


















<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/" target="_blank" rel="noopener">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>

<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>In a second step, we performed a pruning operation, which consists in progressively removing the weights that are the weakest. This time, performance remains optimal over a wide compression range, and we reach half-saturation when we have removed around 99.8% of the weights. This means that the network is able to maintain very good performance, even when only one weight out of 600 has been kept, and therefore, with a computation time increased by a factor of 600. This property, which we didn&rsquo;t expect, seems promising for creating machine learning algorithms that are less energy-hungry.</p>

</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="neuromorphic-models-of-visionhttpslaurentperrinetgithubioslides2024-02-05-udemtransitionfade-1"><a href="https://laurentperrinet.github.io/slides/2024-02-05-udem/?transition=fade" target="_blank" rel="noopener">Neuromorphic models of vision</a></h1>
<h4 id="laurent-perrinethttpslaurentperrinetgithubio-1"><em><a href="https://laurentperrinet.github.io" target="_blank" rel="noopener">Laurent Perrinet</a></em></h4>
<h4 id="u2024-02-05httpslaurentperrinetgithubiotalk2023-12-01-biocomp-seminar-at-udems-school-of-optometry-montréalhttpsoptoumontrealcaecoleenglishu-1"><u><a href="https://laurentperrinet.github.io/talk/2023-12-01-biocomp" target="_blank" rel="noopener">[2024-02-05]</a> <a href="https://opto.umontreal.ca/ecole/english/" target="_blank" rel="noopener">Seminar at UdeM’s School of Optometry, Montréal</a></u></h4>
<img src="https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg" alt="logos" height="130"/>
<p><a href="mailto:laurent.perrinet@univ-amu.fr">laurent.perrinet@univ-amu.fr</a></p>
<aside class="notes">
  In conclusion, we have seen that event-driven cameras open the door to new applications that mimic the performance of the human eye, in terms of computational dynamics, adaptation to light conditions and energy constraints. This technological development has recently been accompanied by the development of neuromorphic chips and innovative algorithms in the form of spiking neural networks. However, there is still a great deal of progress to be made at theoretical level, particularly in the understanding of these spiking neural networks, and we have shown the potential progress that can be made by exploiting the richness of temporal representations, particularly by taking advantage of heterogeneous delays.
Beyond these particular applications to natural image processing, I hope to have succeeded in demonstrating the importance of cross-fertilizing the field of engineering applications in general with biological neuroscience. This new line of research - known as NeuroAI or, more generally, as computational neuroscience - is likely to develop over the next few years. Thank you for your attention.
</aside>
</section>

    </section>
    

    
    
  </div>
</div>



  
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/markdown/markdown.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/notes/notes.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/search/search.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/math/math.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/plugin/zoom/zoom.min.js" crossorigin="anonymous"></script>

  
  
    <script src="https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/plugin.js" integrity="sha256-M6JwAjnRAWmi+sbXURR/yAhWZKYhAw7YXnnLvIxrdGs=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.js" integrity="sha256-l14dklFcW5mWar6w/9KaW0fWVerf3mYr7Wt0+rXzFAA=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.css" integrity="sha256-0fU8HKLaTjgzfaV9CgSqbsN8ilA3zo6zK1M6rlgULd8=" crossorigin="anonymous">
  

  
  

  
  
  <script src="/js/wowchemy-slides.js"></script>

</body>
</html>
