<!DOCTYPE html>
<html lang="en-us">
<head>

  
  
  

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Wowchemy 5.5.0 for Hugo">

  

  <link rel="icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://laurentperrinet.github.io/slides/2023-12-14-jraf/">

  <title> | Novel visual computations</title>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/dist/reveal.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/dist/theme/white.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" id="highlight-theme">

  
  
  <link rel="stylesheet" href="/css/reveal_custom.min.css">
</head>
<body>

  
<div class="reveal">
  <div class="slides">
    
    
    

    
    
    
    
    

    
    

    
    
    
    <section>
    
      <section>
<h1 id="event-based-visionhttpslaurentperrinetgithubioslides2023-12-14-jraftransitionfade"><a href="https://laurentperrinet.github.io/slides/2023-12-14-jraf/?transition=fade" target="_blank" rel="noopener">Event-based vision</a></h1>
<h4 id="adrien-fois--laurent-perrinethttpslaurentperrinetgithubio"><em><a href="https://laurentperrinet.github.io" target="_blank" rel="noopener">Adrien Fois &amp; Laurent Perrinet</a></em></h4>
<h4 id="u2023-12-14httpslaurentperrinetgithubiotalk2023-12-14-jraf-journées-sur-lapprentissage-frugal-jraf-httpsjraf-2023sciencesconforgu"><u><a href="https://laurentperrinet.github.io/talk/2023-12-14-jraf" target="_blank" rel="noopener">[2023-12-14]</a> <a href="https://jraf-2023.sciencesconf.org/" target="_blank" rel="noopener">Journées sur l&rsquo;apprentissage frugal (JRAF) </a></u></h4>
<img src="https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg" alt="logos" height="130"/>
<p><a href="mailto:adrien.fois@univ-amu.fr">adrien.fois@univ-amu.fr</a>
<a href="mailto:laurent.perrinet@univ-amu.fr">laurent.perrinet@univ-amu.fr</a></p>
<aside class="notes">
  <p><em>Hello</em>, can you hear me in the back?</p>
<p>I&rsquo;m Adrien Fois from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit. I&rsquo;m a post-doc under the supervision of Laurent Perrinet, and during this seminar, I&rsquo;ll be presenting <em>event-driven cameras</em>, a new technology in the field of imaging, and the impact of this technology on our understanding of vision. I&rsquo;d like to <em>thank</em> organizers for this opportunity, and all of you for coming. These slides are available from the website of Laurent Perrinet, along with a number of references. The <em>outline</em> of the talk is as follows: first, we&rsquo;ll describe what an event-driven camera is - in particular, by comparing it to a conventional frame-based camera; then, we&rsquo;ll show some examples of applications of these cameras with dedicated algorithms; and finally, we&rsquo;ll present how our knowledge of biological mechanisms in neuroscience can enable us to improve these algorithms.</p>

</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="sensing-light">Sensing light</h1>
<aside class="notes">
  First of all, the objective of <em>imaging</em> is to represent a visual signal, which includes luminous intensity and color, distributed over the visual field to create a realistic representation of a visual scene.
</aside>

    </section>
    

    
    
    
      

<section data-noprocess data-shortcode-slide
  
      
      data-background-image="http://lepassetempsderose.l.e.pic.centerblog.net/fddea7fb.gif"
  >

<aside class="notes">
  Imaging gives us the feeling that we’re seeing a scene right in front of us. For example, this galloping horse seems to move, but it’s not really moving. It’s an <em>illusion</em> called apparent motion. This happens when still images are shown one after another, very quickly, making it look like the scene is moving. Our brains interpret these separate images as a single, moving scene. This technique is the foundation of motion pictures and animation, where frames are displayed quickly enough to give the <em>illusion</em> of fluid motion.
</aside>

    </section>
    

    
    
    
      

<section data-noprocess data-shortcode-slide
  
      
      data-background-image="https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif"
  >

<aside class="notes">
  Imaging techniques have also opened doors to new scientific discoveries. For example, back in the late 19th century, scientists wondered if horses lifted all four feet off the ground when they galloped. It was too fast for our eyes to see. Eadweard Muybridge solved this puzzle using <em>chronophotography</em>, an early form of photography that captures movement. He took a series of photos of a running horse and showed that, yes, there are moments when all four hooves are in the air. This breakthrough helped us understand animal movement better and paved the way for modern cameras.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="representing-spatio-temporal-luminous-information">Representing spatio-temporal luminous information</h2>
<!-- 













<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="http://1.bp.blogspot.com/-odG4Twu0Blc/UrN3ytufKnI/AAAAAAAACRM/dzJNcpV4JfY/s1600/Monty&#43;Python%27s&#43;1.gif" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure> -->














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/movie.gif" alt="" loading="lazy" data-zoomable width="66%" /></div>
  </div></figure>
<aside class="notes">
  To better understand the mechanism behind this technology, let&rsquo;s take a sample video.
Here, I&rsquo;ve taken a grayscale <em>video</em> from an episode from the Monty Python Flying Circus TV series.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="representing-spatio-temporal-luminous-information-1">Representing spatio-temporal luminous information</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/analog_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  <p>&hellip; and we will focus on a <em>single pixel</em> in the space of the visual field
In this way, we can represent the evolution of the <em>log intensity</em> of the light signal as a function of time.</p>
<p><a href="http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The+Horse+in+Motion,+1878.%C2%A0Eadweard+Muybridge+(b.+9+April,+1830)The+first+movie+ever+made,+from+still+photographs..gif">http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The+Horse+in+Motion,+1878.%C2%A0Eadweard+Muybridge+(b.+9+April,+1830)The+first+movie+ever+made,+from+still+photographs..gif</a>
<a href="https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif">https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif</a>
<a href="https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;h=600">https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;h=600</a>
<a href="http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif">http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif</a>
<a href="https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif%22">https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif&quot;</a></p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="frame-based-camera-temporal-discretization">Frame-Based Camera: Temporal discretization</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/frame-based_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  From this representation, expressed in continuous time, we can <em>discretize</em> time and measure the log intensity at regular time intervals. The difference between two images gives the <em>temporal resolution</em>, and its inverse gives the number of images per second. This is the representation classically used in chronophotography, but also in all conventional video stream <em>acquisition and viewing</em> technologies.
This technology is highly efficient for a wide range of signals. However, it does have certain <em>limitations</em>.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="frame-based-camera-aliasing">Frame-Based Camera: Aliasing</h2>














<figure  id="figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://lenzgregor.com/posts/event-cameras/post-rethinking/frames.gif" alt="[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]" loading="lazy" data-zoomable width="85%" /></div>
  </div><figcaption>
      [<a href="https://lenzgregor.com/posts/event-cameras/">Gregor Lenz, 2020</a>]
    </figcaption></figure>
<aside class="notes">
  To illustrate a common limitation, let&rsquo;s take the <em>example</em> of three colored cubes rotating around a circle on a frontal axis. Due to the camera’s temporal resolution and the duration the shutter remains open, the captured images exhibit blur. This makes it challenging to precisely measure the cubes’ movement. As the cubes’ rotation speed increases, we might notice an effect called temporal <em>aliasing</em>, where the movement appears distorted due to the camera’s limitations.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="frame-based-camera-wagon-wheel-illusion">Frame-Based Camera: Wagon-Wheel Illusion</h2>














<figure  id="figure-sam-brinson-2020httpswwwsambrinsoncomnature-of-perception">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://vignette.wikia.nocookie.net/revengeristsconsortium/images/2/25/Whee.gif/revision/latest/scale-to-width-down/340?cb=20141209071330" alt="[[Sam Brinson, 2020](https://www.sambrinson.com/nature-of-perception/)]" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      [<a href="https://www.sambrinson.com/nature-of-perception/">Sam Brinson, 2020</a>]
    </figcaption></figure>
<aside class="notes">
  This phenomenon is particularly striking when we look at a spinning wheel moving at high speed. Sometimes, the wheel spins so fast that in two consecutive images, it appears to rotate backwards. This optical illusion is known as the wagon-wheel illusion. It’s particularly noticeable in car wheels, where the central hub may seem stationary while the wheel itself seems to turn <em>counter</em> to its actual direction on the road. Again this wagon-wheel effect is due to standard camera&rsquo;s limitations.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="event-based-camera">Event-Based Camera</h1>
<aside class="notes">
  Transitioning from conventional frame-based cameras, we now focus on the <em>event camera</em>, a highly promising bio-inspired visual sensor.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-1">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  <p>An event-based camera is equipped with a sensor that, much like common CMOS sensors, converts light into electrical current. Yet, it stands apart from standard frame-based cameras by taking inspiration from the human retina. There are two main differences with respect to frame-based camera:
Firstly each pixel of an event-based camera is <em>independent</em>, functioning without a synchonized global clock.
Secondly, each pixel detect shifts in <em>logarithmic light intensity</em>, generating an binary event only when the change exceeds a <em>threshold</em>. If the change is an increment - meaning the log intensity increased - the event has positive polarity; if it&rsquo;s a decrement, the event has negative polarity.</p>
<p>In summary, an event is asynchronously generated when a pixel-level change in brightness is detected. This leads to a superior temporal resolution and a reduced susceptibility to motion blur, making event-camera ideal for capturing fast-moving scenes. Now, let’s explain how discrete events are produced in response to an analog signal that evolves continuously over time.</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-2">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_0.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  Our signal is analog. It consists of the evolution of the log-intensity (y axis) of a single pixel through time (x axis).
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-3">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_1.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  &hellip; And we can observe that it crosses a threshold. At this precise time, the pixel generates an event. In this case, the event is of positive polarity, as it corresponds to an increase.
TODO : C&rsquo;est pas plutôt un évènement de polarité positive ? (j&rsquo;ai déjà changé la description)
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-4">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_2.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  Then, the signal continue its course in time and cross a threshold again, resulting in the production of a new event with positive polarity.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-5">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_5.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  The log-intensity continues to increase, leading to increments, or in other words, positive polarizations.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-6">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_10.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  Now the signal decreases, resulting in events with negative polarity instead of positive polarity.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-7">Event-Based Camera</h2>
<p>













<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_20.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  Continuing this process, the simple mechanism generates a <em>stream</em> of events for each pixel, comprising a <em>list</em> of occurrence times and their respective polarities.
</aside></p>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-8">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_-1.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-9">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  Let&rsquo;s show it now applied to the whole analog signal.
It&rsquo;s worth noting in particular that, compared with frame-by-frame representations, this one is particularly <em>sparse</em>: in particular, a signal with very few changes can be represented by just a few binary events. This is a very useful feature, not only because it saves <em>bandwidth</em>, but also because it allows us to concentrate the <em>computations</em> around the few events that represent the image. It&rsquo;s also a fundamental feature of neuron function in the brain, and we&rsquo;ll come back to it later.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-10">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<!-- 













<figure  id="figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://lenzgregor.com/posts/event-cameras/post-rethinking/events.gif" alt="[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      [<a href="https://lenzgregor.com/posts/event-cameras/">Gregor Lenz, 2020</a>]
    </figcaption></figure> -->
<aside class="notes">
  <p>Ultimately, we obtain a list of events for each pixels which can be <em>merged</em> to represent the entire image. This list of events includes pixel addresses, times of occurrence and polarities. As events are generated over time, they are naturally sorted by their time of occurences. These events are then transmitted in <em>real-time</em> to the output bus, often through a USB3 connection.
It’s interesting to draw a parallel between this process and the optic nerve, which connects our retina to the brain. In fact, the retina’s output is composed of a million ganglion cells that emit action potentials, constituting the only source of information transmitted through the <em>optic nerve</em>.</p>
<ul>
<li><a href="https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg">https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg</a></li>
</ul>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-11">Event-Based Camera</h2>
<table>
<thead>
<tr>
<th>Sensor</th>
<th>Range</th>
<th>Framerate</th>
<th>Resolution</th>
<th>Power</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human eye</td>
<td>60 (?) dB</td>
<td>300 (?) fps</td>
<td>100 (?) Mpx</td>
<td>10 mW</td>
</tr>
<tr>
<td>DSLR</td>
<td>44.6 dB</td>
<td>120     fps</td>
<td>2&ndash;20   Mpx</td>
<td>30  W</td>
</tr>
<tr>
<td>Ultra-high speed</td>
<td>64   dB</td>
<td>10^4 fps</td>
<td>0.3&ndash;4  Mpx</td>
<td>300 W</td>
</tr>
<tr>
<td>Event-based</td>
<td>120  dB</td>
<td>10^6 fps</td>
<td>0.1&ndash;2  Mpx</td>
<td>30 mW</td>
</tr>
</tbody>
</table>
<aside class="notes">
  <p>Event-driven cameras boast several remarkable properties.
Firstly, their <em>temporal precision</em> is in the microsecond range, allowing for a theoretical frame rate of up to a million images per second. In contrast, a conventional camera typically captures around a hundred images per second, while a high-speed camera may reach 10,000 images per second. Estimating the sampling frequency of human perception is challenging; although 25 frames per second usually suffice for movies, the human eye can discern temporal details at rates between 300 and 1,000 frames per second.
It’s also noteworthy that the <em>spatial resolution</em> of event cameras is generally modest, often in the megapixel range. This is not due to technical constraints but rather reflects the cameras’ common technological applications.
Compared with conventional cameras, which will consume several watts, event cameras consume very little electrical <em>energy</em>, in the order of 10 milliwatts, a consumption equivalent to that of the human eye.
Another key feature is their ability to detect a very wide <em>range</em> of luminosity, reaching 120 dB, which is a million times greater than conventional cameras and thousand times greater than an human eye.</p>
<p><a href="https://en.wikipedia.org/wiki/Event_camera#Functional_description">https://en.wikipedia.org/wiki/Event_camera#Functional_description</a></p>
<p>more in <a href="https://arxiv.org/pdf/1904.08405.pdf">https://arxiv.org/pdf/1904.08405.pdf</a></p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-12">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  But why is detecting a very wide range of luminosity usefull ? The ability to <em>adapt</em> to changing light conditions can be illustrated by revisiting our analog signal and its event representation. Consider, for example, an autonomous car driving in daylight and then entering and exiting a <em>tunnel</em>. This scenario involves changes in brightness by a factor of several thousand.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="event-based-camera-13">Event-Based Camera</h2>














<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_low.svg" alt="" loading="lazy" data-zoomable width="100%" /></div>
  </div></figure>
<aside class="notes">
  Here we have a division by a factor 8 of the signal in the middle section. It will be reported by a frame-based camera. In an event-based camera, this is represented here by a <em>sharp decrement</em> in log intensity space and clearly indicated by events of negative polarity, but we can see that since this is a camera that uses log intensity, dividing the light signal produces the <em>same signal</em> course over time, and therefore events that are identical.  Event-driven cameras are therefore particularly well-suited to <em>dynamic signals</em>, where the lighting context can change drastically.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="event-based-computer-vision">Event-Based Computer vision</h1>
<aside class="notes">
  <p>These cameras therefore look very promising for future applications, particularly for embedded applications, but also for applications linked to scientific experiments. However, we can see that the image <em>representation</em> is completely different, that is, we can no longer consider static images that follow one another at a regular rate, and for which we could have applied the algorithms that have been developed for decades in the field of <em>computer vision</em>. We end up with a signal that corresponds to events that are transmitted as a stream from the camera. And we have to reinvent all computer vision algorithms to make them <em>event-driven</em>.</p>
<p>TODO: the process is active driven by the signal compared to acquired</p>

</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-recognition-dvs-gesture">Always-on Object Recognition: DVS gesture</h2>
<!-- 













<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>













<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>













<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure> -->
<p><img src="https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif"  width="33%"/><img src="https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif"  width="33%"/><img src="https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif"  width="33%"/></p>
<!-- !"" width="33%" >}}













<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure>













<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif" alt="" loading="lazy" data-zoomable width="33%" /></div>
  </div></figure> -->
<aside class="notes">
  <p>We considered a classification task using a classic camera dataset, involving the classification of 10 different types of human gestures. These biological movements are, for example, clapping hands or playing air guitar.</p>
<aside class="notes">
  
    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-recognition">Always-on Object Recognition</h2>
<figure  id="figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23/hots.png" alt="[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-23/">Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023</a>]
    </figcaption></figure>
<aside class="notes">
  So how can we process and learn data coming from an event-based camera ?
Our team, including PhD student Antoine Grimaldi, has enhanced an existing algorithm known as <em>HOTS</em>. This algorithm employs a traditional convolutional and hierarchical structure to process information. It begins with the camera’s event data and progresses through various stacked layers, the last layer giving a high-level representation suitable for tasks like digit recognition—for example, identifying the number eight. A key aspect of HOTS is its conversion of event data into multiplexed, parallel channels that mirror the temporal sequence of events, termed the <em>temporal surface</em>. Each layer represents these temporal surfaces individually. Notably, the algorithm’s learning process is <em>unsupervised</em> at every layer, marking a significant advancement over typical deep learning methods that rely on back-propagating classification errors—which is biologically implausible. Building on HOTS, we’ve improved it by incorporated neurobiological insights, particularly the principle of <em>homeostasis</em>, to better balance the various parallel communication pathways.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-gesture-recognition">Always-on Object Gesture Recognition</h2>
<figure  id="figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23/gesture_offline.png" alt="[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]" loading="lazy" data-zoomable width="90%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-23/">Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023</a>]
    </figcaption></figure>
<aside class="notes">
  <p>To demonstrate our algorithm’s effectiveness, we tested it on a standard dataset that I presented before for classifying <em>10 distinct human gestures</em>, such as clapping, waving, or drumming. With random guessing at 10%, the original HOTS algorithm achieved 70% accuracy after processing all events. However, by adding <em>homeostasis</em>—a concept from neuroscience—we enhanced the algorithm’s performance to 82%. This underscores the value of incorporating neuroscientific principles into machine learning.</p>
<p>Furthermore, we leveraged a key trait of biological systems: the ability to process information continuously, in real time. Traditional algorithms wait to classify until all events are processed. We innovated by enabling our algorithm to classify on-the-fly, in real-time, with each incoming event. This means that as events occur, they’re instantly processed through the layers, reaching the classification layer without delay.</p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="always-on-object-gesture-recognition-1">Always-on Object Gesture Recognition</h2>
<figure  id="figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23/gesture_online.png" alt="[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]" loading="lazy" data-zoomable width="90%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-23/">Grimaldi, Boutin, Sio-Ieng, Benosman &amp; LP, 2023</a>]
    </figcaption></figure>
<aside class="notes">
  What&rsquo;s more interesting is that we were also able to show the <em>evolution</em> of our algorithm’s average performance relative to the dataset and the number of processed events. The blue curve reveals that with fewer than 10 events, performance hovers at chance levels. However, as more events are processed, we observe a steady improvement. Remarkably, with 10,000 events, <em>performance</em> matches that of the original algorithm and further excels with an additional tenfold increase in events. A major advantage of this algorithm is that it can be asked to classify the nature of what it sees in real-time, at any point during the event stream —not just after the entire signal is processed. Online processing is essential in biology. For example, imagine you&rsquo;re on the savannah and a <em>lion</em> jumps out at you. You won&rsquo;t have the time to wait for the video sequence to finish processing before making the right decision, which is to flee.
We’ve also refined our algorithm to select classification events based on precision calculations for each event. By adding a precision <em>threshold</em>, we achieve high performance with merely a hundred events. This reflects a biological network trait where decisions aren’t made incrementally but rather emerge abruptly here after 200 events — and then continue to improve and stabilize.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="event-based-visionhttpslaurentperrinetgithubioslides2023-12-14-jraftransitionfade"><a href="https://laurentperrinet.github.io/slides/2023-12-14-jraf/?transition=fade">Event-based vision</a></h1>
<h4 id="adrien-fois--laurent-perrinethttpslaurentperrinetgithubio"><em><a href="https://laurentperrinet.github.io">Adrien Fois &amp; Laurent Perrinet</a></em></h4>
<h4 id="u2023-12-14httpslaurentperrinetgithubiotalk2023-12-14-jraf-journées-sur-lapprentissage-frugal-jraf-httpsjraf-2023sciencesconforgu"><u><a href="https://laurentperrinet.github.io/talk/2023-12-14-jraf">[2023-12-14]</a> <a href="https://jraf-2023.sciencesconf.org/">Journées sur l&rsquo;apprentissage frugal (JRAF) </a></u></h4>
<img src="https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg" alt="logos" height="130"/>
<p><a href="mailto:adrien.fois@univ-amu.fr">adrien.fois@univ-amu.fr</a>
<a href="mailto:laurent.perrinet@univ-amu.fr">laurent.perrinet@univ-amu.fr</a></p>
<aside class="notes">
  In conclusion, we have seen that event-driven cameras open the door to new applications that mimic the performance of the human eye, in terms of computational dynamics, adaptation to light conditions and energy constraints. This technological development has recently been accompanied by the development of neuromorphic chips and innovative algorithms in the form of spiking neural networks. However, there is still a great deal of progress to be made at theoretical level, particularly in the understanding of these spiking neural networks, and we have shown the potential progress that can be made by exploiting the richness of temporal representations, particularly by taking advantage of heterogeneous delays.
Beyond these particular applications to natural image processing, I hope to have succeeded in demonstrating the importance of cross-fertilizing the field of engineering applications in general with biological neuroscience. This new line of research - known as NeuroAI or, more generally, as computational neuroscience - is likely to develop over the next few years. Thank you for your attention.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="spiking-neural-networks">Spiking Neural Networks</h1>
<aside class="notes">
  I have therefore illustrated the use of <em>event-driven</em> cameras on a particular algorithm. The nice feature of this algorithm is that it processes the stream of events from the camera on an event-by-event basis rather than having to wait for the whole video sequence to finish. Each event has the potential to initiate a series of processes across various layers, allowing for the continuous update of classification values. This type of operation is characteristic of the way neurons work in the brain, that is using an event-based representation of information processing. This is what we call <em>spiking neural networks</em>.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<figure  id="figure-tonic-manualhttpstonicreadthedocsioenlatest_imagesneuron-modelspng">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://tonic.readthedocs.io/en/latest/_images/neuron-models.png" alt="[[Tonic manual](https://tonic.readthedocs.io/en/latest/_images/neuron-models.png)]" loading="lazy" data-zoomable width="70%" /></div>
  </div><figcaption>
      [<a href="https://tonic.readthedocs.io/en/latest/_images/neuron-models.png">Tonic manual</a>]
    </figcaption></figure>
<aside class="notes">
  Traditional neural networks in deep learning typically rely on an analog representation. This is illustrated in this figure, where various analog inputs are integrated and then processed through a non-linear function to output an analog activation value. This basic <em>perceptron</em> principle is at the foundation of all existing neural networks, including convolutional networks that excel in image classification. While effective for static images, this method can be resource-intensive for video processing. An alternative is the use of <em>spiking neurons</em>. Unlike their analog counterparts, spiking neurons process discrete events, which are integrated in the membrane potential. When the membrane potential crosses a theshold, it output an action potential, which can be seen as an event.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-lif-neuron">Spiking Neural Networks: LIF Neuron</h2>
<figure  id="figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif" alt="[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      [Grimaldi <em>et al</em>, 2023, <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/">Precise Spiking Motifs</a>]
    </figcaption></figure>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>This is illustrated in this <em>animation</em>, which shows how we can transform a list of input events by giving them different weights, and then <em>integrate</em> them into the cell&rsquo;s membrane potential. When the membrane potential crosses the spiking theshold, the neuron outputs a spike.</p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-neuromorphic-hardware">Spiking Neural Networks: neuromorphic hardware</h2>
<figure  id="figure-loihi-2">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg" alt="Loihi 2" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      Loihi 2
    </figcaption></figure>
<aside class="notes">
  <p>The introduction of spiking neural networks marks a <em>paradigm shift</em> in computation, in the same way that event-driven cameras have brought a paradigm shift in image representation. These spiking neural networks have led to the creation of innovative algorithms and the development of neuromorphic chips like Intel’s Loihi 2. This chip departs from traditional computing by utilizing a massively parallel array of event-driven processing units. As with event-driven cameras, this has the dual advantage of being very fast and consuming very little energy. The field continues to advance, with new neuromorphic chips being developed that could potentially replace standard CPUs and GPUs.</p>
<figure  id="figure-propheseehttpsdocspropheseeaistableconceptshtml">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg" alt="[Prophesee](https://docs.prophesee.ai/stable/concepts.html)" loading="lazy" data-zoomable width="45%" /></div>
  </div><figcaption>
      <a href="https://docs.prophesee.ai/stable/concepts.html">Prophesee</a>
    </figcaption></figure>
<p>Loihi: <a href="https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg">https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg</a></p>
<p><a href="https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;strip=none&amp;ssl=1">https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;strip=none&amp;ssl=1</a></p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-in-neurobiology">Spiking Neural Networks in neurobiology</h2>
<figure  id="figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="http://i.stack.imgur.com/ixnrz.png" alt="[[Mainen &amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb">Mainen &amp; Sejnowski, 1995</a>]
    </figcaption></figure>
<aside class="notes">
  <strong>2 MINUTE</strong>
Spiking neural networks show great potential for processing data from event-driven cameras. However, <em>neurophysiology</em> studies reveal some unexpected behaviors, very different from the classical perceptron. I will highlight these differences with three examples. The first example is a 1995 study by Mainen and Sejnowski examined a neuron’s reaction to repeated stimulations.
<em>Panel A</em> at the top presents the neuron’s response to multiple stimulations with a 200 picoampere <em>current step</em>. The membrane potential varied across trials, indicating an unpredictable response. Initially, the spikes were synchronized at the onset of stimulation, but coherence diminished over time, leading to no alignment after approximately 750 milliseconds.
In contrast, Panel B at the botton shows the neuron’s response to stimulation with <em>noise</em>. Here, the neuron exhibited highly consistent responses across trials, with membrane potential traces nearly identical. This precision was achieved using <em>frozen</em> noise, a repeated, unchanging stimulus. The study highlights that neurons are less responsive to constant analog values, such as square pulses, and more selective to dynamic signals, responding with remarkable precision in the temporal domain.
</aside>
<!-- 
---
<h2 id="spiking-neural-networks-in-neurobiology-1">Spiking Neural Networks in neurobiology</h2>
<figure  id="figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png" alt="[[Mainen &amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb">Mainen &amp; Sejnowski, 1995</a>]
    </figcaption></figure>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<ul>
<li>reproduucibility</li>
</ul>
</aside> -->

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-in-neurobiology-2">Spiking Neural Networks in neurobiology</h2>
<figure  id="figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png" alt="[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py">Diesmann et al. 1999</a>]
    </figcaption></figure>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>In this second example, I show a simulation reproducing the 1999 paper by Diesmann and colleagues. This <em>theoretical model</em> considers ten interconnected groups, each comprising 100 neurons. Each group is connected to the next one. A key finding is that information transfer across groups depends on the temporal concentration of spikes. Initially, information is too scattered within the first group, leading to a dilution effect in subsequent groups. However, once a threshold is reached, a cluster of synchronous spikes ensures efficient propagation through the network. This non-linear dynamic is characteristic of spiking neural networks, adding a layer of richness, but also a cerain complexity.</p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-in-neurobiology-3">Spiking Neural Networks in neurobiology</h2>
<figure  id="figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg" alt="[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]" loading="lazy" data-zoomable width="99%" /></div>
  </div><figcaption>
      [<a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/">Haimerl et al, 2019</a>]
    </figcaption></figure>
<aside class="notes">
  <strong>2 MINUTE</strong>
A third example shows an experiment conducted by Rosa Cossart&rsquo;s group at INMED and recently published by Haimerl and colleagues. They used <em>calcium fluorescence</em> imaging to track neuronal activity in mice. By arranging the neurons in <em>temporal order of activation</em>, it shows a sequential activation of these neurons, a mechanism which resembles the model mentioned earlier. These patterns closely align with the mouse’s motor behavior, as depicted in the accompanying graph. Notably, these activity sequences remained consistent, even when recorded on the <em>next day</em>, underscoring the importance of temporal dynamics in neural computation.
</aside>
</section>

    </section>
    

    
    
    
    <section>
    
      
<section>
<h1 id="spiking-neural-networks-spiking-motifs">Spiking Neural Networks: Spiking motifs</h1>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>These observations have led us to <em>review</em> neurobiological evidence of neurons encoding information based on the relative timing of spikes. Intriguingly, the conduction <em>delays</em> observed in spike transmission are not merely obstacles. Instead, they could be used to enhance information representation and processing through <em>spiking motifs</em>. This perspective challenges traditional views and opens up new possibilities for understanding information representation, processing and learning.</p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-spiking-motifs-1">Spiking Neural Networks: Spiking motifs</h2>
<figure  id="figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png" alt="[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      [Grimaldi <em>et al</em>, 2023, <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/">Precise Spiking Motifs</a>]
    </figcaption></figure>
<aside class="notes">
  <strong>2 MINUTE</strong>
Consider an ultra-simplified neural network with three presynaptic neurons and two output neurons, connected by <em>heterogeneous</em> delays. With synchronous inputs, the output neurons activate at different times, failing to reach the threshold for an output spike. However, if the delays align the action potentials to arrive simultaneously, the combined input can trigger an output spike at the <em>same instant</em>, as indicated by the red bar.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-spiking-motifs-2">Spiking Neural Networks: Spiking motifs</h2>
<figure  id="figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif" alt="Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)." loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      Review on <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/">Precise Spiking Motifs</a>.
    </figcaption></figure>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>To better grasp this mechanism, let’s revisit the animation of a spiking neuron. Without delays, action potentials reach the neuron’s cell body immediately, where they’re integrated to potentially trigger a spike.</p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-spiking-motifs-3">Spiking Neural Networks: Spiking motifs</h2>
<figure  id="figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif" alt="Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)." loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      Review on <a href="https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/">Precise Spiking Motifs</a>.
    </figcaption></figure>
<aside class="notes">
  <strong>2 MINUTE</strong>
Now using <em>heterogeneous</em> delays, the timing of spike arrival at the cell body varies. Introducing a specific <em>spiking motif</em>, marked by green action potentials, allows these spikes to converge simultaneously due to the delays. This synchronicity results in the neuron generating a new spike.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn">Spiking Neural Networks: HD-SNN</h2>
<video autoplay loop  >
  <source src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4" type="video/mp4">
</video>
<aside class="notes">
  <strong>2 MINUTE</strong>
In applying this theoretical principle, we developed an algorithm to detect movement in images. We began by simulating event data from natural images set in motion along paths similar to those observed during free visual exploration. The event-driven output exhibits distinct characteristics. For instance, rapid movement results in a higher spike rate. Conversely, edges aligned with the motion direction yield minimal changes, leading to fewer spikes. This phenomenon is known as the aperture problem.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-1">Spiking Neural Networks: HD-SNN</h2>
<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://raw.githubusercontent.com/laurentperrinet/figures/7f382a8074552de1a6a0c5728c60d48788b5a9f8/animated_neurons/conv_HDSNN.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="100%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>We then used a neural network with a classical architecture, which we enhanced by using an spike representation that accounts for various synaptic delays values. In this figure, the input is on the left grid, indicating spikes of either positive or negative polarity. This input is processed through multiple channels, represented by green and orange, and generate membrane activity. This activity, in turn, led to the production of output spikes, particularly in synaptic connection nuclei with heterogeneous delays. These delays are key to identifying specific spatio-temporal patterns.</p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-2">Spiking Neural Networks: HD-SNN</h2>
<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="90%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>
<aside class="notes">
  <strong>2 MINUTE</strong>
A key advantage of this network is its differentiability, which allows the application of traditional machine learning techniques, such as supervised learning.
We then see the emergence of various convolution kernels. The graph on the left, marked by red arrows, displays a selection of these kernels oriented in different directions.
It shows the kernels obtained on the spatial representation according to the different columns, and each row represents the different delays from a delay of one on the right to a delay of 12 time steps on the left. Detectors that follow the motion emerge. For example, for the top line from top to bottom. These kernels integrate both positive neurons in red and negative polarity inputs in blue.
vim Such spatio-temporal filtering is observed in neurobiology, but to my knowledge had never been observed in a model of spiking neurons trained under natural conditions.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-3">Spiking Neural Networks: HD-SNN</h2>
<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_raw.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>We will now study the performance of this network in detecting motion in the flow of events entering the network. When we use all the weights of the convolution kernel, we get a very good performance of the order of 99%, represented by the black dot in the top right-hand corner. Note that in the kernels we&rsquo;ve seen emerge, most of the synaptic weights are close to zero, so we might consider removing some of these weights, as this can be shown to reduce the number of event calculations required.</p>
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-4">Spiking Neural Networks: HD-SNN</h2>
<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_shortening.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>
<aside class="notes">
  <strong>2 MINUTE</strong>
This is what we&rsquo;ve done, by first removing the parts of the core corresponding to the longest delays. This &ldquo;shortens&rdquo; the kernel. We quickly observed a degradation in performance, which reached half-saturation when we reduced the number of weights by around 50%. This demonstrates the importance of integrating information that is quite distant and structured over time.
</aside>

    </section>
    

    
    
    
    <section>
    
      
<h2 id="spiking-neural-networks-hd-snn-5">Spiking Neural Networks: HD-SNN</h2>
<figure  id="figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc">
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy.svg" alt="[Grimaldi &amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)" loading="lazy" data-zoomable width="80%" /></div>
  </div><figcaption>
      <a href="https://laurentperrinet.github.io/publication/grimaldi-23-bc/">Grimaldi &amp; LP (2023) Biol Cybernetics</a>
    </figcaption></figure>
<aside class="notes">
  <p><strong>2 MINUTE</strong></p>
<p>In a second step, we performed a pruning operation, which consists in progressively removing the weights that are the weakest. This time, performance remains optimal over a wide compression range, and we reach half-saturation when we have removed around 99.8% of the weights. This means that the network is able to maintain very good performance, even when only one weight out of 600 has been kept, and therefore, with a computation time increased by a factor of 600. This property, which we didn&rsquo;t expect, seems promising for creating machine learning algorithms that are less energy-hungry.</p>
</aside>
</section>
</aside>
</aside>

    </section>
    

    
    
  </div>
</div>



  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/dist/reveal.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/plugin/markdown/markdown.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/plugin/highlight/highlight.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/plugin/notes/notes.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/plugin/search/search.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/plugin/math/math.min.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.2.1/plugin/zoom/zoom.min.js" crossorigin="anonymous"></script>

  
  
    <script src="https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/plugin.js" integrity="sha256-M6JwAjnRAWmi+sbXURR/yAhWZKYhAw7YXnnLvIxrdGs=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.js" integrity="sha256-l14dklFcW5mWar6w/9KaW0fWVerf3mYr7Wt0+rXzFAA=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.css" integrity="sha256-0fU8HKLaTjgzfaV9CgSqbsN8ilA3zo6zK1M6rlgULd8=" crossorigin="anonymous">
  

  

  
  <script src="/js/wowchemy-slides.js"></script>

</body>
</html>
