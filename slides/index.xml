<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slides | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/slides/</link>
      <atom:link href="https://laurentperrinet.github.io/slides/index.xml" rel="self" type="application/rss+xml" />
    <description>Slides</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Mon, 23 Jan 2023 00:00:00 +0000</lastBuildDate>
    <item>
      <title>2023-01-23_game-theory-and-the-brain</title>
      <link>https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain/</guid>
      <description>&lt;h1 id=&#34;game-theory-and-brain-strategies&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;50%&#34; &gt;
&lt;p&gt;&lt;strong&gt;[2023-01-23] Atelier jeu et cerveau&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-01-23-game-theory-and-the-brain&#34;&gt;https://laurentperrinet.github.io/talk/2023-01-23-game-theory-and-the-brain&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;Photo by Naser Tamimi on Unsplash &lt;a href=&#34;https://unsplash.com/fr/photos/yG9pCqSOrAg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://unsplash.com/fr/photos/yG9pCqSOrAg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-1&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;80%&#34; &gt;
&lt;p&gt;&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;Le jeu du cerveau et du hasard, &lt;i&gt;The Conversation&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;What is noise? The uncertainty due to noise is symbolized by dices: a throw of fair dices, even if they are optimally simulated can not be predicted: the outcome is uniformly one facet from 1 to 6,&lt;/li&gt;
&lt;li&gt;I am interested in vision, and uncertainty exists in different forms,&lt;/li&gt;
&lt;li&gt;If we consider the image, can be noise at low contrast, complexity of the object, pose of the dice,&lt;/li&gt;
&lt;li&gt;in this presentation, we will see different facets of noise and uncertainty, and illustrate how our brains may play with it - and delineate a theory for this game. We will also see how it may harness the noise by explicitly representing it in the neural activity.&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;aleatoric-noise&#34;&gt;Aleatoric noise&lt;/h1&gt;
&lt;hr&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-random-points--a&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://a5huynh.github.io/img/2019/rng-example.png&#34; alt=&#34;Random points  (A).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;49%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Random points  (A).
    &lt;/figcaption&gt;&lt;/figure&gt;













&lt;figure  id=&#34;figure-random-points--b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://a5huynh.github.io/img/2019/poisson-disk-example.png&#34; alt=&#34;Random points  (B).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;49%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Random points  (B).
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;img src=&#34;https://a5huynh.github.io/img/2019/rng-example.png&#34; width=&#34;70%&#34; &gt;
&lt;img src=&#34;https://a5huynh.github.io/img/2019/poisson-disk-example.png&#34; width=&#34;70%&#34; &gt;
&lt;p&gt;&lt;a href=&#34;https://a5huynh.github.io/posts/2019/poisson-disk-sampling/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Huynh, generating Poisson disk noise&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;what is noise? it exists at quantum level, but if I were to ask you to draw random points how would it look like?&lt;/li&gt;
&lt;li&gt;Aleatoric comes from alea, the Latin word for â€œdice.â€ Aleatoric uncertainty is the uncertainty introduced by the randomness of an event. For example, the result of flipping a coin is an aleatoric event.&lt;/li&gt;
&lt;li&gt;In your opinion, which of the two is the most random pattern?&lt;/li&gt;
&lt;li&gt;from your responses &amp;hellip;&lt;/li&gt;
&lt;li&gt;the answer is that &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When it comes to true randomness, one of its stranger aspects is that it often behaves differently to peopleâ€™s expectations. Take the two diagrams below â€“ which one do you think is a random distribution, and which has been deliberately created/adjusted?&lt;/p&gt;
&lt;p&gt;randomized dots
Only one of these panels shows a random distribution of dots | Source: Bully for Brontosaurus â€“ Stephen Jay Gould&lt;/p&gt;
&lt;p&gt;If you said the right panel, you are in good company, as this is most peopleâ€™s expectation of what randomness looks like. However, this relatively uniform distribution has been adjusted to ensure the dots are evenly spread. In fact, it is the left panel, with its clumps and voids, that reflects a true random distribution. It is also this tendency for randomness to produce clumps and voids that leads to some unintuitive outcomes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://theconversation.com/daniel-kahneman-on-noise-the-flaw-in-human-judgement-harder-to-detect-than-cognitive-bias-160525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://theconversation.com/daniel-kahneman-on-noise-the-flaw-in-human-judgement-harder-to-detect-than-cognitive-bias-160525&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-instabilitÃ©-etienne-reyhttpslaurentperrinetgithubiopost2018-09-09_artorama&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/post/2018-09-09_artorama/featured.png&#34; alt=&#34;[InstabilitÃ©, Etienne Rey.](https://laurentperrinet.github.io/post/2018-09-09_artorama/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/post/2018-09-09_artorama/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InstabilitÃ©, Etienne Rey.&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;this was for instance used by the artist Etienne Rey to generate large panels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;our perception will generate objects out of nowhere: surfaces, groups, holes&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;this explains many cognitive biases, for instance that we expect noise to have some regularity and that we wish to explain any cluster of events by some god-like divinity&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;going further &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;when going to the same place a few years later &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;the face was gone &amp;hellip;&lt;/li&gt;
&lt;li&gt;conclusion 1: information pops out from noise&lt;/li&gt;
&lt;li&gt;conclusion 2: further information may change the interpretation&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction&#34;&gt;Sequence prediction&lt;/h1&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://github.com/chloepasturel/AnticipatorySPEM/raw/master/2020-03_video-abstract/Bet_eyeMvt/eyeMvt.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;to test this in the lab, we analyzed the response of observers to a sequences of left / right moving dots&lt;/li&gt;
&lt;li&gt;These were presented in multiple blocks of 50 trials for which we recorded eye movements and, on a subsequent day, asked them&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction-1&#34;&gt;Sequence prediction&lt;/h1&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
A: ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ ?
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
B: ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ ?
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
C: ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ ?
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
D: ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ğŸ‘ğŸ¤˜ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ¤˜ ?
&lt;/code&gt;&lt;/pre&gt;&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;to simplify the problem, let&amp;rsquo;s show these sequences as the sequence of these 2 emojis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In sequence A, what do you think the next&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the same question could be asked in an online fashion&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence B, it&amp;rsquo;s certainly the same answer, yet with lower certitude&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence C, you go metal ğŸ¤˜&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence D, it&amp;rsquo;s different there is a clearly a tendance for ğŸ¤˜but that it switches to ğŸ‘&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;is it possible that the brain may detect such switches?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction-2&#34;&gt;Sequence prediction&lt;/h1&gt;














&lt;figure  id=&#34;figure-pasturel-et-al-2020httpslaurentperrinetgithubiopublicationpasturel-montagnini-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis.png&#34; alt=&#34;([Pasturel *et al*, 2020](https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      (&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pasturel &lt;em&gt;et al&lt;/em&gt;, 2020&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;to synthesize, we have a generative model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we found the mathematically optimal problem - and found that both eye movements + bets follow the model with switches&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The aleatoric noise is transformed into a measure of knowledge = epistemic noise&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;epistemic-noise&#34;&gt;Epistemic noise&lt;/h1&gt;
&lt;!-- 
---

# Playing with noise
















&lt;figure  id=&#34;figure-nash-equilibrium-rock-paper-scissorshttpsenwikipediaorgwikirock_paper_scissors&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/67/Rock-paper-scissors.svg&#34; alt=&#34;Nash equilibrium ([Rock paper scissors](https://en.wikipedia.org/wiki/Rock_paper_scissors)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Nash equilibrium (&lt;a href=&#34;https://en.wikipedia.org/wiki/Rock_paper_scissors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rock paper scissors&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;let&amp;rsquo;s go back to game theory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rock paper scissors: Its French name, &amp;ldquo;Chi-fou-mi&amp;rdquo;, is based on the Old Japanese words for &amp;ldquo;one, two, three&amp;rdquo; (&amp;ldquo;hi, fu, mi&amp;rdquo;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nash Equilibrium is a game theory concept that determines the optimal solution in a non-cooperative game in which each player lacks any incentive to change his/her initial strategy. Under the Nash equilibrium, a player does not gain anything from deviating from their initially chosen strategy, assuming the other players also keep their strategies unchanged.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quantamagazine.org/the-game-theory-math-behind-rock-paper-scissors-20180402/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.quantamagazine.org/the-game-theory-math-behind-rock-paper-scissors-20180402/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;

---















&lt;figure  id=&#34;figure-prisoners-dilemma-salem-marafihttpwwwsalemmaraficombusinessprisoners-dilemma&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.salemmarafi.com/wp-content/uploads/2011/10/prisoners_dilemma.jpg&#34; alt=&#34;Prisonerâ€™s Dilemma ([Salem Marafi](http://www.salemmarafi.com/business/prisoners-dilemma/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;60%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Prisonerâ€™s Dilemma (&lt;a href=&#34;http://www.salemmarafi.com/business/prisoners-dilemma/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Salem Marafi&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;


&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;uncertainty comes not from aleatoric noise but from not knowing: epistemic uncertainty&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt; --&gt;
&lt;hr&gt;
&lt;h1 id=&#34;representing-uncertainty&#34;&gt;Representing uncertainty&lt;/h1&gt;














&lt;figure  id=&#34;figure-visual-epistemic-uncertainty-hugo-ladrethttpstheconversationcomle-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.theconversation.com/files/407867/original/file-20210623-17-ai1gc3.png&#34; alt=&#34;Visual epistemic uncertainty ([Hugo Ladret](https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual epistemic uncertainty (&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;in the case of images, a local patch may have the same most likely orientation, yet with different bandwidth (textures)&lt;/li&gt;
&lt;li&gt;the primary visual cortex of mammals like humans is to detect orientations&lt;/li&gt;
&lt;li&gt;will the response be the same for both cases?&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;representing-uncertainty-1&#34;&gt;Representing uncertainty&lt;/h1&gt;














&lt;figure  id=&#34;figure-visual-epistemic-uncertainty-hugo-ladrethttpslaurentperrinetgithubiopublicationladret-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/ladret-23/featured_hubbf63d8d7d25b21f139c2f10354080fc_466086_720x2500_fit_q75_h2_lanczos_3.webp&#34; alt=&#34;Visual epistemic uncertainty ([Hugo Ladret](https://laurentperrinet.github.io/publication/ladret-23/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual epistemic uncertainty (&lt;a href=&#34;https://laurentperrinet.github.io/publication/ladret-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-2&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;80%&#34; &gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;In face of noise, the brain plays a game&lt;/li&gt;
&lt;li&gt;Evolution favors not fitness but adaptability&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-3&#34;&gt;Game theory and brain strategies&lt;/h1&gt;














&lt;figure  id=&#34;figure-aleatoric-uncertainty-pasturel-et-al-2020httpslaurentperrinetgithubiopublicationpasturel-montagnini-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis.png&#34; alt=&#34;Aleatoric uncertainty ([Pasturel *et al*, 2020](https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Aleatoric uncertainty (&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pasturel &lt;em&gt;et al&lt;/em&gt;, 2020&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;The brain uses predictive coding, for instance for sequence learning&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-4&#34;&gt;Game theory and brain strategies&lt;/h1&gt;














&lt;figure  id=&#34;figure-epistemic-uncertainty-hugo-ladrethttpstheconversationcomle-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.theconversation.com/files/407867/original/file-20210623-17-ai1gc3.png&#34; alt=&#34;Epistemic uncertainty ([Hugo Ladret](https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Epistemic uncertainty (&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;For this, it represents explictly uncertainty (epistemic noise)&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022-11-21_flash-lag-effect</title>
      <link>https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/</link>
      <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/</guid>
      <description>&lt;table width=&#34;100%&#34;&gt; 
&lt;tr&gt;
	&lt;th width=&#34;80%&#34;&gt;
	&lt;img src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/header.png&#34; width=&#34;100%&#34; &gt;
	&lt;th width=&#34;20%&#34;&gt;
	&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; width=&#34;100%&#34; &gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;[2022-11-21] Alex Reynaud&amp;rsquo;s lab meeting&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34;&gt;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;!--
---


&lt;table width=&#34;100%&#34;&gt;
&lt;tr&gt;
	&lt;th width=&#34;80%&#34;&gt;
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;figure  &gt;
	  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
	    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/khoei-masson-perrinet-17/header.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
	  &lt;/div&gt;&lt;/figure&gt;	&lt;/th&gt;
	&lt;th width=&#34;20%&#34;&gt;
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;figure  &gt;
	  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
	    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
	  &lt;/div&gt;&lt;/figure&gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;

__[2022-11-21] Alex Reynaud&#39;s lab meeting__

&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34;&gt;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&lt;/a&gt;&lt;/p&gt; --&gt;
&lt;!-- ---

|













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/khoei-masson-perrinet-17/header.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;













&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;29%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;|

__[2022-11-21] Alex Reynaud&#39;s lab meeting__
https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;timing-in-the-visual-pathways&#34;&gt;Timing in the visual pathways&lt;/h2&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-ultra-rapid-visual-processing-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-polychronies/featured.jpg&#34; alt=&#34;Ultra-rapid visual processing ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ultra-rapid visual processing (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-compensating-visual-delays-perrinet-adams--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/tsonga.jpg&#34; alt=&#34;Compensating visual delays ([Perrinet, Adams &amp;amp; Friston 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Compensating visual delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet, Adams &amp;amp; Friston 2014&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-compensating-visual-delays-perrinet-adams--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/figure-tsonga.jpg&#34; alt=&#34;Compensating visual delays ([Perrinet Adams &amp;amp; Friston, 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Compensating visual delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet Adams &amp;amp; Friston, 2014&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;travelling-waves&#34;&gt;Travelling waves?&lt;/h2&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/line_motion.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/phi_motion.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-suppressive-travelling-waves-chemla-et-al-2019httpslaurentperrinetgithubiopublicationchemla-19&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-18_JNLF/master/figures/Chemla_etal2019.png&#34; alt=&#34;Suppressive travelling waves ([Chemla *et al*, 2019](https://laurentperrinet.github.io/publication/chemla-19/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Suppressive travelling waves (&lt;a href=&#34;https://laurentperrinet.github.io/publication/chemla-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chemla &lt;em&gt;et al&lt;/em&gt;, 2019&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;predictive-coding&#34;&gt;Predictive coding&lt;/h2&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_aperture.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;!--
---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_box.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt; --&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_cube.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-based-prediction-perrinet-et-al-2012httpslaurentperrinetgithubiopublicationperrinet-12-pred&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/navier.svg&#34; alt=&#34;Motion-based prediction ([Perrinet *et al*, 2012](https://laurentperrinet.github.io/publication/perrinet-12-pred/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-based-prediction-perrinet-et-al-2012httpslaurentperrinetgithubiopublicationperrinet-12-pred&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/perrinet12pred_figure2.png&#34; alt=&#34;Motion-based prediction ([Perrinet *et al*, 2012](https://laurentperrinet.github.io/publication/perrinet-12-pred/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;61%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- ---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line-nopred_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
 --&gt;
&lt;hr&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;flash-lag-effect&#34;&gt;Flash-lag effect&lt;/h2&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-flash-lag-effect-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_cartoon.jpg&#34; alt=&#34;Flash-lag effect ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Flash-lag effect (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!--
---















&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=10.1371/journal.pcbi.1005068.g002&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;id=10.1371/journal.pcbi.1005068.g002
---















&lt;figure  id=&#34;figure-diagonal-markov&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov
    &lt;/figcaption&gt;&lt;/figure&gt;

---















&lt;figure  id=&#34;figure-diagonal-markov&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_simple.jpg&#34; alt=&#34;Diagonal markov&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov
    &lt;/figcaption&gt;&lt;/figure&gt;

---















&lt;figure  id=&#34;figure-diagonal-markov-pull&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_pull.jpg&#34; alt=&#34;Diagonal markov (pull)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov (pull)
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;!--
---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/PBP_flash_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;


---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/PBP_dot_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;


---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_flash_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;

---











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt; --&gt;
&lt;!-- ---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

Flash-lag effect: MBP ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).

---










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

Flash-lag effect: MBP ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)). --&gt;
&lt;hr&gt;
&lt;p&gt;








  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;








  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;Flash-lag effect: MBP (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/positional-delay.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-flash-lag-effect-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg&#34; alt=&#34;Flash-lag effect ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Flash-lag effect (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!--


---















&lt;figure  id=&#34;figure-qauntitative-result&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg&#34; alt=&#34;Qauntitative result&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Qauntitative result
    &lt;/figcaption&gt;&lt;/figure&gt;


https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg


https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true
MBP_dot_spatial_readout.mp4
MBP_flash_spatial_readout.mp4
MBP_spatial_readout.mp4
PBP_dot_spatial_readout.mp4
PBP_flash_spatial_readout.mp4

https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true

PBP_spatial_readout.mp4


src=&#34;../../publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; khoei-masson-perrinet-17


 create mode 100644 publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4
 create mode 100644 publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4

 --&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram_comp.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-reversal-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_MotionReversal_MBP.jpg&#34; alt=&#34;Motion reversal ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion reversal (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-motion-reversal-smoothed-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_MotionReversal.jpg&#34; alt=&#34;Motion reversal (smoothed) ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion reversal (smoothed) (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag_stop.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-limit-cycles-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_limit_cycles.jpg&#34; alt=&#34;Limit cycles ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Limit cycles (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!--
---















&lt;figure  id=&#34;figure-diagonal-neural&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_neural.jpg&#34; alt=&#34;Diagonal neural&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal neural
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-application-to-the-pulfrich-phenomenonhttpseyewikiaaoorgpulfrich_phenomenon&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://eyewiki.aao.org/w/images/1/e/eb/Pulfrich.png&#34; alt=&#34;Application to the [Pulfrich phenomenon](https://eyewiki.aao.org/Pulfrich_Phenomenon)?&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Application to the &lt;a href=&#34;https://eyewiki.aao.org/Pulfrich_Phenomenon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pulfrich phenomenon&lt;/a&gt;?
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt; + &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022-03-23_UE-neurosciences-computationnelles</title>
      <link>https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/</link>
      <pubDate>Wed, 23 Mar 2022 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/</guid>
      <description>&lt;h1 id=&#34;rÃ©seaux-de-neurones-artificiels-et-apprentissage-machine-appliquÃ©s-Ã -la-comprÃ©hension-de-la-visionhttpsgithubcomlaurentperrinet2022_ue-neurosciences-computationnelles&#34;&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RÃ©seaux de neurones artificiels et apprentissage machine appliquÃ©s Ã  la comprÃ©hension de la vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubiotalk2022-03-23-ue-neurosciences-computationnelles&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2022-03-23-ue-neurosciences-computationnelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2022-03-23httpsameticeuniv-amufrpluginfilephp5559779mod_resourcecontent1planning_neurocomp_m1_2022pdf-master-1-neurosciences-et-sciences-cognitiveshttpsameticeuniv-amufrcourseviewphpid89069u&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/pluginfile.php/5559779/mod_resource/content/1/Planning_Neurocomp_M1_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2022-03-23]&lt;/a&gt; &lt;a href=&#34;https://ametice.univ-amu.fr/course/view.php?id=89069&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master 1 Neurosciences et Sciences Cognitives&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.png&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;principes-de-la-vision&#34;&gt;Principes de la Vision&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;more on &lt;a href=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/master/exampleSite/content/slides/example/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;Ã -quoi-sert-la-vision&#34;&gt;Ã€ quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor-ilya-repin-1884httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_001.jpg&#34; alt=&#34;[An Unexpected Visitor (Ilya Repin, 1884)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Ilya Repin, 1884)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;Ã -quoi-sert-la-vision-1&#34;&gt;Ã€ quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_002.jpg&#34; alt=&#34;[An Unexpected Visitor (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;Ã -quoi-sert-la-vision-2&#34;&gt;Ã€ quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor---age-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_003.jpg&#34; alt=&#34;[An Unexpected Visitor - *Age?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;Age?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;Ã -quoi-sert-la-vision-3&#34;&gt;Ã€ quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor---how-long--yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_006.jpg&#34; alt=&#34;[An Unexpected Visitor - *How long?*  (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;How long?&lt;/em&gt;  (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion_without.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;








  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Kitaoka.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ilusions of brightness or lightness &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-3&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-rotating-snakes-akiyoshi-kitaokahttpwwwritsumeiacjpakitaokaindex-ehtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/42_rotsnakes_main.jpg&#34; alt=&#34;[Rotating Snakes *Akiyoshi KITAOKA*](http://www.ritsumei.ac.jp/~akitaoka/index-e.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating Snakes &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--parÃ©idoliehttpsfrwikipediaorgwikiparc3a9idolie&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt; : &lt;a href=&#34;https://fr.wikipedia.org/wiki/Par%C3%A9idolie&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ParÃ©idolie&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--parÃ©idoliehttpsfrwikipediaorgwikiparc3a9idolie-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt; : &lt;a href=&#34;https://fr.wikipedia.org/wiki/Par%C3%A9idolie&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ParÃ©idolie&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--parÃ©idoliehttpsfrwikipediaorgwikiparc3a9idolie-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt; : &lt;a href=&#34;https://fr.wikipedia.org/wiki/Par%C3%A9idolie&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ParÃ©idolie&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-neurosciences-computationnelles&#34;&gt;Les neurosciences computationnelles&lt;/h2&gt;














&lt;figure  id=&#34;figure-sejnowski--koch---churchland-1998httpwwwhmsharvardedubssneurobornlabnb204paperssejnowski-koch-churchland-science1988pdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Churchland92.png&#34; alt=&#34;[[Sejnowski,  Koch  &amp;amp; Churchland (1998)](http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;35%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sejnowski,  Koch  &amp;amp; Churchland (1998)&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;de-v1-aux-rÃ©seaux-convolutionnels&#34;&gt;De V1 aux rÃ©seaux convolutionnels&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;le-systÃ¨me-visuel&#34;&gt;Le systÃ¨me visuel&lt;/h2&gt;














&lt;figure  id=&#34;figure-systÃ¨me-visuel-humain-wikipediahttpsfrwikipediaorgwikisystc3a8me_visuel_humain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e4/Voies_visuelles3.svg&#34; alt=&#34;[SystÃ¨me visuel humain (Wikipedia)](https://fr.wikipedia.org/wiki/Syst%C3%A8me_visuel_humain)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Syst%C3%A8me_visuel_humain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SystÃ¨me visuel humain (Wikipedia)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;le-cortex-visuel-primaire&#34;&gt;Le cortex visuel primaire&lt;/h2&gt;














&lt;figure  id=&#34;figure-hubel--wiesel-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/scientists.jpg&#34; alt=&#34;[Hubel &amp;amp; Wiesel, 1962]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Hubel &amp;amp; Wiesel, 1962]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hubel--wiesel&#34;&gt;Hubel &amp;amp; Wiesel&lt;/h2&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;[Hubel &amp;amp; Wiesel, 1962]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels--hiÃ©rarchie&#34;&gt;RÃ©seaux convolutionnels : hiÃ©rarchie&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels---math&#34;&gt;RÃ©seaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrÃ¨te uni-dimensionnelle (eg dans le temps) avec un noyau f de rayon $K$:
$$
(f \ast g)[n]=\sum_{m=-K}^{K} f[m] g[n-m]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels---math-1&#34;&gt;RÃ©seaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrÃ¨te d&amp;rsquo;une image (bi-dimensionnelle):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[i, j] g[i-x, j-y]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels--lopÃ©ration-de-convolution&#34;&gt;RÃ©seaux convolutionnels : l&amp;rsquo;opÃ©ration de convolution&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/convolution-layer-a.png?1c517e00cb8d709baf32fc3d39ebae67&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels---math-2&#34;&gt;RÃ©seaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrÃ¨te d&amp;rsquo;une image sur plusieurs canaux de sortie:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y, k] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[k, i, j, k] g[i-x, j-y]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels---math-3&#34;&gt;RÃ©seaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrÃ¨te d&amp;rsquo;une image multi-canaux (eg. RGB) sur plusieurs canaux de sortie (noter &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;l&amp;rsquo;ordre des indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y, k] = \
\sum_{i=-K}^{K} \sum_{j=-K}^{K} \sum_{c=1}^{C} f[k, c, i, j] g[i-x, j-y, c]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels--cnn&#34;&gt;RÃ©seaux convolutionnels : CNN&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-cnn-fr.jpeg&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;mise-en-pratique-dÃ©tecter--apprendre&#34;&gt;Mise en pratique: dÃ©tecter &amp;amp; apprendre&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tutoriel Apprentissage profond&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/blob/master/A_D%C3%A9tecter.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Notebook &lt;code&gt;A_DÃ©tecter.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/blob/master/B_Apprendre.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Notebook &lt;code&gt;B_Apprendre.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;perspectives&#34;&gt;Perspectives&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-convolutionnels--hiÃ©rarchie-1&#34;&gt;RÃ©seaux convolutionnels : hiÃ©rarchie&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;rÃ©seaux-prÃ©dictifs&#34;&gt;RÃ©seaux prÃ©dictifs&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;topographie-dans-v1&#34;&gt;Topographie dans V1&lt;/h2&gt;














&lt;figure  id=&#34;figure-bosking-et-al-1997&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Bosking97Fig4.jpg&#34; alt=&#34;[Bosking *et al*, 1997]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Bosking &lt;em&gt;et al&lt;/em&gt;, 1997]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;














&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recurrent-processing&#34;&gt;Recurrent processing&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamique-de-la-vision&#34;&gt;Dynamique de la vision&lt;/h2&gt;














&lt;figure  id=&#34;figure-thorpe-2001httpslaurentperrinetgithubio2022-01-12_neurocercle21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/scheme_thorpe.jpg&#34; alt=&#34;[[Thorpe (2001)]](https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Thorpe (2001)]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;applications-robotiques&#34;&gt;Applications robotiques&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-communicating-by-event-driven-feed-forward-and-feed-back-communications&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/principe_agile.jpg&#34; alt=&#34;Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-04-03-master-m-4-nc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-04-03-master-m-4-nc/</guid>
      <description>&lt;h1 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-visionhttpslaurentperrinetgithubioslides2023-04-03-master-m-4-nctransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-04-03-master-m-4-nc/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-04-03httpsameticeuniv-amufrpluginfilephp5559779mod_resourcecontent1planning_neurocomp_m1_2022pdf-master-m4nc-de-linstitut-neuromod-cours-prospective-innovation-and-researchhttpsneuromoduniv-cotedazureuu&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/pluginfile.php/5559779/mod_resource/content/1/Planning_Neurocomp_M1_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-04-03]&lt;/a&gt; &lt;a href=&#34;https://neuromod.univ-cotedazur.eu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master M4NC de l&amp;rsquo;institut NeuroMod, cours Prospective Innovation and Research.&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;section data-transition=&#34;slide-in fade-out&#34;&gt;
&lt;h1 id=&#34;principles-of-vision&#34;&gt;Principles of Vision&lt;/h1&gt;














&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;more on &lt;a href=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/master/exampleSite/content/slides/example/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision&#34;&gt;What is the function of vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor-ilya-repin-1884httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_001.jpg&#34; alt=&#34;[An Unexpected Visitor (Ilya Repin, 1884)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Ilya Repin, 1884)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-1&#34;&gt;What is the function of vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_002.jpg&#34; alt=&#34;[An Unexpected Visitor (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-2&#34;&gt;What is the function of vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor---age-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_003.jpg&#34; alt=&#34;[An Unexpected Visitor - *Age?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;Age?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-3&#34;&gt;What is the function of vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor---how-long-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_006.jpg&#34; alt=&#34;[An Unexpected Visitor - *How long?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;How long?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion_without.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;








  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Kitaoka.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ilusions of brightness or lightness &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-3&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-rotating-snakes-akiyoshi-kitaokahttpwwwritsumeiacjpakitaokaindex-ehtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/42_rotsnakes_main.jpg&#34; alt=&#34;[Rotating Snakes *Akiyoshi KITAOKA*](http://www.ritsumei.ac.jp/~akitaoka/index-e.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating Snakes &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;














&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- ---

## What is the function of vision?

---

## What is the algorithm of vision? --&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;computational-neuroscience-of-vision&#34;&gt;Computational neuroscience of vision&lt;/h1&gt;














&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;computational-neuroscience-of-vision-1&#34;&gt;Computational neuroscience of vision&lt;/h2&gt;














&lt;figure  id=&#34;figure-sejnowski-koch--churchland-1998httpwwwhmsharvardedubssneurobornlabnb204paperssejnowski-koch-churchland-science1988pdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Churchland92.png&#34; alt=&#34;[[Sejnowski, Koch &amp;amp; Churchland (1998)](http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sejnowski, Koch &amp;amp; Churchland (1998)&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;anatomy-of-the-human-visual-system&#34;&gt;Anatomy of the Human Visual system&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.readkong.com/static/06/b0/06b09f0235ae7fcf29438ce317c10e60/optogenetic-visual-cortical-prosthesis-9612386-7.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;61%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;human-visual-system--the-hmax-model&#34;&gt;Human Visual system : the HMAX model&lt;/h2&gt;














&lt;figure  id=&#34;figure-serre-and-poggio-2007httpsbiologystackexchangecomquestions10955ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.stack.imgur.com/ZlFnp.png&#34; alt=&#34;[[Serre and Poggio, 2007](https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;65%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Serre and Poggio, 2007&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!-- ---

## Anatomy of the Human Visual system















&lt;figure  id=&#34;figure-wikipediahttpsenwikipediaorgwikivisual_system&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e4/Voies_visuelles3.svg&#34; alt=&#34;[[Wikipedia]](https://en.wikipedia.org/wiki/Visual_system)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Visual_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Wikipedia]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;














&lt;figure  id=&#34;figure-hubel--wiesel-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/scientists.jpg&#34; alt=&#34;[Hubel &amp;amp; Wiesel, 1962]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Hubel &amp;amp; Wiesel, 1962]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel-1&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;[Hubel &amp;amp; Wiesel, 1962]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy&#34;&gt;Convolutional Neural Networks : Hierarchy&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One-dimensional &lt;a href=&#34;https://en.wikipedia.org/wiki/Convolution#Discrete_convolution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discrete convolution&lt;/a&gt; (eg in time) with a kernel $g$ of radius $K$:
$$
(f \ast g)[n]=\sum_{m=-K}^{K} f[n-m] \cdot g[m]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-1&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[x-i, y-j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-2&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-correlation&lt;/strong&gt; of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{i,j} f[x+i, y+j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-3&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/convolution-layer-a.png&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-4&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of an image on several output channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-5&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of a multi-channel (e.g. RGB) image on multiple output channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[k, x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[k, c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--the-hmax-model&#34;&gt;Convolutional Neural Networks : the HMAX model&lt;/h2&gt;














&lt;figure  id=&#34;figure-serre-and-poggio-2006httpsbiologystackexchangecomquestions10955ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.stack.imgur.com/ZlFnp.png&#34; alt=&#34;[[Serre and Poggio, 2006]](https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;65%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Serre and Poggio, 2006]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks-cnns&#34;&gt;Convolutional Neural Networks (CNNs)&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-cnn-fr.jpeg&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy-1&#34;&gt;Convolutional Neural Networks : hierarchy&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--predictive-coding&#34;&gt;Convolutional Neural Networks : Predictive coding&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;














&lt;figure  id=&#34;figure-bosking-et-al-1997&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Bosking97Fig4.jpg&#34; alt=&#34;[Bosking *et al*, 1997]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Bosking &lt;em&gt;et al&lt;/em&gt;, 1997]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography-1&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2022httpslaurentperrinetgithubiopublicationfranciosini-21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/franciosini-21/featured_hu9b69386a773810bf9e1d727831735035_76078_720x2500_fit_q75_h2_lanczos.webp&#34; alt=&#34;[[Boutin *et al*, 2022](https://laurentperrinet.github.io/publication/franciosini-21/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/franciosini-21/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2022&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;dynamics-of-vision&#34;&gt;Dynamics of vision&lt;/h1&gt;














&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;!--
---


## Dynamics of vision















&lt;figure  id=&#34;figure-thorpe-2001httpslaurentperrinetgithubio2022-01-12_neurocercle21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/scheme_thorpe.jpg&#34; alt=&#34;[[Thorpe (2001)]](https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Thorpe (2001)]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;!--
---


## Dynamics of vision















&lt;figure  id=&#34;figure-precise-spiking-motifs-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency-estimate.jpg&#34; alt=&#34;Precise Spiking Motifs] ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Precise Spiking Motifs] (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-1&#34;&gt;Dynamics of vision&lt;/h2&gt;














&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency_bg.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In particular in our group, we are interested in dynamics of neural processing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The visual system is very efficient in generating a decision from the retinal image to the different stages of the visual pathways, here for a macaque monkey, a reaction of finger muscles in about 300 milliseconds.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the process of categorizing an object takes 10 layers&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-2&#34;&gt;Dynamics of vision&lt;/h2&gt;














&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the latencies are of similar in the human brain but merely scaled due to the brain size&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;as a consequence, it is thought that this efficiency is achieved by spikes that is, brief all-or-none events which are passed in the very large network which forms the brain from assemblies of neurons to others.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-3&#34;&gt;Dynamics of vision&lt;/h2&gt;














&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-4&#34;&gt;Dynamics of vision&lt;/h2&gt;














&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/figure-tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston, 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston, 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-5&#34;&gt;Dynamics of vision&lt;/h2&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-6&#34;&gt;Dynamics of vision&lt;/h2&gt;














&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-7&#34;&gt;Dynamics of vision&lt;/h2&gt;
&lt;!-- 








  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;








  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt; --&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/positional-delay.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;Flash-lag effect: MBP (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;)&lt;/p&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;














&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;














&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;














&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;














&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Izhikevich polychronization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;yet the domain is vast, and there s lot to do in SNNs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;














&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;














&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A standard LIF&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;














&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;














&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;event-based cameras&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-1&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-bc/HDSNN_conv.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-2&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-3&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-bc/motion_kernels.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nice kernels&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-4&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;














&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-bc/accuracy.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;frugal computing&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2020-12-10_agileneurobot_anr</title>
      <link>https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr/</guid>
      <description>&lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anr&#34;&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/header.png&#34; alt=&#34;header&#34; height=&#34;450&#34;&gt;
&lt;/a&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;th&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr&#34;&gt;
		PrÃ©sentation du projet - L. Perrinet
    &lt;!-- &lt;img src=&#34;http://www.cnrs.fr/themes/custom/cnrs/logo.svg&#34; alt=&#34;CNRS&#34; height=&#34;15&#34;&gt; --&gt;
    &lt;!-- &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/thumb/2/2c/CNRS.svg/240px-CNRS.svg.png&#34; alt=&#34;CNRS&#34; height=&#34;40&#34;&gt; --&gt;
    &lt;br&gt;
		&lt;u&gt;[2020-12-10] RÃ©union de lancement&lt;/u&gt;
	&lt;/a&gt;
	&lt;/th&gt;
	&lt;th&gt;
  &lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/featured.png&#34; alt=&#34;ANR&#34; height=&#34;80&#34;&gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;agileneurobot-fiche-didentitÃ©&#34;&gt;AgileNeuRobot: Fiche d&amp;rsquo;identitÃ©&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Titre : Robots aÃ©riens agiles bio-mimetiques pour le vol en conditions rÃ©elles&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Title : Bio-mimetic agile aerial robots flying in real-life conditions&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;CES : CE23 - Intelligence Artificielle (ANR-20-CE23-0021)&lt;/li&gt;
&lt;li&gt;DurÃ©e: 3 ans, Ã  partir du 1er mars 2021&lt;/li&gt;
&lt;li&gt;Budget total: 435 kâ‚¬&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;














&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recurrent-processing&#34;&gt;Recurrent processing&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-communicating-by-event-driven-feed-forward-and-feed-back-communications&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/principe_agile.jpg&#34; alt=&#34;Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;consortium&#34;&gt;Consortium:&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/stephane-viollet/avatar.jpg&#34; alt=&#34;SV&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/ryad-benosman/avatar_hu2d5c77d2b84468d5dedad740d610465b_7790_270x270_fill_q75_lanczos_center.jpg&#34; alt=&#34;RB&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/avatar.jpg&#34; alt=&#34;LP&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;StÃ©phane Viollet&lt;/td&gt;
&lt;td&gt;Ryad Benosman&lt;/td&gt;
&lt;td&gt;Laurent Perrinet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julien Diperi&lt;/td&gt;
&lt;td&gt;Sio-HoÃ¯ Ieng&lt;/td&gt;
&lt;td&gt;Emmanuel DaucÃ©&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Inst Sciences Mouvement&lt;/td&gt;
&lt;td&gt;Inst de la Vision&lt;/td&gt;
&lt;td&gt;Inst Neurosci de la Timone&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;gantt-chart-of-project&#34;&gt;Gantt Chart of project&lt;/h2&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/gantt.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022-07-01_grimaldi-22-areadne</title>
      <link>https://laurentperrinet.github.io/slides/2022-07-01_grimaldi-22-areadne/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-07-01_grimaldi-22-areadne/</guid>
      <description>&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/brain-logo-240.jpg&#34; alt=&#34;header&#34; height=&#34;350&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;th&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-07-01_grimaldi-22-areadne&#34;&gt;
		Decoding spiking motifs using neurons with heterogeneous delays
    &lt;!-- &lt;img src=&#34;http://www.cnrs.fr/themes/custom/cnrs/logo.svg&#34; alt=&#34;CNRS&#34; height=&#34;15&#34;&gt; --&gt;
    &lt;!-- &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/thumb/2/2c/CNRS.svg/240px-CNRS.svg.png&#34; alt=&#34;CNRS&#34; height=&#34;40&#34;&gt; --&gt;
    &lt;br&gt;
		&lt;u&gt;[2022-07-01] AREADNE 2022 conference&lt;/u&gt;
	&lt;/a&gt;
	&lt;/th&gt;
	&lt;th&gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;














&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-a-raster-plot&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1a_k.png&#34; alt=&#34;A raster plot..&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A raster plot..
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure--as-a-mixture-of-motifs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1a.png&#34; alt=&#34;.. as a mixture of motifs&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      .. as a mixture of motifs
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure--defined-as-list-of-weights-and-delays&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1b.png&#34; alt=&#34;... defined as list of weights and delays..&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &amp;hellip; defined as list of weights and delays..
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-occurring-from-a-new-raster-plot&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1c.png&#34; alt=&#34;occurring from a new raster plot..&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      occurring from a new raster plot..
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/LIF.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/HSD_conductance_speeds.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-learning&#34;&gt;supervised learning&lt;/h2&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/2022-06-23_Supervised_MC_input_1.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/2022-06-23_Supervised_MC_input_3.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/2022-06-23_Supervised_MC_input_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;









  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-heterogeneous/2022-05-24_Supervised_MC_MC.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;hr&gt;
&lt;h2 id=&#34;learned-heterogeneous-weights&#34;&gt;Learned heterogeneous weights&lt;/h2&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-heterogeneous-delays-as-convolution-kernels&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/2022-06-26_Supervised_nat-causal_kernel.png&#34; alt=&#34;Heterogeneous delays as convolution kernels.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Heterogeneous delays as convolution kernels.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-mask-applied-on-the-weights&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/2022-06-26_Supervised_nat-causal_kernel-mask.png&#34; alt=&#34;Mask applied on the weights.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Mask applied on the weights.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-scatter-of-on-versus-off-weights&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/2022-07-08_Supervised_nat_joint_ON-OFF.png&#34; alt=&#34;Scatter of ON versus OFF weights.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Scatter of ON versus OFF weights.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frugal-computing&#34;&gt;Frugal computing&lt;/h2&gt;














&lt;figure  id=&#34;figure-stable-accuracy-while-pruning-99-weights&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/accuracy.png&#34; alt=&#34;Stable accuracy while pruning ~99% weights.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Stable accuracy while pruning ~99% weights.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://laurentperrinet.github.io/slides/example-slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/example-slides/</guid>
      <description>&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reveal is awesome&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;a--horizontal-slide&#34;&gt;A : Horizontal Slide&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a1--vertical-slide-1&#34;&gt;A.1 : Vertical Slide 1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;a2--vertical-slide-2&#34;&gt;A.2 : Vertical Slide 2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;b--horizontal-slide&#34;&gt;B : Horizontal Slide&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;b1--vertical-slide-1&#34;&gt;B.1 : Vertical Slide 1&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;b2--vertical-slide-2&#34;&gt;B.2 : Vertical Slide 2&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;h2 id=&#34;slide-with-bullets&#34;&gt;Slide with Bullets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;r-markdown&#34;&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you click the &lt;strong&gt;Knit&lt;/strong&gt; button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;slide-with-bullets-1&#34;&gt;Slide with Bullets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bullet 1&lt;/li&gt;
&lt;li&gt;Bullet 2&lt;/li&gt;
&lt;li&gt;Bullet 3&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sub-slide&#34;&gt;sub-slide&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write sub-slide&lt;/li&gt;
&lt;li&gt;Efficiently write sub-slide&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sub-slide-2&#34;&gt;sub-slide 2&lt;/h3&gt;
&lt;p&gt;&lt;video data-autoplay src=&#34;http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;ul&gt;
&lt;li&gt;One&lt;/li&gt;
&lt;/ul&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Two&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;ul&gt;
&lt;li&gt;Three&lt;/li&gt;
&lt;/ul&gt;
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/hulk.png&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/img/hulk.png&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
