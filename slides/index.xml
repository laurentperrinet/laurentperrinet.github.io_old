<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slides | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/slides/</link>
      <atom:link href="https://laurentperrinet.github.io/slides/index.xml" rel="self" type="application/rss+xml" />
    <description>Slides</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Mon, 05 Feb 2024 00:00:00 +0000</lastBuildDate>
    <item>
      <title>2024-02-05-udem.md</title>
      <link>https://laurentperrinet.github.io/slides/2024-02-05-udem/</link>
      <pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2024-02-05-udem/</guid>
      <description>&lt;section&gt;
&lt;h3 id=&#34;neuromorphic-models-of-visionhttpslaurentperrinetgithubioslides2024-02-05-udemtransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2024-02-05-udem/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neuromorphic models of vision&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2024-02-05httpslaurentperrinetgithubiotalk2023-12-01-biocomp-seminar-at-udems-school-of-optometry-montréalhttpsoptoumontrealcaecoleenglishu&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-12-01-biocomp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2024-02-05]&lt;/a&gt; &lt;a href=&#34;https://opto.umontreal.ca/ecole/english/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seminar at UdeM’s School of Optometry, Montréal&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;h4 id=&#34;laurentperrinetuniv-amufrmailtolaurentperrinetuniv-amufr&#34;&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/h4&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;h2 id=&#34;when-brains-meet-computing-machines&#34;&gt;When brains meet computing machines&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Hello&lt;/em&gt;, can you hear me in the back? First of all, I&amp;rsquo;d like to &lt;em&gt;thank&lt;/em&gt; the organizers for this opportunity and all of you for coming.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m Laurent Perrinet from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit, and I&amp;rsquo;m a computational neuroscientist interested in large-scale models of vision. During this seminar for the &amp;ldquo;groupe de recherche de la vision de l&amp;rsquo;UdeM&amp;rdquo;, I&amp;rsquo;ll focus on neuromorphic models by introducing you to &lt;em&gt;event-driven cameras&lt;/em&gt;, a new technology in the field of imaging, and the impact of this technology on our understanding of vision. The &lt;em&gt;outline&lt;/em&gt; of the talk is as follows: first, I will explain the concept of an event-driven camera, especially in comparison to a traditional frame-based camera. Then we&amp;rsquo;ll explore some applications of these cameras using specific algorithms. Finally, we&amp;rsquo;ll look at how our understanding of neuroscience can improve these algorithms.&lt;/p&gt;
&lt;p&gt;Relax, these slides along with a number of references and useful links are available on my website.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;sensing-light&#34;&gt;Sensing light&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  The primary goal of &lt;em&gt;imaging technologies&lt;/em&gt; is to represent a visual signal, i.e. the intensity and color of light as it is distributed across the visual field, in order to create a realistic representation of a visual scene. Let&amp;rsquo;s look at an example.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;http://lepassetempsderose.l.e.pic.centerblog.net/fddea7fb.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  For example, this galloping horse makes us feel like we&amp;rsquo;re seeing this real scene right in front of us. This imaging technique, made possible by the chain of pre-processing from my computer to the projector, appears to move smoothly, but it&amp;rsquo;s actually an &lt;em&gt;illusion&lt;/em&gt; called apparent motion. This is what happens when still images are shown one after another, very quickly, making it appear as if the scene is moving all the time: Our brains interpret these separate images as a single, unified moving scene. This technique is the basis of motion pictures and animation, where frames are displayed quickly enough to create the &lt;em&gt;illusion&lt;/em&gt; of continuous motion. Lowering the frame rate reveals this illusion&amp;hellip;
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&amp;hellip; and this example demonstrates that since numerous years imaging techniques have also opened the door to new scientific discoveries. For example, in the late 19th century, scientists wondered if horses lifted all four hooves off the ground when they galloped. It was too fast for the human eye to see. Eadweard Muybridge solved this mystery using &lt;em&gt;chronophotography&lt;/em&gt;, an early form of photography that captures motion. He took a series of photographs of a horse running and showed that there are moments when all four hooves are in the air. This breakthrough helped us better understand animal movement and paved the way for modern cameras.&lt;/p&gt;
&lt;p&gt;This technique is inspired by the research of [Étienne-Jules &lt;em&gt;Marey&lt;/em&gt;] (&lt;a href=&#34;https://en.wikipedia.org/wiki/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/&lt;/a&gt;Étienne-Jules_Marey), under the term &lt;em&gt;chronophotography&lt;/em&gt;, which is the use of a rifle-like apparatus to photograph a visual scene. This technique allowed Muybridge, in particular, to scientifically demonstrate the mechanism of a horse&amp;rsquo;s gallop. The movie theater became popular only afterwards.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The&amp;#43;Horse&amp;#43;in&amp;#43;Motion,&amp;#43;1878.%C2%A0Eadweard&amp;#43;Muybridge&amp;#43;%28b.&amp;#43;9&amp;#43;April,&amp;#43;1830%29The&amp;#43;first&amp;#43;movie&amp;#43;ever&amp;#43;made,&amp;#43;from&amp;#43;still&amp;#43;photographs..gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The+Horse+in+Motion,+1878.%C2%A0Eadweard+Muybridge+(b.+9+April,+1830)The+first+movie+ever+made,+from+still+photographs..gif&lt;/a&gt;
&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;The use of such dynamic &lt;em&gt;visualization&lt;/em&gt; is crucial in the scientific field, whether in biology or physics, as it allows us to quantify the characteristics of the experiment being conducted, and this is certainly one of the reasons for your presence and an important aspect of your daily work. In the laboratory, for example, we use it in particular to quantify &lt;em&gt;eye movements&lt;/em&gt; when a stimulus is presented to an observer.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;amp;h=600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;h=600&lt;/a&gt;
&lt;a href=&#34;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&lt;/a&gt;
&lt;a href=&#34;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif&#34;&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;representing-light&#34;&gt;Representing light&lt;/h4&gt;
&lt;!-- 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://1.bp.blogspot.com/-odG4Twu0Blc/UrN3ytufKnI/AAAAAAAACRM/dzJNcpV4JfY/s1600/Monty&amp;#43;Python%27s&amp;#43;1.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/movie.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;66%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  To better understand the mechanism behind this technology, let&amp;rsquo;s take a sample video.
Here, I&amp;rsquo;ve taken a grayscale &lt;em&gt;video&lt;/em&gt; from an episode from the Monty Python Flying Circus TV series.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;representing-light-1&#34;&gt;Representing light&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/analog_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &amp;hellip; and we will focus on a &lt;em&gt;single pixel&lt;/em&gt; in the space of the visual field
In this way, we can represent the evolution of the &lt;em&gt;log intensity&lt;/em&gt; of the light signal as a function of time.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;frame-based-camera-temporal-discretization&#34;&gt;Frame-Based Camera: Temporal discretization&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/frame-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  From this representation, expressed in continuous time, we can &lt;em&gt;discretize&lt;/em&gt; time and measure the log intensity at regular time intervals. The difference between two images gives the &lt;em&gt;temporal resolution&lt;/em&gt;, and its inverse gives the number of images per second. This is the representation classically used in chronophotography, but also in all conventional video stream &lt;em&gt;acquisition and viewing&lt;/em&gt; technologies.
This technology is highly efficient for a wide range of signals. However, it does have certain &lt;em&gt;limitations&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;frame-based-camera-temporal-aliasing&#34;&gt;Frame-Based Camera: Temporal Aliasing&lt;/h4&gt;


















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/frames.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  To illustrate a common limitation, let&amp;rsquo;s take the &lt;em&gt;example&lt;/em&gt; of three colored cubes rotating around a circle on a frontal axis. Due to the camera’s temporal resolution and the duration the shutter remains open, the captured images exhibit blur. This makes it challenging to precisely measure the cubes’ movement. As the cubes’ rotation speed increases, we might notice an effect called temporal &lt;em&gt;aliasing&lt;/em&gt;, where the movement appears distorted due to the camera’s limitations.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;frame-based-camera-wagon-wheel-illusion&#34;&gt;Frame-Based Camera: Wagon-Wheel Illusion&lt;/h4&gt;


















&lt;figure  id=&#34;figure-sam-brinson-2020httpswwwsambrinsoncomnature-of-perception&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://vignette.wikia.nocookie.net/revengeristsconsortium/images/2/25/Whee.gif/revision/latest/scale-to-width-down/340?cb=20141209071330&#34; alt=&#34;[[Sam Brinson, 2020](https://www.sambrinson.com/nature-of-perception/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://www.sambrinson.com/nature-of-perception/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sam Brinson, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This phenomenon is particularly striking when we look at a spinning wheel moving at high speed. Sometimes, the wheel spins so fast that in two consecutive images, it appears to rotate backwards. This optical illusion is known as the wagon-wheel illusion. It’s particularly noticeable in car wheels, where the central hub may seem stationary while the wheel itself seems to turn &lt;em&gt;counter&lt;/em&gt; to its actual direction on the road. Again this wagon-wheel effect is due to standard camera&amp;rsquo;s limitations.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-cameras&#34;&gt;Event-Based Cameras&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  Transitioning from conventional frame-based cameras, we now focus on the &lt;em&gt;event-based camera&lt;/em&gt;, a highly promising bio-inspired visual sensor.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-1&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;An event-based camera is equipped with a sensor that converts light into an electrical current, similar to conventional CMOS sensors. However, it differs from standard frame-based cameras in that it is inspired by the human retina. There are two main differences from a frame-based camera (middle graph) that lead to an event-based representation (right graph):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, each pixel of an event-based camera is &lt;em&gt;independent&lt;/em&gt;, operating without a synchronized global clock.&lt;/li&gt;
&lt;li&gt;Second, each pixel detects changes in &lt;em&gt;logarithmic light intensity&lt;/em&gt; and generates a binary event only if the change exceeds a &lt;em&gt;threshold&lt;/em&gt;. If the change is an increment - that is, the log intensity has increased - the event has positive polarity; if it&amp;rsquo;s a decrement, the event has negative polarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, an event is generated asynchronously when a pixel-level change in brightness is detected. This results in superior temporal resolution and reduced susceptibility to motion blur, making event cameras ideal for capturing fast-moving scenes.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-dvs-gesture&#34;&gt;Event-Based Cameras: DVS gesture&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_arm-roll.webp&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_hand-clap.webp&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_air-guitar.webp&#34;  width=&#34;33%&#34;/&gt;&lt;/p&gt;
&lt;!-- 
https://prostheticknowledge.tumblr.com/post/163324909991/dvs128-gesture-dataset-release-from-ibm-research


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34;  width=&#34;33%&#34;/&gt;
&lt;img src=https://user-images.githubusercontent.com/4012178/27771912-cb58ebb8-5f58-11e7-9566-79f3fbc5d9ba.gif&#34;  width=&#34;33%&#34;/&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 

https://github.com/idsc-frazzoli/retina?tab=readme-ov-file
https://user-images.githubusercontent.com/4012178/30553969-2948547a-9ca3-11e7-91e8-159806c7e329.gif --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  Let&amp;rsquo;s take some examples from a classic dataset, DVS gesture. These movements are, for example, clapping hands or playing air guitar. Note that the stream of events is caused by changes in the visual scene, hiding static parts. Let&amp;rsquo;s explain how discrete events are generated in response to the luminous input that continuously evolves over time.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-2&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_0.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Our signal is analog. It consists of the evolution of the log-intensity (y axis) of a single pixel through time (x axis).
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-3&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &amp;hellip; As we follow this trajectory, we can observe that it crosses a threshold. It is at this precise moment that the pixel generates an event. In this case, the event is of positive polarity, since it corresponds to an increase.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-4&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_2.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  The signal then continues its time course and crosses a threshold again, resulting in the production of a new event with positive polarity.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-5&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_5.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  The log-intensity continues to increase, leading to increments, or in other words, positive polarizations.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-6&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_10.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Now the signal decreases, resulting in events with negative polarity instead of positive polarity.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-7&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_20.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Continuing this process, the simple mechanism generates a &lt;em&gt;stream&lt;/em&gt; of events for each pixel, &amp;hellip;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-8&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_-1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &amp;hellip; comprising a &lt;em&gt;list&lt;/em&gt; of occurrence times and their respective polarities.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-9&#34;&gt;Event-Based Cameras&lt;/h4&gt;
&lt;!-- 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Let&amp;rsquo;s now show it applied to the whole analog signal, showing the events below the signal.
It&amp;rsquo;s worth noting that this is particularly &lt;em&gt;sparse&lt;/em&gt; compared to frame-by-frame representations: in particular, a signal with very few changes can be represented by just a few binary events. This is a very useful feature, not only because it saves &lt;em&gt;bandwidth&lt;/em&gt;, but also because it allows us to concentrate the &lt;em&gt;computations&lt;/em&gt; on the few events that represent the image. It&amp;rsquo;s also a fundamental feature of neuron function in the brain. Indeed, neurons communicate sparsely with action potentials, which can be thought of as binary events.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-10&#34;&gt;Event-Based Cameras&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_arm-roll.webp&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_hand-clap.webp&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_air-guitar.webp&#34;  width=&#34;33%&#34;/&gt;&lt;/p&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/events.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;Ultimately, we get a list of events for each pixel that can be &lt;em&gt;merged&lt;/em&gt; to represent the entire image. This list of events includes pixel addresses, times of occurrence, and polarities. Note that since events are generated over time, they are naturally sorted by their time of occurrence. These events are then transmitted in &lt;em&gt;real time&lt;/em&gt; to the output bus, often via a USB3 connection.
It&amp;rsquo;s interesting to draw a parallel between this process and the optic nerve that connects our retina to the brain. In fact, the output of the retina consists of a million ganglion cells that emit action potentials, which are the only source of information transmitted by the &lt;em&gt;optic nerve&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-11&#34;&gt;Event-Based Cameras&lt;/h4&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sensor&lt;/th&gt;
&lt;th&gt;Range&lt;/th&gt;
&lt;th&gt;Framerate&lt;/th&gt;
&lt;th&gt;Resolution&lt;/th&gt;
&lt;th&gt;Power&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Human eye&lt;/td&gt;
&lt;td&gt;60 (?) dB&lt;/td&gt;
&lt;td&gt;300 (?) fps&lt;/td&gt;
&lt;td&gt;100 (?) Mpx&lt;/td&gt;
&lt;td&gt;10 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DSLR&lt;/td&gt;
&lt;td&gt;44.6 dB&lt;/td&gt;
&lt;td&gt;120     fps&lt;/td&gt;
&lt;td&gt;2&amp;ndash;20   Mpx&lt;/td&gt;
&lt;td&gt;30  W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ultra-high speed&lt;/td&gt;
&lt;td&gt;64   dB&lt;/td&gt;
&lt;td&gt;10^4 fps&lt;/td&gt;
&lt;td&gt;0.3&amp;ndash;4  Mpx&lt;/td&gt;
&lt;td&gt;300 W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Event-based&lt;/td&gt;
&lt;td&gt;120  dB&lt;/td&gt;
&lt;td&gt;10^6 fps&lt;/td&gt;
&lt;td&gt;0.1&amp;ndash;2  Mpx&lt;/td&gt;
&lt;td&gt;30 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;Event-driven cameras boast several remarkable properties.
Firstly, their &lt;em&gt;temporal precision&lt;/em&gt; is in the microsecond range, allowing for a theoretical frame rate of up to a million images per second. In contrast, a conventional camera typically captures around a hundred images per second, while a high-speed camera may reach 10,000 images per second. Estimating the sampling frequency of human perception is challenging; although 25 frames per second usually suffice for movies, the human eye can discern temporal details at rates between 300 and 1,000 frames per second.
It’s also noteworthy that the &lt;em&gt;spatial resolution&lt;/em&gt; of event cameras is generally modest, often in the megapixel range. This is not due to technical constraints but rather reflects the cameras’ common technological applications.
Compared with conventional cameras, which will consume several watts, event cameras consume very little electrical &lt;em&gt;energy&lt;/em&gt;, in the order of 10 milliwatts, a consumption equivalent to that of the human eye.
Another key feature is their ability to detect a very wide &lt;em&gt;range&lt;/em&gt; of luminosity, reaching 120 dB, which is a million times greater than conventional cameras and thousand times greater than an human eye.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Event_camera#Functional_description&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Event_camera#Functional_description&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;more in &lt;a href=&#34;https://arxiv.org/pdf/1904.08405.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1904.08405.pdf&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-12&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  But why is detecting a very wide range of luminosity usefull ? The ability to &lt;em&gt;adapt&lt;/em&gt; to changing light conditions can be illustrated by revisiting our analog signal and its event representation. Consider, for example, an autonomous car driving in daylight and then entering and exiting a &lt;em&gt;tunnel&lt;/em&gt;. This scenario involves changes in brightness by a factor of several thousand.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;event-based-cameras-13&#34;&gt;Event-Based Cameras&lt;/h4&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_low.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Here we have a division by a factor 8 of the signal in the middle section. It will be reported by a frame-based camera. In an event-based camera, this is represented here by a &lt;em&gt;sharp decrement&lt;/em&gt; in log intensity space and clearly indicated by events of negative polarity, but we can see that since this is a camera that uses log intensity, dividing the light signal produces the &lt;em&gt;same signal&lt;/em&gt; course over time, and therefore events that are identical.  Event-driven cameras are therefore particularly well-suited to &lt;em&gt;dynamic signals&lt;/em&gt;, where the lighting context can change drastically.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-computer-vision&#34;&gt;Event-Based Computer vision&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;These cameras therefore look very promising for future applications, particularly for embedded applications, but also for applications linked to scientific experiments. However, we can see that the image &lt;em&gt;representation&lt;/em&gt; is completely different, that is, we can no longer consider static images that follow one another at a regular rate, and for which we could have applied the algorithms that have been developed for decades in the field of &lt;em&gt;computer vision&lt;/em&gt;. We end up with a signal that corresponds to events that are transmitted as a stream from the camera. And we have to reinvent all computer vision algorithms to make them &lt;em&gt;event-driven&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;TODO: the process is active driven by the signal compared to acquired&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;always-on-object-recognition-dvs-gesture&#34;&gt;Always-on Object Recognition: DVS gesture&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_arm-roll.webp&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_hand-clap.webp&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/DVSGesture_air-guitar.webp&#34;  width=&#34;33%&#34;/&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  We considered a classification task using a classic camera dataset, involving the classification of 10 different types of human gestures. These movements are, for example, clapping hands or playing air guitar. Note that the stream of events is caused by changes in the visual scene.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;always-on-object-recognition&#34;&gt;Always-on Object Recognition&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/hots.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  So how can we process and learn data coming from an event-based camera ?
My team, including PhD student Antoine Grimaldi, has enhanced an existing algorithm known as &lt;em&gt;HOTS&lt;/em&gt;. This algorithm employs a traditional convolutional and hierarchical structure to process information. It begins with the camera’s event data that are processed three stacked layers, the last layer giving a high-level representation suitable for tasks like digit recognition—for example, identifying the number eight. A key aspect of HOTS is its conversion of event data into multiplexed, parallel channels that mirror the temporal sequence of events, termed the &lt;em&gt;temporal surface&lt;/em&gt;. A temporal surface provides a representation of recent activity, it jumps to one on an event and then exponentially decays through time. Each layer represents these temporal surfaces individually. Notably, the algorithm’s learning process is &lt;em&gt;unsupervised&lt;/em&gt; at every layer, marking a significant advancement over typical deep learning methods that rely on back-propagating classification errors—which is biologically implausible. Building on HOTS, we’ve improved it by incorporated neurobiological insights, particularly the principle of &lt;em&gt;homeostasis&lt;/em&gt;, to better balance the various parallel communication pathways.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;always-on-object-gesture-recognition&#34;&gt;Always-on Object Gesture Recognition&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_offline.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;To demonstrate our algorithm’s effectiveness, we tested it on a standard dataset that I presented before for classifying &lt;em&gt;10 distinct human gestures&lt;/em&gt;, such as clapping, waving, or drumming. With random guessing at 10%, the original HOTS algorithm achieved 70% accuracy after processing all events. However, by adding &lt;em&gt;homeostasis&lt;/em&gt;—an important concept from neuroscience—we enhanced the algorithm’s performance to 82%. This underscores the value of incorporating neuroscientific principles into machine learning. Homeostasis is used to balance the firing rates accross neurons in a neural network. It ensures that all neurons contribute equally over time, avoiding dominance by a few neurons.&lt;/p&gt;
&lt;p&gt;Furthermore, we leveraged a key trait of biological systems: the ability to process information continuously, in real time. Traditional algorithms wait to classify until all events are processed. We innovated by enabling our algorithm to classify on-the-fly, in real-time, with each incoming event. This means that as events occur, they’re instantly processed through the layers, reaching the classification layer without delay.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;always-on-object-gesture-recognition-1&#34;&gt;Always-on Object Gesture Recognition&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_online.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;{&lt;aside class=&#34;notes&#34;&gt;
  What&amp;rsquo;s more interesting is that we were also able to show the &lt;em&gt;evolution&lt;/em&gt; of our algorithm’s average performance relative to the dataset and the number of processed events. The blue curve reveals that with fewer than 10 events, performance hovers at chance levels. However, as more events are processed, we observe a steady improvement. Remarkably, with 10,000 events, &lt;em&gt;performance&lt;/em&gt; matches that of the original algorithm and further excels with an additional tenfold increase in events. A major advantage of this algorithm is that it can be asked to classify the nature of what it sees in real-time, at any point during the event stream —not just after the entire signal is processed. Online processing is essential in biology. For example, imagine you&amp;rsquo;re on the savannah and a &lt;em&gt;lion&lt;/em&gt; jumps out at you. You won&amp;rsquo;t have the time to wait for the video sequence to finish processing before making the right decision, which is to flee.
We’ve also refined our algorithm to select classification events based on precision calculations for each event. By adding a precision &lt;em&gt;threshold&lt;/em&gt;, we achieve high performance with merely a hundred events. This reflects a biological network trait where decisions aren’t made incrementally but rather emerge abruptly here after 200 events — and then continue to improve and stabilize.
&lt;/aside&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  I have therefore illustrated the use of &lt;em&gt;event-driven&lt;/em&gt; cameras on a particular algorithm. The nice feature of this algorithm is that it processes the stream of events from the camera on an event-by-event basis rather than having to wait for the whole video sequence to finish. Each event has the potential to initiate a series of processes across various layers, allowing for the continuous update of classification values. This type of operation is characteristic of the way neurons work in the brain, that is using an event-based representation of information processing. This is what we call &lt;em&gt;spiking neural networks&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-tonic-manualhttpstonicreadthedocsioenlatest_imagesneuron-modelspng&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; alt=&#34;[[Tonic manual](https://tonic.readthedocs.io/en/latest/_images/neuron-models.png)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tonic manual&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Traditional neural networks in deep learning typically rely on an analog representation. This is illustrated in this figure, where various analog inputs are integrated and then processed through a non-linear function to output an analog activation value. This basic &lt;em&gt;perceptron&lt;/em&gt; principle is at the foundation of all existing neural networks, including convolutional networks that excel in image classification. While effective for static images, this method can be resource-intensive for video processing. An alternative is the use of &lt;em&gt;spiking neurons&lt;/em&gt;. Unlike their analog counterparts, spiking neurons process discrete events, which are integrated in the membrane potential. When the membrane potential crosses a theshold, it output an action potential, which can be seen as an event.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-lif-neuron&#34;&gt;Spiking Neural Networks: LIF Neuron&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is illustrated in this &lt;em&gt;animation&lt;/em&gt;, which shows how we can transform a list of input events by giving them different weights, and then &lt;em&gt;integrate&lt;/em&gt; them into the cell&amp;rsquo;s membrane potential. When the membrane potential crosses the spiking theshold, the neuron outputs a spike.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-neuromorphic-hardware&#34;&gt;Spiking Neural Networks: neuromorphic hardware&lt;/h4&gt;


















&lt;figure  id=&#34;figure-loihi-2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg&#34; alt=&#34;Loihi 2&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Loihi 2
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;The introduction of spiking neural networks marks a &lt;em&gt;paradigm shift&lt;/em&gt; in computation, in the same way that event-driven cameras have brought a paradigm shift in image representation. These spiking neural networks have led to the creation of innovative algorithms and the development of neuromorphic chips like Intel’s Loihi 2. This chip departs from traditional computing by utilizing a massively parallel array of event-driven processing units. As with event-driven cameras, this has the dual advantage of being very fast and consuming very little energy. The field continues to advance, with new neuromorphic chips being developed that could potentially replace standard CPUs and GPUs.&lt;/p&gt;
&lt;figure  id=&#34;figure-propheseehttpsdocspropheseeaistableconceptshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; alt=&#34;[Prophesee](https://docs.prophesee.ai/stable/concepts.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://docs.prophesee.ai/stable/concepts.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prophesee&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Loihi: &lt;a href=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;amp;strip=none&amp;amp;ssl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;strip=none&amp;ssl=1&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h4&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://i.stack.imgur.com/ixnrz.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
Spiking neural networks show great potential for processing data from event-driven cameras. However, &lt;em&gt;neurophysiology&lt;/em&gt; studies reveal some unexpected behaviors, very different from the classical perceptron. I will highlight these differences with three examples. The first example is a 1995 study by Mainen and Sejnowski examined a neuron’s reaction to repeated stimulations.
&lt;em&gt;Panel A&lt;/em&gt; at the top presents the neuron’s response to multiple stimulations with a 200 picoampere &lt;em&gt;current step&lt;/em&gt;. The membrane potential varied across trials, indicating an unpredictable response. Initially, the spikes were synchronized at the onset of stimulation, but coherence diminished over time, leading to no alignment after approximately 750 milliseconds.
In contrast, Panel B at the botton shows the neuron’s response to stimulation with &lt;em&gt;noise&lt;/em&gt;. Here, the neuron exhibited highly consistent responses across trials, with membrane potential traces nearly identical. This precision was achieved using &lt;em&gt;frozen&lt;/em&gt; noise, a repeated, unchanging stimulus. The study highlights that neurons are less responsive to constant analog values, such as square pulses, and more selective to dynamic signals, responding with remarkable precision in the temporal domain.
&lt;/aside&gt;
&lt;!-- 
---


#### Spiking Neural Networks in neurobiology



















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;



&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt; --&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h4&gt;


















&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this second example, I show a simulation reproducing the 1999 paper by Diesmann and colleagues. This &lt;em&gt;theoretical model&lt;/em&gt; considers ten interconnected groups, each comprising 100 neurons. Each group is connected to the next one. A key finding is that information transfer across groups depends on the temporal concentration of spikes. Initially, information is too scattered within the first group, leading to a dilution effect in subsequent groups. However, once a threshold is reached, a cluster of synchronous spikes ensures efficient propagation through the network. This non-linear dynamic is characteristic of spiking neural networks, adding a layer of richness, but also a cerain complexity.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h4&gt;


















&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
A third example shows an experiment conducted by Rosa Cossart&amp;rsquo;s group at INMED and recently published by Haimerl and colleagues. They used &lt;em&gt;calcium fluorescence&lt;/em&gt; imaging to track neuronal activity in mice. By arranging the neurons in &lt;em&gt;temporal order of activation&lt;/em&gt;, it shows a sequential activation of these neurons, a mechanism which resembles the model mentioned earlier. These patterns closely align with the mouse’s motor behavior, as depicted in the accompanying graph. Notably, these activity sequences remained consistent, even when recorded on the &lt;em&gt;next day&lt;/em&gt;, underscoring the importance of temporal dynamics in neural computation.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These observations have led us to &lt;em&gt;review&lt;/em&gt; neurobiological evidence of neurons encoding information based on the relative timing of spikes. Intriguingly, the conduction &lt;em&gt;delays&lt;/em&gt; observed in spike transmission are not merely obstacles. Instead, they could be used to enhance information representation and processing through &lt;em&gt;spiking motifs&lt;/em&gt;. This perspective challenges traditional views and opens up new possibilities for understanding information representation, processing and learning.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
Consider an ultra-simplified neural network with three presynaptic neurons and two output neurons, connected by &lt;em&gt;heterogeneous&lt;/em&gt; delays. With synchronous inputs, the output neurons activate at different times, failing to reach the threshold for an output spike. However, if the delays align the action potentials to arrive simultaneously, the combined input can trigger an output spike at the &lt;em&gt;same instant&lt;/em&gt;, as indicated by the red bar.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h4&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To better grasp this mechanism, let’s revisit the animation of a spiking neuron. Without delays, action potentials reach the neuron’s cell body immediately, where they’re integrated to potentially trigger a spike.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-spiking-motifs-3&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h4&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
Now using &lt;em&gt;heterogeneous&lt;/em&gt; delays, the timing of spike arrival at the cell body varies. Introducing a specific &lt;em&gt;spiking motif&lt;/em&gt;, marked by green action potentials, allows these spikes to converge simultaneously due to the delays. This synchronicity results in the neuron generating a new spike.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-hd-snn&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h4&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
In applying this theoretical principle, we developed an algorithm to detect movement in images. We began by simulating event data from natural images set in motion along paths similar to those observed during free visual exploration. The event-driven output exhibits distinct characteristics. For instance, rapid movement results in a higher spike rate. Conversely, edges aligned with the motion direction yield minimal changes, leading to fewer spikes. This phenomenon is known as the aperture problem.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-hd-snn-1&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/laurentperrinet/figures/7f382a8074552de1a6a0c5728c60d48788b5a9f8/animated_neurons/conv_HDSNN.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We then used a neural network with a classical architecture, which we enhanced by using an spike representation that accounts for various synaptic delays values. In this figure, the input is on the left grid, indicating spikes of either positive or negative polarity. This input is processed through multiple channels, represented by green and orange, and generate membrane activity. This activity, in turn, led to the production of output spikes, particularly in synaptic connection nuclei with heterogeneous delays. These delays are key to identifying specific spatio-temporal patterns.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-hd-snn-2&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
A key advantage of this network is its differentiability, which allows the application of traditional machine learning techniques, such as supervised learning.
We then see the emergence of various convolution kernels. The graph on the left, marked by red arrows, displays a selection of these kernels oriented in different directions.
It shows the kernels obtained on the spatial representation according to the different columns, and each row represents the different delays from a delay of one on the right to a delay of 12 time steps on the left. Detectors that follow the motion emerge. For example, for the top line from top to bottom. These kernels integrate both positive neurons in red and negative polarity inputs in blue.
vim Such spatio-temporal filtering is observed in neurobiology, but to my knowledge had never been observed in a model of spiking neurons trained under natural conditions.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-hd-snn-3&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_raw.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will now study the performance of this network in detecting motion in the flow of events entering the network. When we use all the weights of the convolution kernel, we get a very good performance of the order of 99%, represented by the black dot in the top right-hand corner. Note that in the kernels we&amp;rsquo;ve seen emerge, most of the synaptic weights are close to zero, so we might consider removing some of these weights, as this can be shown to reduce the number of event calculations required.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-hd-snn-4&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_shortening.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
This is what we&amp;rsquo;ve done, by first removing the parts of the core corresponding to the longest delays. This &amp;ldquo;shortens&amp;rdquo; the kernel. We quickly observed a degradation in performance, which reached half-saturation when we reduced the number of weights by around 50%. This demonstrates the importance of integrating information that is quite distant and structured over time.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h4 id=&#34;spiking-neural-networks-hd-snn-5&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h4&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a second step, we performed a pruning operation, which consists in progressively removing the weights that are the weakest. This time, performance remains optimal over a wide compression range, and we reach half-saturation when we have removed around 99.8% of the weights. This means that the network is able to maintain very good performance, even when only one weight out of 600 has been kept, and therefore, with a computation time increased by a factor of 600. This property, which we didn&amp;rsquo;t expect, seems promising for creating machine learning algorithms that are less energy-hungry.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h3 id=&#34;neuromorphic-models-of-visionhttpslaurentperrinetgithubioslides2024-02-05-udemtransitionfade-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2024-02-05-udem/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neuromorphic models of vision&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2024-02-05httpslaurentperrinetgithubiotalk2023-12-01-biocomp-seminar-at-udems-school-of-optometry-montréalhttpsoptoumontrealcaecoleenglishu-1&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-12-01-biocomp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2024-02-05]&lt;/a&gt; &lt;a href=&#34;https://opto.umontreal.ca/ecole/english/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seminar at UdeM’s School of Optometry, Montréal&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;h4 id=&#34;laurentperrinetuniv-amufrmailtolaurentperrinetuniv-amufr-1&#34;&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/h4&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;In conclusion, we have seen that event-driven cameras open the door to new applications that mimic the performance of the human eye, in terms of computational dynamics, adaptation to light conditions and energy constraints. This technological development has recently been accompanied by the development of neuromorphic chips and innovative algorithms in the form of spiking neural networks. However, there is still a great deal of progress to be made at theoretical level, particularly in the understanding of these spiking neural networks, and we have shown the potential progress that can be made by exploiting the richness of temporal representations, particularly by taking advantage of heterogeneous delays.
Beyond these particular applications to natural image processing, I hope to have succeeded in demonstrating the importance of cross-fertilizing the field of engineering applications in general with biological neuroscience. This new line of research - known as NeuroAI or, more generally, as computational neuroscience - is likely to develop over the next few years. Thank you for your attention.&lt;/p&gt;
&lt;p&gt;To conclude, we&amp;rsquo;ve explored how event-driven cameras pave the way for new applications. These applications mirror the human eye&amp;rsquo;s performance in terms of computational dynamics, rapid light condition adaptation, and energy efficiency. This tech advancement is complemented by the emergence of neuromorphic chips and innovative algorithms, specifically spiking neural networks. These networks emulate biological neurons, which communicate through binary events known as spikes rather than analog values used in traditionnal neural networks.&lt;/p&gt;
&lt;p&gt;Despite these advancements, there&amp;rsquo;s still much to learn, especially in understanding how spiking neural networks process information. I hope I&amp;rsquo;ve successfully highlighted the importance of integrating engineering applications with neuroscience. This emerging research area, known as NeuroAI or computational neuroscience, is evolving rapidly. The ultimate aim of NeuroAI is to emulate the brain’s performance: it’s like having the computational power of a supercomputer compacted into the size of a soccer ball, using only around 20W of power, which is comparable to the energy consumption of a light bulb.
This emerging research area, known as NeuroAI or computational neuroscience, is set&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>2023-12-14-jraf.md</title>
      <link>https://laurentperrinet.github.io/slides/2023-12-14-jraf/</link>
      <pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-12-14-jraf/</guid>
      <description>&lt;section&gt;
&lt;h1 id=&#34;event-based-visionhttpslaurentperrinetgithubioslides2023-12-14-jraftransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-12-14-jraf/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event-based vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;adrien-fois--laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adrien Fois &amp;amp; Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-12-14httpslaurentperrinetgithubiotalk2023-12-14-jraf-journées-sur-lapprentissage-frugal-jraf-httpsjraf-2023sciencesconforgu&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-12-14-jraf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-12-14]&lt;/a&gt; &lt;a href=&#34;https://jraf-2023.sciencesconf.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Journées sur l&amp;rsquo;apprentissage frugal (JRAF) &lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;mailto:adrien.fois@univ-amu.fr&#34;&gt;adrien.fois@univ-amu.fr&lt;/a&gt;
&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;em&gt;Hello&lt;/em&gt;, can you hear me in the back?&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m Adrien Fois from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit. I&amp;rsquo;m a post-doctoral researcher under the supervision of Laurent Perrinet, and during this seminar, I&amp;rsquo;ll be presenting &lt;em&gt;event-driven cameras&lt;/em&gt;. This innovative imaging technology and its influence on our understanding of vision will be our focus. I&amp;rsquo;d like to &lt;em&gt;thank&lt;/em&gt; organizers for this opportunity, and all of you for coming. You can find these slides and related references on Laurent Perrinet’s website. The &lt;em&gt;outline&lt;/em&gt; of the talk is as follows: initially, I will explain the concept of an event-driven camera, especially in comparison to a traditional frame-based camera. Following that, we’ll explore some applications of these cameras using specific algorithms. Lastly, we’ll delve into how our understanding of neuroscience can enhance these algorithms.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;sensing-light&#34;&gt;Sensing light&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  First of all, the objective of &lt;em&gt;imaging&lt;/em&gt; is to represent a visual signal, which includes luminous intensity and color, distributed over the visual field to create a realistic representation of a visual scene.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;http://lepassetempsderose.l.e.pic.centerblog.net/fddea7fb.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  Imaging gives us the feeling that we’re seeing a scene right in front of us. For example, this galloping horse seems to move smoothly, but it’s actually an &lt;em&gt;illusion&lt;/em&gt; called apparent motion. This happens when still images are shown one after another, very quickly, making it look like the scene is moving. Our brains interpret these separate images as a single, moving scene. This technique is the foundation of motion pictures and animation, where frames are displayed quickly enough to give the &lt;em&gt;illusion&lt;/em&gt; of fluid motion.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  Imaging techniques have also opened doors to new scientific discoveries. For example, back in the late 19th century, scientists wondered if horses lifted all four hooves off the ground when they galloped. It was too fast for our eyes to see. Eadweard Muybridge solved this puzzle using &lt;em&gt;chronophotography&lt;/em&gt;, an early form of photography that captures movement. He took a series of photos of a running horse and showed that, yes, there are moments when all four hooves are in the air. This breakthrough helped us understand animal movement better and paved the way for modern cameras.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representing-spatio-temporal-luminous-information&#34;&gt;Representing spatio-temporal luminous information&lt;/h2&gt;
&lt;!-- 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://1.bp.blogspot.com/-odG4Twu0Blc/UrN3ytufKnI/AAAAAAAACRM/dzJNcpV4JfY/s1600/Monty&amp;#43;Python%27s&amp;#43;1.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/movie.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;66%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  To better understand the mechanism behind this technology, let&amp;rsquo;s take a sample video.
Here, I&amp;rsquo;ve taken a grayscale &lt;em&gt;video&lt;/em&gt; from an episode from the Monty Python Flying Circus TV series.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representing-spatio-temporal-luminous-information-1&#34;&gt;Representing spatio-temporal luminous information&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/analog_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&amp;hellip; and we will focus on a &lt;em&gt;single pixel&lt;/em&gt; in the space of the visual field
In this way, we can represent the evolution of the &lt;em&gt;log intensity&lt;/em&gt; of the light signal as a function of time.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The&amp;#43;Horse&amp;#43;in&amp;#43;Motion,&amp;#43;1878.%C2%A0Eadweard&amp;#43;Muybridge&amp;#43;%28b.&amp;#43;9&amp;#43;April,&amp;#43;1830%29The&amp;#43;first&amp;#43;movie&amp;#43;ever&amp;#43;made,&amp;#43;from&amp;#43;still&amp;#43;photographs..gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The+Horse+in+Motion,+1878.%C2%A0Eadweard+Muybridge+(b.+9+April,+1830)The+first+movie+ever+made,+from+still+photographs..gif&lt;/a&gt;
&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&lt;/a&gt;
&lt;a href=&#34;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;amp;h=600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;h=600&lt;/a&gt;
&lt;a href=&#34;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&lt;/a&gt;
&lt;a href=&#34;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif&#34;&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-temporal-discretization&#34;&gt;Frame-Based Camera: Temporal discretization&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/frame-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  From this representation, expressed in continuous time, we can &lt;em&gt;discretize&lt;/em&gt; time and measure the log intensity at regular time intervals. The difference between two images gives the &lt;em&gt;temporal resolution&lt;/em&gt;, and its inverse gives the number of images per second. This is the representation classically used in chronophotography, but also in all conventional video stream &lt;em&gt;acquisition and viewing&lt;/em&gt; technologies.
This technology is highly efficient for a wide range of signals. However, it does have certain &lt;em&gt;limitations&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-aliasing&#34;&gt;Frame-Based Camera: Aliasing&lt;/h2&gt;


















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/frames.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;85%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  To illustrate a common limitation, let&amp;rsquo;s take the &lt;em&gt;example&lt;/em&gt; of three colored cubes rotating around a circle on a frontal axis. Due to the camera’s temporal resolution and the duration the shutter remains open, the captured images exhibit blur. This makes it challenging to precisely measure the cubes’ movement. As the cubes’ rotation speed increases, we might notice an effect called temporal &lt;em&gt;aliasing&lt;/em&gt;, where the movement appears distorted due to the camera’s limitations.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-wagon-wheel-illusion&#34;&gt;Frame-Based Camera: Wagon-Wheel Illusion&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sam-brinson-2020httpswwwsambrinsoncomnature-of-perception&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://vignette.wikia.nocookie.net/revengeristsconsortium/images/2/25/Whee.gif/revision/latest/scale-to-width-down/340?cb=20141209071330&#34; alt=&#34;[[Sam Brinson, 2020](https://www.sambrinson.com/nature-of-perception/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://www.sambrinson.com/nature-of-perception/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sam Brinson, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This phenomenon is particularly striking when we look at a spinning wheel moving at high speed. Sometimes, the wheel spins so fast that in two consecutive images, it appears to rotate backwards. This optical illusion is known as the wagon-wheel illusion. It’s particularly noticeable in car wheels, where the central hub may seem stationary while the wheel itself seems to turn &lt;em&gt;counter&lt;/em&gt; to its actual direction on the road. Again this wagon-wheel effect is due to standard camera&amp;rsquo;s limitations.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-camera&#34;&gt;Event-Based Camera&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  Transitioning from conventional frame-based cameras, we now focus on the &lt;em&gt;event camera&lt;/em&gt;, a highly promising bio-inspired visual sensor.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-1&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;An event-based camera is equipped with a sensor that, much like common CMOS sensors, converts light into electrical current. Yet, it stands apart from standard frame-based cameras by taking inspiration from the human retina. There are two main differences with respect to frame-based camera:
Firstly each pixel of an event-based camera is &lt;em&gt;independent&lt;/em&gt;, functioning without a synchonized global clock.
Secondly, each pixel detect shifts in &lt;em&gt;logarithmic light intensity&lt;/em&gt;, generating an binary event only when the change exceeds a &lt;em&gt;threshold&lt;/em&gt;. If the change is an increment - meaning the log intensity increased - the event has positive polarity; if it&amp;rsquo;s a decrement, the event has negative polarity.&lt;/p&gt;
&lt;p&gt;In summary, an event is asynchronously generated when a pixel-level change in brightness is detected. This leads to a superior temporal resolution and a reduced susceptibility to motion blur, making event-camera ideal for capturing fast-moving scenes. Now, let’s explain how discrete events are produced in response to an analog signal that evolves continuously over time.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-2&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_0.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Our signal is analog. It consists of the evolution of the log-intensity (y axis) of a single pixel through time (x axis).
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-3&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &amp;hellip; And we can observe that it crosses a threshold. At this precise time, the pixel generates an event. In this case, the event is of positive polarity, as it corresponds to an increase.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-4&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_2.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Then, the signal continue its course in time and cross a threshold again, resulting in the production of a new event with positive polarity.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-5&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_5.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  The log-intensity continues to increase, leading to increments, or in other words, positive polarizations.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-6&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_10.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Now the signal decreases, resulting in events with negative polarity instead of positive polarity.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-7&#34;&gt;Event-Based Camera&lt;/h2&gt;
&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_20.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Continuing this process, the simple mechanism generates a &lt;em&gt;stream&lt;/em&gt; of events for each pixel, comprising a &lt;em&gt;list&lt;/em&gt; of occurrence times and their respective polarities.
&lt;/aside&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-8&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_-1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-9&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Let&amp;rsquo;s show it now applied to the whole analog signal.
It&amp;rsquo;s worth noting in particular that, compared with frame-by-frame representations, this one is particularly &lt;em&gt;sparse&lt;/em&gt;: in particular, a signal with very few changes can be represented by just a few binary events. This is a very useful feature, not only because it saves &lt;em&gt;bandwidth&lt;/em&gt;, but also because it allows us to concentrate the &lt;em&gt;computations&lt;/em&gt; around the few events that represent the image. It&amp;rsquo;s also a fundamental feature of neuron function in the brain. Indeed neurons communicate sparsely with action potentials that can be seen as binary events.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-10&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;!-- 

















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/events.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;Ultimately, we obtain a list of events for each pixels which can be &lt;em&gt;merged&lt;/em&gt; to represent the entire image. This list of events includes pixel addresses, times of occurrence and polarities. Note that as events are generated over time, they are naturally sorted by their time of occurences. These events are then transmitted in &lt;em&gt;real-time&lt;/em&gt; to the output bus, often through a USB3 connection.
It’s interesting to draw a parallel between this process and the optic nerve, which connects our retina to the brain. In fact, the retina’s output is composed of a million ganglion cells that emit action potentials, constituting the only source of information transmitted through the &lt;em&gt;optic nerve&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-11&#34;&gt;Event-Based Camera&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sensor&lt;/th&gt;
&lt;th&gt;Range&lt;/th&gt;
&lt;th&gt;Framerate&lt;/th&gt;
&lt;th&gt;Resolution&lt;/th&gt;
&lt;th&gt;Power&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Human eye&lt;/td&gt;
&lt;td&gt;60 (?) dB&lt;/td&gt;
&lt;td&gt;300 (?) fps&lt;/td&gt;
&lt;td&gt;100 (?) Mpx&lt;/td&gt;
&lt;td&gt;10 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DSLR&lt;/td&gt;
&lt;td&gt;44.6 dB&lt;/td&gt;
&lt;td&gt;120     fps&lt;/td&gt;
&lt;td&gt;2&amp;ndash;20   Mpx&lt;/td&gt;
&lt;td&gt;30  W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ultra-high speed&lt;/td&gt;
&lt;td&gt;64   dB&lt;/td&gt;
&lt;td&gt;10^4 fps&lt;/td&gt;
&lt;td&gt;0.3&amp;ndash;4  Mpx&lt;/td&gt;
&lt;td&gt;300 W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Event-based&lt;/td&gt;
&lt;td&gt;120  dB&lt;/td&gt;
&lt;td&gt;10^6 fps&lt;/td&gt;
&lt;td&gt;0.1&amp;ndash;2  Mpx&lt;/td&gt;
&lt;td&gt;30 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;Event-driven cameras boast several remarkable properties.
Firstly, their &lt;em&gt;temporal precision&lt;/em&gt; is in the microsecond range, allowing for a theoretical frame rate of up to a million images per second. In contrast, a conventional camera typically captures around a hundred images per second, while a high-speed camera may reach 10,000 images per second. Estimating the sampling frequency of human perception is challenging; although 25 frames per second usually suffice for movies, the human eye can discern temporal details at rates between 300 and 1,000 frames per second.
It’s also noteworthy that the &lt;em&gt;spatial resolution&lt;/em&gt; of event cameras is generally modest, often in the megapixel range. This is not due to technical constraints but rather reflects the cameras’ common technological applications.
Compared with conventional cameras, which will consume several watts, event cameras consume very little electrical &lt;em&gt;energy&lt;/em&gt;, in the order of 10 milliwatts, a consumption equivalent to that of the human eye.
Another key feature is their ability to detect a very wide &lt;em&gt;range&lt;/em&gt; of luminosity, reaching 120 dB, which is a million times greater than conventional cameras and thousand times greater than an human eye.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Event_camera#Functional_description&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Event_camera#Functional_description&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;more in &lt;a href=&#34;https://arxiv.org/pdf/1904.08405.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1904.08405.pdf&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-12&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  But why is detecting a very wide range of luminosity usefull ? The ability to &lt;em&gt;adapt&lt;/em&gt; to changing light conditions can be illustrated by revisiting our analog signal and its event representation. Consider, for example, an autonomous car driving in daylight and then entering and exiting a &lt;em&gt;tunnel&lt;/em&gt;. This scenario involves changes in brightness by a factor of several thousand.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-13&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_low.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Here we have a division by a factor 8 of the signal in the middle section. It will be reported by a frame-based camera. In an event-based camera, this is represented here by a &lt;em&gt;sharp decrement&lt;/em&gt; in log intensity space and clearly indicated by events of negative polarity, but we can see that since this is a camera that uses log intensity, dividing the light signal produces the &lt;em&gt;same signal&lt;/em&gt; course over time, and therefore events that are identical.  Event-driven cameras are therefore particularly well-suited to &lt;em&gt;dynamic signals&lt;/em&gt;, where the lighting context can change drastically.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-computer-vision&#34;&gt;Event-Based Computer vision&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;These cameras therefore look very promising for future applications, particularly for embedded applications, but also for applications linked to scientific experiments. However, we can see that the image &lt;em&gt;representation&lt;/em&gt; is completely different, that is, we can no longer consider static images that follow one another at a regular rate, and for which we could have applied the algorithms that have been developed for decades in the field of &lt;em&gt;computer vision&lt;/em&gt;. We end up with a signal that corresponds to events that are transmitted as a stream from the camera. And we have to reinvent all computer vision algorithms to make them &lt;em&gt;event-driven&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;TODO: the process is active driven by the signal compared to acquired&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-recognition-dvs-gesture&#34;&gt;Always-on Object Recognition: DVS gesture&lt;/h2&gt;
&lt;!-- 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;
&lt;p&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34;  width=&#34;33%&#34;/&gt;&lt;/p&gt;
&lt;!-- !&#34;&#34; width=&#34;33%&#34; &gt;}}

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  We considered a classification task using a classic camera dataset, involving the classification of 10 different types of human gestures. These movements are, for example, clapping hands or playing air guitar. Note that the stream of events is caused by changes in the visual scene.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-recognition&#34;&gt;Always-on Object Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/hots.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  So how can we process and learn data coming from an event-based camera ?
My team, including PhD student Antoine Grimaldi, has enhanced an existing algorithm known as &lt;em&gt;HOTS&lt;/em&gt;. This algorithm employs a traditional convolutional and hierarchical structure to process information. It begins with the camera’s event data that are processed three stacked layers, the last layer giving a high-level representation suitable for tasks like digit recognition—for example, identifying the number eight. A key aspect of HOTS is its conversion of event data into multiplexed, parallel channels that mirror the temporal sequence of events, termed the &lt;em&gt;temporal surface&lt;/em&gt;. A temporal surface provides a representation of recent activity, it jumps to one on an event and then exponentially decays through time. Each layer represents these temporal surfaces individually. Notably, the algorithm’s learning process is &lt;em&gt;unsupervised&lt;/em&gt; at every layer, marking a significant advancement over typical deep learning methods that rely on back-propagating classification errors—which is biologically implausible. Building on HOTS, we’ve improved it by incorporated neurobiological insights, particularly the principle of &lt;em&gt;homeostasis&lt;/em&gt;, to better balance the various parallel communication pathways.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-gesture-recognition&#34;&gt;Always-on Object Gesture Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_offline.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;To demonstrate our algorithm’s effectiveness, we tested it on a standard dataset that I presented before for classifying &lt;em&gt;10 distinct human gestures&lt;/em&gt;, such as clapping, waving, or drumming. With random guessing at 10%, the original HOTS algorithm achieved 70% accuracy after processing all events. However, by adding &lt;em&gt;homeostasis&lt;/em&gt;—an important concept from neuroscience—we enhanced the algorithm’s performance to 82%. This underscores the value of incorporating neuroscientific principles into machine learning. Homeostasis is used to balance the firing rates accross neurons in a neural network. It ensures that all neurons contribute equally over time, avoiding dominance by a few neurons.&lt;/p&gt;
&lt;p&gt;Furthermore, we leveraged a key trait of biological systems: the ability to process information continuously, in real time. Traditional algorithms wait to classify until all events are processed. We innovated by enabling our algorithm to classify on-the-fly, in real-time, with each incoming event. This means that as events occur, they’re instantly processed through the layers, reaching the classification layer without delay.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-gesture-recognition-1&#34;&gt;Always-on Object Gesture Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_online.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  What&amp;rsquo;s more interesting is that we were also able to show the &lt;em&gt;evolution&lt;/em&gt; of our algorithm’s average performance relative to the dataset and the number of processed events. The blue curve reveals that with fewer than 10 events, performance hovers at chance levels. However, as more events are processed, we observe a steady improvement. Remarkably, with 10,000 events, &lt;em&gt;performance&lt;/em&gt; matches that of the original algorithm and further excels with an additional tenfold increase in events. A major advantage of this algorithm is that it can be asked to classify the nature of what it sees in real-time, at any point during the event stream —not just after the entire signal is processed. Online processing is essential in biology. For example, imagine you&amp;rsquo;re on the savannah and a &lt;em&gt;lion&lt;/em&gt; jumps out at you. You won&amp;rsquo;t have the time to wait for the video sequence to finish processing before making the right decision, which is to flee.
We’ve also refined our algorithm to select classification events based on precision calculations for each event. By adding a precision &lt;em&gt;threshold&lt;/em&gt;, we achieve high performance with merely a hundred events. This reflects a biological network trait where decisions aren’t made incrementally but rather emerge abruptly here after 200 events — and then continue to improve and stabilize.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-visionhttpslaurentperrinetgithubioslides2023-12-14-jraftransitionfade-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-12-14-jraf/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event-based vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;adrien-fois--laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adrien Fois &amp;amp; Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-12-14httpslaurentperrinetgithubiotalk2023-12-14-jraf-journées-sur-lapprentissage-frugal-jraf-httpsjraf-2023sciencesconforgu-1&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-12-14-jraf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-12-14]&lt;/a&gt; &lt;a href=&#34;https://jraf-2023.sciencesconf.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Journées sur l&amp;rsquo;apprentissage frugal (JRAF) &lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;mailto:adrien.fois@univ-amu.fr&#34;&gt;adrien.fois@univ-amu.fr&lt;/a&gt;
&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;To conclude, we&amp;rsquo;ve explored how event-driven cameras pave the way for new applications. These applications mirror the human eye&amp;rsquo;s performance in terms of computational dynamics, rapid light condition adaptation, and energy efficiency. This tech advancement is complemented by the emergence of neuromorphic chips and innovative algorithms, specifically spiking neural networks. These networks emulate biological neurons, which communicate through binary events known as spikes rather than analog values used in traditionnal neural networks.&lt;/p&gt;
&lt;p&gt;Despite these advancements, there&amp;rsquo;s still much to learn, especially in understanding how spiking neural networks process information. I hope I&amp;rsquo;ve successfully highlighted the importance of integrating engineering applications with neuroscience. This emerging research area, known as NeuroAI or computational neuroscience, is evolving rapidly. The ultimate aim of NeuroAI is to emulate the brain’s performance: it’s like having the computational power of a supercomputer compacted into the size of a soccer ball, using only around 20W of power, which is comparable to the energy consumption of a light bulb.
This emerging research area, known as NeuroAI or computational neuroscience, is set to evolve in the coming years. Thank you for your attention.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  I have therefore illustrated the use of &lt;em&gt;event-driven&lt;/em&gt; cameras on a particular algorithm. The nice feature of this algorithm is that it processes the stream of events from the camera on an event-by-event basis rather than having to wait for the whole video sequence to finish. Each event has the potential to initiate a series of processes across various layers, allowing for the continuous update of classification values. This type of operation is characteristic of the way neurons work in the brain, that is using an event-based representation of information processing. This is what we call &lt;em&gt;spiking neural networks&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-tonic-manualhttpstonicreadthedocsioenlatest_imagesneuron-modelspng&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; alt=&#34;[[Tonic manual](https://tonic.readthedocs.io/en/latest/_images/neuron-models.png)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tonic manual&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Traditional neural networks in deep learning typically rely on an analog representation. This is illustrated in this figure, where various analog inputs are integrated and then processed through a non-linear function to output an analog activation value. This basic &lt;em&gt;perceptron&lt;/em&gt; principle is at the foundation of all existing neural networks, including convolutional networks that excel in image classification. While effective for static images, this method can be resource-intensive for video processing. An alternative is the use of &lt;em&gt;spiking neurons&lt;/em&gt;. Unlike their analog counterparts, spiking neurons process discrete events, which are integrated in the membrane potential. When the membrane potential crosses a theshold, it output an action potential, which can be seen as an event.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-lif-neuron&#34;&gt;Spiking Neural Networks: LIF Neuron&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is illustrated in this &lt;em&gt;animation&lt;/em&gt;, which shows how we can transform a list of input events by giving them different weights, and then &lt;em&gt;integrate&lt;/em&gt; them into the cell&amp;rsquo;s membrane potential. When the membrane potential crosses the spiking theshold, the neuron outputs a spike.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-neuromorphic-hardware&#34;&gt;Spiking Neural Networks: neuromorphic hardware&lt;/h2&gt;


















&lt;figure  id=&#34;figure-loihi-2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg&#34; alt=&#34;Loihi 2&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Loihi 2
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;The introduction of spiking neural networks marks a &lt;em&gt;paradigm shift&lt;/em&gt; in computation, in the same way that event-driven cameras have brought a paradigm shift in image representation. These spiking neural networks have led to the creation of innovative algorithms and the development of neuromorphic chips like Intel’s Loihi 2. This chip departs from traditional computing by utilizing a massively parallel array of event-driven processing units. As with event-driven cameras, this has the dual advantage of being very fast and consuming very little energy. The field continues to advance, with new neuromorphic chips being developed that could potentially replace standard CPUs and GPUs.&lt;/p&gt;
&lt;figure  id=&#34;figure-propheseehttpsdocspropheseeaistableconceptshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; alt=&#34;[Prophesee](https://docs.prophesee.ai/stable/concepts.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://docs.prophesee.ai/stable/concepts.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prophesee&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Loihi: &lt;a href=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;amp;strip=none&amp;amp;ssl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;strip=none&amp;ssl=1&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://i.stack.imgur.com/ixnrz.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
Spiking neural networks show great potential for processing data from event-driven cameras. However, &lt;em&gt;neurophysiology&lt;/em&gt; studies reveal some unexpected behaviors, very different from the classical perceptron. I will highlight these differences with three examples. The first example is a 1995 study by Mainen and Sejnowski examined a neuron’s reaction to repeated stimulations.
&lt;em&gt;Panel A&lt;/em&gt; at the top presents the neuron’s response to multiple stimulations with a 200 picoampere &lt;em&gt;current step&lt;/em&gt;. The membrane potential varied across trials, indicating an unpredictable response. Initially, the spikes were synchronized at the onset of stimulation, but coherence diminished over time, leading to no alignment after approximately 750 milliseconds.
In contrast, Panel B at the botton shows the neuron’s response to stimulation with &lt;em&gt;noise&lt;/em&gt;. Here, the neuron exhibited highly consistent responses across trials, with membrane potential traces nearly identical. This precision was achieved using &lt;em&gt;frozen&lt;/em&gt; noise, a repeated, unchanging stimulus. The study highlights that neurons are less responsive to constant analog values, such as square pulses, and more selective to dynamic signals, responding with remarkable precision in the temporal domain.
&lt;/aside&gt;
&lt;!-- 
---


## Spiking Neural Networks in neurobiology



















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;



&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this second example, I show a simulation reproducing the 1999 paper by Diesmann and colleagues. This &lt;em&gt;theoretical model&lt;/em&gt; considers ten interconnected groups, each comprising 100 neurons. Each group is connected to the next one. A key finding is that information transfer across groups depends on the temporal concentration of spikes. Initially, information is too scattered within the first group, leading to a dilution effect in subsequent groups. However, once a threshold is reached, a cluster of synchronous spikes ensures efficient propagation through the network. This non-linear dynamic is characteristic of spiking neural networks, adding a layer of richness, but also a cerain complexity.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
A third example shows an experiment conducted by Rosa Cossart&amp;rsquo;s group at INMED and recently published by Haimerl and colleagues. They used &lt;em&gt;calcium fluorescence&lt;/em&gt; imaging to track neuronal activity in mice. By arranging the neurons in &lt;em&gt;temporal order of activation&lt;/em&gt;, it shows a sequential activation of these neurons, a mechanism which resembles the model mentioned earlier. These patterns closely align with the mouse’s motor behavior, as depicted in the accompanying graph. Notably, these activity sequences remained consistent, even when recorded on the &lt;em&gt;next day&lt;/em&gt;, underscoring the importance of temporal dynamics in neural computation.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These observations have led us to &lt;em&gt;review&lt;/em&gt; neurobiological evidence of neurons encoding information based on the relative timing of spikes. Intriguingly, the conduction &lt;em&gt;delays&lt;/em&gt; observed in spike transmission are not merely obstacles. Instead, they could be used to enhance information representation and processing through &lt;em&gt;spiking motifs&lt;/em&gt;. This perspective challenges traditional views and opens up new possibilities for understanding information representation, processing and learning.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
Consider an ultra-simplified neural network with three presynaptic neurons and two output neurons, connected by &lt;em&gt;heterogeneous&lt;/em&gt; delays. With synchronous inputs, the output neurons activate at different times, failing to reach the threshold for an output spike. However, if the delays align the action potentials to arrive simultaneously, the combined input can trigger an output spike at the &lt;em&gt;same instant&lt;/em&gt;, as indicated by the red bar.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To better grasp this mechanism, let’s revisit the animation of a spiking neuron. Without delays, action potentials reach the neuron’s cell body immediately, where they’re integrated to potentially trigger a spike.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-3&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
Now using &lt;em&gt;heterogeneous&lt;/em&gt; delays, the timing of spike arrival at the cell body varies. Introducing a specific &lt;em&gt;spiking motif&lt;/em&gt;, marked by green action potentials, allows these spikes to converge simultaneously due to the delays. This synchronicity results in the neuron generating a new spike.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
In applying this theoretical principle, we developed an algorithm to detect movement in images. We began by simulating event data from natural images set in motion along paths similar to those observed during free visual exploration. The event-driven output exhibits distinct characteristics. For instance, rapid movement results in a higher spike rate. Conversely, edges aligned with the motion direction yield minimal changes, leading to fewer spikes. This phenomenon is known as the aperture problem.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-1&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/laurentperrinet/figures/7f382a8074552de1a6a0c5728c60d48788b5a9f8/animated_neurons/conv_HDSNN.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We then used a neural network with a classical architecture, which we enhanced by using an spike representation that accounts for various synaptic delays values. In this figure, the input is on the left grid, indicating spikes of either positive or negative polarity. This input is processed through multiple channels, represented by green and orange, and generate membrane activity. This activity, in turn, led to the production of output spikes, particularly in synaptic connection nuclei with heterogeneous delays. These delays are key to identifying specific spatio-temporal patterns.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-2&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
A key advantage of this network is its differentiability, which allows the application of traditional machine learning techniques, such as supervised learning.
We then see the emergence of various convolution kernels. The graph on the left, marked by red arrows, displays a selection of these kernels oriented in different directions.
It shows the kernels obtained on the spatial representation according to the different columns, and each row represents the different delays from a delay of one on the right to a delay of 12 time steps on the left. Detectors that follow the motion emerge. For example, for the top line from top to bottom. These kernels integrate both positive neurons in red and negative polarity inputs in blue.
vim Such spatio-temporal filtering is observed in neurobiology, but to my knowledge had never been observed in a model of spiking neurons trained under natural conditions.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-3&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_raw.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will now study the performance of this network in detecting motion in the flow of events entering the network. When we use all the weights of the convolution kernel, we get a very good performance of the order of 99%, represented by the black dot in the top right-hand corner. Note that in the kernels we&amp;rsquo;ve seen emerge, most of the synaptic weights are close to zero, so we might consider removing some of these weights, as this can be shown to reduce the number of event calculations required.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-4&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_shortening.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
This is what we&amp;rsquo;ve done, by first removing the parts of the core corresponding to the longest delays. This &amp;ldquo;shortens&amp;rdquo; the kernel. We quickly observed a degradation in performance, which reached half-saturation when we reduced the number of weights by around 50%. This demonstrates the importance of integrating information that is quite distant and structured over time.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-5&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a second step, we performed a pruning operation, which consists in progressively removing the weights that are the weakest. This time, performance remains optimal over a wide compression range, and we reach half-saturation when we have removed around 99.8% of the weights. This means that the network is able to maintain very good performance, even when only one weight out of 600 has been kept, and therefore, with a computation time increased by a factor of 600. This property, which we didn&amp;rsquo;t expect, seems promising for creating machine learning algorithms that are less energy-hungry.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>2023-01-23_game-theory-and-the-brain</title>
      <link>https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain/</guid>
      <description>&lt;h1 id=&#34;game-theory-and-brain-strategies&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;50%&#34; &gt;
&lt;p&gt;&lt;strong&gt;[2023-01-23] Atelier jeu et cerveau&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-01-23-game-theory-and-the-brain&#34;&gt;https://laurentperrinet.github.io/talk/2023-01-23-game-theory-and-the-brain&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;Photo by Naser Tamimi on Unsplash &lt;a href=&#34;https://unsplash.com/fr/photos/yG9pCqSOrAg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://unsplash.com/fr/photos/yG9pCqSOrAg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-1&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;80%&#34; &gt;
&lt;p&gt;&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;Le jeu du cerveau et du hasard, &lt;i&gt;The Conversation&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;What is noise? The uncertainty due to noise is symbolized by dices: a throw of fair dices, even if they are optimally simulated can not be predicted: the outcome is uniformly one facet from 1 to 6,&lt;/li&gt;
&lt;li&gt;I am interested in vision, and uncertainty exists in different forms,&lt;/li&gt;
&lt;li&gt;If we consider the image, can be noise at low contrast, complexity of the object, pose of the dice,&lt;/li&gt;
&lt;li&gt;in this presentation, we will see different facets of noise and uncertainty, and illustrate how our brains may play with it - and delineate a theory for this game. We will also see how it may harness the noise by explicitly representing it in the neural activity.&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;aleatoric-noise&#34;&gt;Aleatoric noise&lt;/h1&gt;
&lt;hr&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-random-points--a&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://a5huynh.github.io/img/2019/rng-example.png&#34; alt=&#34;Random points  (A).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;49%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Random points  (A).
    &lt;/figcaption&gt;&lt;/figure&gt;


















&lt;figure  id=&#34;figure-random-points--b&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://a5huynh.github.io/img/2019/poisson-disk-example.png&#34; alt=&#34;Random points  (B).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;49%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Random points  (B).
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;img src=&#34;https://a5huynh.github.io/img/2019/rng-example.png&#34; width=&#34;70%&#34; &gt;
&lt;img src=&#34;https://a5huynh.github.io/img/2019/poisson-disk-example.png&#34; width=&#34;70%&#34; &gt;
&lt;p&gt;&lt;a href=&#34;https://a5huynh.github.io/posts/2019/poisson-disk-sampling/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Huynh, generating Poisson disk noise&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;what is noise? it exists at quantum level, but if I were to ask you to draw random points how would it look like?&lt;/li&gt;
&lt;li&gt;Aleatoric comes from alea, the Latin word for “dice.” Aleatoric uncertainty is the uncertainty introduced by the randomness of an event. For example, the result of flipping a coin is an aleatoric event.&lt;/li&gt;
&lt;li&gt;In your opinion, which of the two is the most random pattern?&lt;/li&gt;
&lt;li&gt;from your responses &amp;hellip;&lt;/li&gt;
&lt;li&gt;the answer is that &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When it comes to true randomness, one of its stranger aspects is that it often behaves differently to people’s expectations. Take the two diagrams below – which one do you think is a random distribution, and which has been deliberately created/adjusted?&lt;/p&gt;
&lt;p&gt;randomized dots
Only one of these panels shows a random distribution of dots | Source: Bully for Brontosaurus – Stephen Jay Gould&lt;/p&gt;
&lt;p&gt;If you said the right panel, you are in good company, as this is most people’s expectation of what randomness looks like. However, this relatively uniform distribution has been adjusted to ensure the dots are evenly spread. In fact, it is the left panel, with its clumps and voids, that reflects a true random distribution. It is also this tendency for randomness to produce clumps and voids that leads to some unintuitive outcomes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://theconversation.com/daniel-kahneman-on-noise-the-flaw-in-human-judgement-harder-to-detect-than-cognitive-bias-160525&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://theconversation.com/daniel-kahneman-on-noise-the-flaw-in-human-judgement-harder-to-detect-than-cognitive-bias-160525&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-instabilité-etienne-reyhttpslaurentperrinetgithubiopost2018-09-09_artorama&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/post/2018-09-09_artorama/featured.png&#34; alt=&#34;[Instabilité, Etienne Rey.](https://laurentperrinet.github.io/post/2018-09-09_artorama/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/post/2018-09-09_artorama/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Instabilité, Etienne Rey.&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;this was for instance used by the artist Etienne Rey to generate large panels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;our perception will generate objects out of nowhere: surfaces, groups, holes&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;this explains many cognitive biases, for instance that we expect noise to have some regularity and that we wish to explain any cluster of events by some god-like divinity&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;going further &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;when going to the same place a few years later &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;the face was gone &amp;hellip;&lt;/li&gt;
&lt;li&gt;conclusion 1: information pops out from noise&lt;/li&gt;
&lt;li&gt;conclusion 2: further information may change the interpretation&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction&#34;&gt;Sequence prediction&lt;/h1&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://github.com/chloepasturel/AnticipatorySPEM/raw/master/2020-03_video-abstract/Bet_eyeMvt/eyeMvt.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;to test this in the lab, we analyzed the response of observers to a sequences of left / right moving dots&lt;/li&gt;
&lt;li&gt;These were presented in multiple blocks of 50 trials for which we recorded eye movements and, on a subsequent day, asked them&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction-1&#34;&gt;Sequence prediction&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A: 👍👍👍👍🤘👍👍👍👍👍🤘👍👍👍👍🤘👍👍👍👍👍🤘👍🤘👍👍👍👍👍🤘 ?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;B: 👍🤘🤘🤘👍👍👍🤘🤘👍🤘👍🤘👍👍🤘👍🤘👍👍👍🤘👍🤘👍🤘🤘🤘👍🤘 ?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;C: 👍🤘🤘🤘👍🤘👍👍🤘🤘🤘🤘🤘🤘👍👍🤘👍🤘🤘🤘👍🤘👍🤘🤘🤘🤘🤘👍 ?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;D: 🤘🤘🤘🤘🤘👍🤘🤘🤘👍🤘🤘🤘🤘👍🤘👍👍👍👍👍🤘👍🤘👍👍👍👍👍🤘 ?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;to simplify the problem, let&amp;rsquo;s show these sequences as the sequence of these 2 emojis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In sequence A, what do you think the next&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the same question could be asked in an online fashion&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence B, it&amp;rsquo;s certainly the same answer, yet with lower certitude&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence C, you go metal 🤘&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;in sequence D, it&amp;rsquo;s different there is a clearly a tendance for 🤘but that it switches to 👍&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;is it possible that the brain may detect such switches?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;sequence-prediction-2&#34;&gt;Sequence prediction&lt;/h1&gt;


















&lt;figure  id=&#34;figure-pasturel-et-al-2020httpslaurentperrinetgithubiopublicationpasturel-montagnini-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis.png&#34; alt=&#34;([Pasturel *et al*, 2020](https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      (&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pasturel &lt;em&gt;et al&lt;/em&gt;, 2020&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;to synthesize, we have a generative model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we found the mathematically optimal problem - and found that both eye movements + bets follow the model with switches&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The aleatoric noise is transformed into a measure of knowledge = epistemic noise&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;epistemic-noise&#34;&gt;Epistemic noise&lt;/h1&gt;
&lt;!-- 
---

# Playing with noise




















&lt;figure  id=&#34;figure-nash-equilibrium-rock-paper-scissorshttpsenwikipediaorgwikirock_paper_scissors&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/67/Rock-paper-scissors.svg&#34; alt=&#34;Nash equilibrium ([Rock paper scissors](https://en.wikipedia.org/wiki/Rock_paper_scissors)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Nash equilibrium (&lt;a href=&#34;https://en.wikipedia.org/wiki/Rock_paper_scissors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rock paper scissors&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;


&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;let&amp;rsquo;s go back to game theory&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rock paper scissors: Its French name, &amp;ldquo;Chi-fou-mi&amp;rdquo;, is based on the Old Japanese words for &amp;ldquo;one, two, three&amp;rdquo; (&amp;ldquo;hi, fu, mi&amp;rdquo;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nash Equilibrium is a game theory concept that determines the optimal solution in a non-cooperative game in which each player lacks any incentive to change his/her initial strategy. Under the Nash equilibrium, a player does not gain anything from deviating from their initially chosen strategy, assuming the other players also keep their strategies unchanged.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quantamagazine.org/the-game-theory-math-behind-rock-paper-scissors-20180402/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.quantamagazine.org/the-game-theory-math-behind-rock-paper-scissors-20180402/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;

---



















&lt;figure  id=&#34;figure-prisoners-dilemma-salem-marafihttpwwwsalemmaraficombusinessprisoners-dilemma&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://www.salemmarafi.com/wp-content/uploads/2011/10/prisoners_dilemma.jpg&#34; alt=&#34;Prisoner’s Dilemma ([Salem Marafi](http://www.salemmarafi.com/business/prisoners-dilemma/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;60%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Prisoner’s Dilemma (&lt;a href=&#34;http://www.salemmarafi.com/business/prisoners-dilemma/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Salem Marafi&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;



&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;uncertainty comes not from aleatoric noise but from not knowing: epistemic uncertainty&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt; --&gt;
&lt;hr&gt;
&lt;h1 id=&#34;representing-uncertainty&#34;&gt;Representing uncertainty&lt;/h1&gt;


















&lt;figure  id=&#34;figure-visual-epistemic-uncertainty-hugo-ladrethttpstheconversationcomle-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.theconversation.com/files/407867/original/file-20210623-17-ai1gc3.png&#34; alt=&#34;Visual epistemic uncertainty ([Hugo Ladret](https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual epistemic uncertainty (&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;in the case of images, a local patch may have the same most likely orientation, yet with different bandwidth (textures)&lt;/li&gt;
&lt;li&gt;the primary visual cortex of mammals like humans is to detect orientations&lt;/li&gt;
&lt;li&gt;will the response be the same for both cases?&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;representing-uncertainty-1&#34;&gt;Representing uncertainty&lt;/h1&gt;


















&lt;figure  id=&#34;figure-visual-epistemic-uncertainty-hugo-ladrethttpslaurentperrinetgithubiopublicationladret-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/ladret-23/featured_hubbf63d8d7d25b21f139c2f10354080fc_466086_720x2500_fit_q75_h2_lanczos_3.webp&#34; alt=&#34;Visual epistemic uncertainty ([Hugo Ladret](https://laurentperrinet.github.io/publication/ladret-23/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual epistemic uncertainty (&lt;a href=&#34;https://laurentperrinet.github.io/publication/ladret-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-2&#34;&gt;Game theory and brain strategies&lt;/h1&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/publication/perrinet-21-hasard/featured.jpg&#34; width=&#34;80%&#34; &gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;In face of noise, the brain plays a game&lt;/li&gt;
&lt;li&gt;Evolution favors not fitness but adaptability&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-3&#34;&gt;Game theory and brain strategies&lt;/h1&gt;


















&lt;figure  id=&#34;figure-aleatoric-uncertainty-pasturel-et-al-2020httpslaurentperrinetgithubiopublicationpasturel-montagnini-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/synthesis.png&#34; alt=&#34;Aleatoric uncertainty ([Pasturel *et al*, 2020](https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Aleatoric uncertainty (&lt;a href=&#34;https://laurentperrinet.github.io/publication/pasturel-montagnini-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pasturel &lt;em&gt;et al&lt;/em&gt;, 2020&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;The brain uses predictive coding, for instance for sequence learning&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;game-theory-and-brain-strategies-4&#34;&gt;Game theory and brain strategies&lt;/h1&gt;


















&lt;figure  id=&#34;figure-epistemic-uncertainty-hugo-ladrethttpstheconversationcomle-jeu-du-cerveau-et-du-hasard-159388&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.theconversation.com/files/407867/original/file-20210623-17-ai1gc3.png&#34; alt=&#34;Epistemic uncertainty ([Hugo Ladret](https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Epistemic uncertainty (&lt;a href=&#34;https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Ladret&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;For this, it represents explictly uncertainty (epistemic noise)&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-01-23_game-theory-and-the-brain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022-11-21_flash-lag-effect</title>
      <link>https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/</link>
      <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect/</guid>
      <description>&lt;table width=&#34;100%&#34;&gt; 
&lt;tr&gt;
	&lt;th width=&#34;80%&#34;&gt;
	&lt;img src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/header.png&#34; width=&#34;100%&#34; &gt;
	&lt;th width=&#34;20%&#34;&gt;
	&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; width=&#34;100%&#34; &gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;[2022-11-21] Alex Reynaud&amp;rsquo;s lab meeting&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34;&gt;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;!--
---


&lt;table width=&#34;100%&#34;&gt;
&lt;tr&gt;
	&lt;th width=&#34;80%&#34;&gt;
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;figure  &gt;
	  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
	    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/khoei-masson-perrinet-17/header.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
	  &lt;/div&gt;&lt;/figure&gt;
	&lt;/th&gt;
	&lt;th width=&#34;20%&#34;&gt;
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;figure  &gt;
	  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
	    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
	  &lt;/div&gt;&lt;/figure&gt;

	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;

__[2022-11-21] Alex Reynaud&#39;s lab meeting__

&lt;p style=&#34;color:blue;font-size:25px;&#34;&gt;
&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34;&gt;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&lt;/a&gt;&lt;/p&gt; --&gt;
&lt;!-- ---

|

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/khoei-masson-perrinet-17/header.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/coverart.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;29%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
|

__[2022-11-21] Alex Reynaud&#39;s lab meeting__
https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;timing-in-the-visual-pathways&#34;&gt;Timing in the visual pathways&lt;/h2&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-ultra-rapid-visual-processing-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-polychronies/featured.jpg&#34; alt=&#34;Ultra-rapid visual processing ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ultra-rapid visual processing (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-compensating-visual-delays-perrinet-adams--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/tsonga.jpg&#34; alt=&#34;Compensating visual delays ([Perrinet, Adams &amp;amp; Friston 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Compensating visual delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet, Adams &amp;amp; Friston 2014&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-compensating-visual-delays-perrinet-adams--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/figure-tsonga.jpg&#34; alt=&#34;Compensating visual delays ([Perrinet Adams &amp;amp; Friston, 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Compensating visual delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet Adams &amp;amp; Friston, 2014&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;travelling-waves&#34;&gt;Travelling waves?&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/line_motion.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/phi_motion.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-suppressive-travelling-waves-chemla-et-al-2019httpslaurentperrinetgithubiopublicationchemla-19&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-18_JNLF/master/figures/Chemla_etal2019.png&#34; alt=&#34;Suppressive travelling waves ([Chemla *et al*, 2019](https://laurentperrinet.github.io/publication/chemla-19/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Suppressive travelling waves (&lt;a href=&#34;https://laurentperrinet.github.io/publication/chemla-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chemla &lt;em&gt;et al&lt;/em&gt;, 2019&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;predictive-coding&#34;&gt;Predictive coding&lt;/h2&gt;
&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_aperture.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;!--
---












  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_box.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
 --&gt;
&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/aperture_cube.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-motion-based-prediction-perrinet-et-al-2012httpslaurentperrinetgithubiopublicationperrinet-12-pred&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/navier.svg&#34; alt=&#34;Motion-based prediction ([Perrinet *et al*, 2012](https://laurentperrinet.github.io/publication/perrinet-12-pred/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-motion-based-prediction-perrinet-et-al-2012httpslaurentperrinetgithubiopublicationperrinet-12-pred&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/perrinet12pred_figure2.png&#34; alt=&#34;Motion-based prediction ([Perrinet *et al*, 2012](https://laurentperrinet.github.io/publication/perrinet-12-pred/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;61%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!-- ---












  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line-nopred_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;


---












  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

 --&gt;
&lt;hr&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/line_particles.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;Motion-based prediction (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-12-pred/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &lt;em&gt;et al&lt;/em&gt;, 2012&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;flash-lag-effect&#34;&gt;Flash-lag effect&lt;/h2&gt;
&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-flash-lag-effect-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_cartoon.jpg&#34; alt=&#34;Flash-lag effect ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Flash-lag effect (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!--
---



















&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;amp;id=10.1371/journal.pcbi.1005068.g002&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;


https://journals.plos.org/ploscompbiol/article/figure/image?size=large&amp;id=10.1371/journal.pcbi.1005068.g002
---



















&lt;figure  id=&#34;figure-diagonal-markov&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov
    &lt;/figcaption&gt;&lt;/figure&gt;


---



















&lt;figure  id=&#34;figure-diagonal-markov&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_simple.jpg&#34; alt=&#34;Diagonal markov&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov
    &lt;/figcaption&gt;&lt;/figure&gt;


---



















&lt;figure  id=&#34;figure-diagonal-markov-pull&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_pull.jpg&#34; alt=&#34;Diagonal markov (pull)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov (pull)
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!--
---













  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/PBP_flash_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;



---













  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/PBP_dot_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;



---













  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_flash_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;


---













  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true&#34; type=&#34;video/mp4?raw=true&#34;&gt;
&lt;/video&gt;
 --&gt;
&lt;!-- ---












  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;


Flash-lag effect: MBP ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).

---












  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;


Flash-lag effect: MBP ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)). --&gt;
&lt;hr&gt;
&lt;p&gt;










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/p&gt;
&lt;p&gt;Flash-lag effect: MBP (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).&lt;/p&gt;
&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/positional-delay.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-flash-lag-effect-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg&#34; alt=&#34;Flash-lag effect ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Flash-lag effect (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!--


---



















&lt;figure  id=&#34;figure-qauntitative-result&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg&#34; alt=&#34;Qauntitative result&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Qauntitative result
    &lt;/figcaption&gt;&lt;/figure&gt;



https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE.jpg


https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true
MBP_dot_spatial_readout.mp4
MBP_flash_spatial_readout.mp4
MBP_spatial_readout.mp4
PBP_dot_spatial_readout.mp4
PBP_flash_spatial_readout.mp4

https://github.com/laurentperrinet/Khoei_2017_PLoSCB/blob/master/figures/MBP_dot_spatial_readout.mp4?raw=true

PBP_spatial_readout.mp4


src=&#34;../../publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; khoei-masson-perrinet-17


 create mode 100644 publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4
 create mode 100644 publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4

 --&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram_comp.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-motion-reversal-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_MotionReversal_MBP.jpg&#34; alt=&#34;Motion reversal ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion reversal (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-motion-reversal-smoothed-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_MotionReversal.jpg&#34; alt=&#34;Motion reversal (smoothed) ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Motion reversal (smoothed) (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag_stop.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-probability-distributions-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_histogram.jpg&#34; alt=&#34;Probability distributions ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Probability distributions (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-limit-cycles-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_limit_cycles.jpg&#34; alt=&#34;Limit cycles ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Limit cycles (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!--
---



















&lt;figure  id=&#34;figure-diagonal-neural&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov_neural.jpg&#34; alt=&#34;Diagonal neural&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal neural
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-application-to-the-pulfrich-phenomenonhttpseyewikiaaoorgpulfrich_phenomenon&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://eyewiki.aao.org/w/images/1/e/eb/Pulfrich.png&#34; alt=&#34;Application to the [Pulfrich phenomenon](https://eyewiki.aao.org/Pulfrich_Phenomenon)?&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Application to the &lt;a href=&#34;https://eyewiki.aao.org/Pulfrich_Phenomenon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pulfrich phenomenon&lt;/a&gt;?
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-11-21_flash-lag-effect&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt; + &lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022-03-23_UE-neurosciences-computationnelles</title>
      <link>https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/</link>
      <pubDate>Wed, 23 Mar 2022 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/</guid>
      <description>&lt;h1 id=&#34;réseaux-de-neurones-artificiels-et-apprentissage-machine-appliqués-à-la-compréhension-de-la-visionhttpsgithubcomlaurentperrinet2022_ue-neurosciences-computationnelles&#34;&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Réseaux de neurones artificiels et apprentissage machine appliqués à la compréhension de la vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubiotalk2022-03-23-ue-neurosciences-computationnelles&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2022-03-23-ue-neurosciences-computationnelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2022-03-23httpsameticeuniv-amufrpluginfilephp5559779mod_resourcecontent1planning_neurocomp_m1_2022pdf-master-1-neurosciences-et-sciences-cognitiveshttpsameticeuniv-amufrcourseviewphpid89069u&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/pluginfile.php/5559779/mod_resource/content/1/Planning_Neurocomp_M1_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2022-03-23]&lt;/a&gt; &lt;a href=&#34;https://ametice.univ-amu.fr/course/view.php?id=89069&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master 1 Neurosciences et Sciences Cognitives&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.png&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;principes-de-la-vision&#34;&gt;Principes de la Vision&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;more on &lt;a href=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/master/exampleSite/content/slides/example/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision&#34;&gt;À quoi sert la vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-ilya-repin-1884httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_001.jpg&#34; alt=&#34;[An Unexpected Visitor (Ilya Repin, 1884)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Ilya Repin, 1884)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision-1&#34;&gt;À quoi sert la vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_002.jpg&#34; alt=&#34;[An Unexpected Visitor (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision-2&#34;&gt;À quoi sert la vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---age-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_003.jpg&#34; alt=&#34;[An Unexpected Visitor - *Age?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;Age?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision-3&#34;&gt;À quoi sert la vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---how-long--yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_006.jpg&#34; alt=&#34;[An Unexpected Visitor - *How long?*  (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;How long?&lt;/em&gt;  (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion_without.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;










  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Kitaoka.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ilusions of brightness or lightness &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-3&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-rotating-snakes-akiyoshi-kitaokahttpwwwritsumeiacjpakitaokaindex-ehtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/42_rotsnakes_main.jpg&#34; alt=&#34;[Rotating Snakes *Akiyoshi KITAOKA*](http://www.ritsumei.ac.jp/~akitaoka/index-e.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating Snakes &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--paréidoliehttpsfrwikipediaorgwikiparc3a9idolie&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt; : &lt;a href=&#34;https://fr.wikipedia.org/wiki/Par%C3%A9idolie&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paréidolie&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--paréidoliehttpsfrwikipediaorgwikiparc3a9idolie-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt; : &lt;a href=&#34;https://fr.wikipedia.org/wiki/Par%C3%A9idolie&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paréidolie&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelleshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--paréidoliehttpsfrwikipediaorgwikiparc3a9idolie-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Les illusions visuelles&lt;/a&gt; : &lt;a href=&#34;https://fr.wikipedia.org/wiki/Par%C3%A9idolie&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paréidolie&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsfrwikipediaorgwikicydonia_mensae&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://fr.wikipedia.org/wiki/Cydonia_Mensae)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Cydonia_Mensae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;les-neurosciences-computationnelles&#34;&gt;Les neurosciences computationnelles&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sejnowski--koch---churchland-1998httpwwwhmsharvardedubssneurobornlabnb204paperssejnowski-koch-churchland-science1988pdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Churchland92.png&#34; alt=&#34;[[Sejnowski,  Koch  &amp;amp; Churchland (1998)](http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;35%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sejnowski,  Koch  &amp;amp; Churchland (1998)&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;de-v1-aux-réseaux-convolutionnels&#34;&gt;De V1 aux réseaux convolutionnels&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;le-système-visuel&#34;&gt;Le système visuel&lt;/h2&gt;


















&lt;figure  id=&#34;figure-système-visuel-humain-wikipediahttpsfrwikipediaorgwikisystc3a8me_visuel_humain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e4/Voies_visuelles3.svg&#34; alt=&#34;[Système visuel humain (Wikipedia)](https://fr.wikipedia.org/wiki/Syst%C3%A8me_visuel_humain)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;40%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Syst%C3%A8me_visuel_humain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Système visuel humain (Wikipedia)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;le-cortex-visuel-primaire&#34;&gt;Le cortex visuel primaire&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hubel--wiesel-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/scientists.jpg&#34; alt=&#34;[Hubel &amp;amp; Wiesel, 1962]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Hubel &amp;amp; Wiesel, 1962]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;hubel--wiesel&#34;&gt;Hubel &amp;amp; Wiesel&lt;/h2&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;[Hubel &amp;amp; Wiesel, 1962]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels--hiérarchie&#34;&gt;Réseaux convolutionnels : hiérarchie&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels---math&#34;&gt;Réseaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrète uni-dimensionnelle (eg dans le temps) avec un noyau f de rayon $K$:
$$
(f \ast g)[n]=\sum_{m=-K}^{K} f[m] g[n-m]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels---math-1&#34;&gt;Réseaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrète d&amp;rsquo;une image (bi-dimensionnelle):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[i, j] g[i-x, j-y]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels--lopération-de-convolution&#34;&gt;Réseaux convolutionnels : l&amp;rsquo;opération de convolution&lt;/h2&gt;


















&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/convolution-layer-a.png?1c517e00cb8d709baf32fc3d39ebae67&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels---math-2&#34;&gt;Réseaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrète d&amp;rsquo;une image sur plusieurs canaux de sortie:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y, k] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[k, i, j, k] g[i-x, j-y]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels---math-3&#34;&gt;Réseaux convolutionnels :  Math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution discrète d&amp;rsquo;une image multi-canaux (eg. RGB) sur plusieurs canaux de sortie (noter &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;l&amp;rsquo;ordre des indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y, k] = \
\sum_{i=-K}^{K} \sum_{j=-K}^{K} \sum_{c=1}^{C} f[k, c, i, j] g[i-x, j-y, c]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels--cnn&#34;&gt;Réseaux convolutionnels : CNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-cnn-fr.jpeg&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;mise-en-pratique-détecter--apprendre&#34;&gt;Mise en pratique: détecter &amp;amp; apprendre&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tutoriel Apprentissage profond&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/blob/master/A_D%C3%A9tecter.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Notebook &lt;code&gt;A_Détecter.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/blob/master/B_Apprendre.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Notebook &lt;code&gt;B_Apprendre.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;perspectives&#34;&gt;Perspectives&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels--hiérarchie-1&#34;&gt;Réseaux convolutionnels : hiérarchie&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;réseaux-prédictifs&#34;&gt;Réseaux prédictifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;topographie-dans-v1&#34;&gt;Topographie dans V1&lt;/h2&gt;


















&lt;figure  id=&#34;figure-bosking-et-al-1997&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Bosking97Fig4.jpg&#34; alt=&#34;[Bosking *et al*, 1997]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Bosking &lt;em&gt;et al&lt;/em&gt;, 1997]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;


















&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;recurrent-processing&#34;&gt;Recurrent processing&lt;/h2&gt;


















&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamique-de-la-vision&#34;&gt;Dynamique de la vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-thorpe-2001httpslaurentperrinetgithubio2022-01-12_neurocercle21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/scheme_thorpe.jpg&#34; alt=&#34;[[Thorpe (2001)]](https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Thorpe (2001)]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;applications-robotiques&#34;&gt;Applications robotiques&lt;/h2&gt;


















&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-communicating-by-event-driven-feed-forward-and-feed-back-communications&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/principe_agile.jpg&#34; alt=&#34;Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://laurentperrinet.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-hugo-blox-builder&#34;&gt;Create slides in Markdown with Hugo Blox Builder&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://hugoblox.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Blox Builder&lt;/a&gt; | &lt;a href=&#34;https://docs.hugoblox.com/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com/pdf-export/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} One {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} Three {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  {{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.hugoblox.com/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-04-03-master-m-4-nc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-04-03-master-m-4-nc/</guid>
      <description>&lt;section&gt;
&lt;h1 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-visionhttpslaurentperrinetgithubioslides2023-04-03-master-m-4-nctransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-04-03-master-m-4-nc/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-04-03httpsameticeuniv-amufrpluginfilephp5559779mod_resourcecontent1planning_neurocomp_m1_2022pdf-master-m4nc-de-linstitut-neuromod-cours-prospective-innovation-and-researchhttpsneuromoduniv-cotedazureuu&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/pluginfile.php/5559779/mod_resource/content/1/Planning_Neurocomp_M1_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-04-03]&lt;/a&gt; &lt;a href=&#34;https://neuromod.univ-cotedazur.eu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master M4NC de l&amp;rsquo;institut NeuroMod, cours Prospective Innovation and Research.&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Contact me @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;objective= understand biological vision&lt;/li&gt;
&lt;li&gt;interaction between artificial and natural NNs&lt;/li&gt;
&lt;li&gt;outline&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;principles-of-vision&#34;&gt;Principles of Vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;cut in different levels: Marr (+ Poggio)&lt;/li&gt;
&lt;li&gt;arbitrary, but useful division of labor&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-ilya-repin-1884httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_001.jpg&#34; alt=&#34;[An Unexpected Visitor (Ilya Repin, 1884)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Ilya Repin, 1884)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;seeing= interacting with the visual world&lt;/li&gt;
&lt;li&gt;social animals: looking at emotions&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-1&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_002.jpg&#34; alt=&#34;[An Unexpected Visitor (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;active: the eye is always moving&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fr.wikipedia.org/wiki/Alfred_Iarbous&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://fr.wikipedia.org/wiki/Alfred_Iarbous&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-2&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---age-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_003.jpg&#34; alt=&#34;[An Unexpected Visitor - *Age?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;Age?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;active: depends on task&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-3&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---how-long-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_006.jpg&#34; alt=&#34;[An Unexpected Visitor - *How long?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;How long?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;consistency of eye traces&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;










  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Kitaoka.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ilusions of brightness or lightness &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion_without.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;effect of context -&amp;gt; 3D&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-3&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-rotating-snakes-akiyoshi-kitaokahttpwwwritsumeiacjpakitaokaindex-ehtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/42_rotsnakes_main.jpg&#34; alt=&#34;[Rotating Snakes *Akiyoshi KITAOKA*](http://www.ritsumei.ac.jp/~akitaoka/index-e.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating Snakes &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;principles-of-vision-1&#34;&gt;Principles of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;computational-neuroscience-of-vision&#34;&gt;Computational neuroscience of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;computational-neuroscience-of-vision-1&#34;&gt;Computational neuroscience of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sejnowski-koch--churchland-1998httpwwwhmsharvardedubssneurobornlabnb204paperssejnowski-koch-churchland-science1988pdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Churchland92.png&#34; alt=&#34;[[Sejnowski, Koch &amp;amp; Churchland (1998)](http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sejnowski, Koch &amp;amp; Churchland (1998)&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;anatomy-of-the-human-visual-system&#34;&gt;Anatomy of the Human Visual system&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.readkong.com/static/06/b0/06b09f0235ae7fcf29438ce317c10e60/optogenetic-visual-cortical-prosthesis-9612386-7.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;61%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;human-visual-system--the-hmax-model&#34;&gt;Human Visual system : the HMAX model&lt;/h2&gt;


















&lt;figure  id=&#34;figure-serre-and-poggio-2007httpsbiologystackexchangecomquestions10955ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.stack.imgur.com/ZlFnp.png&#34; alt=&#34;[[Serre and Poggio, 2007](https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;65%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Serre and Poggio, 2007&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!-- ---

## Anatomy of the Human Visual system



















&lt;figure  id=&#34;figure-wikipediahttpsenwikipediaorgwikivisual_system&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e4/Voies_visuelles3.svg&#34; alt=&#34;[[Wikipedia]](https://en.wikipedia.org/wiki/Visual_system)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Visual_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Wikipedia]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hubel--wiesel-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/scientists.jpg&#34; alt=&#34;[Hubel &amp;amp; Wiesel, 1962]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Hubel &amp;amp; Wiesel, 1962]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel-1&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;[Hubel &amp;amp; Wiesel, 1962]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy&#34;&gt;Convolutional Neural Networks : Hierarchy&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One-dimensional &lt;a href=&#34;https://en.wikipedia.org/wiki/Convolution#Discrete_convolution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discrete convolution&lt;/a&gt; (eg in time) with a kernel $g$ of radius $K$:
$$
(f \ast g)[n]=\sum_{m=-K}^{K} f[n-m] \cdot g[m]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-1&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[x-i, y-j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-2&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-correlation&lt;/strong&gt; of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[x+i, y+j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-3&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;


















&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/convolution-layer-a.png&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-4&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of an image defined on several  channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-5&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of a multi-channel image for multiple output channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[k, x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[k, c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--the-hmax-model&#34;&gt;Convolutional Neural Networks : the HMAX model&lt;/h2&gt;


















&lt;figure  id=&#34;figure-serre-and-poggio-2006httpsbiologystackexchangecomquestions10955ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.stack.imgur.com/ZlFnp.png&#34; alt=&#34;[[Serre and Poggio, 2006]](https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;65%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Serre and Poggio, 2006]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;sota&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks-cnns&#34;&gt;Convolutional Neural Networks (CNNs)&lt;/h2&gt;


















&lt;figure  id=&#34;figure-jérémie--lp-2023httpslaurentperrinetgithubiopublicationjeremie-23-ultra-fast-cat&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.mdpi.com/vision/vision-07-00029/article_deploy/html/images/vision-07-00029-g003.png&#34; alt=&#34;[[Jérémie &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/jeremie-23-ultra-fast-cat/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/jeremie-23-ultra-fast-cat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jérémie &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy-1&#34;&gt;Convolutional Neural Networks : hierarchy&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;backpropagation is not bioplausible&lt;/li&gt;
&lt;li&gt;modification&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--predictive-coding&#34;&gt;Convolutional Neural Networks : Predictive coding&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;adding sparse coding + feedback&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--predictive-coding-1&#34;&gt;Convolutional Neural Networks : Predictive coding&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/BoutinFranciosiniChavaneRuffierPerrinet20face.png&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;interpretable features&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;


















&lt;figure  id=&#34;figure-bosking-et-al-1997&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Bosking97Fig4.jpg&#34; alt=&#34;[Bosking *et al*, 1997]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Bosking &lt;em&gt;et al&lt;/em&gt;, 1997]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography-1&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2022httpslaurentperrinetgithubiopublicationfranciosini-21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/franciosini-21/featured_hu9b69386a773810bf9e1d727831735035_76078_720x2500_fit_q75_h2_lanczos.webp&#34; alt=&#34;[[Boutin *et al*, 2022](https://laurentperrinet.github.io/publication/franciosini-21/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/franciosini-21/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2022&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;computational-neuroscience-of-vision-2&#34;&gt;Computational neuroscience of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;dynamics-of-vision&#34;&gt;Dynamics of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!--
---


## Dynamics of vision



















&lt;figure  id=&#34;figure-thorpe-2001httpslaurentperrinetgithubio2022-01-12_neurocercle21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/scheme_thorpe.jpg&#34; alt=&#34;[[Thorpe (2001)]](https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Thorpe (2001)]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!--
---


## Dynamics of vision



















&lt;figure  id=&#34;figure-precise-spiking-motifs-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency-estimate.jpg&#34; alt=&#34;Precise Spiking Motifs] ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Precise Spiking Motifs] (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-1&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency_bg.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In particular in our group, we are interested in dynamics of neural processing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The visual system is very efficient in generating a decision from the retinal image to the different stages of the visual pathways, here for a macaque monkey, a reaction of finger muscles in about 300 milliseconds.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the process of categorizing an object takes 10 layers&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-2&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the latencies are of similar in the human brain but merely scaled due to the brain size&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;as a consequence, it is thought that this efficiency is achieved by spikes that is, brief all-or-none events which are passed in the very large network which forms the brain from assemblies of neurons to others.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-3&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-4&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/figure-tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston, 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston, 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-5&#34;&gt;Dynamics of vision&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-6&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-7&#34;&gt;Dynamics of vision&lt;/h2&gt;
&lt;!-- 










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
 --&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/positional-delay.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;Flash-lag effect: MBP (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-8&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-leaky-integrate-and-fire-neuron&#34;&gt;Spiking Neural Networks: Leaky Integrate-and-Fire Neuron&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A standard LIF&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://i.stack.imgur.com/ixnrz.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-3&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Izhikevich polychronization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;yet the domain is vast, and there s lot to do in SNNs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A standard LIF&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;event-based cameras&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-1&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/HDSNN_conv.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-2&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-3&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nice kernels&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-4&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/accuracy.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;frugal computing&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h2 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-vision&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;more on &lt;a href=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/master/exampleSite/content/slides/example/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-visionhttpslaurentperrinetgithubioslides2023-04-03-master-m-4-nctransitionfade-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-04-03-master-m-4-nc/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-04-03httpsameticeuniv-amufrpluginfilephp5559779mod_resourcecontent1planning_neurocomp_m1_2022pdf-master-m4nc-de-linstitut-neuromod-cours-prospective-innovation-and-researchhttpsneuromoduniv-cotedazureuu-1&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/pluginfile.php/5559779/mod_resource/content/1/Planning_Neurocomp_M1_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-04-03]&lt;/a&gt; &lt;a href=&#34;https://neuromod.univ-cotedazur.eu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master M4NC de l&amp;rsquo;institut NeuroMod, cours Prospective Innovation and Research.&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Contact me @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-04-05-ue-neurosciences-computationnelles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-04-05-ue-neurosciences-computationnelles/</guid>
      <description>&lt;section&gt;
&lt;h1 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-visionhttpslaurentperrinetgithubioslides2023-04-05-ue-neurosciences-computationnellestransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-04-05-ue-neurosciences-computationnelles/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-04-05httpsameticeuniv-amufrcourseviewphpid95116-master-1-neurosciences-et-sciences-cognitiveshttpssciencesuniv-amufrfrformationmastersmaster-neurosciencesu&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/course/view.php?id=95116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-04-05]&lt;/a&gt; &lt;a href=&#34;https://sciences.univ-amu.fr/fr/formation/masters/master-neurosciences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master 1 Neurosciences et Sciences Cognitives.&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Contact me @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;objective= understand biological vision&lt;/li&gt;
&lt;li&gt;interaction between artificial and natural NNs&lt;/li&gt;
&lt;li&gt;outline&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;principles-of-vision&#34;&gt;Principles of Vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;cut in different levels: Marr (+ Poggio)&lt;/li&gt;
&lt;li&gt;arbitrary, but useful division of labor&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-ilya-repin-1884httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_001.jpg&#34; alt=&#34;[An Unexpected Visitor (Ilya Repin, 1884)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Ilya Repin, 1884)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;seeing= interacting with the visual world&lt;/li&gt;
&lt;li&gt;social animals: looking at emotions&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-1&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_002.jpg&#34; alt=&#34;[An Unexpected Visitor (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;active: the eye is always moving&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fr.wikipedia.org/wiki/Alfred_Iarbous&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://fr.wikipedia.org/wiki/Alfred_Iarbous&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-2&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---age-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_003.jpg&#34; alt=&#34;[An Unexpected Visitor - *Age?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;Age?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;active: depends on task&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-3&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---how-long-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_006.jpg&#34; alt=&#34;[An Unexpected Visitor - *How long?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;How long?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;consistency of eye traces&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;










  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Kitaoka.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ilusions of brightness or lightness &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion_without.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;effect of context -&amp;gt; 3D&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-3&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-rotating-snakes-akiyoshi-kitaokahttpwwwritsumeiacjpakitaokaindex-ehtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/42_rotsnakes_main.jpg&#34; alt=&#34;[Rotating Snakes *Akiyoshi KITAOKA*](http://www.ritsumei.ac.jp/~akitaoka/index-e.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating Snakes &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae (1976) *Viking Orbiter image*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (1976) &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae (2007) *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae (2007) &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;principles-of-vision-1&#34;&gt;Principles of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;computational-neuroscience-of-vision&#34;&gt;Computational neuroscience of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;computational-neuroscience-of-vision-1&#34;&gt;Computational neuroscience of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sejnowski-koch--churchland-1998httpwwwhmsharvardedubssneurobornlabnb204paperssejnowski-koch-churchland-science1988pdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Churchland92.png&#34; alt=&#34;[[Sejnowski, Koch &amp;amp; Churchland (1998)](http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sejnowski, Koch &amp;amp; Churchland (1998)&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;anatomy-of-the-human-visual-system&#34;&gt;Anatomy of the Human Visual system&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.readkong.com/static/06/b0/06b09f0235ae7fcf29438ce317c10e60/optogenetic-visual-cortical-prosthesis-9612386-7.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;61%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;human-visual-system--the-hmax-model&#34;&gt;Human Visual system : the HMAX model&lt;/h2&gt;


















&lt;figure  id=&#34;figure-serre-and-poggio-2007httpsbiologystackexchangecomquestions10955ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.stack.imgur.com/ZlFnp.png&#34; alt=&#34;[[Serre and Poggio, 2007](https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;65%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Serre and Poggio, 2007&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!-- ---

## Anatomy of the Human Visual system



















&lt;figure  id=&#34;figure-wikipediahttpsenwikipediaorgwikivisual_system&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e4/Voies_visuelles3.svg&#34; alt=&#34;[[Wikipedia]](https://en.wikipedia.org/wiki/Visual_system)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Visual_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Wikipedia]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hubel--wiesel-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/scientists.jpg&#34; alt=&#34;[Hubel &amp;amp; Wiesel, 1962]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Hubel &amp;amp; Wiesel, 1962]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel-1&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;[Hubel &amp;amp; Wiesel, 1962]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy&#34;&gt;Convolutional Neural Networks : Hierarchy&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One-dimensional &lt;a href=&#34;https://en.wikipedia.org/wiki/Convolution#Discrete_convolution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discrete convolution&lt;/a&gt; (eg in time) with a kernel $g$ of radius $K$:
$$
(f \ast g)[n]=\sum_{m=-K}^{K} f[n-m] \cdot g[m]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-1&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[x-i, y-j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-2&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-correlation&lt;/strong&gt; of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[x+i, y+j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-3&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;


















&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/convolution-layer-a.png&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-4&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of an image defined on several  channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-5&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of a multi-channel image for multiple output channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[k, x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[k, c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--the-hmax-model&#34;&gt;Convolutional Neural Networks : the HMAX model&lt;/h2&gt;


















&lt;figure  id=&#34;figure-serre-and-poggio-2006httpsbiologystackexchangecomquestions10955ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.stack.imgur.com/ZlFnp.png&#34; alt=&#34;[[Serre and Poggio, 2006]](https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;65%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Serre and Poggio, 2006]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;sota&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks-cnns&#34;&gt;Convolutional Neural Networks (CNNs)&lt;/h2&gt;


















&lt;figure  id=&#34;figure-jérémie--lp-2023httpslaurentperrinetgithubiopublicationjeremie-23-ultra-fast-cat&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.mdpi.com/vision/vision-07-00029/article_deploy/html/images/vision-07-00029-g003.png&#34; alt=&#34;[[Jérémie &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/jeremie-23-ultra-fast-cat/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/jeremie-23-ultra-fast-cat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jérémie &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy-1&#34;&gt;Convolutional Neural Networks : hierarchy&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;backpropagation is not bioplausible&lt;/li&gt;
&lt;li&gt;modification&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--predictive-coding&#34;&gt;Convolutional Neural Networks : Predictive coding&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;adding sparse coding + feedback&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--predictive-coding-1&#34;&gt;Convolutional Neural Networks : Predictive coding&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/BoutinFranciosiniChavaneRuffierPerrinet20face.png&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;interpretable features&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;


















&lt;figure  id=&#34;figure-bosking-et-al-1997&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Bosking97Fig4.jpg&#34; alt=&#34;[Bosking *et al*, 1997]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Bosking &lt;em&gt;et al&lt;/em&gt;, 1997]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography-1&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2022httpslaurentperrinetgithubiopublicationfranciosini-21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/franciosini-21/featured_hu9b69386a773810bf9e1d727831735035_76078_720x2500_fit_q75_h2_lanczos.webp&#34; alt=&#34;[[Boutin *et al*, 2022](https://laurentperrinet.github.io/publication/franciosini-21/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/franciosini-21/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2022&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;computational-neuroscience-of-vision-2&#34;&gt;Computational neuroscience of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;dynamics-of-vision&#34;&gt;Dynamics of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!--
---


## Dynamics of vision



















&lt;figure  id=&#34;figure-thorpe-2001httpslaurentperrinetgithubio2022-01-12_neurocercle21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/scheme_thorpe.jpg&#34; alt=&#34;[[Thorpe (2001)]](https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Thorpe (2001)]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!--
---


## Dynamics of vision



















&lt;figure  id=&#34;figure-precise-spiking-motifs-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency-estimate.jpg&#34; alt=&#34;Precise Spiking Motifs] ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Precise Spiking Motifs] (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-1&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency_bg.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In particular in our group, we are interested in dynamics of neural processing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The visual system is very efficient in generating a decision from the retinal image to the different stages of the visual pathways, here for a macaque monkey, a reaction of finger muscles in about 300 milliseconds.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the process of categorizing an object takes 10 layers&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-2&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the latencies are of similar in the human brain but merely scaled due to the brain size&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;as a consequence, it is thought that this efficiency is achieved by spikes that is, brief all-or-none events which are passed in the very large network which forms the brain from assemblies of neurons to others.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-3&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-4&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/figure-tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston, 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston, 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-5&#34;&gt;Dynamics of vision&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-6&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-7&#34;&gt;Dynamics of vision&lt;/h2&gt;
&lt;!-- 










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
 --&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/positional-delay.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;Flash-lag effect: MBP (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-8&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-leaky-integrate-and-fire-neuron&#34;&gt;Spiking Neural Networks: Leaky Integrate-and-Fire Neuron&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A standard LIF&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://i.stack.imgur.com/ixnrz.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-3&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Izhikevich polychronization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;yet the domain is vast, and there s lot to do in SNNs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A standard LIF&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;event-based cameras&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-1&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/HDSNN_conv.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-2&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-3&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nice kernels&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-4&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/accuracy.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;frugal computing&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h2 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-vision&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;more on &lt;a href=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/master/exampleSite/content/slides/example/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-visionhttpslaurentperrinetgithubioslides2023-04-05-ue-neurosciences-computationnellestransitionfade-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-04-05-ue-neurosciences-computationnelles/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-04-05httpsameticeuniv-amufrcourseviewphpid95116-master-1-neurosciences-et-sciences-cognitiveshttpssciencesuniv-amufrfrformationmastersmaster-neurosciencesu-1&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/course/view.php?id=95116&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-04-05]&lt;/a&gt; &lt;a href=&#34;https://sciences.univ-amu.fr/fr/formation/masters/master-neurosciences&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master 1 Neurosciences et Sciences Cognitives.&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Contact me @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;objective= understand biological vision&lt;/li&gt;
&lt;li&gt;interaction between artificial and natural NNs&lt;/li&gt;
&lt;li&gt;outline&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-05-10-phd-program_neurosciences-computationnelles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-05-10-phd-program_neurosciences-computationnelles/</guid>
      <description>&lt;section&gt;
&lt;h1 id=&#34;interactions-between-machine-learning-artificial-neural-networks-and-our-understanding-of-biological-visionhttpslaurentperrinetgithubioslides2023-05-10-phd-program_neurosciences-computationnellestransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-05-10-phd-program_neurosciences-computationnelles/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactions between machine learning, artificial neural networks and our understanding of biological vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-05-10httpslaurentperrinetgithubiotalk2023-05-10-phd-program-neurosciences-computationnelles-neuroschool-phd-program-in-neurosciencehttpsneuro-marseilleorgentrainingphd-program-computation-neuroscienceu&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-05-10]&lt;/a&gt; &lt;a href=&#34;https://neuro-marseille.org/en/training/phd-program/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroSchool PhD Program in Neuroscience&lt;/a&gt;: Computation Neuroscience&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles/qrcode.png&#34; alt=&#34;qrcode&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;Contact me @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;!-- ![logo](https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg) 
![QR code](https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles/qrcode.png) --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;welcome to the course on COMPUTATIONAL NEUROSCIENCE 2023 entitled &amp;ldquo;Machine learning to analyze complex data&amp;rdquo;&lt;/li&gt;
&lt;li&gt;objective= understand models of biological vision which are the inspiration for modern deep learning&lt;/li&gt;
&lt;li&gt;outcome= interaction between artificial and natural NNs&lt;/li&gt;
&lt;li&gt;outline= principles / CNNs / challenges / solutions&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;principles-of-vision&#34;&gt;Principles of Vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;break down problem in three different levels: Marr (+ Poggio)&lt;/li&gt;
&lt;li&gt;arbitrary, but useful division of labor&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-ilya-repin-1884httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_001.jpg&#34; alt=&#34;[An Unexpected Visitor (Ilya Repin, 1884)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Ilya Repin, 1884)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;seeing= interacting with the visual world&lt;/li&gt;
&lt;li&gt;social animals: looking at emotions&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-1&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_002.jpg&#34; alt=&#34;[An Unexpected Visitor (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;active: the eye is always moving&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fr.wikipedia.org/wiki/Alfred_Iarbous&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://fr.wikipedia.org/wiki/Alfred_Iarbous&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&amp;ldquo;1) examine the painting freely&amp;rdquo;&lt;/li&gt;
&lt;li&gt;consistency of eye traces / interindividual differences&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-2&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---age-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_004.jpg&#34; alt=&#34;[An Unexpected Visitor - *Age?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;Age?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;active: depends on task:&lt;/li&gt;
&lt;li&gt;&amp;ldquo;3) assess the ages of the characters&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what-is-the-function-of-vision-3&#34;&gt;What is the function of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-an-unexpected-visitor---how-long-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_007.jpg&#34; alt=&#34;[An Unexpected Visitor - *How long?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;How long?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;&amp;ldquo;6) surmise how long the “unexpected visitor” had been away&amp;rdquo;&lt;/li&gt;
&lt;li&gt;adaptive and efficient system&amp;hellip;&lt;/li&gt;
&lt;li&gt;yet, surprisingly&amp;hellip;.&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-rotating-snakes-akiyoshi-kitaokahttpwwwritsumeiacjpakitaokaindex-ehtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/42_rotsnakes_main.jpg&#34; alt=&#34;[Rotating Snakes *Akiyoshi KITAOKA*](http://www.ritsumei.ac.jp/~akitaoka/index-e.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating Snakes &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;the visual system experiences &amp;ldquo;hallucinations&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-1976-viking-orbiter-imagehttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Face-on-mars.jpg&#34; alt=&#34;[Cydonia Mensae, 1976, *Viking Orbiter image*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae, 1976, &lt;em&gt;Viking Orbiter image&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;these hallucinations may appear to be&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;real&lt;/li&gt;
&lt;li&gt;persistent&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_low.png&#34; alt=&#34;[Cydonia Mensae, 2007, *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae, 2007, &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  in that specific case&amp;hellip;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions--pareidoliahttpsenwikipediaorgwikipareidolia-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt; : &lt;a href=&#34;https://en.wikipedia.org/wiki/Pareidolia&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pareidolia&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cydonia-mensae-2007-mars-global-surveyorhttpsenwikipediaorgwikicydonia_mars&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Viking_moc_face_20m_high.png&#34; alt=&#34;[Cydonia Mensae, 2007, *Mars Global Surveyor*](https://en.wikipedia.org/wiki/Cydonia_(Mars))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Cydonia_%28Mars%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cydonia Mensae, 2007, &lt;em&gt;Mars Global Surveyor&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;more date = less ambiguity&lt;/li&gt;
&lt;li&gt;beware: models may also hallucinate&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-context&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;: Context&lt;/h2&gt;
&lt;p&gt;










  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Kitaoka.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ilusions of brightness or lightness &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;these may be of low level&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-context-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;: Context&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion_without.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;visual-illusionshttpslaurentperrinetgithubiopublicationperrinet-19-illusions-context-2&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-19-illusions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual illusions&lt;/a&gt;: Context&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;of showing an effect of context -&amp;gt; 3D&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;principles-of-vision-1&#34;&gt;Principles of vision?&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;computational-neuroscience-of-vision&#34;&gt;Computational neuroscience of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;computational-neuroscience-of-vision-1&#34;&gt;Computational neuroscience of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sejnowski-koch--churchland-1998httpwwwhmsharvardedubssneurobornlabnb204paperssejnowski-koch-churchland-science1988pdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Churchland92.png&#34; alt=&#34;[[Sejnowski, Koch &amp;amp; Churchland, 1998](http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;50%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sejnowski, Koch &amp;amp; Churchland, 1998&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;anatomy-of-the-human-visual-system&#34;&gt;Anatomy of the Human Visual system&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.readkong.com/static/06/b0/06b09f0235ae7fcf29438ce317c10e60/optogenetic-visual-cortical-prosthesis-9612386-7.jpg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;61%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;human-visual-system--the-hmax-model&#34;&gt;Human Visual system : the HMAX model&lt;/h2&gt;


















&lt;figure  id=&#34;figure-serre-and-poggio-2006httpsbiologystackexchangecomquestions10955ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.stack.imgur.com/ZlFnp.png&#34; alt=&#34;[[Serre and Poggio, 2006]](https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;65%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://biology.stackexchange.com/questions/10955/ventral-stream-pathway-and-architecture-proposed-by-poggios-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Serre and Poggio, 2006]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy&#34;&gt;Convolutional Neural Networks : Hierarchy&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks-cnns&#34;&gt;Convolutional Neural Networks (CNNs)&lt;/h2&gt;


















&lt;figure  id=&#34;figure-jérémie--lp-2023httpslaurentperrinetgithubiopublicationjeremie-23-ultra-fast-cat&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.mdpi.com/vision/vision-07-00029/article_deploy/html/images/vision-07-00029-g003.png&#34; alt=&#34;[[Jérémie &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/jeremie-23-ultra-fast-cat/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/jeremie-23-ultra-fast-cat/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jérémie &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;sota&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;!-- ---

## Anatomy of the Human Visual system



















&lt;figure  id=&#34;figure-wikipediahttpsenwikipediaorgwikivisual_system&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e4/Voies_visuelles3.svg&#34; alt=&#34;[[Wikipedia]](https://en.wikipedia.org/wiki/Visual_system)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Visual_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Wikipedia]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;


















&lt;figure  id=&#34;figure-hubel--wiesel-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/scientists.jpg&#34; alt=&#34;[Hubel &amp;amp; Wiesel, 1962]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Hubel &amp;amp; Wiesel, 1962]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;primary-visual-cortex-hubel--wiesel-1&#34;&gt;Primary visual cortex: Hubel &amp;amp; Wiesel&lt;/h2&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles/hubel_wiesel.webm&#34; type=&#34;video/webm&#34;&gt;
&lt;/video&gt;

&lt;p&gt;[Hubel &amp;amp; Wiesel, 1962] - from &lt;a href=&#34;https://www.youtube.com/@Neuroslicer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Neuroslicer&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KE952yueVLA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=KE952yueVLA&lt;/a&gt; -
&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles/hubel_wiesel.webm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles/hubel_wiesel.webm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;simple cell 4:09&lt;/li&gt;
&lt;li&gt;excerpt &lt;a href=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--hierarchy-1&#34;&gt;Convolutional Neural Networks : hierarchy&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;backpropagation is not bioplausible&lt;/li&gt;
&lt;li&gt;modification&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;One-dimensional &lt;a href=&#34;https://en.wikipedia.org/wiki/Convolution#Discrete_convolution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discrete convolution&lt;/a&gt; (eg in time) with a kernel $g$ of radius $K$:
$$
(f \ast g)[n]=\sum_{m=-K}^{K} f[n-m] \cdot g[m]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-1&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Convolution of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[x-i, y-j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-2&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross-correlation&lt;/strong&gt; of an image (two-dimensional) with a kernel $g$ of radius $K\times K$:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{i=-K}^{K} \sum_{j=-K}^{K} f[x+i, y+j] \cdot g[i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-3&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;


















&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/convolution-layer-a.png&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-4&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of an image defined on several  channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--mathematics-5&#34;&gt;Convolutional Neural Networks : Mathematics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Correlation of a multi-channel image for multiple output channels (note &lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the order of the indices&lt;/a&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast \tilde{g})[k, x, y] = \sum_{c=1}^{C} \sum_{i,j} f[c, x+i, y+j] \cdot g[k, c, i, j]
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--predictive-coding&#34;&gt;Convolutional Neural Networks : Predictive coding&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1.svg&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;adding sparse coding + feedback&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--predictive-coding-1&#34;&gt;Convolutional Neural Networks : Predictive coding&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/BoutinFranciosiniChavaneRuffierPerrinet20face.png&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;interpretable features&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;


















&lt;figure  id=&#34;figure-bosking-et-al-1997&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Bosking97Fig4.jpg&#34; alt=&#34;[Bosking *et al*, 1997]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Bosking &lt;em&gt;et al&lt;/em&gt;, 1997]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;convolutional-neural-networks--topography-1&#34;&gt;Convolutional Neural Networks : Topography&lt;/h2&gt;


















&lt;figure  id=&#34;figure-boutin-et-al-2022httpslaurentperrinetgithubiopublicationfranciosini-21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/franciosini-21/featured_hu9b69386a773810bf9e1d727831735035_76078_720x2500_fit_q75_h2_lanczos.webp&#34; alt=&#34;[[Boutin *et al*, 2022](https://laurentperrinet.github.io/publication/franciosini-21/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/franciosini-21/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2022&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;computational-neuroscience-of-vision-2&#34;&gt;Computational neuroscience of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;dynamics-of-vision&#34;&gt;Dynamics of vision&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;!--
---


## Dynamics of vision



















&lt;figure  id=&#34;figure-thorpe-2001httpslaurentperrinetgithubio2022-01-12_neurocercle21&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/scheme_thorpe.jpg&#34; alt=&#34;[[Thorpe, 2001]](https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/#/2/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Thorpe, 2001]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!--
---


## Dynamics of vision



















&lt;figure  id=&#34;figure-precise-spiking-motifs-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency-estimate.jpg&#34; alt=&#34;Precise Spiking Motifs] ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Precise Spiking Motifs] (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-1&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency_bg.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In particular in our group, we are interested in dynamics of neural processing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The visual system is very efficient in generating a decision from the retinal image to the different stages of the visual pathways, here for a macaque monkey, a reaction of finger muscles in about 300 milliseconds.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the process of categorizing an object takes 10 layers&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-2&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-visual-latencies-see-reviewhttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/visual-latency.jpg&#34; alt=&#34;Visual latencies ([see review](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Visual latencies (&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see review&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;1 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the latencies are of similar in the human brain but merely scaled due to the brain size&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;as a consequence, it is thought that this efficiency is achieved by spikes that is, brief all-or-none events which are passed in the very large network which forms the brain from assemblies of neurons to others.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-3&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-4&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sensorimotor-delays-perrinet--friston-2014httpslaurentperrinetgithubiopublicationperrinet-adams-friston-14&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/figure-tsonga.jpg&#34; alt=&#34;Sensorimotor delays ([Perrinet &amp;amp; Friston, 2014](https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/))&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;75%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sensorimotor delays (&lt;a href=&#34;https://laurentperrinet.github.io/publication/perrinet-adams-friston-14/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perrinet &amp;amp; Friston, 2014&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-5&#34;&gt;Dynamics of vision&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/perrinet-19-temps/flash_lag.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-6&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diagonal-markov-model-khoei-et-al-2017httpslaurentperrinetgithubiopublicationkhoei-masson-perrinet-17&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/Khoei_2017_PLoSCB/raw/master/figures/FLE_DiagonalMarkov.jpg&#34; alt=&#34;Diagonal markov model ([Khoei *et al*, 2017](https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/)).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Diagonal markov model (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;).
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-7&#34;&gt;Dynamics of vision&lt;/h2&gt;
&lt;!-- 










  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/PBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/MBP_spatial_readout.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
 --&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/sciblog/files/2016-07-07_EDP-proba/figures/positional-delay.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;Flash-lag effect: MBP (&lt;a href=&#34;https://laurentperrinet.github.io/publication/khoei-masson-perrinet-17/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Khoei &lt;em&gt;et al&lt;/em&gt;, 2017&lt;/a&gt;)&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;dynamics-of-vision-8&#34;&gt;Dynamics of vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-leaky-integrate-and-fire-neuron&#34;&gt;Spiking Neural Networks: Leaky Integrate-and-Fire Neuron&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A standard LIF&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://i.stack.imgur.com/ixnrz.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-3&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Izhikevich polychronization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;yet the domain is vast, and there s lot to do in SNNs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This hypothesis is reviewed with respect to our knowledge of the neurobiology, for instance in the hippocampus of rodents. We also review&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A standard LIF&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;event-based cameras&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-1&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/HDSNN_conv.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-2&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A nice HSD neuron&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, we show how precise spike times may be used to detect the direction of motion from such a stream of events in an ultrafast fashion.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-3&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nice kernels&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neuromorphic-engineering-4&#34;&gt;Spiking Neural Networks in neuromorphic engineering&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-hd-snn-neural-network&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/accuracy.png&#34; alt=&#34;The HD-SNN neural network.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The HD-SNN neural network.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;frugal computing&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h2 id=&#34;artificial-neural-networks-and-machine-learning-applied-to-the-understanding-of-biological-vision&#34;&gt;Artificial neural networks and machine learning applied to the understanding of biological vision&lt;/h2&gt;


















&lt;figure  id=&#34;figure-marr-1982httpsoutdexyz2020-01-12overappreciated-arguments-marrs-three-levelshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://outde.xyz/img/Rawski/Marr/3Lvls.jpg&#34; alt=&#34;[[Marr, 1982](https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://outde.xyz/2020-01-12/overappreciated-arguments-marrs-three-levels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marr, 1982&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;more on &lt;a href=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/master/exampleSite/content/slides/example/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h1 id=&#34;interactions-between-machine-learning-artificial-neural-networks-and-our-understanding-of-biological-visionhttpslaurentperrinetgithubioslides2023-05-10-phd-program_neurosciences-computationnellestransitionfade-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-05-10-phd-program_neurosciences-computationnelles/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactions between machine learning, artificial neural networks and our understanding of biological vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-05-10httpslaurentperrinetgithubiotalk2023-05-10-phd-program-neurosciences-computationnelles-neuroschool-phd-program-in-neurosciencehttpsneuro-marseilleorgentrainingphd-program-computation-neuroscienceu-1&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-05-10]&lt;/a&gt; &lt;a href=&#34;https://neuro-marseille.org/en/training/phd-program/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeuroSchool PhD Program in Neuroscience&lt;/a&gt;: Computation Neuroscience&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles/qrcode.png&#34; alt=&#34;qrcode&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;Contact me @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;!-- ![logo](https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg) 
![QR code](https://laurentperrinet.github.io/talk/2023-05-10-phd-program-neurosciences-computationnelles/qrcode.png) --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;thanks for your attention&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-09-08_fresnel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-09-08_fresnel/</guid>
      <description>&lt;section&gt;
&lt;h1 id=&#34;event-based-visionhttpslaurentperrinetgithubioslides2023-09-08_fresneltransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-09-08_fresnel/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event-based vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-09-08httpslaurentperrinetgithubiotalk2023-09-08-fresnel-séminaire-institut-fresnelhttpswwwfresnelfrspipspipphparticle2453langfru&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-09-08-fresnel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-09-08]&lt;/a&gt; &lt;a href=&#34;https://www.fresnel.fr/spip/spip.php?article2453&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Séminaire institut Fresnel&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;!-- &lt;img src=&#34;https://laurentperrinet.github.io/talk/2023-09-08_fresnel/qrcode.png&#34; alt=&#34;qrcode&#34; height=&#34;130&#34;/&gt; --&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-09-08-fresnel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2023-09-08-fresnel/&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;em&gt;Hello&lt;/em&gt;, I&amp;rsquo;m Laurent Perrinet from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit, and during this seminar at the Institut Fresnel, I&amp;rsquo;ll be presenting &lt;em&gt;event-driven cameras&lt;/em&gt;, a new technology in the field of imaging, and the impact of this technology on our understanding of vision. I&amp;rsquo;d like to &lt;em&gt;thank&lt;/em&gt; Loic le Goff for his kind invitation, and all of you for coming. These slides are available from my website, along with a number of references. The &lt;em&gt;outline&lt;/em&gt; of the talk is as follows: first, we&amp;rsquo;ll describe what an event-driven camera is - in particular, by comparing it to a conventional camera; then, we&amp;rsquo;ll show some examples of applications of these cameras with dedicated algorithms; and finally, we&amp;rsquo;ll present how our knowledge of biological mechanisms in neuroscience can enable us to improve these algorithms.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;sensing-light&#34;&gt;Sensing light&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  First of all, the general aim of &lt;em&gt;imaging&lt;/em&gt; is to represent a light signal, i.e. a luminous intensity, a color, distributed over the visual field, giving us a vivid impression of the visual scene before our eyes.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;http://lepassetempsderose.l.e.pic.centerblog.net/fddea7fb.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  This is perfectly illustrated in this &lt;em&gt;galloping horse&lt;/em&gt;. We get a &lt;em&gt;vivid&lt;/em&gt; impression of movement. Thanks to a rapid sequence of still images consistent with the scene being represented. This technique clearly exploits a visual &lt;em&gt;illusion&lt;/em&gt;, because we know that at each point in the visual space, the light signal is made up of a &lt;em&gt;continuous&lt;/em&gt; stream of an analogous signal representing the energy of the photos.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  This technique is inspired by the research carried out by Étienne-Jules &lt;em&gt;Marey&lt;/em&gt; (&lt;a href=&#34;https://en.wikipedia.org/wiki/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/&lt;/a&gt;Étienne-Jules_Marey), who gave his name to the ISM, under the term &lt;em&gt;chronophotography&lt;/em&gt;, which notably enabled later Muybridge to demonstrate the mechanism of a horse&amp;rsquo;s gallop. In particular, Marey literally used a camera mounted on a &lt;em&gt;gun&lt;/em&gt;-like structure to shoot a visual scene.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://media.giphy.com/media/4Y8PqJGFJ21CE/giphy.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  The use of such dynamic &lt;em&gt;visualization&lt;/em&gt; is crucial in the scientific field, whether in biology or physics, as it enables us to quantify the characteristics of the experiment being carried out - I&amp;rsquo;m thinking, for example, of quantifying the movements and number of bacteria in a biological assay.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  In the laboratory, we use it in particular to quantify &lt;em&gt;eye movements&lt;/em&gt; when a stimulus is presented to an observer.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representing-spatio-temporal-luminous-information&#34;&gt;Representing spatio-temporal luminous information&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/analog_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;To better understand the mechanism behind this technology, let&amp;rsquo;s imagine that we represent a &lt;em&gt;single pixel&lt;/em&gt; in the space of the visual field. Here, I&amp;rsquo;ve taken a grayscale &lt;em&gt;video&lt;/em&gt; from an episode from the Monty Python Flying Circus TV series. In this way, we can represent the evolution of the &lt;em&gt;log intensity&lt;/em&gt; of the light signal as a function of time.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The&amp;#43;Horse&amp;#43;in&amp;#43;Motion,&amp;#43;1878.%C2%A0Eadweard&amp;#43;Muybridge&amp;#43;%28b.&amp;#43;9&amp;#43;April,&amp;#43;1830%29The&amp;#43;first&amp;#43;movie&amp;#43;ever&amp;#43;made,&amp;#43;from&amp;#43;still&amp;#43;photographs..gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The+Horse+in+Motion,+1878.%C2%A0Eadweard+Muybridge+(b.+9+April,+1830)The+first+movie+ever+made,+from+still+photographs..gif&lt;/a&gt;
&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&lt;/a&gt;
&lt;a href=&#34;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;amp;h=600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;h=600&lt;/a&gt;
&lt;a href=&#34;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&lt;/a&gt;
&lt;a href=&#34;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif&#34;&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-temporal-discretization&#34;&gt;Frame-Based Camera: Temporal discretization&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/frame-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  From this representation, expressed in continuous time, we can &lt;em&gt;discretize&lt;/em&gt; time and measure the log intensity at regular time intervals. The difference between two images gives the &lt;em&gt;temporal resolution&lt;/em&gt;, and its inverse gives the number of images per second. This is the representation classically used in chronophotography, but also in all conventional video stream &lt;em&gt;acquisition and viewing&lt;/em&gt; technologies.
This technology is highly efficient for a wide range of signals. However, it does have certain &lt;em&gt;limitations&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-aliasing&#34;&gt;Frame-Based Camera: Aliasing&lt;/h2&gt;


















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/frames.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;85%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Let&amp;rsquo;s take the &lt;em&gt;example&lt;/em&gt; of three colored cubes rotating in a frontal axis along a circle. Because of temporal resolution and the length of time the shutter is open, the images captured at each instant can produce a certain amount of &lt;em&gt;blur&lt;/em&gt;, and movement can become increasingly difficult to estimate. If the movement of the cubes begins to accelerate, temporal &lt;em&gt;aliasing&lt;/em&gt; can be observed.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-wagon-wheel-illusion&#34;&gt;Frame-Based Camera: Wagon-Wheel Illusion&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sam-brinson-2020httpswwwsambrinsoncomnature-of-perception&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://vignette.wikia.nocookie.net/revengeristsconsortium/images/2/25/Whee.gif/revision/latest/scale-to-width-down/340?cb=20141209071330&#34; alt=&#34;[[Sam Brinson, 2020](https://www.sambrinson.com/nature-of-perception/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://www.sambrinson.com/nature-of-perception/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sam Brinson, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This phenomenon is particularly striking when we look at a spinning &lt;em&gt;wheel&lt;/em&gt; at high speed, and this wheel&amp;rsquo;s rotational speed is such that two successive images give the illusion that the movement is in the opposite direction to the real, physical moment. It&amp;rsquo;s striking here in this car wheel, where you can perceive that the central hub appears motionless, and the wheel is perceived as turning in the &lt;em&gt;opposite direction&lt;/em&gt; to the physical rolling motion on the road.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-camera&#34;&gt;Event-Based Camera&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  Now let&amp;rsquo;s introduce the &lt;em&gt;event camera&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-1&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This consists of a conventional sensor which, like most CMOS-type sensors, transforms visual energy into an electric current. However, there are two fundamental differences, inspired by our knowledge of the retina, which is the sensor of vision. Firstly, each pixel of this sensor is &lt;em&gt;independent&lt;/em&gt; and is not cadenced according to a global clock. Secondly, each pixel will follow the evolution of the log intensity and signal an event when an increment or decrement exceeds a threshold. Let&amp;rsquo;s explain this mechanism in relation to our analog signal.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-2&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  First of all, the signal will evolve over time, and we can see here that it may cross a &lt;em&gt;threshold&lt;/em&gt;. An event will then be produced by this pixel. Here, the &lt;em&gt;event&lt;/em&gt; is of negative polarity, as it corresponds to an decrement.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-3&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_2.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-4&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_5.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Then, the signal will continue its course in time and cross a threshold again, possibly once more, at which point a new event will be produced. Here, we&amp;rsquo;re also seeing increments, ie positive polarizations.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-5&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_10.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-6&#34;&gt;Event-Based Camera&lt;/h2&gt;
&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_20.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  And so on, this simple mechanism will produce a &lt;em&gt;stream&lt;/em&gt; of events for each pixel, this &lt;em&gt;list&lt;/em&gt; being made up of the times of occurrence and the corresponding polarities.
&lt;/aside&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-7&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_-1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-8&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Let&amp;rsquo;s show it now applied to the whole analog signal.
It&amp;rsquo;s worth noting in particular that, compared with frame-by-frame representations, this one is particularly &lt;em&gt;sparse&lt;/em&gt;: in particular, a signal with very few changes can be represented by just a few events. This is a very useful feature, not only because it saves &lt;em&gt;bandwidth&lt;/em&gt;, but also because it allows us to concentrate the &lt;em&gt;computations&lt;/em&gt; around the few events that represent the image. It&amp;rsquo;s also a fundamental feature of neuron function in the brain, and we&amp;rsquo;ll come back to it later.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-9&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;!-- 

















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/events.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;Finally, we obtain a list of events for each pixels  which can be &lt;em&gt;merged&lt;/em&gt; for the image as a whole, forming a list of events, including pixel addresses, times of occurrence and polarities. As they are generated over time, they are naturally arranged in order of occurrence. All these events are then transmitted in &lt;em&gt;real time&lt;/em&gt; to the output bus, typically by means of a USB3 connection. Note the analogy between this representation and the one made in the optic nerve that connects our retina to the rest of the brain: indeed, the million ganglion cells that make up the retina&amp;rsquo;s output emit action potentials, which are the only source of information that leaves the retina via the &lt;em&gt;optic nerve&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-10&#34;&gt;Event-Based Camera&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sensor&lt;/th&gt;
&lt;th&gt;Range&lt;/th&gt;
&lt;th&gt;Framerate&lt;/th&gt;
&lt;th&gt;Resolution&lt;/th&gt;
&lt;th&gt;Power&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Human eye&lt;/td&gt;
&lt;td&gt;60 (?) dB&lt;/td&gt;
&lt;td&gt;300 (?) fps&lt;/td&gt;
&lt;td&gt;100 (?) Mpx&lt;/td&gt;
&lt;td&gt;10 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DSLR&lt;/td&gt;
&lt;td&gt;44.6 dB&lt;/td&gt;
&lt;td&gt;120     fps&lt;/td&gt;
&lt;td&gt;2&amp;ndash;20   Mpx&lt;/td&gt;
&lt;td&gt;30  W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ultra-high speed&lt;/td&gt;
&lt;td&gt;64   dB&lt;/td&gt;
&lt;td&gt;10^4 fps&lt;/td&gt;
&lt;td&gt;0.3&amp;ndash;4  Mpx&lt;/td&gt;
&lt;td&gt;300 W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Event-based&lt;/td&gt;
&lt;td&gt;120  dB&lt;/td&gt;
&lt;td&gt;10^6 fps&lt;/td&gt;
&lt;td&gt;0.1&amp;ndash;2  Mpx&lt;/td&gt;
&lt;td&gt;30 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;There are several properties of event-driven cameras that make them remarkable. First of all, the &lt;em&gt;temporal precision&lt;/em&gt; of events is of the order of microseconds, enabling a theoretical frame rate of the order of a million images per second to be reached. This can be compared with a conventional camera, which is of the order of a hundred images per second, or with a high-speed camera, which can reach 10,000 images per second. It is difficult to estimate the sampling frequency of human perception, because while 25 frames per second is often sufficient for movie viewing, it has been shown that the human eye can distinguish temporal details up to 300 or even 1,000 frames per second. It&amp;rsquo;s worth noting that the &lt;em&gt;spatial resolution&lt;/em&gt; of these event cameras is often relatively modest, in the order of megapixels, but this is not a technical limitation, but rather due to the technological applications in which these cameras are commonly used. Compared with conventional cameras, which will consume several watts, event cameras consume very little electrical &lt;em&gt;energy&lt;/em&gt;, in the order of 10 milliwatts, a consumption equivalent to that of the human eye.  Another important feature of these cameras is their ability to detect a very wide &lt;em&gt;range&lt;/em&gt; of luminosity, far exceeding that of conventional cameras at 120 dB (a factor of a million, compared with the human eye&amp;rsquo;s factor of 1 in a thousand between full moon and full sun),&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Event_camera#Functional_description&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Event_camera#Functional_description&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;more in &lt;a href=&#34;https://arxiv.org/pdf/1904.08405.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1904.08405.pdf&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-11&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This ability to &lt;em&gt;adapt&lt;/em&gt; to changing light conditions can be illustrated by going back to our analog signal and its event representation, and imagining. A typical example would be an autonomous car driving in daylight, entering and leaving a &lt;em&gt;tunnel&lt;/em&gt;, involving changes in brightness by a factor of several thousand.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-12&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_low.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Here we have a division by a factor 8 of the signal in the middle section. It will be reported by a frame-based camera. In an event-based camera, this is represented here by a &lt;em&gt;sharp decrement&lt;/em&gt; in log intensity space and clearly indicated by events of negative polarity, but we can see that since this is a camera that uses log intensity, dividing the light signal produces the &lt;em&gt;same signal&lt;/em&gt; course over time, and therefore events that are identical.  Event-driven cameras are therefore particularly well-suited to &lt;em&gt;dynamic signals&lt;/em&gt;, where the lighting context can change drastically.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-computer-vision&#34;&gt;Event-Based Computer vision&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;These cameras therefore look very promising for future applications, particularly for embedded applications, but also for applications linked to scientific experiments. However, we can see that the image &lt;em&gt;representation&lt;/em&gt; is completely different, that is, we can no longer consider static images that follow one another at a regular rate, and for which we could have applied the algorithms that have been developed for decades in the field of &lt;em&gt;computer vision&lt;/em&gt;. We end up with a signal that corresponds to events that are transmitted as a stream from the camera. And we have to reinvent all computer vision algorithms to make them &lt;em&gt;event-driven&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;TODO: the process is active driven by the signal compared to acquired&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-recognition&#34;&gt;Always-on Object Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/hots.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  The first algorithm we developed with Antoine Grimaldi, who is a PhD student, and in collaboration with Sio Ieng and Ryad Benosman of Sorbonne University, who are recognized researchers in the development of this type of camera, is an improvement on an existing algorithm, &lt;em&gt;HOTS&lt;/em&gt;. This algorithm uses a relatively classical convolutional and hierarchical information processing architecture, which passes information &amp;ldquo;forward&amp;rdquo; from the camera and its event representation, and then through different processing layers to converge on a high-level representation that can be used for classification, in this case to recognize the identity of the digit presented as input, i.e. an eight digit. A fundamental feature of this algorithm is that it transforms the event representation into multiplexed, parallel channels, which analogously represent the temporal pattern of events, or &amp;ldquo;&lt;em&gt;temporal surface&lt;/em&gt;&amp;rdquo;. These are represented in the different layers by the individual temporal surfaces. An interesting feature of this algorithm is that learning in each of the layers is &lt;em&gt;unsupervised&lt;/em&gt;, which is a significant improvement over conventional deep learning algorithms that assume that a classification error signal can be back-propagated along the entire hierarchy, which is notoriously incorrect. Starting from this algorithm, we improved it by including neuro-biological knowledge, especially about the balance between different parallel communication pathways by including &lt;em&gt;homeostasis&lt;/em&gt; rules.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-gesture-recognition&#34;&gt;Always-on Object Gesture Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_offline.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;To illustrate the results of our algorithm, we applied a classic camera dataset involving the classification of 10 different types of human &lt;em&gt;gestures&lt;/em&gt;. These biological movements are, for example, clapping hands, saying hello or a drum movement. The chance level is therefore at 10%, and we have observed that when all events have been processed, the &lt;em&gt;original&lt;/em&gt; algorithm achieves a performance of around 70%. By adding &lt;em&gt;homeostasis&lt;/em&gt;, we have reached a higher level of 82%, demonstrating the usefulness of using neuroscientific knowledge to improve machine learning algorithms.&lt;/p&gt;
&lt;p&gt;We also built on a fundamental characteristic of biological systems. In fact, this kind of algorithm is classically used to process the flow of events, but classification is only used as a last resort when all the events have been processed. We have modified the algorithm so that this classification can be done &lt;em&gt;online&lt;/em&gt;, in real time, event by event. In this way, processing in the various layers is triggered by the arrival of each event, which is propagated from the camera through all the layers to the classification layer.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-gesture-recognition-1&#34;&gt;Always-on Object Gesture Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_online.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  What&amp;rsquo;s more interesting is that we were also able to show the &lt;em&gt;evolution&lt;/em&gt; of the average performance obtained on a data set, and as a function of the number of events processed by the algorithm. The blue curve shows that if below 10 events, we remain at the level of chance, we then experience a gradual increase in performance that reaches the level of the original algorithm with ten thousand events, and exceeds this &lt;em&gt;performance&lt;/em&gt; when we have even 10 times more. A major advantage of this algorithm is that it can be asked to classify the nature of what it sees in its event camera, not once the entire signal has been processed by the system, but at any time. This characteristic is essential in biology. For example, imagine you&amp;rsquo;re on the savannah and a &lt;em&gt;lion&lt;/em&gt; jumps out at you. You won&amp;rsquo;t have the flexibility to wait for the video sequence to finish processing before making the right decision, which is to flee. Another variant in our algorithm consists of selecting the output classification events based on a calculation of the precision for each event. By using a &lt;em&gt;threshold&lt;/em&gt; on this precision, we can achieve a very good level of performance, with just a hundred events, and so achieve a characteristic that is common in biological networks, i.e. that a decision is not taken gradually, but emerges abruptly (here after 200 events) and then improves and stabilizes.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  We have therefore illustrated the use of &lt;em&gt;event-driven&lt;/em&gt; cameras on a particular algorithm. This algorithm has the particularity of processing the flow of events coming from the camera event by event, so that potentially each of these events triggers a cascade of mechanisms in the different processing layers, and thus enables a classification value to be updated at any given moment. This type of operation is characteristic of the way neurons work in the brain, i.e. using an event-based representation of information processing. This is what we call &lt;em&gt;spiking neural networks&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-tonic-manualhttpstonicreadthedocsioenlatest_imagesneuron-modelspng&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; alt=&#34;[[Tonic manual](https://tonic.readthedocs.io/en/latest/_images/neuron-models.png)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tonic manual&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Indeed, most neural networks used in deep learning use an analog representation. This is illustrated in this figure, which represents the various analog inputs to a formal neuron as they are linearly integrated by the synapses, then transformed by a non-linear function to generate an activation which is itself analog. This basic &lt;em&gt;perceptron&lt;/em&gt; principle is at the foundation of all existing neural networks, and in particular enables the construction of convolutional-type networks which are currently the champions for image classification, having outperformed human performance for several years. However, while this is true for static images, it can become prohibitively expensive with videos. This is why it can be interesting to use &lt;em&gt;spiking&lt;/em&gt; neurons instead, which, instead of receiving an analog input, will receive events that will trigger cascades of mechanisms in the neuronal cell, notably represented by the cell&amp;rsquo;s membrane potential. Typically, we&amp;rsquo;ll include a threshold for triggering action potential in this cell, which will generate new output events on the cell&amp;rsquo;s axon.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-lif-neuron&#34;&gt;Spiking Neural Networks: LIF Neuron&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is illustrated in this &lt;em&gt;animation&lt;/em&gt;, which shows how we can transform a list of input events by giving them different weights, and then &lt;em&gt;integrate&lt;/em&gt; them into the cell&amp;rsquo;s soma to generate output events.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-neuromorphic-hardware&#34;&gt;Spiking Neural Networks: neuromorphic hardware&lt;/h2&gt;


















&lt;figure  id=&#34;figure-loihi-2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg&#34; alt=&#34;Loihi 2&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Loihi 2
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;This new type of representation represents a &lt;em&gt;paradigm shift&lt;/em&gt; in computation, in the same way that event-driven cameras have brought with them a paradigm shift in image representation. The development of these two new algorithms, which use impulse neural networks, is accompanied by the development of new neuromorphic chips, such as the Loihi 2 chip developed by Intel, which replaces a central computing unit with a massively parallelized &lt;em&gt;array&lt;/em&gt; of elementary event-driven computing units. As with event-driven cameras, this has the dual advantage of being very fast and consuming very little energy. Other types of &lt;em&gt;neuromorphic chips&lt;/em&gt; are currently being developed and may soon be used instead of conventional CPUs or GPUs.&lt;/p&gt;
&lt;figure  id=&#34;figure-propheseehttpsdocspropheseeaistableconceptshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; alt=&#34;[Prophesee](https://docs.prophesee.ai/stable/concepts.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://docs.prophesee.ai/stable/concepts.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prophesee&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Loihi: &lt;a href=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;amp;strip=none&amp;amp;ssl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;strip=none&amp;ssl=1&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://i.stack.imgur.com/ixnrz.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Spiking neural networks therefore seem very promising for processing the output of event-driven cameras, but the study of &lt;em&gt;neurophysiology&lt;/em&gt; shows us that their operation can sometimes seem incongruous and far from the perceptron. In this first example, taken from an article by Mainen and Sejnowski from 1995, we see the response of the same neuron to several &lt;em&gt;repetitions&lt;/em&gt; of a stimulation in panel A. At the top, we see the membrane potential of this neuron in response to a 200 Pico ampere &lt;em&gt;current step&lt;/em&gt;, which shows that the membrane potential is not reproducible across different trials. This is illustrated by showing the spike response over time for the different trials, which shows a strong alignment at the start of stimulation, but that this diffuses little by little, so that after around 750 milliseconds there is no longer any coherence between the different trials. The situation is different in panel B, where the neuron is stimulated with &lt;em&gt;noise&lt;/em&gt;. In this case, the responses are so precise for the different trials that the membrane potential traces are overlapping almost exactly. The subtlety of this paper lies in its use of a &lt;em&gt;frozen&lt;/em&gt; noise, i.e. one that is repeated unchanged across trials. In this way, it demonstrates that neurons are not so much sensitive to analog values presented in the form of square pulses, but rather to dynamic signals for which they will respond with very high precision in the dynamic domain.&lt;/p&gt;

&lt;/aside&gt;
&lt;!-- 
---


## Spiking Neural Networks in neurobiology



















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;



&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this other example, I show a simulation that reproduces the 1999 paper by Diesmann and colleagues. This &lt;em&gt;theoretical model&lt;/em&gt; considers ten groups of 100 neurons that are connected from group to group. An interesting property of this system is to show that for the same stimulation, i.e. for the same number of spikes, information can propagate from group to group only if it is sufficiently &lt;em&gt;concentrated in time&lt;/em&gt;. For the first two groups, the information is too dispersed in the first group and spreads progressively and increasingly in subsequent groups. Above a certain threshold, the information formed by a group of relatively synchronous spikes is correctly transmitted to the various groups in the network. This &lt;em&gt;non-linear&lt;/em&gt; behavior is one of the characteristics of spiking networks, giving them a certain richness, but also a certain complexity.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
A third example shows an experiment conducted by Rosa Cossart&amp;rsquo;s group at INMED and recently published by Haimerl and colleagues. It shows the results of &lt;em&gt;calcium fluorescence&lt;/em&gt; imaging recordings in mice. By arranging the different neurons in &lt;em&gt;temporal order of activation&lt;/em&gt;, it shows a sequential activation of these neurons, a mechanism which resembles the model mentioned earlier. These activation groups are strongly correlated with the &lt;em&gt;motor behavior&lt;/em&gt; of the mouse, as described in the graph at the top. Of particular interest is the fact that these sequences of activity are stable over time and can be recorded on a &lt;em&gt;subsequent day&lt;/em&gt;. This illustrates the importance of dynamics in the integration of neural computations.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These observations have led us to &lt;em&gt;review&lt;/em&gt; neurobiological evidence around the existence of a neural representation that would use the relative time of spikes as a means of representing information. In particular, it is possible to use the conduction &lt;em&gt;delays&lt;/em&gt; that exist in the transmission of spikes from one neuron to another. It may seem paradoxical, but these delays are not simply a constraint, but can help to improve our ability to represent information by way of &lt;em&gt;spiking motifs&lt;/em&gt;.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we consider, for example, this ultra-simplified network consisting of three presynaptic neurons and two output neurons connected by &lt;em&gt;heterogeneous&lt;/em&gt; delays, then we can see that a &lt;em&gt;synchronous&lt;/em&gt; input will generate membrane activity in the two output neurons at different times, so the threshold will never be reached, and these neurons will not produce an output impulse. On the other hand, if these delays are such that the action potentials converge on the neuron at the same instant, then these contributions will be able to sum up at the &lt;em&gt;same instant&lt;/em&gt; and produce an output spike, as denoted here by the red bar.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To better understand this mechanism, let&amp;rsquo;s return to our animation of a spiking neuron. Action potentials arrive at the neuron and are &lt;em&gt;immediately&lt;/em&gt; transmitted to the neuron&amp;rsquo;s cell body to be integrated and potentially generate a spike.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-3&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When using &lt;em&gt;heterogeneous&lt;/em&gt; delays, the situation is different, as the information will take a differential time to arrive or not at the neuron&amp;rsquo;s cell body. Note that if we include a particular &lt;em&gt;spiking motif&lt;/em&gt;, which we have here highlighted by green action potentials, then these converge at the same instant thanks to the delay. We will therefore have a detection in the neuron in the form of a new impulse.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
We used this theoretical principle in an algorithm for detecting movement in an image. To do this, we first generated event data using natural images that are set in motion along trajectories that resemble those produced by free exploration of the visual scene. You&amp;rsquo;ll notice several features of the event-driven output, such as the fact that faster motion generates more spikes, or that edges oriented parallel to one direction produce few changes, and therefore little spike output - the so-called aperture problem.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-1&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/laurentperrinet/figures/7f382a8074552de1a6a0c5728c60d48788b5a9f8/animated_neurons/conv_HDSNN.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We then used a neural network with a classical architecture, which we enhanced by using an impulse representation that takes into account different possible synaptic delays. In this figure, we have represented the input in the left grid, which represents the occurrence of spikes of positive or negative polarity. Then we have represented different processing channels denoted by the colors green and orange, which are applied to this input to produce membrane activity. As illustrated above, this activity will produce output pulses, notably in synaptic connection nuclei, with heterogeneous delays corresponding to the detection of precise spatio-temporal patterns.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-2&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One advantage of this network is that it is differentiable, enabling us to apply classical machine learning methods, notably supervised learning. We then see the emergence of different convolution kernels, and here I represent a subset of its kernels for different directions, as denoted by the red arrows on the left of the graph. It shows the kernels obtained on the spatial representation according to the different columns, and each row represents the different delays from a delay of one on the right to a delay of 12 time steps on the left. Detectors that follow the motion emerge. For example, for the top line from top to bottom. These kernels integrate both positive neurons in red and negative polarity inputs in blue.
Such spatio-temporal filtering is observed in neurobiology, but to my knowledge had never been observed in a model of spiking neurons trained under natural conditions.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-3&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_raw.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will now study the performance of this network in detecting motion in the flow of events entering the network. When we use all the weights of the convolution kernel, we get a very good performance of the order of 99%, represented by the black dot in the top right-hand corner. Note that in the kernels we&amp;rsquo;ve seen emerge, most of the synaptic weights are close to zero, so we might consider removing some of these weights, as this can be shown to reduce the number of event calculations required.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-4&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_shortening.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
This is what we&amp;rsquo;ve done, by first removing the parts of the core corresponding to the longest delays. This &amp;ldquo;shortens&amp;rdquo; the kernel. We quickly observed a degradation in performance, which reached half-saturation when we reduced the number of weights by around 50%. This demonstrates the importance of integrating information that is quite distant and structured over time.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-5&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a second step, we performed a pruning operation, which consists in progressively removing the weights that are the weakest. This time, performance remains optimal over a wide compression range, and we reach half-saturation when we have removed around 99.8% of the weights. This means that the network is able to maintain very good performance, even when only one weight out of 600 has been kept, and therefore, with a computation time increased by a factor of 600. This property, which we didn&amp;rsquo;t expect, seems promising for creating machine learning algorithms that are less energy-hungry.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-visionhttpslaurentperrinetgithubioslides2023-09-08_fresneltransitionfade-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-09-08_fresnel/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event-based vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-09-08httpslaurentperrinetgithubiotalk2023-09-08-fresnel-séminaire-institut-fresnelhttpswwwfresnelfrspipspipphparticle2453langfru-1&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-09-08-fresnel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-09-08]&lt;/a&gt; &lt;a href=&#34;https://www.fresnel.fr/spip/spip.php?article2453&amp;amp;lang=fr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Séminaire institut Fresnel&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;!-- &lt;img src=&#34;https://laurentperrinet.github.io/talk/2023-09-08_fresnel/qrcode.png&#34; alt=&#34;qrcode&#34; height=&#34;130&#34;/&gt; --&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  In conclusion, we have seen that event-driven cameras open the door to new applications that mimic the performance of the human eye, in terms of computational dynamics, adaptation to light conditions and energy constraints. This technological development has recently been accompanied by the development of neuromorphic chips and innovative algorithms in the form of spiking neural networks. However, there is still a great deal of progress to be made at theoretical level, particularly in the understanding of these spiking neural networks, and we have shown the potential progress that can be made by exploiting the richness of temporal representations, particularly by taking advantage of heterogeneous delays.
Beyond these particular applications to natural image processing, I hope to have succeeded in demonstrating the importance of cross-fertilizing the field of engineering applications in general with biological neuroscience. This new line of research - known as NeuroAI or, more generally, as computational neuroscience - is likely to develop over the next few years. Thank you for your attention.
&lt;/aside&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-09-27_icann/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-09-27_icann/</guid>
      <description>&lt;h2 id=&#34;__accurate-detection-of-spiking-motifs-by-learning-heterogeneous-delays-of-a-spiking-neural-networkhttpslaurentperrinetgithubioslides2023-09-27_icanntransitionfade__&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-09-27_icann/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;icann-workshop-on-___recent-advances-in-snnshttpse-nnsorgicann2023wp-contentuploadssites7202304icann2023-asnn-cfppdf___&#34;&gt;ICANN workshop on &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://e-nns.org/icann2023/wp-content/uploads/sites/7/2023/04/ICANN2023-ASNN-CfP.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recent Advances in SNNs&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;!-- &lt;img src=&#34;https://laurentperrinet.github.io/talk/2023-09-27_icann/qrcode.png&#34; alt=&#34;qrcode&#34; height=&#34;130&#34;/&gt; --&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-09-27-icann&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2023-09-27-icann&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;em&gt;Hello&lt;/em&gt;, I&amp;rsquo;m Laurent Perrinet from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit, and during this talk at this ICANN workshop on Recent Advances in SNNs, I&amp;rsquo;ll be presenting a method for the &lt;em&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/em&gt;, and how it may also impact the design of SNNs. I&amp;rsquo;d like to &lt;em&gt;thank&lt;/em&gt; Sander Bohté and Sebastian Otte for the organization of this workshop and you for listening. These slides are available from my web-site, along with a number of references. The &lt;em&gt;outline&lt;/em&gt; of the talk is as follows: first, I&amp;rsquo;ll describe how one may perform computations using Heterogeneous Delays - and present a toy model example; then, I&amp;rsquo;ll show real scale example quantifying the performance on synthetic data ; and finally, I&amp;rsquo;ll present how this SNN is in fact differentiable and may be extended for future applications.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;core-mechanism-of-spiking-motif-detection&#34;&gt;Core Mechanism of Spiking Motif Detection&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/izhikevich_left.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  The core idea of the method follows the use of polychronous groups as defined by Izhikevich in 2006. Suppose three presynaptic neurons are connected to two postsynaptic neurons by certains weights and certain delays, which correspond to the time it takes for a spike to travel from one neuron to the next.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;core-mechanism-of-spiking-motif-detection-1&#34;&gt;Core Mechanism of Spiking Motif Detection&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/izhikevich_middle.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  If we assume these delays are different, then if presynaptic neurons are activated synchronously, then postsynaptic currents do not match in time, such that the membrane potential is not reached.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;core-mechanism-of-spiking-motif-detection-2&#34;&gt;Core Mechanism of Spiking Motif Detection&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/izhikevich.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  However, if the timing of presynaptic spikes forms a &lt;em&gt;spiking motif&lt;/em&gt; such that they reach the soma of neuron b_1 at the same time then this neuron will be selectively activated.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;from-generating-raster-plots-to-inferring-spiking-motifs&#34;&gt;From generating raster plots to inferring spiking motifs&lt;/h2&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-a_k.svg&#34; width=&#34;42%&#34;&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-b.svg&#34; width=&#34;42%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-c.svg&#34; width=&#34;42%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-a.svg&#34; width=&#34;42%&#34;&gt; 
&lt;/span&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;em&gt;A&lt;/em&gt; In this work, this principle was framed in a probabilistic setting such that we could provide an optimal scheme for detecting generic spiking motifs which may be superposed at random times. Starting with 10 presynaptic inputs, this model allows to generate a synthetic raster plot as the combination of four different spiking motifs.
&lt;em&gt;B&lt;/em&gt; These motifs are defined by a positive (red) or negative (blue) contribution to the spiking probability which are represented here.
&lt;em&gt;C&lt;/em&gt; Applying a Bayesian approach, we may define four formal spiking neurons which will integrate the incoming spiking information from the presynaptic neurons - this analog signal can then be thresholded to give the detection of each spiking motif (vertical) bar which was here always exact with respect to the ground truth (stars).
&lt;em&gt;D&lt;/em&gt; The beauty of this is that we can recover in the presynaptic raster plot the contribution of each spiking motif to the original raster plot.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;detecting-spiking-motifs-using-heterogeneous-delays&#34;&gt;Detecting spiking motifs using heterogeneous delays&lt;/h2&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_N_SMs.svg&#34; width=&#34;31%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_N_pre.svg&#34; width=&#34;31%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_N_SM_time.svg&#34; width=&#34;31%&#34;&gt; 
&lt;/span&gt;
&lt;aside class=&#34;notes&#34;&gt;
  This was a toy example and let&amp;rsquo;s now quantify the performance of this method in real scale settings by measuring the accuracy of finding the right SM at the right time. For this we will compare our method to a classical approach using the correlation.
First, by increasing the number of motifs, we show that the accuracy of our method (in blue) is very high and outperforms the cross-correlation method (red), in particular as the number of SMs increases. The same trend is shown also when the number of presynaptic inputs increases from a low to a high dimension. Finally, the number of possible delays is a crucial parameter and enough heterogenous delays are necessary to reach a good performance.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;detecting-spiking-motifs-using-heterogeneous-delays-1&#34;&gt;Detecting spiking motifs using heterogeneous delays&lt;/h2&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_xcorr-supervised.svg&#34; width=&#34;62%&#34;&gt; 
&lt;/span&gt;
&lt;aside class=&#34;notes&#34;&gt;
  An advantage of our method is that is is fully differentiable. We thus applied a supervised learning method and starting with random weights, we could recover the spiking motifs, as is shown here in this cross-correlagram of the weights of the learned werights with respect to the ground truth.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;__accurate-detection-of-spiking-motifs-by-learning-heterogeneous-delays-of-a-spiking-neural-networkhttpslaurentperrinetgithubioslides2023-09-27_icanntransitionfade__-1&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-09-27_icann/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;icann-workshop-on-___recent-advances-in-snnshttpse-nnsorgicann2023wp-contentuploadssites7202304icann2023-asnn-cfppdf___-1&#34;&gt;ICANN workshop on &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://e-nns.org/icann2023/wp-content/uploads/sites/7/2023/04/ICANN2023-ASNN-CfP.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Recent Advances in SNNs&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-09-27-icann&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2023-09-27-icann&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;As a conclusion, this heterogenous delay spiking neural network provides an efficient neural computation. It has some limitations that we detail in the paper, notably that it works on discrete time and that it is supervised, yet we hope to deliver soon an unsupervised learning method using this computational brick which could be used to build novel SNNs - we did that for detecting motion in event-based data - but also to analyse neurobiological data.&lt;/p&gt;
&lt;p&gt;Thanks for your attention, slides are also available online&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;__accurate-detection-of-spiking-motifs-by-learning-heterogeneous-delays-of-a-spiking-neural-networkhttpslaurentperrinetgithubioslides2023-09-27_icanntransitionfade__-2&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-09-27_icann/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/talk/2023-09-27-icann/qrcode.png&#34; alt=&#34;qrcode&#34; width=&#34;45%&#34;/&gt;
&lt;p&gt;&lt;sup&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-09-27-icann&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2023-09-27-icann&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &amp;hellip; by scanning this qrcode!
&lt;/aside&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-11-07-snufa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-11-07-snufa/</guid>
      <description>&lt;h2 id=&#34;__accurate-detection-of-spiking-motifs-by-learning-heterogeneous-delays-of-a-spiking-neural-networkhttpslaurentperrinetgithubioslides2023-11-07-snufatransitionfade__&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-11-07-snufa/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;___snufa-spiking-neural-networks-as-universal-function-approximatorshttpssnufanet2023___&#34;&gt;&lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://snufa.net/2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SNUFA: Spiking Neural networks as Universal Function Approximators&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;!-- &lt;img src=&#34;https://laurentperrinet.github.io/talk/2023-09-27_icann/qrcode.png&#34; alt=&#34;qrcode&#34; height=&#34;130&#34;/&gt; --&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-11-07-snufa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2023-11-07-snufa&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;em&gt;Hello&lt;/em&gt;, I&amp;rsquo;m Laurent Perrinet from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit, and during this talk at SNUFA, I&amp;rsquo;ll be presenting a method for the &lt;em&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/em&gt;, and how it may also impact the design of SNNs. The &lt;em&gt;outline&lt;/em&gt; of the talk is as follows: first, I&amp;rsquo;ll describe how one may perform computations using Heterogeneous Delays - and present a toy model example; then, I&amp;rsquo;ll show real scale example quantifying the performance on synthetic data ; and finally, I&amp;rsquo;ll present how this SNN is in fact differentiable and may be extended for future applications.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;core-mechanism-of-spiking-motif-detection&#34;&gt;Core Mechanism of Spiking Motif Detection&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/izhikevich_left.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  The core idea of the method follows the use of polychronous groups as defined by Izhikevich in 2006. Suppose three presynaptic neurons are connected to two postsynaptic neurons by certains weights and certain delays, which correspond to the time it takes for a spike to travel from one neuron to the next.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;core-mechanism-of-spiking-motif-detection-1&#34;&gt;Core Mechanism of Spiking Motif Detection&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/izhikevich_middle.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  If we assume these delays are different, then if presynaptic neurons are activated synchronously, then postsynaptic currents do not match in time, such that the membrane potential is not reached.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;core-mechanism-of-spiking-motif-detection-2&#34;&gt;Core Mechanism of Spiking Motif Detection&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/izhikevich.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  However, if the timing of presynaptic spikes forms a &lt;em&gt;spiking motif&lt;/em&gt; such that they reach the soma of neuron b_1 at the same time then this neuron will be selectively activated.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;from-generating-raster-plots-to-inferring-spiking-motifs&#34;&gt;From generating raster plots to inferring spiking motifs&lt;/h2&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-a_k.svg&#34; width=&#34;42%&#34;&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-b.svg&#34; width=&#34;42%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-c.svg&#34; width=&#34;42%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_toy-a.svg&#34; width=&#34;42%&#34;&gt; 
&lt;/span&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;em&gt;A&lt;/em&gt; In this work, this principle was framed in a probabilistic setting such that we could provide an optimal scheme for detecting generic spiking motifs which may be superposed at random times. Starting with 10 presynaptic inputs, this model allows to generate a synthetic raster plot as the combination of four different spiking motifs.
&lt;em&gt;B&lt;/em&gt; These motifs are defined by a positive (red) or negative (blue) contribution to the spiking probability which are represented here.
&lt;em&gt;C&lt;/em&gt; Applying a Bayesian approach, we may define four formal spiking neurons which will integrate the incoming spiking information from the presynaptic neurons - this analog signal can then be thresholded to give the detection of each spiking motif (vertical) bar which was here always exact with respect to the ground truth (stars).
&lt;em&gt;D&lt;/em&gt; The beauty of this is that we can recover in the presynaptic raster plot the contribution of each spiking motif to the original raster plot.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;detecting-spiking-motifs-using-heterogeneous-delays&#34;&gt;Detecting spiking motifs using heterogeneous delays&lt;/h2&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_N_SMs.svg&#34; width=&#34;31%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_N_pre.svg&#34; width=&#34;31%&#34;&gt;     
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_N_SM_time.svg&#34; width=&#34;31%&#34;&gt; 
&lt;/span&gt;
&lt;aside class=&#34;notes&#34;&gt;
  This was a toy example and let&amp;rsquo;s now quantify the performance of this method in real scale settings by measuring the accuracy of finding the right SM at the right time. For this we will compare our method to a classical approach using the correlation.
First, by increasing the number of motifs, we show that the accuracy of our method (in blue) is very high and outperforms the cross-correlation method (red), in particular as the number of SMs increases. The same trend is shown also when the number of presynaptic inputs increases from a low to a high dimension. Finally, the number of possible delays is a crucial parameter and enough heterogenous delays are necessary to reach a good performance.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;detecting-spiking-motifs-using-heterogeneous-delays-supervised-learning&#34;&gt;Detecting spiking motifs using heterogeneous delays: supervised learning&lt;/h2&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;img src=&#34;https://github.com/laurentperrinet/2023-07-20_HDSNN-ICANN/raw/master/figures/THC_xcorr-supervised.svg&#34; width=&#34;62%&#34;&gt; 
&lt;/span&gt;
&lt;aside class=&#34;notes&#34;&gt;
  An advantage of our method is that it is fully differentiable. We thus applied a supervised learning method and starting with random weights, we could recover the spiking motifs, as is shown here in this cross-correlagram of the weights of the learned werights with respect to the ground truth.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;__accurate-detection-of-spiking-motifs-by-learning-heterogeneous-delays-of-a-spiking-neural-networkhttpslaurentperrinetgithubioslides2023-11-07-snufatransitionfade__-1&#34;&gt;&lt;strong&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-11-07-snufa/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network&lt;/a&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;___snufa-spiking-neural-networks-as-universal-function-approximatorshttpssnufanet2023___-1&#34;&gt;&lt;em&gt;&lt;strong&gt;&lt;a href=&#34;https://snufa.net/2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SNUFA: Spiking Neural networks as Universal Function Approximators&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-11-07-snufa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laurentperrinet.github.io/talk/2023-11-07-snufa&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;As a conclusion, this heterogenous delay spiking neural network provides an efficient neural computation. It has some limitations that we detail in the paper, notably that it works on discrete time and that it is supervised, yet we hope to deliver soon an unsupervised learning method using this computational brick which could be used to build novel SNNs - we did that for detecting motion in event-based data - but also to analyse neurobiological data.&lt;/p&gt;
&lt;p&gt;Thanks for your attention, slides are also available online&lt;/p&gt;

&lt;/aside&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://laurentperrinet.github.io/slides/2023-12-01-biocomp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2023-12-01-biocomp/</guid>
      <description>&lt;section&gt;
&lt;h1 id=&#34;event-based-visionhttpslaurentperrinetgithubioslides2023-12-01-biocomptransitionfade&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-12-01-biocomp/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event-based vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-12-01httpslaurentperrinetgithubiotalk2023-12-01-biocomp-séminaire-colloque-biocomp-2023httpgdr-biocompfrcolloque-biocomp-2023u&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-12-01-biocomp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-12-01]&lt;/a&gt; &lt;a href=&#34;http://gdr-biocomp.fr/colloque-biocomp-2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Séminaire colloque BioComp 2023&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;em&gt;Hello&lt;/em&gt;, can you hear me in the back?&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m Laurent Perrinet from the Institut des Neurosciences de la Timone, a joint AMU / CNRS unit, and during this seminar at the BioComp 2023 colloquium, I&amp;rsquo;ll be presenting &lt;em&gt;event-driven cameras&lt;/em&gt;, a new technology in the field of imaging, and the impact of this technology on our understanding of vision. I&amp;rsquo;d like to &lt;em&gt;thank&lt;/em&gt; organizers for this opportunity, and all of you for coming. These slides are available from my website, along with a number of references. The &lt;em&gt;outline&lt;/em&gt; of the talk is as follows: first, we&amp;rsquo;ll describe what an event-driven camera is - in particular, by comparing it to a conventional camera; then, we&amp;rsquo;ll show some examples of applications of these cameras with dedicated algorithms; and finally, we&amp;rsquo;ll present how our knowledge of biological mechanisms in neuroscience can enable us to improve these algorithms.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;sensing-light&#34;&gt;Sensing light&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  First of all, the general aim of &lt;em&gt;imaging&lt;/em&gt; is to represent a visual signal, i.e. a luminous intensity, a color, distributed over the visual field, giving us a vivid impression of the visual scene before our eyes.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;http://lepassetempsderose.l.e.pic.centerblog.net/fddea7fb.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  This is perfectly illustrated in this &lt;em&gt;galloping horse&lt;/em&gt;. We get a &lt;em&gt;vivid&lt;/em&gt; impression of movement. Thanks to a rapid sequence of still images consistent with the scene being represented. This technique clearly exploits a visual &lt;em&gt;illusion&lt;/em&gt;, because we know that at each point in the visual space, the light signal is made up of a &lt;em&gt;continuous&lt;/em&gt; stream of an analogous signal representing the energy of the photos.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  This technique is inspired by the research carried out by &lt;a href=&#34;https://en.wikipedia.org/wiki/%c3%89tienne-Jules_Marey&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Étienne-Jules &lt;em&gt;Marey&lt;/em&gt;&lt;/a&gt;, under the term &lt;em&gt;chronophotography&lt;/em&gt;, litterally shooting scene with a gun-like apparatus to shoot a visual scene. It notably enabled later Muybridge to scientifically demonstrate the mechanism of a horse&amp;rsquo;s gallop.
&lt;/aside&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;https://media.giphy.com/media/4Y8PqJGFJ21CE/giphy.gif&#34;
  &gt;

&lt;aside class=&#34;notes&#34;&gt;
  The use of such dynamic &lt;em&gt;visualization&lt;/em&gt; is crucial in the scientific field, whether in biology or physics, as it enables us to quantify the characteristics of the experiment being carried out - I&amp;rsquo;m thinking, for example, of quantifying the movements and number of bacteria in a biological assay.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representing-spatio-temporal-luminous-information&#34;&gt;Representing spatio-temporal luminous information&lt;/h2&gt;
&lt;!-- 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://1.bp.blogspot.com/-odG4Twu0Blc/UrN3ytufKnI/AAAAAAAACRM/dzJNcpV4JfY/s1600/Monty&amp;#43;Python%27s&amp;#43;1.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/movie.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;25%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  To better understand the mechanism behind this technology, let&amp;rsquo;s take a sample video.
Here, I&amp;rsquo;ve taken a grayscale &lt;em&gt;video&lt;/em&gt; from an episode from the Monty Python Flying Circus TV series.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;representing-spatio-temporal-luminous-information-1&#34;&gt;Representing spatio-temporal luminous information&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/analog_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&amp;hellip; and we will focus on a &lt;em&gt;single pixel&lt;/em&gt; in the space of the visual field
In this way, we can represent the evolution of the &lt;em&gt;log intensity&lt;/em&gt; of the light signal as a function of time.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The&amp;#43;Horse&amp;#43;in&amp;#43;Motion,&amp;#43;1878.%C2%A0Eadweard&amp;#43;Muybridge&amp;#43;%28b.&amp;#43;9&amp;#43;April,&amp;#43;1830%29The&amp;#43;first&amp;#43;movie&amp;#43;ever&amp;#43;made,&amp;#43;from&amp;#43;still&amp;#43;photographs..gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://4.bp.blogspot.com/-AHprBxkfu5o/UJ-lqR7GsmI/AAAAAAAAHpo/VJzY7HMuXe0/s1600/The+Horse+in+Motion,+1878.%C2%A0Eadweard+Muybridge+(b.+9+April,+1830)The+first+movie+ever+made,+from+still+photographs..gif&lt;/a&gt;
&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://upload.wikimedia.org/wikipedia/commons/0/07/The_Horse_in_Motion-anim.gif&lt;/a&gt;
&lt;a href=&#34;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;amp;h=600&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hackaday.com/wp-content/uploads/2018/04/saccades.gif?w=600&amp;h=600&lt;/a&gt;
&lt;a href=&#34;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://38.media.tumblr.com/831aada3328557146e214efe1cb867a5/tumblr_mslrotKPS01snyrdto1_500.gif&lt;/a&gt;
&lt;a href=&#34;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif%22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.filmsranked.com/wp-content/uploads/2020/05/two-fencers.gif&#34;&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-temporal-discretization&#34;&gt;Frame-Based Camera: Temporal discretization&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/frame-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  From this representation, expressed in continuous time, we can &lt;em&gt;discretize&lt;/em&gt; time and measure the log intensity at regular time intervals. The difference between two images gives the &lt;em&gt;temporal resolution&lt;/em&gt;, and its inverse gives the number of images per second. This is the representation classically used in chronophotography, but also in all conventional video stream &lt;em&gt;acquisition and viewing&lt;/em&gt; technologies.
This technology is highly efficient for a wide range of signals. However, it does have certain &lt;em&gt;limitations&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-aliasing&#34;&gt;Frame-Based Camera: Aliasing&lt;/h2&gt;


















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/frames.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;85%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Let&amp;rsquo;s take the &lt;em&gt;example&lt;/em&gt; of three colored cubes rotating in a frontal axis along a circle. Because of temporal resolution and the length of time the shutter is open, the images captured at each instant can produce a certain amount of &lt;em&gt;blur&lt;/em&gt;, and movement can become increasingly difficult to estimate. If the movement of the cubes begins to accelerate, temporal &lt;em&gt;aliasing&lt;/em&gt; can be observed.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;frame-based-camera-wagon-wheel-illusion&#34;&gt;Frame-Based Camera: Wagon-Wheel Illusion&lt;/h2&gt;


















&lt;figure  id=&#34;figure-sam-brinson-2020httpswwwsambrinsoncomnature-of-perception&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://vignette.wikia.nocookie.net/revengeristsconsortium/images/2/25/Whee.gif/revision/latest/scale-to-width-down/340?cb=20141209071330&#34; alt=&#34;[[Sam Brinson, 2020](https://www.sambrinson.com/nature-of-perception/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://www.sambrinson.com/nature-of-perception/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sam Brinson, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This phenomenon is particularly striking when we look at a spinning &lt;em&gt;wheel&lt;/em&gt; at high speed, and this wheel&amp;rsquo;s rotational speed is such that two successive images give the illusion that the movement is in the opposite direction to the real, physical moment. It&amp;rsquo;s striking here in this car wheel, where you can perceive that the central hub appears motionless, and the wheel is perceived as turning in the &lt;em&gt;opposite direction&lt;/em&gt; to the physical rolling motion on the road.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-camera&#34;&gt;Event-Based Camera&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  Now let&amp;rsquo;s introduce the &lt;em&gt;event camera&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-1&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This consists of a conventional sensor which, like most CMOS-type sensors, transforms visual energy into an electric current. However, there are two fundamental differences, inspired by our knowledge of the retina, which is the sensor of vision. Firstly, each pixel of this sensor is &lt;em&gt;independent&lt;/em&gt; and is not cadenced according to a global clock. Secondly, each pixel will follow the evolution of the log intensity and signal an event when an increment or decrement exceeds a threshold. Let&amp;rsquo;s explain this mechanism in relation to our analog signal.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-2&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_0.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  First of all, the signal will evolve over time, &amp;hellip;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-3&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &amp;hellip; and we can see here that it may cross a &lt;em&gt;threshold&lt;/em&gt;. An event will then be produced by this pixel. Here, the &lt;em&gt;event&lt;/em&gt; is of negative polarity, as it corresponds to a decrement.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-4&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_2.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-5&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_5.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Then, the signal will continue its course in time and cross a threshold again, possibly once more, at which point a new event will be produced. Here, we&amp;rsquo;re also seeing increments, ie positive polarizations.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-6&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_10.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-7&#34;&gt;Event-Based Camera&lt;/h2&gt;
&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_20.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  And so on, this simple mechanism will produce a &lt;em&gt;stream&lt;/em&gt; of events for each pixel, this &lt;em&gt;list&lt;/em&gt; being made up of the times of occurrence and the corresponding polarities.
&lt;/aside&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-8&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw_-1.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-9&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_raw.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Let&amp;rsquo;s show it now applied to the whole analog signal.
It&amp;rsquo;s worth noting in particular that, compared with frame-by-frame representations, this one is particularly &lt;em&gt;sparse&lt;/em&gt;: in particular, a signal with very few changes can be represented by just a few events. This is a very useful feature, not only because it saves &lt;em&gt;bandwidth&lt;/em&gt;, but also because it allows us to concentrate the &lt;em&gt;computations&lt;/em&gt; around the few events that represent the image. It&amp;rsquo;s also a fundamental feature of neuron function in the brain, and we&amp;rsquo;ll come back to it later.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-10&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;!-- 

















&lt;figure  id=&#34;figure-gregor-lenz-2020httpslenzgregorcompostsevent-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://lenzgregor.com/posts/event-cameras/post-rethinking/events.gif&#34; alt=&#34;[[Gregor Lenz, 2020](https://lenzgregor.com/posts/event-cameras/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://lenzgregor.com/posts/event-cameras/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gregor Lenz, 2020&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;Finally, we obtain a list of events for each pixels  which can be &lt;em&gt;merged&lt;/em&gt; for the image as a whole, forming a list of events, including pixel addresses, times of occurrence and polarities. As they are generated over time, they are naturally arranged in order of occurrence. All these events are then transmitted in &lt;em&gt;real time&lt;/em&gt; to the output bus, typically by means of a USB3 connection. Note the analogy between this representation and the one made in the optic nerve that connects our retina to the rest of the brain: indeed, the million ganglion cells that make up the retina&amp;rsquo;s output emit action potentials, which are the only source of information that leaves the retina via the &lt;em&gt;optic nerve&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.researchgate.net/profile/Guido-Croon/publication/313221316/figure/fig2/AS:668997448134663@1536512829861/Picture-of-the-event-based-camera-employed-in-this-work-the-DVS_W640.jpg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-11&#34;&gt;Event-Based Camera&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sensor&lt;/th&gt;
&lt;th&gt;Range&lt;/th&gt;
&lt;th&gt;Framerate&lt;/th&gt;
&lt;th&gt;Resolution&lt;/th&gt;
&lt;th&gt;Power&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Human eye&lt;/td&gt;
&lt;td&gt;60 (?) dB&lt;/td&gt;
&lt;td&gt;300 (?) fps&lt;/td&gt;
&lt;td&gt;100 (?) Mpx&lt;/td&gt;
&lt;td&gt;10 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DSLR&lt;/td&gt;
&lt;td&gt;44.6 dB&lt;/td&gt;
&lt;td&gt;120     fps&lt;/td&gt;
&lt;td&gt;2&amp;ndash;20   Mpx&lt;/td&gt;
&lt;td&gt;30  W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ultra-high speed&lt;/td&gt;
&lt;td&gt;64   dB&lt;/td&gt;
&lt;td&gt;10^4 fps&lt;/td&gt;
&lt;td&gt;0.3&amp;ndash;4  Mpx&lt;/td&gt;
&lt;td&gt;300 W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Event-based&lt;/td&gt;
&lt;td&gt;120  dB&lt;/td&gt;
&lt;td&gt;10^6 fps&lt;/td&gt;
&lt;td&gt;0.1&amp;ndash;2  Mpx&lt;/td&gt;
&lt;td&gt;30 mW&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;There are several properties of event-driven cameras that make them remarkable. First of all, the &lt;em&gt;temporal precision&lt;/em&gt; of events is of the order of microseconds, enabling a theoretical frame rate of the order of a million images per second to be reached. This can be compared with a conventional camera, which is of the order of a hundred images per second, or with a high-speed camera, which can reach 10,000 images per second. It is difficult to estimate the sampling frequency of human perception, because while 25 frames per second is often sufficient for movie viewing, it has been shown that the human eye can distinguish temporal details up to 300 or even 1,000 frames per second. It&amp;rsquo;s worth noting that the &lt;em&gt;spatial resolution&lt;/em&gt; of these event cameras is often relatively modest, in the order of megapixels, but this is not a technical limitation, but rather due to the technological applications in which these cameras are commonly used. Compared with conventional cameras, which will consume several watts, event cameras consume very little electrical &lt;em&gt;energy&lt;/em&gt;, in the order of 10 milliwatts, a consumption equivalent to that of the human eye.  Another important feature of these cameras is their ability to detect a very wide &lt;em&gt;range&lt;/em&gt; of luminosity, far exceeding that of conventional cameras at 120 dB (a factor of a million, compared with the human eye&amp;rsquo;s factor of 1 in a thousand between full moon and full sun),&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Event_camera#Functional_description&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Event_camera#Functional_description&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;more in &lt;a href=&#34;https://arxiv.org/pdf/1904.08405.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1904.08405.pdf&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-12&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  This ability to &lt;em&gt;adapt&lt;/em&gt; to changing light conditions can be illustrated by going back to our analog signal and its event representation, and imagining. A typical example would be an autonomous car driving in daylight, entering and leaving a &lt;em&gt;tunnel&lt;/em&gt;, involving changes in brightness by a factor of several thousand.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;event-based-camera-13&#34;&gt;Event-Based Camera&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/figures/raw/main/event-based/event-based_signal_low.svg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Here we have a division by a factor 8 of the signal in the middle section. It will be reported by a frame-based camera. In an event-based camera, this is represented here by a &lt;em&gt;sharp decrement&lt;/em&gt; in log intensity space and clearly indicated by events of negative polarity, but we can see that since this is a camera that uses log intensity, dividing the light signal produces the &lt;em&gt;same signal&lt;/em&gt; course over time, and therefore events that are identical.  Event-driven cameras are therefore particularly well-suited to &lt;em&gt;dynamic signals&lt;/em&gt;, where the lighting context can change drastically.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-computer-vision&#34;&gt;Event-Based Computer vision&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;These cameras therefore look very promising for future applications, particularly for embedded applications, but also for applications linked to scientific experiments. However, we can see that the image &lt;em&gt;representation&lt;/em&gt; is completely different, that is, we can no longer consider static images that follow one another at a regular rate, and for which we could have applied the algorithms that have been developed for decades in the field of &lt;em&gt;computer vision&lt;/em&gt;. We end up with a signal that corresponds to events that are transmitted as a stream from the camera. And we have to reinvent all computer vision algorithms to make them &lt;em&gt;event-driven&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;TODO: the process is active driven by the signal compared to acquired&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-recognition-dvs-gesture&#34;&gt;Always-on Object Recognition: DVS gesture&lt;/h2&gt;
&lt;!-- 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;
&lt;p&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34;  width=&#34;33%&#34;/&gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34;  width=&#34;33%&#34;/&gt;&lt;/p&gt;
&lt;!-- !&#34;&#34; width=&#34;33%&#34; &gt;}}

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;33%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-recognition&#34;&gt;Always-on Object Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/hots.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  The first algorithm we developed with Antoine Grimaldi, who is a PhD student, and in collaboration with Sio Ieng and Ryad Benosman of Sorbonne University, who are recognized researchers in the development of this type of camera, is an improvement on an existing algorithm, &lt;em&gt;HOTS&lt;/em&gt;. This algorithm uses a relatively classical convolutional and hierarchical information processing architecture, which passes information &amp;ldquo;forward&amp;rdquo; from the camera and its event representation, and then through different processing layers to converge on a high-level representation that can be used for classification, in this case to recognize the identity of the digit presented as input, i.e. an eight digit. A fundamental feature of this algorithm is that it transforms the event representation into multiplexed, parallel channels, which analogously represent the temporal pattern of events, or &amp;ldquo;&lt;em&gt;temporal surface&lt;/em&gt;&amp;rdquo;. These are represented in the different layers by the individual temporal surfaces. An interesting feature of this algorithm is that learning in each of the layers is &lt;em&gt;unsupervised&lt;/em&gt;, which is a significant improvement over conventional deep learning algorithms that assume that a classification error signal can be back-propagated along the entire hierarchy, which is notoriously incorrect. Starting from this algorithm, we improved it by including neuro-biological knowledge, especially about the balance between different parallel communication pathways by including &lt;em&gt;homeostasis&lt;/em&gt; rules.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-gesture-recognition&#34;&gt;Always-on Object Gesture Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_offline.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;To illustrate the results of our algorithm, we applied a classic camera dataset involving the classification of 10 different types of human &lt;em&gt;gestures&lt;/em&gt;. These biological movements are, for example, clapping hands, saying hello or a drum movement. The chance level is therefore at 10%, and we have observed that when all events have been processed, the &lt;em&gt;original&lt;/em&gt; algorithm achieves a performance of around 70%. By adding &lt;em&gt;homeostasis&lt;/em&gt;, we have reached a higher level of 82%, demonstrating the usefulness of using neuroscientific knowledge to improve machine learning algorithms.&lt;/p&gt;
&lt;p&gt;We also built on a fundamental characteristic of biological systems. In fact, this kind of algorithm is classically used to process the flow of events, but classification is only used as a last resort when all the events have been processed. We have modified the algorithm so that this classification can be done &lt;em&gt;online&lt;/em&gt;, in real time, event by event. In this way, processing in the various layers is triggered by the arrival of each event, which is propagated from the camera through all the layers to the classification layer.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;always-on-object-gesture-recognition-1&#34;&gt;Always-on Object Gesture Recognition&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-boutin-sio-ieng-benosman--lp-2023httpslaurentperrinetgithubiopublicationgrimaldi-23&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/gesture_online.png&#34; alt=&#34;[[Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023](https://laurentperrinet.github.io/publication/grimaldi-23/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi, Boutin, Sio-Ieng, Benosman &amp;amp; LP, 2023&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  What&amp;rsquo;s more interesting is that we were also able to show the &lt;em&gt;evolution&lt;/em&gt; of the average performance obtained on a data set, and as a function of the number of events processed by the algorithm. The blue curve shows that if below 10 events, we remain at the level of chance, we then experience a gradual increase in performance that reaches the level of the original algorithm with ten thousand events, and exceeds this &lt;em&gt;performance&lt;/em&gt; when we have even 10 times more. A major advantage of this algorithm is that it can be asked to classify the nature of what it sees in its event camera, not once the entire signal has been processed by the system, but at any time. This characteristic is essential in biology. For example, imagine you&amp;rsquo;re on the savannah and a &lt;em&gt;lion&lt;/em&gt; jumps out at you. You won&amp;rsquo;t have the flexibility to wait for the video sequence to finish processing before making the right decision, which is to flee. Another variant in our algorithm consists of selecting the output classification events based on a calculation of the precision for each event. By using a &lt;em&gt;threshold&lt;/em&gt; on this precision, we can achieve a very good level of performance, with just a hundred events, and so achieve a characteristic that is common in biological networks, i.e. that a decision is not taken gradually, but emerges abruptly (here after 200 events) and then improves and stabilizes.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  We have therefore illustrated the use of &lt;em&gt;event-driven&lt;/em&gt; cameras on a particular algorithm. This algorithm has the particularity of processing the flow of events coming from the camera event by event, so that potentially each of these events triggers a cascade of mechanisms in the different processing layers, and thus enables a classification value to be updated at any given moment. This type of operation is characteristic of the way neurons work in the brain, i.e. using an event-based representation of information processing. This is what we call &lt;em&gt;spiking neural networks&lt;/em&gt;.
&lt;/aside&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-tonic-manualhttpstonicreadthedocsioenlatest_imagesneuron-modelspng&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; alt=&#34;[[Tonic manual](https://tonic.readthedocs.io/en/latest/_images/neuron-models.png)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;70%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://tonic.readthedocs.io/en/latest/_images/neuron-models.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tonic manual&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  Indeed, most neural networks used in deep learning use an analog representation. This is illustrated in this figure, which represents the various analog inputs to a formal neuron as they are linearly integrated by the synapses, then transformed by a non-linear function to generate an activation which is itself analog. This basic &lt;em&gt;perceptron&lt;/em&gt; principle is at the foundation of all existing neural networks, and in particular enables the construction of convolutional-type networks which are currently the champions for image classification, having outperformed human performance for several years. However, while this is true for static images, it can become prohibitively expensive with videos. This is why it can be interesting to use &lt;em&gt;spiking&lt;/em&gt; neurons instead, which, instead of receiving an analog input, will receive events that will trigger cascades of mechanisms in the neuronal cell, notably represented by the cell&amp;rsquo;s membrane potential. Typically, we&amp;rsquo;ll include a threshold for triggering action potential in this cell, which will generate new output events on the cell&amp;rsquo;s axon.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-lif-neuron&#34;&gt;Spiking Neural Networks: LIF Neuron&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is illustrated in this &lt;em&gt;animation&lt;/em&gt;, which shows how we can transform a list of input events by giving them different weights, and then &lt;em&gt;integrate&lt;/em&gt; them into the cell&amp;rsquo;s soma to generate output events.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-neuromorphic-hardware&#34;&gt;Spiking Neural Networks: neuromorphic hardware&lt;/h2&gt;


















&lt;figure  id=&#34;figure-loihi-2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg&#34; alt=&#34;Loihi 2&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Loihi 2
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;This new type of representation represents a &lt;em&gt;paradigm shift&lt;/em&gt; in computation, in the same way that event-driven cameras have brought with them a paradigm shift in image representation. The development of these two new algorithms, which use impulse neural networks, is accompanied by the development of new neuromorphic chips, such as the Loihi 2 chip developed by Intel, which replaces a central computing unit with a massively parallelized &lt;em&gt;array&lt;/em&gt; of elementary event-driven computing units. As with event-driven cameras, this has the dual advantage of being very fast and consuming very little energy. Other types of &lt;em&gt;neuromorphic chips&lt;/em&gt; are currently being developed and may soon be used instead of conventional CPUs or GPUs.&lt;/p&gt;
&lt;figure  id=&#34;figure-propheseehttpsdocspropheseeaistableconceptshtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; alt=&#34;[Prophesee](https://docs.prophesee.ai/stable/concepts.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://docs.prophesee.ai/stable/concepts.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prophesee&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Loihi: &lt;a href=&#34;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://d1fmx1rbmqrxrr.cloudfront.net/zdnet/optim/i/edit/ne/2019/Pierre%20temp/Intel%20Loihi__w630.jpg&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;amp;strip=none&amp;amp;ssl=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cdn.cnx-software.com/wp-content/uploads/2022/09/Intel-Loihi-2.jpg?lossy=0&amp;strip=none&amp;ssl=1&lt;/a&gt;&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://i.stack.imgur.com/ixnrz.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Spiking neural networks therefore seem very promising for processing the output of event-driven cameras, but the study of &lt;em&gt;neurophysiology&lt;/em&gt; shows us that their operation can sometimes seem incongruous and far from the perceptron. In this first example, taken from an article by Mainen and Sejnowski from 1995, we see the response of the same neuron to several &lt;em&gt;repetitions&lt;/em&gt; of a stimulation in panel A. At the top, we see the membrane potential of this neuron in response to a 200 Pico ampere &lt;em&gt;current step&lt;/em&gt;, which shows that the membrane potential is not reproducible across different trials. This is illustrated by showing the spike response over time for the different trials, which shows a strong alignment at the start of stimulation, but that this diffuses little by little, so that after around 750 milliseconds there is no longer any coherence between the different trials. The situation is different in panel B, where the neuron is stimulated with &lt;em&gt;noise&lt;/em&gt;. In this case, the responses are so precise for the different trials that the membrane potential traces are overlapping almost exactly. The subtlety of this paper lies in its use of a &lt;em&gt;frozen&lt;/em&gt; noise, i.e. one that is repeated unchanged across trials. In this way, it demonstrates that neurons are not so much sensitive to analog values presented in the form of square pulses, but rather to dynamic signals for which they will respond with very high precision in the dynamic domain.&lt;/p&gt;

&lt;/aside&gt;
&lt;!-- 
---


## Spiking Neural Networks in neurobiology



















&lt;figure  id=&#34;figure-mainen--sejnowski-1995httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_2_mainensejnowski1995ipynb&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/replicating_MainenSejnowski1995.png&#34; alt=&#34;[[Mainen &amp;amp; Sejnowski, 1995](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_2_MainenSejnowski1995.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mainen &amp;amp; Sejnowski, 1995&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;



&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reproduucibility&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt; --&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-1&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-diesmann-et-al-1999httpsgithubcomspikeai2022_polychronies-reviewblobmainsrcfigure_3_diesmann_et_al_1999py&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/Diesmann_et_al_1999.png&#34; alt=&#34;[[Diesmann et al. 1999](https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://github.com/SpikeAI/2022_polychronies-review/blob/main/src/Figure_3_Diesmann_et_al_1999.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diesmann et al. 1999&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this other example, I show a simulation that reproduces the 1999 paper by Diesmann and colleagues. This &lt;em&gt;theoretical model&lt;/em&gt; considers ten groups of 100 neurons that are connected from group to group. An interesting property of this system is to show that for the same stimulation, i.e. for the same number of spikes, information can propagate from group to group only if it is sufficiently &lt;em&gt;concentrated in time&lt;/em&gt;. For the first two groups, the information is too dispersed in the first group and spreads progressively and increasingly in subsequent groups. Above a certain threshold, the information formed by a group of relatively synchronous spikes is correctly transmitted to the various groups in the network. This &lt;em&gt;non-linear&lt;/em&gt; behavior is one of the characteristics of spiking networks, giving them a certain richness, but also a certain complexity.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-in-neurobiology-2&#34;&gt;Spiking Neural Networks in neurobiology&lt;/h2&gt;


















&lt;figure  id=&#34;figure-haimerl-et-al-2019httpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/haimerl2019.jpg&#34; alt=&#34;[[Haimerl et al, 2019](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;99%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Haimerl et al, 2019&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
A third example shows an experiment conducted by Rosa Cossart&amp;rsquo;s group at INMED and recently published by Haimerl and colleagues. It shows the results of &lt;em&gt;calcium fluorescence&lt;/em&gt; imaging recordings in mice. By arranging the different neurons in &lt;em&gt;temporal order of activation&lt;/em&gt;, it shows a sequential activation of these neurons, a mechanism which resembles the model mentioned earlier. These activation groups are strongly correlated with the &lt;em&gt;motor behavior&lt;/em&gt; of the mouse, as described in the graph at the top. Of particular interest is the fact that these sequences of activity are stable over time and can be recorded on a &lt;em&gt;subsequent day&lt;/em&gt;. This illustrates the importance of dynamics in the integration of neural computations.
&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;spiking-neural-networks-spiking-motifs&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These observations have led us to &lt;em&gt;review&lt;/em&gt; neurobiological evidence around the existence of a neural representation that would use the relative time of spikes as a means of representing information. In particular, it is possible to use the conduction &lt;em&gt;delays&lt;/em&gt; that exist in the transmission of spikes from one neuron to another. It may seem paradoxical, but these delays are not simply a constraint, but can help to improve our ability to represent information by way of &lt;em&gt;spiking motifs&lt;/em&gt;.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-1&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi-et-al-2023-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/izhikevich.png&#34; alt=&#34;[Grimaldi *et al*, 2023, [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Grimaldi &lt;em&gt;et al&lt;/em&gt;, 2023, &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we consider, for example, this ultra-simplified network consisting of three presynaptic neurons and two output neurons connected by &lt;em&gt;heterogeneous&lt;/em&gt; delays, then we can see that a &lt;em&gt;synchronous&lt;/em&gt; input will generate membrane activity in the two output neurons at different times, so the threshold will never be reached, and these neurons will not produce an output impulse. On the other hand, if these delays are such that the action potentials converge on the neuron at the same instant, then these contributions will be able to sum up at the &lt;em&gt;same instant&lt;/em&gt; and produce an output spike, as denoted here by the red bar.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-2&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/LIF.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To better understand this mechanism, let&amp;rsquo;s return to our animation of a spiking neuron. Action potentials arrive at the neuron and are &lt;em&gt;immediately&lt;/em&gt; transmitted to the neuron&amp;rsquo;s cell body to be integrated and potentially generate a spike.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-spiking-motifs-3&#34;&gt;Spiking Neural Networks: Spiking motifs&lt;/h2&gt;


















&lt;figure  id=&#34;figure-review-on-precise-spiking-motifshttpslaurentperrinetgithubiopublicationgrimaldi-22-polychronies&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/SpikeAI/2022_polychronies-review/raw/main/figures/HSD.gif&#34; alt=&#34;Review on [Precise Spiking Motifs](https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/).&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Review on &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-polychronies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Precise Spiking Motifs&lt;/a&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When using &lt;em&gt;heterogeneous&lt;/em&gt; delays, the situation is different, as the information will take a differential time to arrive or not at the neuron&amp;rsquo;s cell body. Note that if we include a particular &lt;em&gt;spiking motif&lt;/em&gt;, which we have here highlighted by green action potentials, then these converge at the same instant thanks to the delay. We will therefore have a detection in the neuron in the form of a new impulse.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/FastMotionDetection_input.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
We used this theoretical principle in an algorithm for detecting movement in an image. To do this, we first generated event data using natural images that are set in motion along trajectories that resemble those produced by free exploration of the visual scene. You&amp;rsquo;ll notice several features of the event-driven output, such as the fact that faster motion generates more spikes, or that edges oriented parallel to one direction produce few changes, and therefore little spike output - the so-called aperture problem.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-1&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/laurentperrinet/figures/7f382a8074552de1a6a0c5728c60d48788b5a9f8/animated_neurons/conv_HDSNN.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We then used a neural network with a classical architecture, which we enhanced by using an impulse representation that takes into account different possible synaptic delays. In this figure, we have represented the input in the left grid, which represents the occurrence of spikes of positive or negative polarity. Then we have represented different processing channels denoted by the colors green and orange, which are applied to this input to produce membrane activity. As illustrated above, this activity will produce output pulses, notably in synaptic connection nuclei, with heterogeneous delays corresponding to the detection of precise spatio-temporal patterns.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-2&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/motion_kernels.png&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One advantage of this network is that it is differentiable, enabling us to apply classical machine learning methods, notably supervised learning. We then see the emergence of different convolution kernels, and here I represent a subset of its kernels for different directions, as denoted by the red arrows on the left of the graph. It shows the kernels obtained on the spatial representation according to the different columns, and each row represents the different delays from a delay of one on the right to a delay of 12 time steps on the left. Detectors that follow the motion emerge. For example, for the top line from top to bottom. These kernels integrate both positive neurons in red and negative polarity inputs in blue.
Such spatio-temporal filtering is observed in neurobiology, but to my knowledge had never been observed in a model of spiking neurons trained under natural conditions.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-3&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_raw.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will now study the performance of this network in detecting motion in the flow of events entering the network. When we use all the weights of the convolution kernel, we get a very good performance of the order of 99%, represented by the black dot in the top right-hand corner. Note that in the kernels we&amp;rsquo;ve seen emerge, most of the synaptic weights are close to zero, so we might consider removing some of these weights, as this can be shown to reduce the number of event calculations required.&lt;/p&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-4&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy_shortening.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;strong&gt;2 MINUTE&lt;/strong&gt;
This is what we&amp;rsquo;ve done, by first removing the parts of the core corresponding to the longest delays. This &amp;ldquo;shortens&amp;rdquo; the kernel. We quickly observed a degradation in performance, which reached half-saturation when we reduced the number of weights by around 50%. This demonstrates the importance of integrating information that is quite distant and structured over time.
&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks-hd-snn-5&#34;&gt;Spiking Neural Networks: HD-SNN&lt;/h2&gt;


















&lt;figure  id=&#34;figure-grimaldi--lp-2023-biol-cyberneticshttpslaurentperrinetgithubiopublicationgrimaldi-23-bc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/quant_accuracy.svg&#34; alt=&#34;[Grimaldi &amp;amp; LP (2023) Biol Cybernetics](https://laurentperrinet.github.io/publication/grimaldi-23-bc/)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-23-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grimaldi &amp;amp; LP (2023) Biol Cybernetics&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;p&gt;&lt;strong&gt;2 MINUTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a second step, we performed a pruning operation, which consists in progressively removing the weights that are the weakest. This time, performance remains optimal over a wide compression range, and we reach half-saturation when we have removed around 99.8% of the weights. This means that the network is able to maintain very good performance, even when only one weight out of 600 has been kept, and therefore, with a computation time increased by a factor of 600. This property, which we didn&amp;rsquo;t expect, seems promising for creating machine learning algorithms that are less energy-hungry.&lt;/p&gt;

&lt;/aside&gt;
&lt;/section&gt;
&lt;hr&gt;
&lt;section&gt;
&lt;h1 id=&#34;event-based-visionhttpslaurentperrinetgithubioslides2023-12-01-biocomptransitionfade-1&#34;&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2023-12-01-biocomp/?transition=fade&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Event-based vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubio-1&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2023-12-01httpslaurentperrinetgithubiotalk2023-12-01-biocomp-séminaire-colloque-biocomp-2023httpgdr-biocompfrcolloque-biocomp-2023u-1&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2023-12-01-biocomp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2023-12-01]&lt;/a&gt; &lt;a href=&#34;http://gdr-biocomp.fr/colloque-biocomp-2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Séminaire colloque BioComp 2023&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.jpg&#34; alt=&#34;logos&#34; height=&#34;130&#34;/&gt;
&lt;p&gt;&lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  In conclusion, we have seen that event-driven cameras open the door to new applications that mimic the performance of the human eye, in terms of computational dynamics, adaptation to light conditions and energy constraints. This technological development has recently been accompanied by the development of neuromorphic chips and innovative algorithms in the form of spiking neural networks. However, there is still a great deal of progress to be made at theoretical level, particularly in the understanding of these spiking neural networks, and we have shown the potential progress that can be made by exploiting the richness of temporal representations, particularly by taking advantage of heterogeneous delays.
Beyond these particular applications to natural image processing, I hope to have succeeded in demonstrating the importance of cross-fertilizing the field of engineering applications in general with biological neuroscience. This new line of research - known as NeuroAI or, more generally, as computational neuroscience - is likely to develop over the next few years. Thank you for your attention.
&lt;/aside&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>2020-12-10_agileneurobot_anr</title>
      <link>https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr/</guid>
      <description>&lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anr&#34;&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/header.png&#34; alt=&#34;header&#34; height=&#34;450&#34;&gt;
&lt;/a&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;th&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr&#34;&gt;
		Présentation du projet - L. Perrinet
    &lt;!-- &lt;img src=&#34;http://www.cnrs.fr/themes/custom/cnrs/logo.svg&#34; alt=&#34;CNRS&#34; height=&#34;15&#34;&gt; --&gt;
    &lt;!-- &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/thumb/2/2c/CNRS.svg/240px-CNRS.svg.png&#34; alt=&#34;CNRS&#34; height=&#34;40&#34;&gt; --&gt;
    &lt;br&gt;
		&lt;u&gt;[2020-12-10] Réunion de lancement&lt;/u&gt;
	&lt;/a&gt;
	&lt;/th&gt;
	&lt;th&gt;
  &lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/featured.png&#34; alt=&#34;ANR&#34; height=&#34;80&#34;&gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;agileneurobot-fiche-didentité&#34;&gt;AgileNeuRobot: Fiche d&amp;rsquo;identité&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Titre : Robots aériens agiles bio-mimetiques pour le vol en conditions réelles&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Title : Bio-mimetic agile aerial robots flying in real-life conditions&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;CES : CE23 - Intelligence Artificielle (ANR-20-CE23-0021)&lt;/li&gt;
&lt;li&gt;Durée: 3 ans, à partir du 1er mars 2021&lt;/li&gt;
&lt;li&gt;Budget total: 435 k€&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;


















&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;recurrent-processing&#34;&gt;Recurrent processing&lt;/h2&gt;


















&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-communicating-by-event-driven-feed-forward-and-feed-back-communications&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/principe_agile.jpg&#34; alt=&#34;Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;consortium&#34;&gt;Consortium:&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/stephane-viollet/avatar.jpg&#34; alt=&#34;SV&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/ryad-benosman/avatar_hu2d5c77d2b84468d5dedad740d610465b_7790_270x270_fill_q75_lanczos_center.jpg&#34; alt=&#34;RB&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/avatar.jpg&#34; alt=&#34;LP&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Stéphane Viollet&lt;/td&gt;
&lt;td&gt;Ryad Benosman&lt;/td&gt;
&lt;td&gt;Laurent Perrinet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julien Diperi&lt;/td&gt;
&lt;td&gt;Sio-Hoï Ieng&lt;/td&gt;
&lt;td&gt;Emmanuel Daucé&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Inst Sciences Mouvement&lt;/td&gt;
&lt;td&gt;Inst de la Vision&lt;/td&gt;
&lt;td&gt;Inst Neurosci de la Timone&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;gantt-chart-of-project&#34;&gt;Gantt Chart of project&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anr/gantt.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022-07-01_grimaldi-22-areadne</title>
      <link>https://laurentperrinet.github.io/slides/2022-07-01_grimaldi-22-areadne/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-07-01_grimaldi-22-areadne/</guid>
      <description>&lt;img src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/brain-logo-240.jpg&#34; alt=&#34;header&#34; height=&#34;350&#34;&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;th&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2022-07-01_grimaldi-22-areadne&#34;&gt;
		Decoding spiking motifs using neurons with heterogeneous delays
    &lt;!-- &lt;img src=&#34;http://www.cnrs.fr/themes/custom/cnrs/logo.svg&#34; alt=&#34;CNRS&#34; height=&#34;15&#34;&gt; --&gt;
    &lt;!-- &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/thumb/2/2c/CNRS.svg/240px-CNRS.svg.png&#34; alt=&#34;CNRS&#34; height=&#34;40&#34;&gt; --&gt;
    &lt;br&gt;
		&lt;u&gt;[2022-07-01] AREADNE 2022 conference&lt;/u&gt;
	&lt;/a&gt;
	&lt;/th&gt;
	&lt;th&gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;


















&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../grant/anr-anr/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-a-raster-plot&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1a_k.png&#34; alt=&#34;A raster plot..&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A raster plot..
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure--as-a-mixture-of-motifs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1a.png&#34; alt=&#34;.. as a mixture of motifs&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      .. as a mixture of motifs
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure--defined-as-list-of-weights-and-delays&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1b.png&#34; alt=&#34;... defined as list of weights and delays..&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &amp;hellip; defined as list of weights and delays..
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-occurring-from-a-new-raster-plot&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/figure_1c.png&#34; alt=&#34;occurring from a new raster plot..&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;95%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      occurring from a new raster plot..
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/LIF.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/HSD_conductance_speeds.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;
&lt;h2 id=&#34;supervised-learning&#34;&gt;supervised learning&lt;/h2&gt;
&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/2022-06-23_Supervised_MC_input_1.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/2022-06-23_Supervised_MC_input_3.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/2022-06-23_Supervised_MC_input_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;











  





&lt;video autoplay loop  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/talk/2022-06-19-neuro-vision-heterogeneous/2022-05-24_Supervised_MC_MC.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;hr&gt;
&lt;h2 id=&#34;learned-heterogeneous-weights&#34;&gt;Learned heterogeneous weights&lt;/h2&gt;
&lt;hr&gt;


















&lt;figure  id=&#34;figure-heterogeneous-delays-as-convolution-kernels&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/2022-06-26_Supervised_nat-causal_kernel.png&#34; alt=&#34;Heterogeneous delays as convolution kernels.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Heterogeneous delays as convolution kernels.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-mask-applied-on-the-weights&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/2022-06-26_Supervised_nat-causal_kernel-mask.png&#34; alt=&#34;Mask applied on the weights.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Mask applied on the weights.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;


















&lt;figure  id=&#34;figure-scatter-of-on-versus-off-weights&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/2022-07-08_Supervised_nat_joint_ON-OFF.png&#34; alt=&#34;Scatter of ON versus OFF weights.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Scatter of ON versus OFF weights.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;frugal-computing&#34;&gt;Frugal computing&lt;/h2&gt;


















&lt;figure  id=&#34;figure-stable-accuracy-while-pruning-99-weights&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../publication/grimaldi-22-areadne/accuracy.png&#34; alt=&#34;Stable accuracy while pruning ~99% weights.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Stable accuracy while pruning ~99% weights.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/publication/grimaldi-22-areadne/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
