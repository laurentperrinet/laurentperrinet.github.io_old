<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Slides | Novel visual computations</title>
    <link>https://laurentperrinet.github.io/slides/</link>
      <atom:link href="https://laurentperrinet.github.io/slides/index.xml" rel="self" type="application/rss+xml" />
    <description>Slides</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author&#39;s copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder. 
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License Please note that multiple distribution, publication or commercial usage of copyrighted papers included in this website would require submission of a permission request addressed to the journal in which the paper appeared. </copyright><lastBuildDate>Wed, 23 Mar 2022 09:00:00 +0000</lastBuildDate>
    <item>
      <title>2022-03-23_UE-neurosciences-computationnelles</title>
      <link>https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/</link>
      <pubDate>Wed, 23 Mar 2022 09:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2022-03-23_ue-neurosciences-computationnelles/</guid>
      <description>&lt;h1 id=&#34;réseaux-de-neurones-artificiels-et-apprentissage-machine-appliqués-à-la-compréhension-de-la-visionhttpsgithubcomlaurentperrinet2022_ue-neurosciences-computationnelles&#34;&gt;&lt;a href=&#34;https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Réseaux de neurones artificiels et apprentissage machine appliqués à la compréhension de la vision&lt;/a&gt;&lt;/h1&gt;
&lt;h4 id=&#34;laurent-perrinethttpslaurentperrinetgithubiotalk2022-03-23-ue-neurosciences-computationnelles&#34;&gt;&lt;em&gt;&lt;a href=&#34;https://laurentperrinet.github.io/talk/2022-03-23-ue-neurosciences-computationnelles/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Laurent Perrinet&lt;/a&gt;&lt;/em&gt;&lt;/h4&gt;
&lt;h4 id=&#34;u2022-03-23httpsameticeuniv-amufrpluginfilephp5559779mod_resourcecontent1planning_neurocomp_m1_2022pdf-master-1-neurosciences-et-sciences-cognitiveshttpsameticeuniv-amufrcourseviewphpid89069u&#34;&gt;&lt;u&gt;&lt;a href=&#34;https://ametice.univ-amu.fr/pluginfile.php/5559779/mod_resource/content/1/Planning_Neurocomp_M1_2022.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2022-03-23]&lt;/a&gt; &lt;a href=&#34;https://ametice.univ-amu.fr/course/view.php?id=89069&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Master 1 Neurosciences et Sciences Cognitives&lt;/a&gt;&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/perrinet_curriculum-vitae.tex/raw/master/troislogos.png&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;principes-de-la-vision&#34;&gt;Principes de la Vision&lt;/h1&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;li&gt;more on &lt;a href=&#34;https://raw.githubusercontent.com/wowchemy/starter-hugo-academic/master/exampleSite/content/slides/example/index.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision&#34;&gt;À quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor-ilya-repin-1884httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_001.jpg&#34; alt=&#34;[An Unexpected Visitor (Ilya Repin, 1884)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Ilya Repin, 1884)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision-1&#34;&gt;À quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_002.jpg&#34; alt=&#34;[An Unexpected Visitor (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision-2&#34;&gt;À quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor---age-yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_003.jpg&#34; alt=&#34;[An Unexpected Visitor - *Age?* (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;Age?&lt;/em&gt; (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;à-quoi-sert-la-vision-3&#34;&gt;À quoi sert la vision?&lt;/h2&gt;














&lt;figure  id=&#34;figure-an-unexpected-visitor---how-long--yarbus-1965httpswwwcabinetmagazineorgissues30archibaldphp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://www.cabinetmagazine.org/issues/30/cabinet_030_archibald_sasha_006.jpg&#34; alt=&#34;[An Unexpected Visitor - *How long?*  (Yarbus, 1965)](https://www.cabinetmagazine.org/issues/30/archibald.php)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.cabinetmagazine.org/issues/30/archibald.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Unexpected Visitor - &lt;em&gt;How long?&lt;/em&gt;  (Yarbus, 1965)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelles&#34;&gt;Les illusions visuelles&lt;/h2&gt;














&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion_without.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelles-1&#34;&gt;Les illusions visuelles&lt;/h2&gt;














&lt;figure  id=&#34;figure-hering-illusionhttpsenwikipediaorgwikihering_illusion&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Hering_illusion.svg&#34; alt=&#34;[Hering illusion](https://en.wikipedia.org/wiki/Hering_illusion)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://en.wikipedia.org/wiki/Hering_illusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hering illusion&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelles-2&#34;&gt;Les illusions visuelles&lt;/h2&gt;
&lt;p&gt;








  





&lt;video controls  &gt;
  &lt;source src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/Kitaoka.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ilusions of brightness or lightness &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-illusions-visuelles-3&#34;&gt;Les illusions visuelles&lt;/h2&gt;














&lt;figure  id=&#34;figure-rotating-snakes-akiyoshi-kitaokahttpwwwritsumeiacjpakitaokaindex-ehtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2022-01-12_NeuroCercle/figures/42_rotsnakes_main.jpg&#34; alt=&#34;[Rotating Snakes *Akiyoshi KITAOKA*](http://www.ritsumei.ac.jp/~akitaoka/index-e.html)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;http://www.ritsumei.ac.jp/~akitaoka/index-e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rotating Snakes &lt;em&gt;Akiyoshi KITAOKA&lt;/em&gt;&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;de-v1-aux-réseaux-convolutionnels&#34;&gt;De V1 aux réseaux convolutionnels&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;les-neurosciences-computationnelles&#34;&gt;Les neurosciences computationnelles&lt;/h2&gt;














&lt;figure  id=&#34;figure-sejnowski--koch---churchland-1998httpwwwhmsharvardedubssneurobornlabnb204paperssejnowski-koch-churchland-science1988pdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Churchland92.png&#34; alt=&#34;[[Sejnowski,  Koch  &amp;amp; Churchland (1998)](http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;35%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;http://www.hms.harvard.edu/bss/neuro/bornlab/nb204/papers/sejnowski-koch-churchland-science1988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sejnowski,  Koch  &amp;amp; Churchland (1998)&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;le-système-visuel&#34;&gt;Le système visuel&lt;/h2&gt;














&lt;figure  id=&#34;figure-système-visuel-humain-wikipediahttpsfrwikipediaorgwikisystc3a8me_visuel_humain&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/e/e4/Voies_visuelles3.svg&#34; alt=&#34;[Système visuel humain (Wikipedia)](https://fr.wikipedia.org/wiki/Syst%C3%A8me_visuel_humain)&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;45%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://fr.wikipedia.org/wiki/Syst%C3%A8me_visuel_humain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Système visuel humain (Wikipedia)&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;le-cortex-visuel-primaire&#34;&gt;Le cortex visuel primaire&lt;/h2&gt;














&lt;figure  id=&#34;figure-hubel--wiesel-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/scientists.jpg&#34; alt=&#34;[Hubel &amp;amp; Wiesel, 1962]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Hubel &amp;amp; Wiesel, 1962]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;hubel--wiesel&#34;&gt;Hubel &amp;amp; Wiesel&lt;/h2&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://raw.githubusercontent.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/master/figures/ComplexDirSelCortCell250_title.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;[Hubel &amp;amp; Wiesel, 1962]&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;topographie-dans-v1&#34;&gt;Topographie dans V1&lt;/h2&gt;














&lt;figure  id=&#34;figure-bosking-et-al-1997&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/laurentperrinet/2019-04-03_a_course_on_vision_and_modelization/raw/master/figures/Bosking97Fig4.jpg&#34; alt=&#34;[Bosking *et al*, 1997]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;80%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [Bosking &lt;em&gt;et al&lt;/em&gt;, 1997]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels--hiérarchie&#34;&gt;Réseaux convolutionnels : hiérarchie&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.png&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels--cnn&#34;&gt;Réseaux convolutionnels : CNN&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-cnn-fr.jpeg&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels--lopération-de-convolution&#34;&gt;Réseaux convolutionnels : l&amp;rsquo;opération de convolution&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/convolution-layer-a.png?1c517e00cb8d709baf32fc3d39ebae67&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-convolutionnels---math&#34;&gt;Réseaux convolutionnels :  Math&lt;/h2&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;ul&gt;
&lt;li&gt;Convolution discrète dans le temps:
$$
(f \ast g)[n]=\sum_{m=-\infty}^{\infty} f[m] g[n-m]
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;ul&gt;
&lt;li&gt;Convolution discrète bi-dimensionnelle:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y] = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} f[i, j] g[i-x, j-x]
$$
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;ul&gt;
&lt;li&gt;Convolution discrète bi-dimensionnelle multi-canaux:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
(f \ast g)[x, y, k] = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} f[i, j, k] g[i-x, j-x]
$$
&lt;/span&gt;
&lt;hr&gt;
&lt;h1 id=&#34;perspectives&#34;&gt;Perspectives&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;réseaux-prédictifs&#34;&gt;Réseaux prédictifs&lt;/h2&gt;














&lt;figure  id=&#34;figure-boutin-et-al-2021httpslaurentperrinetgithubiopublicationboutin-franciosini-chavane-ruffier-perrinet-20&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/2019-04-03_a_course_on_vision_and_modelization/figures/boutin-franciosini-ruffier-perrinet-19_figure1_a.png&#34; alt=&#34;[[Boutin *et al*, 2021](https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://laurentperrinet.github.io/publication/boutin-franciosini-chavane-ruffier-perrinet-20/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boutin &lt;em&gt;et al&lt;/em&gt;, 2021&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;














&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anb/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recurrent-processing&#34;&gt;Recurrent processing&lt;/h2&gt;














&lt;figure  id=&#34;figure-amidi--amidihttpsstanfordedushervineteachingcs-230cheatsheet-convolutional-neural-networks&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png&#34; alt=&#34;[[Amidi &amp;amp; Amidi](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)]&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      [&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amidi &amp;amp; Amidi&lt;/a&gt;]
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recurrent-processing-1&#34;&gt;Recurrent processing&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-communicating-by-event-driven-feed-forward-and-feed-back-communications&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anb/principe_agile.jpg&#34; alt=&#34;Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;90%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2020-12-10_agileneurobot_anr</title>
      <link>https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr/</guid>
      <description>&lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anb&#34;&gt;
&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anb/header.png&#34; alt=&#34;header&#34; height=&#34;450&#34;&gt;
&lt;/a&gt;
&lt;table&gt;
&lt;tr&gt;
	&lt;th&gt;&lt;a href=&#34;https://laurentperrinet.github.io/slides/2020-12-10_agileneurobot_anr&#34;&gt;
		Présentation du projet - L. Perrinet
    &lt;!-- &lt;img src=&#34;http://www.cnrs.fr/themes/custom/cnrs/logo.svg&#34; alt=&#34;CNRS&#34; height=&#34;15&#34;&gt; --&gt;
    &lt;!-- &lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/thumb/2/2c/CNRS.svg/240px-CNRS.svg.png&#34; alt=&#34;CNRS&#34; height=&#34;40&#34;&gt; --&gt;
    &lt;br&gt;
		&lt;u&gt;[2020-12-10] Réunion de lancement&lt;/u&gt;
	&lt;/a&gt;
	&lt;/th&gt;
	&lt;th&gt;
  &lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anb/featured.png&#34; alt=&#34;ANR&#34; height=&#34;80&#34;&gt;
	&lt;/th&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;agileneurobot-fiche-didentité&#34;&gt;AgileNeuroBot: Fiche d&amp;rsquo;identité&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Titre : Robots aériens agiles bio-mimetiques pour le vol en conditions réelles&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Title : Bio-mimetic agile aerial robots flying in real-life conditions&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;CES : CE23 - Intelligence Artificielle (ANR-20-CE23-0021)&lt;/li&gt;
&lt;li&gt;Durée: 3 ans, à partir du 1er mars 2021&lt;/li&gt;
&lt;li&gt;Budget total: 435 k€&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;spiking-neural-networks&#34;&gt;Spiking Neural Networks&lt;/h2&gt;














&lt;figure  id=&#34;figure-from-frame-based-to-event-based-cameras&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anb/event_driven_computations.png&#34; alt=&#34;From frame-based to event-based cameras.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      From frame-based to event-based cameras.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;recurrent-processing&#34;&gt;Recurrent processing&lt;/h2&gt;














&lt;figure  id=&#34;figure-our-system-is-divided-into-3-units-to-process-visual-inputs-communicating-by-event-driven-feed-forward-and-feed-back-communications&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anb/principe_agile.jpg&#34; alt=&#34;Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Our system is divided into 3 units to process visual inputs communicating by event-driven, feed-forward and feed-back communications.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h2 id=&#34;consortium&#34;&gt;Consortium:&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/stephane-viollet/avatar.jpg&#34; alt=&#34;SV&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/ryad-benosman/avatar_hu2d5c77d2b84468d5dedad740d610465b_7790_270x270_fill_q75_lanczos_center.jpg&#34; alt=&#34;RB&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;th&gt;&lt;img src=&#34;https://laurentperrinet.github.io/author/laurent-u-perrinet/avatar.jpg&#34; alt=&#34;LP&#34; height=&#34;150&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Stéphane Viollet&lt;/td&gt;
&lt;td&gt;Ryad Benosman&lt;/td&gt;
&lt;td&gt;Laurent Perrinet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Julien Diperi&lt;/td&gt;
&lt;td&gt;Sio-Hoï Ieng&lt;/td&gt;
&lt;td&gt;Emmanuel Daucé&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Inst Sciences Mouvement&lt;/td&gt;
&lt;td&gt;Inst de la Vision&lt;/td&gt;
&lt;td&gt;Inst Neurosci de la Timone&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;














&lt;figure  id=&#34;figure-gantt-chart-of-project-organization&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://laurentperrinet.github.io/grant/anr-anb/gantt.png&#34; alt=&#34;Gantt Chart of project organization.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Gantt Chart of project organization.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;Ask info @ &lt;a href=&#34;mailto:laurent.perrinet@univ-amu.fr&#34;&gt;laurent.perrinet@univ-amu.fr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More info @ &lt;a href=&#34;https://laurentperrinet.github.io/grant/anr-anb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web-site&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://laurentperrinet.github.io/slides/example-slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://laurentperrinet.github.io/slides/example-slides/</guid>
      <description>&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sub-slide&#34;&gt;sub-slide&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write sub-slide&lt;/li&gt;
&lt;li&gt;Efficiently write sub-slide&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;sub-slide-2&#34;&gt;sub-slide 2&lt;/h3&gt;
&lt;p&gt;&lt;video data-autoplay src=&#34;http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4&#34;&gt;&lt;/video&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/hulk.png&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/hulk.png&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
