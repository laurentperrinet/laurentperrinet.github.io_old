<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: January 19, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.6" />
  

  
  












  
  










  







  
  

  
  
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.4eb72b3c09083e6876124d44f28a6a86.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" disabled>
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" >
  

  
  



























  
  
  






  <meta name="author" content="Laurent U Perrinet" />





  

<meta name="description" content="Scientific website of Laurent Udo Perrinet." />



<link rel="alternate" hreflang="en-us" href="https://laurentperrinet.github.io/" />
<link rel="canonical" href="https://laurentperrinet.github.io/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#bbdefb" />










  
  






<meta property="twitter:card" content="summary" />
<meta property="twitter:image" content="https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_512x512_fill_lanczos_center_3.png" />



  
    
  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Novel visual computations" />
<meta property="og:url" content="https://laurentperrinet.github.io/" />
<meta property="og:title" content="Novel visual computations" />
<meta property="og:description" content="Scientific website of Laurent Udo Perrinet." /><meta property="og:image" content="https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta property="og:updated_time" content="2024-01-01T00:00:00&#43;00:00" />
  





<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "https://laurentperrinet.github.io/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "https://laurentperrinet.github.io/"
}
</script>


  
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "@id": "https://laurentperrinet.github.io/",
  "name": "Novel visual computations",
  "logo": "https://laurentperrinet.github.io/media/icon_hu064773317e508d3c994dd612a46c4bf9_247868_192x192_fill_lanczos_center_3.png",
  
  
  
  
  "url": "https://laurentperrinet.github.io/"
}
</script>

  




  
  
  
    <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
  

  
  
    <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Novel visual computations" />
  

  


  
  <title>Novel visual computations</title>

  
  
  
    <link rel="me" href="https://neuromatch.social/@laurentperrinet" />

  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  dark "  >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.3c4b5f677f88bb5526061cb52d934de6.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Novel visual computations</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured" data-target="#featured"><span>Latest</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts" data-target="#posts"><span>Events</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects" data-target="#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#people" data-target="#people"><span>People</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications" data-target="#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks" data-target="#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#grants" data-target="#grants"><span>Grants</span></a>
          </li>

          
          

          

          
          
          
            
              
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/sciblog/" target="_blank" rel="noopener"><span>BlogBook</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact" data-target="#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    


  









  
  

<span class="js-widget-page d-none"></span>





  
  
  
  




  

























  
  








  
  
  
    
    
      
    
    
  
  
  



  





























<section id="hero" class="home-section wg-hero dark "  >
 <div class="home-section-bg  bg-image parallax" style="background-image: linear-gradient(90deg, #4bb4e3, #2b94c3);background-image: url(&#39;https://laurentperrinet.github.io/media/Etienne-Rey-TROPIQUE_small_hu50bfd4f812fe208e9369347915a474e9_105148_1920x1920_fit_q75_h2_lanczos.webp&#39;);filter: brightness(0.5);">
   
 </div>
  <div class="container">

  

    










    
      <h1 class="hero-title">Novel visual computations</h1>
    

    
      <div class="hero-lead"><p>Let&rsquo;s admit it: brains are not computers. Indeed, computers are still deceptive compared to biological perceptual systems. Think about rapidly detecting a novel object in clutter. Think about performing this with little supervision at a low energetic cost&hellip;</p>
<p>To narrow the gap between neuroscience and the theory of sensory processing computations, I am interested in <em>bridging  geometrical regularities found in natural scenes with the properties of neural computations</em> as they are observed in  sensory processes or behavior.</p>
<!-- Place this tag where you want the button to render. -->
<p><a class="github-button" href="https://github.com/laurentperrinet" data-size="large" data-show-count="true" aria-label="Follow @laurentperrinet on GitHub">Follow @laurentperrinet on GitHub</a>.<script async defer src="https://buttons.github.io/buttons.js"></script></p>
<p><a href="https://neuromatch.social/@laurentperrinet" class="mastodon-share-button">@laurentperrinet@neuromatch.social</a><script async="" src="https://fo.llow.social/button.js"></script></p>
<!-- <iframe class="mastodon-timeline-embed" src="https://fo.llow.social/embed/@laurentperrinet@neuromatch.social" style="width:100%;height:0px"></iframe><script src="https://fo.llow.social/embed.js" async></script> --></div>
    

    
    

    
    

  
  


  

  </div>
</section>


  






























































<section id="about" class="home-section wg-about  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    









  










<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">

      
      
      <img class="avatar avatar-circle"
           width="270" height="270"
           src="/author/laurent-u-perrinet/avatar_hufdd75f582622d56af8f81b6f2821d19b_501026_270x270_fill_lanczos_center_3.png" alt="Laurent U Perrinet">
      

      <div class="portrait-title">

        <h2>Laurent U Perrinet</h2>

        <h3>Researcher in Computational Neuroscience</h3>

        
        <h3>
          <a href="https://www.int.univ-amu.fr/PERRINET-Laurent" target="_blank" rel="noopener">
          <span></span>
          </a>
        </h3>
        
      </div>

      <ul class="network-icon" aria-hidden="true">
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="/#contact"  aria-label="envelope">
            <i class="fas fa-envelope big-icon"></i>
          </a>
        </li>
        
        
        
        
          
        
        
        
        
        
          
        
        <li>
          <a href="https://neuromatch.social/@laurentperrinet" target="_blank" rel="noopener" aria-label="mastodon">
            <i class="fab fa-mastodon big-icon"></i>
          </a>
        </li>
        
        
        
        
        
        
        
        
          
        
        <li>
          <a href="https://orcid.org/0000-0002-9536-010X" target="_blank" rel="noopener" aria-label="orcid">
            <i class="ai ai-orcid big-icon"></i>
          </a>
        </li>
        
      </ul>

    </div>
  </div>
  <div class="col-12 col-lg-8">

    
    <h1>Biography</h1>

    <div class="article-style">
      <p><a href="https://laurentperrinet.github.io/" target="_blank" rel="noopener">Laurent Perrinet</a> is a computational neuroscientist specialized in large scale neural network models of low-level vision, perception and action, currently at the &ldquo;Institut de Neurosciences de la Timone&rdquo; (France), a joint research unit (CNRS / Aix-Marseille Université). He co-authored more than 40 articles in computational neuroscience and computer vision. He graduated from the aeronautics engineering school SUPAERO, in Toulouse (France) with a signal processing and applied mathematics degree. He received a PhD in Cognitive Science in 2003 on the mathematical analysis of temporal spike coding of images by using a multi-scale and adaptive representation of natural scenes. His research program is focusing in bridging the complex dynamics of realistic, large-scale models of spiking neurons with functional models of low-level vision. In particular, as part of the FACETS and BrainScaleS consortia, he has developed experimental protocols in collaboration with neurophysiologists to characterize the response of population of neurons. Recently, he extended models of visual processing in the framework of predictive processing in collaboration with the team of Karl Friston at the University College of London. This method aims at characterizing the processing of dynamical flow of information as an active inference process. His current challenge within the <a href="https://www.int.univ-amu.fr/spip.php?page=equipe&equipe=NeOpTo&lang=en">NeOpTo team</a> is to translate, or <em>compile</em> in computer terminology, this mathematical formalism with the event-based nature of neural information with the aim of pushing forward the frontiers of Artificial Intelligence systems.</p>

    </div>

    <div class="row">

      
      <div class="col-md-5">
        <div class="section-subheading">Interests</div>
        <ul class="ul-interests mb-0">
          
          <li>Computational Neuroscience</li>
          
          <li>Machine Learning</li>
          
          <li>Vision</li>
          
        </ul>
      </div>
      

      
      <div class="col-md-7">
        <div class="section-subheading">Education</div>
        <ul class="ul-edu fa-ul mb-0">
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Habilitation à diriger des recherches, 2014</p>
              <p class="institution">Aix-Marseille Université</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">PhD. in Cognitive Science, 2003</p>
              <p class="institution">Université P. Sabatier, Toulouse, France</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">M.S. in Engineering, 1998</p>
              <p class="institution">SupAéro, Toulouse, France</p>
            </div>
          </li>
          
        </ul>
      </div>
      

    </div>
  </div>
</div>


  

  </div>
</section>


  






























































<section id="featured" class="home-section wg-collection  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Lastest Publications</h1>
          
        </div>
      
    
  

    










  








  
  





















  




<div class="col-12 col-lg-8">

  

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/christian-casanova/">Christian Casanova</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Submitted</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-24-sparse/" >Kernel Heterogeneity Improves Sparseness of Natural Images Representations</a>
  </div>

  
  <a href="/publication/ladret-24-sparse/"  class="summary-link">
    <div class="article-style">
      <p>Both biological and artificial neural networks inherently balance their performance with their operational cost, which balances their computational abilities. Typically, an efficient neuromorphic neural network is one that learns representations that reduce the redundancies and dimensionality of its input. This is for instance achieved in sparse coding, and sparse representations derived from natural images yield representations that are heterogeneous, both in their sampling of input features and in the variance of those features. Here, we investigated the connection between natural images&rsquo; structure, particularly oriented features, and their corresponding sparse codes. We showed that representations of input features scattered across multiple levels of variance substantially improve the sparseness and resilience of sparse codes, at the cost of reconstruction performance. This echoes the structure of the model&rsquo;s input, allowing to account for the heterogeneously aleatoric structures of natural images. We demonstrate that learning kernel from natural images produces heterogeneity by balancing between approximate and dense representations, which improves all reconstruction metrics. Using a parametrized control of the kernels&rsquo; heterogeneity used by a convolutional sparse coding algorithm, we show that heterogeneity emphasizes sparseness, while homogeneity improves representation granularity. In a broader context, these encoding strategy can serve as inputs to deep convolutional neural networks. We prove that such variance-encoded sparse image datasets enhance computational efficiency, emphasizing the benefits of kernel heterogeneity to leverage naturalistic and variant input structures and possible applications to improve the throughput of neuromorphic hardware.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-24-sparse/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laurentperrinet.github.io/publication/ladret-24-sparse/" target="_blank" rel="noopener">
    URL</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2312.14685" target="_blank" rel="noopener">
    Preprint</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-23-formes-et-perception/" >Formes &amp; perception</a>
  </div>

  
  <a href="/publication/perrinet-23-formes-et-perception/"  class="summary-link">
    <div class="article-style">
      <p>Article de dissémination : la perception visuelle, telle qu&rsquo;elle paut être comprise à travers illusions visuelles.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-23-formes-et-perception/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/art-science/">
    Project
  </a>
  

  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laurentperrinet.github.io/2023-01-31_formes-et-perception" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/urbano-miguel-nunes/">Urbano Miguel Nunes</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/sio-hoi-ieng/">Sio-Hoi Ieng</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Conference on Computer Vision 2023 (ICCV2023)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/nunes-23-iccv/" >
      <div class="img-hover-zoom">
        <img src="/publication/nunes-23-iccv/featured_hud89fddd85482f6903a3794009e962548_2916044_5140f39cfa16ee5b7f45436f339f6043.webp" height="455" width="808"
            class="article-banner" alt="Time-to-Contact Map by Joint Estimation of Up-to-Scale Inverse Depth and Global Motion using a Single Event Camera" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/nunes-23-iccv/" >Time-to-Contact Map by Joint Estimation of Up-to-Scale Inverse Depth and Global Motion using a Single Event Camera</a>
  </div>

  
  <a href="/publication/nunes-23-iccv/"  class="summary-link">
    <div class="article-style">
      <p>Event cameras asynchronously report brightness changes with a temporal resolution in the order of microseconds, which makes them inherently suitable to address problems that involve rapid motion perception, such as ventral landing and fast obstacle avoidance. These problems are typically addressed by estimating a single global time-to-contact (TTC) measure, which explicitly assumes that the surface/obstacle is planar and fronto-parallel. We relax this assumption by proposing an incremental event-based method to estimate the TTC that jointly estimates the (up-to scale) inverse depth and global motion using a single event camera. The proposed method is reliable and fast while asynchronously maintaining a TTC map (TTCM), which provides per-pixel TTC estimates. As a side product, the proposed method can also estimate per-event optical flow. We achieve state-of-the-art performances on TTC estimation in terms of accuracy and runtime per event while achieving competitive performance on optical flow estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/nunes-23-iccv/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Nunes_Time-to-Contact_Map_by_ICCV_2023_supplemental.pdf" target="_blank" rel="noopener">
    PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.science/hal-04230502" target="_blank" rel="noopener">
    HAL</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openaccess.thecvf.com/content/ICCV2023/html/Nunes_Time-to-Contact_Map_by_Joint_Estimation_of_Up-to-Scale_Inverse_Depth_and_ICCV_2023_paper.html" target="_blank" rel="noopener">
    ICCV</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/neuromorphic-paris/ETTCM" target="_blank" rel="noopener">
    code</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>ICANN Special Session on Recent Advances in Spiking Neural Networks</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-23-icann/" >Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network</a>
  </div>

  
  <a href="/publication/perrinet-23-icann/"  class="summary-link">
    <div class="article-style">
      <p>Recently, interest has grown in exploring the hypothesis that neural activity conveys information through precise spiking motifs. To investigate this phenomenon, various algorithms have been proposed to detect such motifs in Single Unit Activity (SUA) recorded from populations of neurons. In this study, we present a novel detection model based on the inversion of a generative model of raster plot synthesis. Using this generative model, we derive an optimal detection procedure that takes the form of logistic regression combined with temporal convolution. A key advantage of this model is its differentiability, which allows us to formulate a supervised learning approach using a gradient descent on the binary cross-entropy loss. To assess the model&rsquo;s ability to detect spiking motifs in synthetic data, we first perform numerical evaluations. This analysis highlights the advantages of using spiking motifs over traditional firing rate based population codes. We then successfully demonstrate that our learning method can recover synthetically generated spiking motifs, indicating its potential for further applications. In the future, we aim to extend this method to real neurobiological data, where the ground truth is unknown, to explore and detect spiking motifs in a more natural and biologically relevant context.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-23-icann/perrinet-23-icann.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-23-icann/cite.bib">
  Cite
</a>








  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2023-09-27_icann/" target="_blank">
    Slides
  </a>
  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laurentperrinet.github.io/publication/perrinet-23-icann/" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>32nd International Conference on Artificial Neural Networks (ICANN 2023)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/jeremie-23-icann/" >Retinotopy improves the categorisation and localisation of visual objects in CNNs</a>
  </div>

  
  <a href="/publication/jeremie-23-icann/"  class="summary-link">
    <div class="article-style">
      <p> to be presented at the 32nd International Conference on Artificial Neural Networks (ICANN 2023) in Heraklion (Greece). </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/jeremie-23-icann/jeremie-23-icann.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/jeremie-23-icann/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/ilias-rentzeperis/">Ilias Rentzeperis</a></span>, <span >
      <a href="/author/luca-calatroni/">Luca Calatroni</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/dario-prandi/">Dario Prandi</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>PLOS Computational Biology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/rentzeperis-23/" >
      <div class="img-hover-zoom">
        <img src="/publication/rentzeperis-23/featured_hu891461c13ec458d0dcc3791474a938bc_266264_9572615309994c5b578497099ce3ddce.webp" height="455" width="808"
            class="article-banner" alt="Beyond $\ell_1$ sparse coding in V1" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/rentzeperis-23/" >Beyond $\ell_1$ sparse coding in V1</a>
  </div>

  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2301.10002" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/rentzeperis-23" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rentzeperis-23/cite.bib">
  Cite
</a>





  
    
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1011459" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Biological Cybernetics</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-23-bc/" >Learning heterogeneous delays in a layer of spiking neurons for fast motion detection</a>
  </div>

  
  <a href="/publication/grimaldi-23-bc/"  class="summary-link">
    <div class="article-style">
      <p>read the paper online (paywall) or read the reprint as PDF
full code with extensive Supplementary Material
join the Zotero group to add and discuss more items
this paper is a follow-up of Antoine Grimaldi, Camille Besnainou, Hugo Ladret, Laurent U Perrinet (2022).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-23-bc/grimaldi-23-bc.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-23-bc/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00422-023-00975-8" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/sio-hoi-ieng/">Sio-Hoi Ieng</a></span>, <span >
      <a href="/author/ryad-benosman/">Ryad Benosman</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>In revision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-23/" >A Robust Event-Driven Approach to Always-on Object Recognition</a>
  </div>

  
  <a href="/publication/grimaldi-23/"  class="summary-link">
    <div class="article-style">
      <p>We propose a neuromimetic architecture able to perform always-on pattern recognition. To achieve this, we extended an existing event-based algorithm [1], which introduced novel spatio-temporal features as a Hierarchy Of Time-Surfaces (HOTS). Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient event-based pattern recognition architecture. Inspired by neuroscience, we extended this method to increase its performance. Our first contribution was to add a homeostatic gain control on the activity of neurons to improve the learning of spatio-temporal patterns [2]. A second contribution is to draw an analogy between the HOTS algorithm and Spiking Neural Networks (SNN). Following that analogy, our last contribution is to modify the classification layer and remodel the offline pattern categorization method previously used into an online and event-driven one. This classifier uses the spiking output of the network to define novel time surfaces and we then perform online classification with a neuromimetic implementation of a multinomial logistic regression. Not only do these improvements increase consistently the performances of the network, they also make this event-driven pattern recognition algorithm online and bio-realistic. Results were validated on different datasets: DVS barrel [3], Poker-DVS [4] and N-MNIST [5]. We foresee to develop the SNN version of the method and to extend this fully event-driven approach to more naturalistic tasks, notably for always-on, ultra-fast object categorization.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-23/grimaldi-23.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-23/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.36227/techrxiv.18003077" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laurentperrinet.github.io/publication/grimaldi-23/" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/nelson-cortes/">Nelson Cortes</a></span>, <span >
      <a href="/author/lamyae-ikan/">Lamyae Ikan</a></span>, <span >
      <a href="/author/frederic-chavane/">Frederic Chavane</a></span>, <span >
      <a href="/author/christian-casanova/">Christian Casanova</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Nature Communications Biology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/ladret-23/" >
      <div class="img-hover-zoom">
        <img src="/publication/ladret-23/featured_hubbf63d8d7d25b21f139c2f10354080fc_466086_d02f09bfa0488c2f0f986b19c696943c.webp" height="455" width="808"
            class="article-banner" alt="Cortical recurrence supports resilience to sensory variance in the primary visual cortex" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-23/" >Cortical recurrence supports resilience to sensory variance in the primary visual cortex</a>
  </div>

  
  <a href="/publication/ladret-23/"  class="summary-link">
    <div class="article-style">
      <p>Our daily endeavors occur in a complex visual environment, whose intrinsic variability challenges the way we integrate information to make decisions. By processing myriads of parallel sensory inputs, our brain is theoretically able to compute the variance of its environment, a cue known to guide our behavior. Yet, the neurobiological and computational basis of such variance computations are still poorly understood. Here, we quantify the dynamics of sensory variance modulations of cat primary visual cortex neurons. We report two archetypal neuronal responses, one of which is resilient to changes in variance and co-encodes the sensory feature and its variance, improving the population encoding of orientation. The existence of these variance-specific responses can be accounted for by a model of intracortical recurrent connectivity. We thus propose that local recurrent circuits process uncertainty as a generic computation, advancing our understanding of how the brain handles naturalistic inputs.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/ladret-23/ladret-23.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-23/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/s42003-023-05042-3" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>ICLR 2023 SNN Workshop</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-23-iclr/" >Convolutional Sparse Coding is improved by heterogeneous uncertainty modeling</a>
  </div>

  
  <a href="/publication/ladret-23-iclr/"  class="summary-link">
    <div class="article-style">
      <p>Aleatoric uncertainty characterizes the variability of features found in natural images, and echoes the epistemic uncertainty ubiquitously found in computer vision models. We explore this &lsquo;&lsquo;uncertainty in, uncertainty out&rsquo;&rsquo; relationship by generating convolutional sparse coding dictionaries with parametric epistemic uncertainty. This improves sparseness, resilience and reconstruction of natural images by providing the model a way to explicitly represent the aleatoric uncertainty of its input. We demonstrate how hierarchical processing can make use of this scheme by training a deep convolutional neural network to classify a sparse-coded CIFAR10 dataset, showing that encoding uncertainty in a sparse code is as efficient as using conventional images, with additional beneficial computational properties. Overall, this work empirically demonstrates the advantage of partitioning epistemic uncertainty in sparse coding algorithms.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/ladret-23-iclr/ladret-23-iclr.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-23-iclr/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/jeremie-23-ultra-fast-cat/" >
      <div class="img-hover-zoom">
        <img src="/publication/jeremie-23-ultra-fast-cat/featured_huab10630c6b830d85988701eae28ef374_131848_c6600f7f5490c8e1eda608b2be6e6642.webp" height="455" width="808"
            class="article-banner" alt="Ultra-Fast Image Categorization in biology and in neural models" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/jeremie-23-ultra-fast-cat/" >Ultra-Fast Image Categorization in biology and in neural models</a>
  </div>

  
  <a href="/publication/jeremie-23-ultra-fast-cat/"  class="summary-link">
    <div class="article-style">
      <p>Humans are able to robustly categorize images and can, for instance, detect the presence of an animal in a briefly flashed image in as little as 120 ms. Initially inspired by neuroscience, deep-learning algorithms literally bloomed up in the last decade such that the accuracy of machines is at present superior to humans for visual recognition tasks. However, these artificial networks are usually trained and evaluated on very specific tasks, for instance on the 1000 separate categories of IMAGENET. In that regard, biological visual systems are more flexible and efficient compared to artificial systems on generic ecological tasks. In order to deepen this comparison, we retrained the standard VGG Convolutional Neural Network (CNN) on two independent tasks which are ecologically relevant for humans: one task defined as detecting the presence of an animal and the other as detecting the presence of an artifact. We show that retraining the network achieves human-like performance level which is reported in psychophysical tasks. We also compare the accuracy of the detection on an image-by-image basis. This showed in particular that the two models perform better when combining their outputs. Indeed, animals (e.g. lions) tend to be less present in photographs containing artifacts (e.g. buildings). These re-trained models could reproduce some unexpected behavioral observations from humans psychophysics such as the robustness to rotations (e.g. upside-down or slanted image) or to a grayscale transformation. Finally, we quantitatively tested the number of layers of the CNN which are necessary to reach such a performance, showing that a good accuracy for ultra-fast categorization could be reached with only a few layers, challenging the belief that image recognition would require a deep sequential analysis of visual objects. We expect to apply this framework to guide future model-based psychophysical experiments and biomimetic deep neuronal architectures designed for such tasks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://arxiv.org/abs/2205.03635" target="_blank" rel="noopener">
  Preprint
</a>




  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/jeremie-23-ultra-fast-cat/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3390/vision7020029" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/nelson-cortes/">Nelson Cortes</a></span>, <span >
      <a href="/author/lamyae-ikan/">Lamyae Ikan</a></span>, <span >
      <a href="/author/frederic-chavane/">Frederic Chavane</a></span>, <span >
      <a href="/author/christian-casanova/">Christian Casanova</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Computational and Systems Neuroscience (Cosyne) 2023</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-23-cosyne/" >Resilience to sensory uncertainty in the primary visual cortex</a>
  </div>

  
  <a href="/publication/ladret-23-cosyne/"  class="summary-link">
    <div class="article-style">
      <p>Our daily endeavors occur in a complex visual environment, whose intrinsic variability shapes the way we integrate information to make decisions. By processing thousands of parallel sensory inputs, our brain is theoretically able to compute the uncertainty of its environment, which would allow it to perform Bayesian integration of its internal representations and its new sensory inputs to drive optimal inference. While there is convincing evidence that humans do compute this sensory uncertainty to guide their behavior, the actual neurobiological and computational principles on which uncertainty computations rely are still poorly understood. Here, we generated naturalistic stimuli of controlled uncertainty and performed a model-based analysis of their electrophysiological correlates in the primary visual cortex. Firstly, we report two layer-specific neuronal responses : infragranular layer neurons were vulnerable to increments of uncertainty, contrarily to supragranular neurons who were resilient, to the point of sometimes reducing uncertainty from the input. Secondly, we used neural decoding to show that these two responses have two different functional population roles: vulnerable neurons encode only the sensory feature (here, orientation) of the input, while resilient neurons co-encode both the sensory feature and its uncertainty. Finally, we implemented a recurrent leaky integrate-and-fire neural network to mechanistically demonstrate that these different types of responses to uncertainty can be explained by different types of recurrent connectivity between cortical neurons. Overall, we provide neurobiological and computational evidences which pinpoint recurrent interactions as the neural substrate of computations on sensory uncertainty. This fits theoretical considerations on canonical microcircuit in the cortex, potentially establishing uncertainty computations as a new general role for local recurrent cortical connectivity.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.world-wide.org/cosyne-23/resilience-sensory-uncertainty-primary-88600879/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-23-cosyne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GDR Vision, Toulouse, 2023</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-23-gdr/" >Learning heterogeneous delays of spiking neurons for motion detection</a>
  </div>

  
  <a href="/publication/grimaldi-23-gdr/"  class="summary-link">
    <div class="article-style">
      <p>The response of a biological neuron depends largely on the precise timing of presynaptic spikes that reach the basal dendritic tree. However, most neuronal models do not take advantage of this minute temporal dimension, especially in exploiting the variety of synaptic delays on the dendritic tree. A notable exception is the polychronization model, a recurrent model of spiking neurons including fixed and random heterogeneous delays and in which the weights are learned using Spike-Time Dependent Plasticity. The output raster plot displays repeated activations of prototypical spiking motifs called Polychronous Groups. Importantly, these motifs seem to be highly relevant in experimental neuroscience. Here, by extending the model of~[3], we develop a spiking neural network model for the efficient detection of PGs: By defining the generation of the raster plot as a probabilistic combination of PGs, we build and train the network in order to optimize the inversion of this generative model.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://gdr-vision-2023.sciencesconf.org/browse?forward-action=index&amp;forward-controller=browse&amp;docid=442297&amp;lang=en" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-23-gdr/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://gdr-vision-2023.sciencesconf.org/browse?forward-action=index&amp;forward-controller=browse&amp;docid=442297&amp;lang=en" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>In preparation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/jeremie-23-ccn/" >Retinotopy improves the categorisation and localisation of visual objects in CNNs</a>
  </div>

  
  <a href="/publication/jeremie-23-ccn/"  class="summary-link">
    <div class="article-style">
      <p>to be presented at the Computational Cognitive Neuroscience Society Meeting 2023 in Oxford
see a follow-up presentation in: Jean-Nicolas Jérémie, Emmanuel Daucé, Laurent U Perrinet (2023). Retinotopy improves the categorisation and localisation of visual objects in CNNs.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/jeremie-23-ccn" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/jeremie-23-ccn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/amelie-gruel/">Amélie Gruel</a></span>, <span >
      <a href="/author/dalia-hareb/">Dalia Hareb</a></span>, <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/jean-martinet/">Jean Martinet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/bernabe-linares-barranco/">Bernabé Linares-Barranco</a></span>, <span >
      <a href="/author/teresa-serrano-gotarredona/">Teresa Serrano-Gotarredona</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2023
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Biological Cybernetics</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/gruel-23-bc/" >Stakes of Neuromorphic Foveation: a promising future for embedded event cameras</a>
  </div>

  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.researchsquare.com/article/rs-2120721" target="_blank" rel="noopener">
  Preprint
</a>




  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/gruel-23-bc/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/amelie-gruel/">Amélie Gruel</a></span>, <span >
      <a href="/author/camille-besnainou/">Camille Besnainou</a></span>, <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/jean-martinet/">Jean Martinet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/grimaldi-22-polychronies/" >
      <div class="img-hover-zoom">
        <img src="/publication/grimaldi-22-polychronies/featured_huccbaf5c656b908d0604d6653ee0cb883_2498963_a2d563c532f9fd1868fbf8a3bc19dbed.webp" height="455" width="808"
            class="article-banner" alt="Precise spiking motifs in neurobiological and neuromorphic data" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-22-polychronies/" >Precise spiking motifs in neurobiological and neuromorphic data</a>
  </div>

  
  <a href="/publication/grimaldi-22-polychronies/"  class="summary-link">
    <div class="article-style">
      <p>Why do neurons communicate through spikes? By definition, spikes are all-or-none neural events which occur at continuous times. In other words, spikes are on one side binary, existing or not without further details, and on the other can occur at any asynchronous time, without the need for a centralized clock. This stands in stark contrast to the analog representation of values and the discretized timing classically used in digital processing and at the base of modern-day neural networks. As neural systems almost systematically use this so-called event-based representation in the living world, a better understanding of this phenomenon remains a fundamental challenge in neurobiology in order to better interpret the profusion of recorded data. With the growing need for intelligent embedded systems, it also emerges as a new computing paradigm to enable the efficient operation of a new class of sensors and event-based computers, called neuromorphic, which could enable significant gains in computation time and energy consumption, a major societal issue in the era of the digital economy and global warming. In this review paper, we provide evidence from biology, theory and engineering that the precise timing of spikes plays a crucial role in our understanding of the efficiency of neural networks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-22-polychronies/grimaldi-22-polychronies.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-22-polychronies/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3390/brainsci13010068" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/camille-besnainou/">Camille Besnainou</a></span>, <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of ICIP 2022</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-22-icip/" >Learning heterogeneous delays of spiking neurons for motion detection</a>
  </div>

  
  <a href="/publication/grimaldi-22-icip/"  class="summary-link">
    <div class="article-style">
      <p>The response of a biological neuron depends largely on the precise timing of presynaptic spikes that reach the basal dendritic tree. However, most neuronal models do not take advantage of this minute temporal dimension, especially in exploiting the variety of synaptic delays on the dendritic tree. A notable exception is the polychronization model, a recurrent model of spiking neurons including fixed and random heterogeneous delays and in which the weights are learned using Spike-Time Dependent Plasticity. The output raster plot displays repeated activations of prototypical spiking motifs called Polychronous Groups. Importantly, these motifs seem to be highly relevant in experimental neuroscience. Here, by extending the model of~[3], we develop a spiking neural network model for the efficient detection of PGs: By defining the generation of the raster plot as a probabilistic combination of PGs, we build and train the network in order to optimize the inversion of this generative model.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-22-icip/grimaldi-22-icip.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-22-icip/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/ICIP46576.2022.9897394" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://2022.ieeeicip.org/" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/camille-besnainou/">Camille Besnainou</a></span>, <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Bernstein Conference 2022</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-22-bernstein/" >Detection of precise spiking motifs using spike-time dependent weight and delay plasticity</a>
  </div>

  
  <a href="/publication/grimaldi-22-bernstein/"  class="summary-link">
    <div class="article-style">
      <p>The spiking response of a biological neuron depends on the precise timing of afferent spikes. This temporal aspect of the neuronal code is essential in understanding information processing in neurobiology. In this model, raster plot analysis showed repeated activation of specific spiking motifs, which exhibit a precise temporal sequence of neural activations. Our first contribution is to develop a model for the efficient detection of temporal spiking motifs based on a layer of neurons with hetero-synaptic delays. Indeed, the variety of synaptic delays on the dendritic tree allows synchronizing synaptic inputs as they reach the basal dendritic tree. Second, we propose a bio-plausible unsupervised learning rule on both weights and delays through the derivation of a loss function which depends on the membrane potential of the spiking neuron and a sparseness regularization. We demonstrate on synthetic data that such a layer of spiking neurons is able to learn different repeating spatio-temporal motifs embedded in the spike train. Then, we test the robustness of the detection accuracy of the model by adding Poisson noise and compare it to a layer of Leaky-Integrate and Fire neurons trained with STDP. Results show a large improvement in performances when adding temporal delays for computations and a great increase in robustness to noise. We show that using synaptic delays for neuronal computations highly increases the representational capacities of a single neuron and its resilience to noise. .</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-22-bernstein/grimaldi-22-bernstein.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-22-bernstein/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>PLoS Computational Biology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/franciosini-21/" >
      <div class="img-hover-zoom">
        <img src="/publication/franciosini-21/featured_hu9b69386a773810bf9e1d727831735035_76078_152a6b9bc927d5a8a7c6da22c61e8f07.webp" height="455" width="808"
            class="article-banner" alt="Pooling in a predictive model of V1 explains functional and structural diversity across species" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/franciosini-21/" >Pooling in a predictive model of V1 explains functional and structural diversity across species</a>
  </div>

  
  <a href="/publication/franciosini-21/"  class="summary-link">
    <div class="article-style">
      <p>Neurons in the primary visual cortex are selective to orientation with various degrees of selectivity to the spatial phase, from high selectivity in simple cells to low selectivity in complex cells. Various computational models have suggested a possible link between the presence of phase invariant cells and the existence of cortical orientation maps in higher mammals&rsquo; V1. These models, however, do not explain the emergence of complex cells in animals that do not show orientation maps. In this study, we build a model of V1 based on a convolutional network called Sparse Deep Predictive Coding (SDPC) and show that a single computational mechanism, pooling, allows the SDPC model to account for the emergence of complex cells as well as cortical orientation maps in V1, as observed in distinct species of mammals. By using different pooling functions, our model developed complex cells in networks that exhibit orientation maps (e.g., like in carnivores and primates) or not (e.g., rodents and lagomorphs). The SDPC can therefore be viewed as a unifying framework that explains the diversity of structural and functional phenomena observed in V1. In particular, we show that orientation maps emerge naturally as the most cost-efficient structure to generate complex cells under the predictive coding principle.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/2021.04.19.440444" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/franciosini-21" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/franciosini-21/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1010270" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the FENS Forum 2022</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-22-fens/" >Learning heterogeneous delays of spiking neurons for motion detection</a>
  </div>

  
  <a href="/publication/grimaldi-22-fens/"  class="summary-link">
    <div class="article-style">
      <p>How to efficiently make use of time in neural computations? ⏱️
With @laurentperrinet we developed a model of spiking neuron including, in addition to synaptic weights, synaptic delays. https://t.co/eztnd5CUMn
Come see this work on Tuesday morning at #FENS2022 poster n° 547 🪩</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-22-fens/grimaldi-22-fens.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-22-fens/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the FENS Forum 2022</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-22-fens/" >Recurrent cortical connectivity in the primary visual cortex supports robust encoding of natural sensory inputs</a>
  </div>

  
  <a href="/publication/ladret-22-fens/"  class="summary-link">
    <div class="article-style">
      <p>Given prior activity on #NeuroTwitter these past days, you probably can guess that this is my #FENS2022 tweet ! Come chat with me about natural vision, recurrent processing and neural decoding on Monday afternoon, poster S04-550 !</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/ladret-22-fens/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-22-fens/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the FENS Forum 2022</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/jeremie-22-fens/" >
      <div class="img-hover-zoom">
        <img src="/publication/jeremie-22-fens/featured_hu8ce5cef2df994d7c5bc7a5734d5b5546_272223_8f27f39d06dff367d3e626a14f926154.webp" height="455" width="808"
            class="article-banner" alt="Ultra-rapid visual search in natural images using active deep learning" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/jeremie-22-fens/" >Ultra-rapid visual search in natural images using active deep learning</a>
  </div>

  
  <a href="/publication/jeremie-22-fens/"  class="summary-link">
    <div class="article-style">
      <p>👉🏼 Jean-Nicolas Jérémie @JnJerem 📍Poster session at #FENS2022 🎯 Ultra-rapid visual search in natural images using active deep learning.#FENSAmbassador @SocNeuro_Tweets @laurentperrinet 1/2 pic.twitter.com/ldGJMCQb4c
&mdash; Mélina Cordeau (@CordeauMelina) July 11, 2022 This work extends to natural scenes a previous work on visual search on a simplified task formulated in Emmanuel Daucé, Pierre Albigès, Laurent U Perrinet (2020).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/jeremie-22-fens/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/jeremie-22-fens/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/ladret-22-areadne/" >
      <div class="img-hover-zoom">
        <img src="/publication/ladret-22-areadne/featured_hu2d7300cbe84f6664449c518669cd7c38_181312_fd5b6d64f93f11bf24125c45c038b522.webp" height="455" width="808"
            class="article-banner" alt="A resilient neural code in V1 to process natural images" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-22-areadne/" >A resilient neural code in V1 to process natural images</a>
  </div>

  
  <a href="/publication/ladret-22-areadne/"  class="summary-link">
    <div class="article-style">
      <p>On a daily basis, the primary visual cortex (V1) detects oriented elements from sensory inputs made of orientation distributions. To remain selective to a large variety of possible input configurations, V1 has to account for the precision of these inputs. Here, we decode the population activity of V1 to uncover a neural code which achieves invariance to input precision. Extracellular recordings were made from 247 V1 neurons in anesthetized cats in response to visually presented naturalistic textures (a). These textures were generated from two parameters : orientation and orientation precision. Using multinomial logistic regression, we were able to recover these two parameters from the population activity.  We report two previously unknown types of neurons in V1 : predominantly infragranular neurons that encode solely orientation, and predominantly supragranular neurons which co-encode both orientation and its precision. Using a simple mean-rate population model, we observed that recurrent cortical inhibition can single-handedly account for the existence of these two types of neurons.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/ladret-22-areadne/ladret-22-areadne.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-22-areadne/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://areadne.org/" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/camille-besnainou/">Camille Besnainou</a></span>, <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-22-areadne/" >Decoding spiking motifs using neurons with heterogeneous delays</a>
  </div>

  
  <a href="/publication/grimaldi-22-areadne/"  class="summary-link">
    <div class="article-style">
      <p>The response of a biological neuron depends largely on the precise timing of presynaptic spikes that reach the basal dendritic tree. However, most neuronal models do not take advantage of this minute temporal dimension, especially in exploiting the variety of synaptic delays on the dendritic tree. A notable exception is the polychronization model, a recurrent model of spiking neurons including fixed and random heterogeneous delays and in which the weights are learned using Spike-Time Dependent Plasticity. The output raster plot displays repeated activations of prototypical spiking motifs called Polychronous Groups. Importantly, these motifs seem to be highly relevant in experimental neuroscience. Here, by extending the model of~[3], we develop a spiking neural network model for the efficient detection of PGs: By defining the generation of the raster plot as a probabilistic combination of PGs, we build and train the network in order to optimize the inversion of this generative model.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-22-areadne/grimaldi-22-areadne.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-22-areadne/cite.bib">
  Cite
</a>








  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2022-07-01_grimaldi-22-areadne/" target="_blank">
    Slides
  </a>
  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://areadne.org/" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/jeremie-22-areadne/" >
      <div class="img-hover-zoom">
        <img src="/publication/jeremie-22-areadne/featured_hu2b6eceb318b61346e0adae3da60992f3_303229_c47925667e65b0504f4555bdbb9be1ad.webp" height="455" width="808"
            class="article-banner" alt="Ultra-rapid visual search in natural images using active deep learning" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/jeremie-22-areadne/" >Ultra-rapid visual search in natural images using active deep learning</a>
  </div>

  
  <a href="/publication/jeremie-22-areadne/"  class="summary-link">
    <div class="article-style">
      <p>Visual search, that is, the simultaneous localization and detection of a visual target of interest, is a vital task. Applied to the case of natural scenes, searching for example to an animal (either a prey, a predator or a partner) constitutes a challenging problem due to large variability over numerous visual dimensions such as shape, pose, size, texture or position. Yet, biological visual systems are able to perform such detection efficiently in  briefly flashed scenes and in a very short amount of time.Deep convolutional neuronal networks (CNNs) were shown to be well fitted to the image classification task, providing with human (or even super-human) performance. Previous models also managed to solve the visual search task, by roughly dividing the image into sub-areas. This is at the cost, however, of computer-intensive parallel processing on relatively low-resolution image samples. Taking inspiration from natural vision systems, we develop here a model that builds over the anatomical visual processing pathways observed in mammals, namely the What and the Where pathways. It operates in two steps, one by selecting regions of interest, before knowing their actual visual content, through an ultra-fast/low resolution analysis of the full visual field, and the second providing a detailed categorization over the detailed foveal selected region attained with a saccade.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/jeremie-22-areadne/jeremie-22-areadne.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/jeremie-22-areadne/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://areadne.org/" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/ilias-rentzeperis/">Ilias Rentzeperis</a></span>, <span >
      <a href="/author/luca-calatroni/">Luca Calatroni</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/dario-prandi/">Dario Prandi</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/rentzeperis-22-areadne/" >Which sparsity problem does the brain solve?</a>
  </div>

  
  <a href="/publication/rentzeperis-22-areadne/"  class="summary-link">
    <div class="article-style">
      <p>Experimental evidence suggests that activity in sensory cortices is sparse in that only few neurons out of a large pool that could respond to sensed stimuli, are active at a time. Generative learning models that aim to replicate sensory systems could deviate from sparse activity patterns when representing noisy signals. We ask: are there biologically plausible implementations that will maintain sparse activations for different levels of noise while representing the underlying signal? A family of generative algorithms modelling sensory systems represent a stimulus as a linear sum of an overcomplete dictionary of vectors with their corresponding coefficients taking the role of activations. Olshausen and Field [1] showed that a learning algorithm that is set to reconstruct natural images with sparse activations develops vectors with properties, found in the receptive fields of neurons in V1, i.e. they are localized, band-pass, and oriented.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/rentzeperis-22-areadne/rentzeperis-22-areadne.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rentzeperis-22-areadne/cite.bib">
  Cite
</a>





  
    
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://areadne.org/" target="_blank" rel="noopener">
    URL</a>


  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/frederic-chavane/">Frederic Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/james-rankin/">James Rankin</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Brain Structure and Function</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/chavane-22/" >Revisiting Horizontal Connectivity Rules in V1: From like-to-like towards like-to-All</a>
  </div>

  
  <a href="/publication/chavane-22/"  class="summary-link">
    <div class="article-style">
      <p>Horizontal connections in the primary visual cortex of carnivores, ungulates and primates organize on a near-regular lattice. Given the similar length scale for the regularity found in cortical orientation maps, the currently accepted theoretical standpoint is that these maps are underpinned by a like-to-like connectivity rule: horizontal axons connect preferentially to neurons with similar preferred orientation. However, there is reason to doubt the rule&rsquo;s explanatory power, since a growing number of quantitative studies show that the like-to-like connectivity preference and bias mostly observed at short-range scale, are highly variable on a neuron-to-neuron level and depend on the origin of the presynaptic neuron. Despite the wide availability of published data, the accepted model of visual processing has never been revised. Here,~we review three lines of independent evidence supporting a much-needed revision of the like-to-like connectivity rule, ranging from anatomy to population functional measures, computational models and to theoretical approaches. We advocate an alternative, distance-dependent connectivity rule that is consistent with new structural and functional evidence: from like-to-like bias at short horizontal distance to like-to-all at long horizontal distance. This generic rule accounts for the observed high heterogeneity in interactions between the orientation and retinotopic domains, that we argue is necessary to process non-trivial stimuli in a task-dependent manner.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00429-022-02455-4" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/chavane-22/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00429-022-02455-4" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/frederic-v-barthelemy/">Frédéric V Barthélemy</a></span>, <span >
      <a href="/author/jerome-fleuriet/">Jérome Fleuriet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>eNeuro</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/barthelemy-22/" >A Behavioral Receptive Field for Ocular Following in Monkeys: Spatial Summation and Its Spatial Frequency Tuning</a>
  </div>

  
  <a href="/publication/barthelemy-22/"  class="summary-link">
    <div class="article-style">
      <p>In human and non-human primates, reflexive tracking eye movements can be initiated at very short latency in response to a rapid shift of the image. Previous studies in humans have shown that only a part of the central visual field is optimal for driving ocular following responses. Herein, we have investigated spatial summation of motion information across a wide range of spatial frequencies and speeds of drifting gratings by recording short-latency ocular following responses in macaque monkeys. We show that optimal stimulus size for driving ocular responses cover a small ($&lt;$20$,^∘$ diameter), central part of the visual field that shrinks with higher spatial frequency. This signature of linear motion integration remains invariant with speed and temporal frequency. For low and medium spatial frequencies, we found a strong suppressive influence from surround motion, evidenced by a decrease of response amplitude for stimulus sizes larger than optimal. Such suppression disappears with gratings at high frequencies. The contribution of peripheral motion was investigated by presenting grating annuli of increasing eccentricity. We observed an exponential decay of response amplitude with grating eccentricity, the decrease being faster for higher spatial frequencies. Weaker surround suppression can thus be explained by sparser eccentric inputs at high frequencies. A Difference-of-Gaussians model best renders the antagonistic contributions of peripheral and central motions. Its best-fit parameters coincide with several, well-known spatial properties of area MT neuronal populations. These results describe the mechanism by which central motion information is automatically integrated in a context-dependent manner to drive ocular responses.Significance statementOcular following is driven by visual motion at ultra-short latency in both humans and monkeys. Its dynamics reflect the properties of low-level motion integration. Here, we show that a strong center-surround suppression mechanism modulates initial eye velocity. Its spatial properties are dependent upon visual inputs&rsquo; spatial frequency but are insensitive to either its temporal frequency or speed. These properties are best described with a Difference-of-Gaussian model of spatial integration. The model parameters reflect many spatial characteristics of motion sensitive neuronal populations in monkey area MT. Our results further outline the computational properties of the behavioral receptive field underpinning automatic, context-dependent motion integration.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/hal-03741144" target="_blank" rel="noopener">
  Preprint
</a>




  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/barthelemy-22/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1523/ENEURO.0374-21.2022" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/ghassan-dabane/">Ghassan Dabane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IJCNN 2022 : International Joint Conference on Neural Networks</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/dabane-22/" >
      <div class="img-hover-zoom">
        <img src="/publication/dabane-22/featured_hu4493f00bae18993c8c782fca98f4c15f_232098_dedd638c1974235ff6a9cc8fada0344f.webp" height="455" width="808"
            class="article-banner" alt="What You See Is What You Transform: Foveated Spatial Transformers as a Bio-Inspired Attention Mechanism" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/dabane-22/" >What You See Is What You Transform: Foveated Spatial Transformers as a Bio-Inspired Attention Mechanism</a>
  </div>

  
  <a href="/publication/dabane-22/"  class="summary-link">
    <div class="article-style">
      <p>Convolutional Neural Networks have been considered the go-to option for object recognition in computer vision for the last couple of years. However, their invariance to object&rsquo;s translations is still deemed as a weak point and remains limited to small translations only via their max-pooling layers. One bio-inspired approach considers the What/Where pathway separation in Mammals to overcome this limitation. This approach works as a nature-inspired attention mechanism, another classical approach of which is Spatial Transformers. These allow an adaptive endto-end learning of different classes of spatial transformations throughout training. In this work, we overview Spatial Transformers as an attention-only mechanism and compare them with the What/Where model. We show that the use of attention restricted or ``Foveated&rsquo;&rsquo; Spatial Transformer Networks, coupled alongside a curriculum learning training scheme and an efficient log-polar visual space entry, provides better performance when compared to the What/Where model, all this without the need for any extra supervision whatsoever.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/dabane-22/dabane-22.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dabane-22/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.36227/techrxiv.16550391" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Champalimaud Research Symposium (CRS21)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-21-crs/" >Decoding orientation distributions from noisy observations in V1</a>
  </div>

  
  <a href="/publication/ladret-21-crs/"  class="summary-link">
    <div class="article-style">
      <p>This poster is presented in the following paper (published in Nature Comm Biology): Hugo Ladret, Nelson Cortes, Lamyae Ikan, Frederic Chavane, Christian Casanova, Laurent U Perrinet (2023). Cortical recurrence supports resilience to sensory variance in the primary visual cortex.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/ladret-21-crs/ladret-21-crs.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-21-crs/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Champalimaud Research Symposium (CRS21)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/jeremie-21-crs/" >
      <div class="img-hover-zoom">
        <img src="/publication/jeremie-21-crs/featured_huf5f6aceac04e4b928c84e0731223c841_1107029_b2820ccc39cae3b245b4a4ca4a294fcb.webp" height="455" width="808"
            class="article-banner" alt="Ultra-fast categorization of images containing animals in vivo and in computo" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/jeremie-21-crs/" >Ultra-fast categorization of images containing animals in vivo and in computo</a>
  </div>

  
  <a href="/publication/jeremie-21-crs/"  class="summary-link">
    <div class="article-style">
      <p>see a follow-up in: Jean-Nicolas Jérémie, Laurent U Perrinet (2023). Ultra-Fast Image Categorization in biology and in neural models. Vision. Preprint Cite DOI see an extension to visual search in: Jean-Nicolas Jérémie, Emmanuel Daucé, Laurent U Perrinet (2022).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/jeremie-21-crs/jeremie-21-crs.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/jeremie-21-crs/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/sio-hoi-ieng/">Sio-Hoi Ieng</a></span>, <span >
      <a href="/author/ryad-benosman/">Ryad Benosman</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Champalimaud Research Symposium (CRS21)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-21-crs/" >From event-based computations to a bio-plausible Spiking Neural Network</a>
  </div>

  
  <a href="/publication/grimaldi-21-crs/"  class="summary-link">
    <div class="article-style">
      <p>We propose a neuromimetic online classifier for always-on digit recognition. To achieve this, we extend an existing event-based algorithm which introduced novel spatio-temporal features: time surfaces. Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and create an efficient hierarchical event-based pattern recognition architecture. Its formalism was previously adapted in the computational neuroscience domain by showing it may be implemented using a Spiking Neural Network (SNN) of leaky integrate-and-fire models and Hebbian learning. Here, we add an online classification layer using a multinomial logistic regression which is compatible with a neural implementation. A decision can be taken at any arbitrary time by taking the argmax of the probability values associated to each class. We extend the parallel with computational neuroscience by demonstrating that this classification layer is also equivalent to a layer of spiking neurons with a Hebbian-like learning mechanism. Our method obtains state-of-the-art performances on the N-MNIST dataset and we show that it is robust to both spatial and temporal jitter. As a summary, we were able to develop a neuromimetic SNN model for online digit classification. We aim at pursuing the study of this architecture for natural scenes and hope to offer insights on the efficiency of neural computations, and in particular how mechanisms of decision-making may be formed.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-21-crs/grimaldi-21-crs.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-21-crs/cite.bib">
  Cite
</a>











  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=aIt5OAleMR8" target="_blank" rel="noopener">
  Video
</a>





  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/alberto-arturo-vergani/">Alberto Arturo Vergani</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Bernstein Conference 2021</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/vergani-21-bernstein/" >Simulating anticipatory activity in a 1D Spiking Neural Network Model</a>
  </div>

  
  <a href="/publication/vergani-21-bernstein/"  class="summary-link">
    <div class="article-style">
      <p> poster number: 94 scheduled on Wednesday, Sep 22, 18:00 CEST. https://abstracts.g-node.org/conference/BC21/abstracts#/uuid/05f81f30-d5d5-4467-b977-f28e9bed65f0 </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vergani-21-bernstein/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.12751/nncn.bc2021.p094" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/sio-hoi-ieng/">Sio-Hoi Ieng</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/ryad-benosman/">Ryad Benosman</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Content-Based Multimedia Indexing (CBMI) 2021</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-21-cbmi/" >A homeostatic gain control mechanism to improve event-driven object recognition</a>
  </div>

  
  <a href="/publication/grimaldi-21-cbmi/"  class="summary-link">
    <div class="article-style">
      <p>We propose a neuromimetic architecture able to perform pattern recognition. To achieve this, we extended the existing event-based algorithm from Lagorce et al (2017) which introduced novel spatio-temporal features: time surfaces. Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and create an efficient hierarchical event-based pattern recognition architecture. Inspired by biological findings and the efficient coding hypothesis, our main contribution is to integrate homeostatic regulation into the Hebbian learning rule. Indeed, in order to be optimally informative, average neural activity within a layer should be equally balanced across neurons. We used that principle to regularize neurons within the same layer by setting a gain depending on their past activity and such that they emit spikes with balanced firing rates. The efficiency of this technique was first demonstrated through a robust improvement in spatio-temporal patterns which were learnt during the training phase. In order to compare with state-of-the-art methods, we replicated past results on the same dataset as Lagorce et al (2017) and extended results in this study to the widely used N-MNIST dataset.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/hal-03336554" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-21-cbmi/grimaldi-21-cbmi.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-21-cbmi/cite.bib">
  Cite
</a>











  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=KxX4pZKexCo&amp;t=3335s" target="_blank" rel="noopener">
  Video
</a>



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/CBMI50038.2021.9461901" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/sio-hoi-ieng/">Sio-Hoi Ieng</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/ryad-benosman/">Ryad Benosman</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Computational and Systems Neuroscience (Cosyne) 2021</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/grimaldi-21-cosyne/" >A robust bio-inspired approach to event-driven object recognition</a>
  </div>

  
  <a href="/publication/grimaldi-21-cosyne/"  class="summary-link">
    <div class="article-style">
      <p>We propose a neuromimetic  architecture able to perform online pattern recognition. To achieve this, we extended the existing event-based algorithm from Lagorce et al (2017) which introduced novel spatio-temporal features: time-surfaces. Built from asynchronous events acquired by a neuromorphic camera, these time surfaces allow to code the local dynamics of a visual scene and to create an efficient hierarchical event-based pattern recognition architecture. Inspired by biological findings and the efficient coding hypothesis, our main contribution is to integrate homeostatic regulation to the Hebbian learning rule. Indeed, in order to be optimally informative, average neural activity within a layer should be equally balanced across neurons. We used that principle to regularize neurons within the same layer by setting a gain depending on their past activity and such that they emit spikes with balanced firing rates. The efficiency of this technique was first demonstrated through a robust improvement in spatio-temporal patterns which were learned during the training phase. We validated classification performance with the widely used N-MNIST dataset reaching 87.3 percent accuracy with homeostasis compared to 72.5 percent accuracy without homeostasis. Finally, by studying the impact of input jitter on classification highlights resilience of this method. We expect to extend this fully event-driven approach to more naturalistic tasks, notably for ultra-fast object categorization.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-21-cosyne/grimaldi-21-cosyne.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-21-cosyne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>PLoS Computational Biology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/" >
      <div class="img-hover-zoom">
        <img src="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/featured_hu574754a65aa93c4d768eb15e6a7a5fb0_205921_c6c35d91dd94fb243fe28dd73fe1c17f.webp" height="455" width="808"
            class="article-banner" alt="Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/" >Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</a>
  </div>

  
  <a href="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/"  class="summary-link">
    <div class="article-style">
      <p>Both neurophysiological and psychophysical experiments have pointed out the crucial role of recurrent and feedback connections to process context-dependent information in the early visual cortex. While numerous models have accounted for feedback effects at either neural or representational level, none of them were able to bind those two levels of analysis. Is it possible to describe feedback effects at both levels using the same model? We answer this question by combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical and convolutional framework. In this Sparse Deep Predictive Coding (SDPC) model, the SC component models the internal recurrent processing within each layer, and the PC component describes the interactions between layers using feedforward and feedback connections. Here, we train a 2-layered SDPC on two different databases of images, and we interpret it as a model of the early visual system (V1~&amp;~V2). We first demonstrate that once the training has converged, SDPC exhibits oriented and localized receptive fields in V1 and more complex features in V2. Second, we analyze the effects of feedback on the neural organization beyond the classical receptive field of V1 neurons using interaction maps. These maps are similar to association fields and reflect the Gestalt principle of good continuation. We demonstrate that feedback signals reorganize interaction maps and modulate neural activity to promote contour integration. Third, we demonstrate at the representational level that the SDPC feedback connections are able to overcome noise in input images. Therefore, the SDPC captures the association field principle at the neural level which results in better disambiguation of blurred images at the representational level.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1902.07651" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/boutin-franciosini-chavane-ruffier-perrinet-20.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/VictorBoutin/InteractionMap" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1008629" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>The Conversation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-21-hasard/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-21-hasard/featured_huce1b6086cb367af0158844edbf0be9bf_1140496_3a38750ab0c2bc394ef43357b8b9cc73.webp" height="455" width="808"
            class="article-banner" alt="Le jeu du cerveau et du hasard" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-21-hasard/" >Le jeu du cerveau et du hasard</a>
  </div>

  
  <a href="/publication/perrinet-21-hasard/"  class="summary-link">
    <div class="article-style">
      <p>Dans la pièce de théâtre la plus célèbre de Marivaux Le jeu de l&rsquo;amour et du hasard, l&rsquo;auteur joue à inverser le rôle des personnages, et le hasard est invité à guider leurs destins. De la même façon, notre cerveau est ballotté au gré du hasard, aussi bien dans une loterie que dans les incertitudes et ambiguı̈tés révélées dans la vision par les illusions d&rsquo;optique. Au point que l&rsquo;on peut attribuer à un esprit malin le fait que la tartine tombe du côté de la confiture, ou que la fiche du câble USB soit toujours dans le mauvais sens. Le hasard s&rsquo;invite comme un personnage à part entière dans la cognition, et on peut s&rsquo;interroger du rôle que celui-ci peut jouer dans le fonctionnement de notre cerveau.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-21-hasard/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-21-hasard/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  











  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/nelson-cortes/">Nelson Cortes</a></span>, <span >
      <a href="/author/lamyae-ikan/">Lamyae Ikan</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/christian-casanova/">Christian Casanova</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-21-sfn/" >Modulation of orientation selectivity by orientation precision</a>
  </div>

  
  <a href="/publication/ladret-21-sfn/"  class="summary-link">
    <div class="article-style">
      <p>The primary visual cortex (V1) processes complex mixtures of orientations to build neural representations of our everyday visual environment. It remains unclear how V1 adapts to the highly volatile distributions of orientations found in natural images. We used naturalistic stimuli and measured the response of V1 neurons to orientation distributions of varying bandwidth. Although broad distributions decreased single neuron tuning, a neurally plausible decoder could robustly retrieve the orientations of stimuli from the population activity at all bandwidths. This decoder demonstrates that V1 population co-encodes orientation and its precision, enhancing population decoding performances compared to sole orientation decoding. This internal representation is mediated by temporally distinct neural dynamics and supports a precision-weighted description of neuronal message passing in the visual cortex, in line with predictive processing theories.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.abstractsonline.com/pp8/#!/10485/presentation/22078" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-21-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IWAI 2020</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/dauce-20-iwai/" >Visual search as active inference</a>
  </div>

  
  <a href="/publication/dauce-20-iwai/"  class="summary-link">
    <div class="article-style">
      <p>a follow-up of: Emmanuel Daucé, Pierre Albigès, Laurent U Perrinet (2020). A dual foveal-peripheral visual processing model implements efficient saccade selection. Journal of Vision. Preprint PDF Cite Code DOI the mathematical details are described as a talk the 1st International WS on #ActiveInference #IWAI2020 at @ECMLPKDD https://t.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/dauce-20-iwai/dauce-20-iwai.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dauce-20-iwai/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/2020-09-14_IWAI/" target="_blank" rel="noopener">
  Code
</a>








  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/2020-09-14_IWAI" target="_blank" rel="noopener">
  Slides
</a>





<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-030-64919-7_17" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Computational and Systems Neuroscience (Cosyne) 2020</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/franciosini-20-cosyne/" >Modelling Complex-cells and topological structure in the visual cortex of mammals using Sparse Predictive Coding</a>
  </div>

  
  <a href="/publication/franciosini-20-cosyne/"  class="summary-link">
    <div class="article-style">
      <p>Cells in the primary visual cortex of mammals (V1) have historically been divided into two classes: simple and complex. Simple cells exhibit a rectified linear response to oriented visual stimuli while complex cells show various degrees of invariance with respect to the stimulus&rsquo; phase (position). The existence of these two populations can be explained by hierarchical models where simple cells feed information into complex cells through a non-linear spatial pooling [1]. Nevertheless, how the brain develops this structure remains an open question. One of the most successful theories to model hierarchical processing in the brain is Predictive Coding (PC): a framework introduced by Rao &amp; Ballard [2] that exploits feedback and feedforward connectivity to solve a Bayesian inference problem. We extended the classical PC to account for a sparse representation of the input data (natural images) and a convolutional structure to allow translation invariance. We demonstrate that this framework, called Sparse Deep Predictive Coding (SDPC) [3], can easily replicate complex-like neurons when a non-linear pooling is included between the layers. In particular, we show that a large population of complex-like neurons, showing various degrees of phase invariance, emerges in the 2nd layer of the model when the pooling function is extended to include not only neighboring spatial locations but also neighboring neurons with different tuning properties. We trained various networks on natural images (STL-10 data-set). To quantify the complex behavior of the model neurons, we used the modulation ratio F1/F0[4]: if F1/F0 ≥ 1 the cell is identified as simple-like, if F1/F0&lt;1 the cell is complex-like. In all the tested settings the 1st layer of the network exhibits simple-like neurons, while the 2nd layer always presents a fraction of complex-like cells. We observed the emergence of different behaviors by introducing different types on non-linear pooling. A striking emergent property of our model is that this non-linearities induce a topographical structure on the neurons of the network. This organization shows qualitatively strong similarities with that found in V1.  Importantly, this organization is solely a consequence of the feedback connection coming from the 2nd layer: enforcing the pooling across neighboring neurons constrains neighboring simple-cells to encode for similar features. In particular, edge-like filters with similar orientations and frequencies, but different phases, tend to be grouped together. We were able to reproduce similar effects with two different types of pooling (l2-pooling, max-pooling) and different network sizes, obtaining different degrees of topographical organization and different ratios of complex-like cells. A possible explanation of this phenomenon lies in the computational bottleneck caused by a reduction in the network layers&rsquo; size. This is also in line with previous studies that showed how cortical networks with the same functional characterization can explain different behavior with different structural parameters [5]. The novelty of this model lies in its ability to highlight the link between structure and function in a neural network. This study addresses a long-debated question on the function and role of the diversity of topographical structure in the visual cortex of mammals across species. References: <em>[1]</em>  Ko Sakai and Shigeru Tanaka. <em>Spatial pooling in the second-order spatial structure of cortical complex cells</em>. In: Vision Research 40.7 (2000), pp. 855&ndash;871. <em>[2]</em>  Rajesh PN Rao and Dana H Ballard. <em>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</em>. In: Nature neuroscience2.1 (1999), p. 79. <em>[3]</em>  Victor Boutin et al. <em>Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</em>. In: arXiv preprint arXiv:1902.07651 (2019). <em>[4]</em>  Bernt C Skottun et al. <em>Classifying simple and complex cells on the basis of response modulation</em>. In: Vision research 31.7-8 (1991), pp. 1078&ndash;1086. <em>[5]</em> Jaeson Jang, Min Song, and Se-Bum Paik. <em>Classification of columnar and salt-and-pepper organization in mammalian visual cortex</em>. In: bioRxiv (2019), p. 698043</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/franciosini-20-cosyne/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/franciosini-20-cosyne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/pierre-albiges/">Pierre Albigès</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/dauce-20/" >A dual foveal-peripheral visual processing model implements efficient saccade selection</a>
  </div>

  
  <a href="/publication/dauce-20/"  class="summary-link">
    <div class="article-style">
      <p>In computer vision, the visual search task consists in extracting a scarce and specific visual information (the target) from a large and crowded visual display. This task is usually implemented by scanning the different possible target identities at all possible spatial positions, hence with strong computational load. The human visual system employs a different strategy, combining a foveated sensor with the capacity to rapidly move the center of fixation using saccades. Saccade-based visual exploration can be idealized as an inference process, assuming that the target position and category are independently drawn from a common generative process. Knowing that process, visual processing is then separated in two specialized pathways, the where pathway mainly conveying information about target position in peripheral space, and the what pathway mainly conveying information about the category of the target. We consider here a dual neural network architecture learning independently where to look and then at what to see. This allows in particular to infer target position in retinotopic coordinates, independently to its category. This framework was tested on a simple task of finding digits in a large, cluttered image. Simulation results demonstrate the benefit of specifically learning where to look before actually knowing the target category. The approach is also energy-efficient as it includes the strong compression rate performed at the sensor level, by retina and V1 encoding, which is preserved up to the action selection level, highlighting the advantages of bio-mimetic strategies with regards to traditional computer vision when computing resources are at stake.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/725879" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/dauce-20/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dauce-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/WhereIsMyMNIST" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/jov.20.8.22" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>SIGMA'2020 (Signal, Image, Geometry, Modelling, Approximation)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/franciosini-20-sigma/" >Modelling Complex-cells and topological structure in the visual cortex of mammals using Sparse Predictive Coding</a>
  </div>

  
  <a href="/publication/franciosini-20-sigma/"  class="summary-link">
    <div class="article-style">
      <p>Cells in the primary visual cortex of mammals (V1) have historically been divided into two classes: simple and complex. Simple cells exhibit a rectified linear response to oriented visual stimuli while complex cells show various degrees of invariance with respect to the stimulus&rsquo; phase (position). The existence of these two populations can be explained by hierarchical models where simple cells feed information into complex cells through a non-linear spatial pooling [1]. Nevertheless, how the brain develops this structure remains an open question. One of the most successful theories to model hierarchical processing in the brain is Predictive Coding (PC): a framework introduced by Rao &amp; Ballard [2] that exploits feedback and feedforward connectivity to solve a Bayesian inference problem. We extended the classical PC to account for a sparse representation of the input data (natural images) and a convolutional structure to allow translation invariance. We demonstrate that this framework, called Sparse Deep Predictive Coding (SDPC) [3], can easily replicate complex-like neurons when a non-linear pooling is included between the layers. In particular, we show that a large population of complex-like neurons, showing various degrees of phase invariance, emerges in the 2nd layer of the model when the pooling function is extended to include not only neighboring spatial locations but also neighboring neurons with different tuning properties. We trained various networks on natural images (STL-10 data-set). To quantify the complex behavior of the model neurons, we used the modulation ratio F1/F0[4]: if F1/F0 ≥ 1 the cell is identified as simple-like, if F1/F0&lt;1 the cell is complex-like. In all the tested settings the 1st layer of the network exhibits simple-like neurons, while the 2nd layer always presents a fraction of complex-like cells. We observed the emergence of different behaviors by introducing different types on non-linear pooling. A striking emergent property of our model is that this non-linearities induce a topographical structure on the neurons of the network. This organization shows qualitatively strong similarities with that found in V1.  Importantly, this organization is solely a consequence of the feedback connection coming from the 2nd layer: enforcing the pooling across neighboring neurons constrains neighboring simple-cells to encode for similar features. In particular, edge-like filters with similar orientations and frequencies, but different phases, tend to be grouped together. We were able to reproduce similar effects with two different types of pooling (l2-pooling, max-pooling) and different network sizes, obtaining different degrees of topographical organization and different ratios of complex-like cells. A possible explanation of this phenomenon lies in the computational bottleneck caused by a reduction in the network layers&rsquo; size. This is also in line with previous studies that showed how cortical networks with the same functional characterization can explain different behavior with different structural parameters [5]. The novelty of this model lies in its ability to highlight the link between structure and function in a neural network. This study addresses a long-debated question on the function and role of the diversity of topographical structure in the visual cortex of mammals across species. References: <em>[1]</em>  Ko Sakai and Shigeru Tanaka. <em>Spatial pooling in the second-order spatial structure of cortical complex cells</em>. In: Vision Research 40.7 (2000), pp. 855&ndash;871. <em>[2]</em>  Rajesh PN Rao and Dana H Ballard. <em>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</em>. In: Nature neuroscience2.1 (1999), p. 79. <em>[3]</em>  Victor Boutin et al. <em>Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</em>. In: arXiv preprint arXiv:1902.07651 (2019). <em>[4]</em>  Bernt C Skottun et al. <em>Classifying simple and complex cells on the basis of response modulation</em>. In: Vision research 31.7-8 (1991), pp. 1078&ndash;1086. <em>[5]</em> Jaeson Jang, Min Song, and Se-Bum Paik. <em>Classification of columnar and salt-and-pepper organization in mammalian visual cortex</em>. In: bioRxiv (2019), p. 698043</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://conferences.cirm-math.fr/2152.html" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/franciosini-20-sigma/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neural Computation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/" >
      <div class="img-hover-zoom">
        <img src="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/featured_hufa81b0053ae9bd90fdb48530672a0347_810086_29165bf7f0f890afd58fd12d2aea84c7.webp" height="455" width="808"
            class="article-banner" alt="Effect of top-down connections in Hierarchical Sparse Coding" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/" >Effect of top-down connections in Hierarchical Sparse Coding</a>
  </div>

  
  <a href="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/"  class="summary-link">
    <div class="article-style">
      <p>Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the 2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks are trained on 4 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we show that the 2L-SPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the 2L-SPC features are more generic and informative.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2002.00892" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/boutin-franciosini-ruffier-perrinet-20-feedback.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1162/neco_a_01325" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/chloe-pasturel/">Chloé Pasturel</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>PLoS Computational Biology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/pasturel-montagnini-perrinet-20/" >
      <div class="img-hover-zoom">
        <img src="/publication/pasturel-montagnini-perrinet-20/featured_huf01a0a99995bf40c2e40e412d13550ad_125526_c14131647fa03c8d183be122e75b8c91.webp" height="455" width="808"
            class="article-banner" alt="Humans adapt their anticipatory eye movements to the volatility of visual motion properties" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/pasturel-montagnini-perrinet-20/" >Humans adapt their anticipatory eye movements to the volatility of visual motion properties</a>
  </div>

  
  <a href="/publication/pasturel-montagnini-perrinet-20/"  class="summary-link">
    <div class="article-style">
      <p>Humans are able to accurately track a moving object with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling high<em>resolution visual analysis. When predictive information is available about target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. It is generally assumed that the role of anticipatory eye movements is to limit the behavioral impairment due to eye</em>to<em>target position and velocity mismatch. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM, as measured during a fixed duration gap before target ramp</em>motion onset. This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. However, such estimate may become particularly challenging in a dynamic context, where the probabilistic contingencies vary in time in an unpredictable way. In addition, whether and how the information processing underlying the buildup of aSPEM is linked to an explicit estimate of probabilities is unknown. We developed a new paired* task paradigm in order to address these two questions. In a first session, participants observe a target moving horizontally with constant speed from the center either to the right or left across trials. The probability of either motion direction changes randomly in time. Participants are asked to estimate &quot;how much they are confident that the target will move to the right or left in the next trial&quot; and to adjust the cursor&rsquo;s position on the screen accordingly. In a second session the participants eye movements are recorded during the observation of the same sequence of random<em>direction trials. In parallel, we are developing new automatic routines for the advanced analysis of oculomotor traces. In order to extract the relevant parameters of the oculomotor responses (latency, gain, initial acceleration, catch</em>up saccades), we developed new tools based on best*fitting procedure of predefined patterns (i.e. the typical smooth pursuit velocity profile).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/784116" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/hal-02394142" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/pasturel-montagnini-perrinet-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1007438" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/sandrine-chemla/">Sandrine Chemla</a></span>, <span >
      <a href="/author/arjan-boonman/">Arjan Boonman</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/frederic-chavane/">Frederic Chavane</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>bioRxiv</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/benvenuti-22/" >Anticipatory Responses along Motion Trajectories in Awake Monkey Area V1</a>
  </div>

  
  <a href="/publication/benvenuti-22/"  class="summary-link">
    <div class="article-style">
      <p>What are the neural mechanisms underlying motion integration of translating objects? Visual motion integration is generally conceived of as a feedforward, hierarchical, information processing. However, feedforward models fail to account for many contextual effects revealed using natural moving stimuli. In particular, a translating object evokes a sequence of transient feedforward responses in the primary visual cortex but also propagations of activity through horizontal and feedback pathways. We investigated how these pathways shape the representation of a translating bar in monkey V1. We show that, for long trajectories, spiking activity builds-up hundreds of milliseconds before the bar enters the neurons receptive fields. Using VSDI and LFP recordings guided by a phenomenological model of propagation dynamics, we demonstrate that this anticipatory response arises from the interplay between horizontal and feedback networks driving V1 neurons well ahead of their feedforward inputs. This mechanism could subtend several perceptual contextual effects observed with translating objects.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/2020.03.26.010017" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/benvenuti-22/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10/ggqj77" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>The Philosophy and Science of Predictive Processing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-20/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-20/featured_hu866bb364fa3f7e57c931c8b0a8acfb69_228114_97eee2ff78e3b0d456bfa192fbaa5b20.webp" height="455" width="808"
            class="article-banner" alt="From the retina to action: Dynamics of predictive processing in the visual system" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-20/" >From the retina to action: Dynamics of predictive processing in the visual system</a>
  </div>

  
  <a href="/publication/perrinet-20/"  class="summary-link">
    <div class="article-style">
      <p>Within the central nervous system, visual areas are essential in transforming the raw luminous signal into a representation which efficiently conveys information about the environment. This process is constrained by the necessity of being robust and rapid. Indeed, there exists both a wide variety of potential changes in the geometrical characteristics of the visual scene and also a necessity to be able to respond as quickly as possible to the incoming sensory stream, for instance to drive a movement of the eyes to the location of a potential danger. Decades of study in neurophysiology and psychophysics at the different levels of vision have shown that this system takes advantage of a priori knowledge about the structure of visual information, such as the regularity in the shape and motion of visual objects. As such, the predictive processing framework offers a unified theory to explain a variety of visual mechanisms. However, we still lack a global normative approach unifying those mechanisms and we will review here some recent and promising approaches. First, we will describe Active Inference, a form of predictive processing equipped with the ability to actively sample the visual space. Then, we will extend this paradigm to the case where information is distributed on a topography, such as is the case for retinotopically organized visual areas. In particular, we will compare such models in light of recent neurophysiological data showing the role of traveling waves in shaping visual processing. Finally, we will propose some lines of research to understand how these functional models may be implemented at the neural level. In particular, we will review potential models of cortical processing in terms of prototypical micro-circuits. These allow to separate the different flows of information, from feed-forward prediction error to feed-back anticipation error. Still, the design of such a generic predictive processing circuit is still not fully understood and we will enumerate some possible implementations using biomimetic neural networks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/Perrinet20PredictiveProcessing_manubot/v/latest/index.html" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-20/perrinet-20.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/Perrinet19PredictiveProcessing" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.5040/9781350099784.ch-005" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Annals of Eye Science</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-20-aes/" >Learning dynamics in a neural network model of the primary visual cortex</a>
  </div>

  
  <a href="/publication/ladret-20-aes/"  class="summary-link">
    <div class="article-style">
      <p><em>Background</em>: The primary visual cortex (V1) is a key component of the visual system that builds some of the first levels of coherent visual representations from sparse visual inputs. While the study of its dynamics has been the focus of many computational models for the past years, there is still relatively few research works that put an emphasis on both synaptic plasticity in V1 and biorealism in the context of learning visual inputs. Here, we present a recurrent spiking neural network that is capable of spike timing dependent plasticity (STDP) and we demonstrate its capacity to discriminate spatio-temporal orientation patterns in noisy natural images.  <em>Methods</em>: A two stage model was developed. First, natural images flux (be it videos/gratings/camera) were converted into spikes, using a difference of gaussians (DOG) approach. This transformation approximates the retina-lateral geniculate nucleus (LGN) organization. Secondly, a spiking neural network was build using PyNN simulator, mimicking cortical neurons dynamics and plasticity, as well as V1 topology. This network was then fed with spikes generated by the first model and its ability to build visual representations was assessed using control gratings inputs.  <em>Results</em>: The neural network exhibited several interesting properties. After a short period of learning, it was capable of learning multiples orientations and reducing noise in such learned feature, compared to the inputs. These learned features were stable even after increasing the noise in inputs and were found to not only encoding the spatial properties of the input, but also its temporal aspects (i.e., the time of each grating presentation  <em>Conclusions</em>: Our work shows that topological structuring of the cortical neural networks, combined with simple plasticity rules, are sufficient to drive strong learning dynamics of natural images properties. This computational model fits many properties found in the literature and provides some theoritical explanations for the shape of tuning curve of certain layers of V1. Further investigations are now conducted to validate its properties against the neuronal responses of rodents, using identical visual stimuli.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://aes.amegroups.com/article/view/5214" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-20-aes/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of NCCD, Capbreton</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-19-nccd/" >A dynamic model for decoding direction and orientation in macaque primary visual cortex</a>
  </div>

  
  <a href="/publication/perrinet-19-nccd/"  class="summary-link">
    <div class="article-style">
      <p>When objects are in motion, the local orientation of their contours and the direction of motion are two essential components of visual information which are processed in parallel in the early visual areas. Generally, to probe a neuron&rsquo;s response property to moving stimuli, bars or gratings are drifted across neuron&rsquo;s receptive field at various angles. The resulting tuning curve will reflect the &quot;confound&quot; selectivity to both the orientation and direction of motion orthogonal to the orientation. Focusing on the primary visual cortex of the macaque monkey (V1), we challenged different models for the joint representation of orientation and direction within the neural activity. Precisely, we considered the response of V1 neurons to an oriented moving bar to investigate whether, and how, the information about the bar&rsquo;s orientation and direction could be encoded dynamically at the population activity level. For that purpose, we used a decoding approach based on a space-time receptive field model that encodes jointly orientation and direction. Then, using this model and a maximum likelihood paradigm, we inferred the most likely representation for a given network activity [1, 2]. We tested this model on surrogate data and on extracellular recordings in area V1 of awake macaque monkeys in response to oriented bars moving in 12 different directions. Using a cross-validation method we could robustly decode both the orientation and the direction of the bar within the classical receptive field (cRF). Furthermore, this decoding approach shows different properties: First, information about the orientation and direction of the bar is emerging before entering the cRF. Second, when testing different orientations with the same direction, our approach unravels that we can ``unconfound&rsquo;&rsquo; the information about direction and orientation by decoding them independently. Finally, our results demonstrate that the orientation and the direction of motion of an ambiguous moving bar can be progressively decoded in V1. This is a signature of a dynamic solution to the aperture problem in area V1, similarly to what was already found in area MT [3].  [1] M. Jazayeri and J.A. Movshon. Optimal representation of sensory information by neural populations. Nature Neuroscience, 9(5):690&ndash;696, 2006. [2] W. Taouali, G. Benvenuti, P. Wallisch, F. Chavane, L. Perrinet. Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts. Journal of Neurophysiology, 2015. [3] C. Pack, R. Born. Temporal dynamics of a neural solution to the aperture problem in visual area MT of macaque brain. Nature, 409(6823), 1040&ndash;1042. 2001.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-nccd/perrinet-19-nccd.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-19-nccd/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/sandrine-chemla/">Sandrine Chemla</a></span>, <span >
      <a href="/author/alexandre-reynaud/">Alexandre Reynaud</a></span>, <span >
      <a href="/author/matteo-divolo/">Matteo diVolo</a></span>, <span >
      <a href="/author/yann-zerlaut/">Yann Zerlaut</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/alain-destexhe/">Alain Destexhe</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Neuroscience</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/chemla-19/" >
      <div class="img-hover-zoom">
        <img src="/publication/chemla-19/featured_hufb805262c50656667e99df7e657cf7df_702482_5565e5b4dcf4e9a678422bae439482d5.webp" height="455" width="808"
            class="article-banner" alt="Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey V1" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/chemla-19/" >Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey V1</a>
  </div>

  
  <a href="/publication/chemla-19/"  class="summary-link">
    <div class="article-style">
      <p>Traveling waves have recently been observed in different animal species, brain areas and behavioral states. However, it is still unclear what are their functional roles. In the case of cortical visual processing, waves propagate across retinotopic maps and can hereby generate interactions between spatially and temporally separated instances of feedforward driven activity. Such interactions could participate in processing long-range apparent motion stimuli, an illusion for which no clear neuronal mechanisms have yet been proposed. Using this paradigm in awake monkeys, we show that suppressive traveling waves produce to a spatio-temporal normalization of apparent motion stimuli. Our study suggests that cortical waves shape the representation of illusory moving stimulus within retinotopic maps for an straightforward read-out by downstream areas.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.science/hal-02190752" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.jneurosci.org/content/39/22/4282" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/chemla-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1523/JNEUROSCI.2792-18.2019" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>SIGMA'2020 (Signal, Image, Geometry, Modelling, Approximation)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-20-sigma/" >Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</a>
  </div>

  
  <a href="/publication/boutin-20-sigma/"  class="summary-link">
    <div class="article-style">
      <p>Both neurophysiological and psychophysical experiments have pointed out the crucial role of recurrent and feedback connections to process context-dependent information in the early visual cortex. While numerous models have accounted for feedback effects at either neural or representational level, none of them were able to bind those two levels of analysis. Is it possible to describe feedback effects at both levels using the same model? We answer this question by combining Predictive Coding (PC) and Sparse Coding (SC) into a hierarchical and convolutional framework. In this Sparse Deep Predictive Coding (SDPC) model, the SC component models the internal recurrent processing within each layer, and the PC component describes the interactions between layers using feedforward and feedback connections. Here, we train a 2-layered SDPC on two different databases of images, and we interpret it as a model of the early visual system (V1~&amp;~V2). We first demonstrate that once the training has converged, SDPC exhibits oriented and localized receptive fields in V1 and more complex features in V2. Second, we analyze the effects of feedback on the neural organization beyond the classical receptive field of V1 neurons using interaction maps. These maps are similar to association fields and reflect the Gestalt principle of good continuation. We demonstrate that feedback signals reorganize interaction maps and modulate neural activity to promote contour integration. Third, we demonstrate at the representational level that the SDPC feedback connections are able to overcome noise in input images. Therefore, the SDPC captures the association field principle at the neural level which results in better disambiguation of blurred images at the representational level.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1902.07651" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://conferences.cirm-math.fr/2152.html" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-20-sigma/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/cesar-u-ravello/">Cesar U Ravello</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/maria-jose-escobar/">Maria Jose Escobar</a></span>, <span >
      <a href="/author/adrian-g-palacios/">Adrián G Palacios</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Scientific Reports</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/ravello-19/" >
      <div class="img-hover-zoom">
        <img src="/publication/ravello-19/featured_huab0a6242a9a1d9a67cb873158befa6d8_178263_72b6624f6fef3554944d593ac2d05dc9.webp" height="455" width="808"
            class="article-banner" alt="Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ravello-19/" >Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli</a>
  </div>

  
  <a href="/publication/ravello-19/"  class="summary-link">
    <div class="article-style">
      <p>Motion detection represents one of the critical tasks of the visual system and has motivated a large body of research. However, it remains unclear precisely why the response of retinal ganglion cells (RGCs) to simple artificial stimuli does not predict their response to complex, naturalistic stimuli. To explore this topic, we use Motion Clouds (MC), which are synthetic textures that preserve properties of natural images and are merely parameterized, in particular by modulating the spatiotemporal spectrum complexity of the stimulus by adjusting the frequency bandwidths. By stimulating the retina of the diurnal rodent, Octodon degus with MC we show that the RGCs respond to increasingly complex stimuli by narrowing their adjustment curves in response to movement. At the level of the population, complex stimuli produce a sparser code while preserving movement information; therefore, the stimuli are encoded more efficiently. Interestingly, these properties were observed throughout different populations of RGCs. Thus, our results reveal that the response at the level of RGCs is modulated by the naturalness of the stimulus -in particular for motion- which suggests that the tuning to the statistics of natural images already emerges at the level of the retina.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-02007905" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038%2Fs41598-018-36861-8" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ravello-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/s41598-018-36861-8" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>NeuroFrance 2019, International Conference from the Société des Neurosciences, Marseille, France</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/franciosini-perrinet-19-neurofrance/" >A hierarchical, multi-layer convolutional sparse coding algorithm based on predictive coding</a>
  </div>

  
  <a href="/publication/franciosini-perrinet-19-neurofrance/"  class="summary-link">
    <div class="article-style">
      <p>Sparse coding holds the idea that signals can be concisely described as a linear mixture of few components (called atoms) picked from a bigger set of primary kernels (called dictionary). This framework has long been used to model the strategy employed by mammals´ primary visual cortex (V1) to detect low-level features, in particular, oriented edges in natural scenes. Differently, predictive coding is a prominent tool used to model hierarchical neural dynamics: high-level cortical layers predict at best the activity of lower-level ones and this prediction is sent back through of a feedback connection between the layers. This defines a recursive loop in which prediction error is integrated to the sensory input and fed forward to refine the quality of the prediction. We propose a Sparse Deep Predictive Coding algorithm (SDPC) that exploits convolutional dictionaries and a feedback information flow for meaningful, hierarchical feature learning in static images. The proposed architecture allows us to add arbitrary non-linear spatial transformation stages between each layer of the hierarchical sparse representations, such as Max-Pooling or Spatial Transformer layers. SPDC consists of a dynamical system in the form of a convolutional neural network, analogous to the model proposed by Rao and Ballard, 1999. The state variables are sparse feature maps encoding the input and the feedback signals while the parameters of the system are convolutional dictionaries optimized through Hebbian learning. We observed that varying the strength of the feedback modulates the overall sparsity of low-level representations (lower feedback scales correspond to a less sparse activity), but without changing the exponential shape of the distribution of the sparse prior. This model could shed light on the role of sparsity and feedback modulation in hierarchical feature learning with important applications in signal processing (data compression), computer vision (by extending it to dynamic scenes) and computational neuroscience, notably by using more complex priors like group sparsity to model topological organization in the brain cortex.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/franciosini-perrinet-19-neurofrance/franciosini-perrinet-19-neurofrance.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/franciosini-perrinet-19-neurofrance/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-19-hulk/" >An adaptive homeostatic algorithm for the unsupervised learning of visual features</a>
  </div>

  
  <a href="/publication/perrinet-19-hulk/"  class="summary-link">
    <div class="article-style">
      <p>The formation of structure in the visual system, that is, of the connections between cells within neural populations, is by large an unsupervised learning process: the emergence of this architecture is mostly self-organized. In the primary visual cortex of mammals, for example, one can observe during development the formation of cells selective to localized, oriented features which results in the development of a representation of contours in area V1. We modeled such a process using sparse Hebbian learning algorithms. These algorithms alternate a coding step to encode the information with a learning step to find the proper encoder. We identified here a major difficulty of classical solutions in their ability to deduce a good representation while knowing immature encoders, and to learn good encoders with a non-optimal representation. To solve this problem, we propose to introduce a new regulation process between learning and coding, called homeostasis. It is compatible with a neuromimetic architecture and allows for a more efficient emergence of localized filters sensitive to orientation. The key to this algorithm lies in a simple adaptation mechanism based on non-linear functions that reconciles the antagonistic processes that occur at the coding and learning time scales. We tested this unsupervised algorithm with this homeostasis rule for a series of learning algorithms coupled with different neural coding algorithms. In addition, we propose a simplification of this optimal homeostasis rule by implementing a simple heuristic on the probability of activation of neurons. Compared to the optimal homeostasis rule, we show that this heuristic allows to implement a faster unsupervised learning algorithm while retaining much of its effectiveness. These results demonstrate the potential application of such a strategy in computer vision and machine learning and we illustrate it with a result in a convolutional neural network.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-hulk/" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-hulk/perrinet-19-hulk.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-19-hulk/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/SpikeAI/HULK" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3390/vision3030047" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>The Conversation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-19-illusions/" >Illusions et hallucinations visuelles : une porte sur la perception</a>
  </div>

  
  <a href="/publication/perrinet-19-illusions/"  class="summary-link">
    <div class="article-style">
      <p>Les illusions visuelles sont des créations d&rsquo;artistes, de scientifiques et plus récemment, grâce aux réseaux sociaux, du grand public qui proposent des situations souvent incongrues, dans lesquelles l&rsquo;eau remonte une cascade, les personnes volent dans les airs ou des serpents se mettent à tourner. Au-delà de leur indéniable coté ludique, ces illusions nous apprennent beaucoup sur le fonctionnement du cerveau, notamment quand celles-ci se transforment en hallucinations visuelles, dépassant ainsi les limites des capacités de notre perception. En tant que chercheur en Neurosciences à l&rsquo;Institut de Neurosciences de la Timone à Marseille, je vous dévoilerai des aspects du fonctionnement du cerveau qui sont souvent méconnus. En particulier, nous verrons pourquoi un magicien peut tromper nos sens ou comment des objets peuvent voyager dans le temps. Surtout nous essaierons de comprendre le fonctionnement de notre perception visuelle sur les bases d&rsquo;une théorie de la vision non pas comme une simple caméra qui enregistre des images mais comme un processus actif en relation avec le monde qui nous entoure.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://theconversation.com/illusions-et-hallucinations-visuelles-une-porte-sur-la-perception-117389" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-illusions/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-19-illusions/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  







  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=jJKTdlChefc" target="_blank" rel="noopener">
  Video
</a>





  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Annual Computational Neuroscience Meeting: CNS</em>2019, Barcelona*
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/franciosini-perrinet-19-cns/" >Modelling Complex Cells of Early Visual Cortex using Predictive Coding</a>
  </div>

  
  <a href="/publication/franciosini-perrinet-19-cns/"  class="summary-link">
    <div class="article-style">
      <p>see a follow-up in: Victor Boutin, Angelo Franciosini, Frédéric Chavane, Franck Ruffier, Laurent U Perrinet (2021). Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system. PLoS Computational Biology.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/franciosini-perrinet-19-cns/franciosini-perrinet-19-cns.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/franciosini-perrinet-19-cns/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/nelson-cortes/">Nelson Cortes</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/christian-casanova/">Christian Casanova</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-19-sfn/" >Orientation selectivity to synthetic natural patterns in a cortical-like model of the cat primary visual cortex</a>
  </div>

  
  <a href="/publication/ladret-19-sfn/"  class="summary-link">
    <div class="article-style">
      <p>A key property of the neurons in the primary visual cortex (V1) is their selectivity to oriented stimuli in the visual field. Orientation selectivity allows the segmentation of objects in natural visual scenes, which is the first step in building integrative representations from retinal inputs. As such, V1 has always been of central interest in creating artificial neural networks and the recent years have seen a growing interest in the creation of explainable yet robust and adaptive models of cortical visual processes, for fundamental or applied purposes. One notable challenge for those models is to behave reliably in generic natural environments, where information is usually hidden in noise, while most models are typically studied with oriented gratings. Here we show that a simple biologically inspired neural network accounts for orientation selectivity to natural-like textures in the cat&rsquo;s primary visual cortex. Our spiking neural network (SNN) is made of point neurons organized in recurrent and hierarchical layers based on the structure of cortical layers IV and II/III. We found that Spike-timing plasticity and synaptic recurrence allowed the SNN to self-organize its connections weights and reproduce the activity of neurons recorded with laminar probes in cortical areas 17 and 18 of cats, notably orientation tuning responses. After less than 5 seconds of stimulus presentation, the SNN displays narrow orientation selectivity (bandwidth = 10 degrees) characteristic of sparse representations, removes noise from the input and learns the structure of natural pattern repetitions. Our results support the use of natural stimuli to study theoretical and experimental cortical dynamics. Furthermore, this model encourages using SNNs to reduce complexity in cortical networks as a method to understand the separate contribution of different components in the laminar organization of the cortex. From an applied perspective, the computations this network performs could also be used as an alternative to classical blackbox Deep Learning models used in artificial vision.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.abstractsonline.com/pp8/#!/7883/presentation/65859" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-19-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/" >
      <div class="img-hover-zoom">
        <img src="/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/featured_hu70dd320aa41adc38f6ad78e524a56b49_2548965_1553ae6b1aca739556e4b35e5b0cb56d.webp" height="455" width="808"
            class="article-banner" alt="Sparse Deep Predictive Coding to model visual object recognition" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/" >Sparse Deep Predictive Coding to model visual object recognition</a>
  </div>

  
  <a href="/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/"  class="summary-link">
    <div class="article-style">
      <p>If you’re at #sfn2019 and have an interest in #sparse #deep Predictive Coding, checkout @VictorBoutin ‘s poster 403.16 / P20:https://t.co/2VLEsl98oU
It shows today + comes with a (timely) preprint https://t.co/FfKi9tjqrN !</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/boutin-franciosini-chavane-ruffier-perrinet-19-sfn.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-chavane-ruffier-perrinet-19-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>The Conversation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-19-temps/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-19-temps/featured_hu39ce41fdb9413fd0582ed06508b1a283_464510_3cdbc2460ce7e354568ebcd0ea97e233.webp" height="455" width="808"
            class="article-banner" alt="Temps et cerveau : comment notre perception nous fait voyager dans le temps" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-19-temps/" >Temps et cerveau : comment notre perception nous fait voyager dans le temps</a>
  </div>

  
  <a href="/publication/perrinet-19-temps/"  class="summary-link">
    <div class="article-style">
      <p>Lorsque nous observons un sablier, lorsque nous fixons notre regard sur les grains de sable qui tombent, nous avons le sentiment que le temps s&rsquo;écoule de façon continue. Nous pensons qu&rsquo;il en est ainsi depuis la naissance du monde, et que rien ne peut contredire cette vérité universelle. Pourtant, nos perceptions sensorielles et les neurones qui en sont à l&rsquo;origine ont une toute autre manière de scander le temps. Une manière subjective et sensuelle, au sens propre du terme.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-temps/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-19-temps/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  











  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GdR Robotics - 2019-06-05</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-franciosini-ruffier-perrinet-19-gdr-robotics/" >Top-down connection in Hierarchical Sparse Coding</a>
  </div>

  
  <a href="/publication/boutin-franciosini-ruffier-perrinet-19-gdr-robotics/"  class="summary-link">
    <div class="article-style">
      <p>The brain has to solve inverse problems to correctly interpret sensory data and infer the set of causes that generated the sensory inputs. When imposing sparse prior and hierarchical structure this problem is called Hierarchical Sparse Coding (HSC). Predictive Coding (PC) is a computational neuroscience framework suggesting that each layer  predicts the activity of the lower layer via feedback connections. The error between predicted and actual response is then sent back to the next higher level via feed-forward connections to correct the estimation of the representation. While computer scientists often solved HSC using a stacking of Lasso sub-problems that we will call Hierarchical Lasso (Hi-La), we propose to leverage PC into a hierarchical and sparse model called Sparse Deep Predictive Coding (SDPC) network. This poster shows computational differences between SDPC and Hi-La.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-ruffier-perrinet-19-gdr-robotics/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jonathan-vacher/">Jonathan Vacher</a></span>, <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-peyre/">Gabriel Peyré</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neural Computation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/vacher-16/" >
      <div class="img-hover-zoom">
        <img src="/publication/vacher-16/featured_hu56f82771ae6c9a8748a4d8b10dc277c0_702202_090e5561b5a979e30c5c87b155ec22d2.webp" height="455" width="808"
            class="article-banner" alt="Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/vacher-16/" >Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures</a>
  </div>

  
  <a href="/publication/vacher-16/"  class="summary-link">
    <div class="article-style">
      <p>A common practice to account for psychophysical biases in vision is to frame them as consequences of a dynamic process relying on optimal inference with respect to a generative model. The present study details the complete formulation of such a generative model intended to probe visual motion perception. It is first derived in a set of axiomatic steps constrained by biological plausibility. We then extend previous contributions by detailing three equivalent formulations of the Gaussian dynamic texture model. First, the composite dynamic textures are constructed by the random aggregation of warped patterns, which can be viewed as 3D Gaussian fields. Second, these textures are cast as solutions to a stochastic partial differential equation (sPDE). This essential step enables real time, on-the-fly, texture synthesis using time-discretized auto-regressive processes. It also allows for the derivation of a local motion-energy model, which corresponds to the log-likelihood of the probability density. The log-likelihoods are finally essential for the construction of a Bayesian inference framework. We use the model to probe speed perception in humans psychophysically using zoom-like changes in stimulus spatial frequency content. The likelihood is contained within the generative model and we chose a slow speed prior consistent with previous literature. We then validated the fitting process of the model using synthesized data. The human data replicates previous findings that relative perceived speed is positively biased by spatial frequency increments. The effect cannot be fully accounted for by previous models, but the current prior acting on the spatio-temporal likelihoods has proved necessary in accounting for the perceptual bias.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1611.01390" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/vacher-16/vacher-16.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vacher-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1162/neco_a_01142" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/damasse-18/" >Reinforcement effects in anticipatory smooth eye movements</a>
  </div>

  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01901640v1" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://jov.arvojournals.org/article.aspx?articleid=2707670" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/18.11.14" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GDR Vision, Paris, 2018</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-18-gdr/" >A low-cost, accessible eye tracking framework</a>
  </div>

  
  <a href="/publication/perrinet-18-gdr/"  class="summary-link">
    <div class="article-style">
      <p>Recording eye movements is a technique that attracts an increasing number of scientists, but also in the general public. Indeed, this allows to quantitatively measure a number of useful dimensions of perception and behavior in general. However, most existing trackers rely on expensive or technically complex solutions. Here, we propose a simple framework to record eye movements using any camera, such as a webcam. As a proof of concept, the recorded image is processed in real-time to detect from a simple sub-set of eye movements : left, center, right or blink. The processing is based on two stages. First, we use a pre-trained computer vision algorithm to extract the image of the face. Second, we used a classical deep-learning architecture to learn to classify these sub-images. This network is a 3 layered convolutional neural network, for which we optimized performance as measured by the accuracy with cross-validation on a wide range of the network&rsquo;s hyper-parameters. Over a dataset of more than 1000 images, this network achieves an average accuracy of approximately 97 percent. We also provide with an integration with the psychopy library which shows that frames can be processed on a standard laptop at a rate of approximately 25 Hz.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/Perrinet18gdr" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-18-gdr/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/chloe-pasturel/">Chloé Pasturel</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Grenoble Workshop on Models and Analysis of Eye Movements, Grenoble, France</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/pasturel-18-anemo/" >
      <div class="img-hover-zoom">
        <img src="/publication/pasturel-18-anemo/featured_hud981166d3338c72353edb3cbff0322ca_693209_31deec1f27c460d71b192e282cda2e60.webp" height="455" width="808"
            class="article-banner" alt="ANEMO: Quantitative tools for the ANalysis of Eye MOvements" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/pasturel-18-anemo/" >ANEMO: Quantitative tools for the ANalysis of Eye MOvements</a>
  </div>

  
  <a href="/publication/pasturel-18-anemo/"  class="summary-link">
    <div class="article-style">
      <p> see a write-up in &ldquo;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&rdquo; as presented at https://eyemovements.sciencesconf.org/ get the poster code : https://github.com/invibe/ANEMO/ </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.science/hal-04157003" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/pasturel-18-anemo/pasturel-18-anemo.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/pasturel-18-anemo/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/open-science/">
    Project
  </a>
  











  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/chloe-pasturel/">Chloé Pasturel</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Grenoble Workshop on Models and Analysis of Eye Movements, Grenoble, France</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/pasturel-18-grenoble/" >Estimating and anticipating a dynamic probabilistic bias in visual motion direction</a>
  </div>

  
  <a href="/publication/pasturel-18-grenoble/"  class="summary-link">
    <div class="article-style">
      <p>Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high<em>resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye</em>to<em>target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp</em>motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired* task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate &quot;how much they are confident that the target will move to the right or left in the next trial&quot;  and to adjust the cursor&rsquo;s position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random<em>direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch</em>up  saccades),  we  developed  new  tools  based  on  best*fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/pasturel-18-grenoble" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/pasturel-18-grenoble/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/chloe-pasturel/">Chloé Pasturel</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/pasturel-18/" >Estimating and anticipating a dynamic probabilistic bias in visual motion direction</a>
  </div>

  
  <a href="/publication/pasturel-18/"  class="summary-link">
    <div class="article-style">
      <p>Humans are able to accurately track a moving object with a combination of saccades and smooth eye  movements.  These  movements  allow  us  to  align  and  stabilize  the  object  on  the  fovea,  thus  enabling  high-resolution  visual  analysis.  When  predictive  information  is  available  about  target  motion,  anticipatory  smooth  pursuit  eye  movements  (aSPEM)  are  efficiently  generated  before  target  appearance,  which  reduce  the  typical  sensorimotor  delay  between  target  motion  onset  and  foveation.  It  is  generally  assumed  that  the  role  of  anticipatory  eye  movements  is  to  limit  the  behavioral  impairment  due  to  eye<em>to</em>target  position  and  velocity  mismatch.  By  manipulating  the  probability  for  target  motion  direction  we  were  able  to  bias  the  direction  and  mean  velocity  of  aSPEM, as measured during a fixed duration gap before target ramp<em>motion onset. This suggests that  probabilistic information may be used to inform the internal representation of motion prediction for  the  initiation  of  anticipatory  movements.  However,  such  estimate  may  become  particularly  challenging  in  a  dynamic  context,  where  the  probabilistic  contingencies  vary  in  time  in  an  unpredictable way. In addition, whether and how the information processing underlying the buildup  of  aSPEM  is  linked  to  an  explicit  estimate  of  probabilities  is  unknown.  We  developed  a  new  paired</em> task  paradigm  in  order  to  address  these  two  questions.  In  a  first  session,  participants  observe  a  target  moving  horizontally  with  constant  speed  from  the  center  either  to  the  right  or  left  across  trials. The probability of either motion direction changes randomly in time. Participants are asked to  estimate &quot;how much they are confident that the target will move to the right or left in the next trial&quot;  and to adjust the cursor&rsquo;s position on the screen accordingly. In a second session the participants eye  movements are recorded during the observation of the same sequence of random direction trials. In  parallel,  we  are  developing  new  automatic  routines  for  the  advanced  analysis  of  oculomotor  traces.  In  order  to  extract  the  relevant  parameters  of  the  oculomotor  responses  (latency,  gain,  initial  acceleration,  catch<em>up  saccades),  we  developed  new  tools  based  on  best</em>fitting  procedure  of  predefined patterns (i.e. the typical smooth pursuit velocity profile).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/pasturel-18" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/pasturel-18/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/chloepasturel/AnticipatorySPEM/" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>iTwist, 2018</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-franciosini-ruffier-perrinet-18-itwist/" >From biological vision to unsupervised hierarchical sparse coding</a>
  </div>

  
  <a href="/publication/boutin-franciosini-ruffier-perrinet-18-itwist/"  class="summary-link">
    <div class="article-style">
      <p>The formation of connections between neural cells is essentially emerging from an unsupervised learning process. During the development of primary visual cortex (V1) of mammals, for example, one may observe the emergence of cells selective to localized and oriented features. This leads to the development of a rough contour-based representation of the retinal image in area V1. We modeled the formation of this representation along the thalamo-cortical pathway using a sparse unsupervised learning algorithm in a hierarchical network. This algorithm alternates (i) a coding phase to encode the information and (ii) a learning phase to find the proper encoder (also called dictionary). We replicated and adapted the Multi-Layer Convolutional Sparse Coding (ML-CSC) model from Michael Elad&rsquo;s group.  As an application, we have trained our implementation on a database containing images from faces. The extracted features show similarities with some of the neuron&rsquo;s receptive field found in V1 and beyond. Furthermore, our results demonstrate the potential application of such a strategy to the fast classification of images, for example in hierarchical and dynamical architectures.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1812.01335" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/boutin-franciosini-ruffier-perrinet-18-itwist/boutin-franciosini-ruffier-perrinet-18-itwist.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-ruffier-perrinet-18-itwist/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/julien-dupeyroux/">Julien Dupeyroux</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/julien-r-serres/">Julien R Serres</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/stephane-viollet/">Stéphane Viollet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>ISCAS2018, IEEE International Symposium on Circuits and Systems</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/dupeyroux-boutin-serres-perrinet-viollet-18/" >M2APix: a bio-inspired auto-adaptive visual sensor for robust ground height estimation</a>
  </div>

  
  <a href="/publication/dupeyroux-boutin-serres-perrinet-viollet-18/"  class="summary-link">
    <div class="article-style">
      <p>This paper presents for the first time the embedded stand-alone version of the bio-inspired M2APix (Michaelis-Menten auto-adaptive pixels) sensor as a ventral optic flow sensor to endow glider-type unmanned aerial vehicles with autonomous landing ability. Assuming the aircraft is equipped with any reliable speed measurement system such as a global positioning system or an inertial measurement unit, we can use the velocity of the glider to determine with high precision its height while landing. This information is robust to different outdoor lighting conditions and over different kinds of textured ground, a crucial property to control the landing phase of the aircraft.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01899440" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/abstract/document/8351433" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dupeyroux-boutin-serres-perrinet-viollet-18/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Curves and Surfaces 2018, Arcachon</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/franciosini-perrinet-18-cs/" >On the Origins of Hierarchy in Visual Processing</a>
  </div>

  
  <a href="/publication/franciosini-perrinet-18-cs/"  class="summary-link">
    <div class="article-style">
      <p>It is widely assumed that visual processing follows a forward sequence of processing steps along a hierarchy of laminar sub-populations of the neural system. Taking the example of the early visual system of mammals, most models are consequently organized in layers from the retina to visual cortical areas, until a decision is taken using the representation that is formed in the highest layer. Typically, features of higher complexity (position, orientation, size, curvature, &hellip;) are successively extracted in distinct layers (Carandini, 2012). This is prevalent in most deep learning algorithms and stems from a long history of feed-forward architectures. Though this proved to be highly successful, the origin of such architectures is not known i̧teSerre07. Using a generic unsupervised learning algorithm, we first trained a simple one-layer convolutional neural network on a seta of natural images with a growing number of neurons. By doing this, we could quantitatively manipulate the complexity of the representation that emerges from such learning and analyze if sub-populations within the layer could be grouped by their similarity, hence justifying the emergence of a hierarchical processing. As shown in previous studies (Olshausen, 1996), such an algorithm converges to a weight matrix that has strong analogies with the receptive fields of simple cells located in the Primary Visual Cortex of mammals (V1).  This result extends naturally to a cortical representation of the input image that encodes second-order features (edges) as neural responses arranged in a three-dimensional space, where the third dimension can be seen as a model of hyper-columns of the Primary Visual Cortex. From this bio-inspired encoding, we were able to define contours in images as simple smooth trajectories in a cortical representation space. This simple model shows that hierarchical processing may originate from the neural encoding of different visual transformations within natural images: respectively translation, rotations and zooms, which correspond to rigid translation in the cortical space. The model can be further extended to reproduce the effect of complex cells in V1 (max pooling) and feedback signals from higher cortical areas. We predict that invariance to more complex transformation like shearing (perspective) and viewpoint changes (looming) will emerge as these additional steps in sensory processing are taken into account.  Indeed, a higher level of complexity can be introduced as the cortical representation is extended from smooth trajectories (space domain) to smooth surfaces (space-time domain). As such, this justifies the extension of a simple sparse network formalism to translation invariant neural networks (such as the convolutional neural networks used in deep learning) that is able to generalize geometrical transformations, such as translation, rotations, and zooms, in an invariant bio-inspired representation (Perrinet, 2015). This should provide some key insights into higher-order features such as co-occurrences, but also to novel categorization architectures. Indeed, such features were recently found to be sufficient to allow the categorization of images containing an animal (Perrinet and Bednar, 2015). Crucially, as the geometrical transformations develop in time, we expect that the detection of these features is made robust by dynamical processes.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/franciosini-perrinet-18-cs/franciosini-perrinet-18-cs.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/franciosini-perrinet-18-cs/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GDR Vision, Paris, 2018</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/ladret-18-gdr/" >
      <div class="img-hover-zoom">
        <img src="/publication/ladret-18-gdr/featured_hu191789e3d69ae28ef26600a7225c6be0_263059_5d82d6ffc8e1e303508715198b92a57b.webp" height="455" width="808"
            class="article-banner" alt="Selectivity to oriented patterns of different precisions" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ladret-18-gdr/" >Selectivity to oriented patterns of different precisions</a>
  </div>

  
  <a href="/publication/ladret-18-gdr/"  class="summary-link">
    <div class="article-style">
      <p>The selectivity of the visual system to oriented patterns is very well documented in a wide range of species, especially in mammals. In particular, neurons of the primary visual cortex are anatomically grouped by their preference to a given oriented visual stimulus. Interactions between such groups of neurons have been successfully modeled using recurrently-connected network of spiking neurons, so called &quot;ring models&quot;. Nonetheless, this selectivity is most often studied with crystal-like patterns such as gratings. Here, we studied the ability of human observers to discriminate texture-like patterns for which we could quantitatively tune the precision of their oriented content and we propose a generic model to explain such results. The first contribution shows that the discrimination threshold as a function of the precision did not vary smoothly as would be expected, but more in a binary, &quot;all or none&quot; fashion. Our second contribution is to propose a novel model of orientation selectivity that is based on deep-learning techniques, which performance we evaluated in the same task. This model has human-like performance in term of accuracy and exhibits qualitatively similar psychophysical curves. One hypothesis that such a structure allows for the system to be robust to noise in its visual inputs.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/hugoladret/InternshipM1/raw/master/2018-06_POSTER_final.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-18-gdr/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/hugoladret/InternshipM1" target="_blank" rel="noopener">
  Code
</a>














  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/kiana-mansour-pour/">Kiana Mansour-Pour</a></span>, <span >
      <a href="/author/nikos-gekas/">Nikos Gekas</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision, Vol.18, 345, proceedings of VSS</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mansour-18-vss/" >Speed uncertainty and motion perception with naturalistic random textures</a>
  </div>

  
  <a href="/publication/mansour-18-vss/"  class="summary-link">
    <div class="article-style">
      <p>It is still not fully understood how visual system integrates motion energy across different spatial and temporal frequencies to build a coherent percept of the global motion under the complex, noisy naturalistic conditions. We addressed this question by manipulating local speed variability distribution (i. e. speed bandwidth) using a well-controlled class of broadband random-texture stimuli called Motion Clouds (MCs) with continuous naturalistic spatiotemporal frequency spectra (Sanz-Leon et al., 2012, ; Simoncini et al., 2012). In a first 2AFC experiment on speed discrimination, participants had to compare the speed of a broad speed bandwidth MC (range: 0.05-8$,^∘$/s) moving at 1 of 5 possible mean speeds (ranging from 5 to 13 $,^∘$/s) to that of another MC with a small speed bandwidth (SD: 0.05 $,^∘$/s), always moving at a mean speed of 10$,^∘$/s . We found that MCs with larger speed bandwidth (between 0.05-0.5$,^∘$/s) were perceived moving faster. Within this range, speed uncertainty results in over-estimating stimulus velocity. However, beyond a critical bandwidth (SD: 0.5 $,^∘$/s), perception of a coherent speed was lost. In a second 2AFC experiment on direction discrimination, participants had to estimate the motion direction of moving MCs with different speed bandwidths. We found that for large band MCs participant could no longer discriminate motion direction. These results suggest that when increasing speed bandwidth from small to large range, the observer experiences different perceptual regimes. We then decided to run a Maximum Likelihood Difference Scaling (Knoblauch &amp; Maloney, 2008) experiment with our speed bandwidth stimuli to investigate these different possible perceptual regimes. We identified three regimes within this space that correspond to motion coherency, motion transparency and motion incoherency. These results allow to further characterize the shape of the interactions kernel observed between different speed tuned channels and different spatiotemporal scales (Gekas et al ., 2017) that underlies global velocity estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/mansour-18-vss" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mansour-18-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/18.10.345" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Doc2AMU Doctoral Day - 2018-11-23</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-franciosini-ruffier-perrinet-18-doctoral-day/" >Unsupervised Hierarchical Sparse Coding algorithm inspired by Biological Vision</a>
  </div>

  
  <a href="/publication/boutin-franciosini-ruffier-perrinet-18-doctoral-day/"  class="summary-link">
    <div class="article-style">
      <p>The brain has to solve inverse problems to correctly interpret sensory data and infer the set of causes that generated the sensory inputs. Such a problem is typically ill-posed, and thus requires constraint the narrow down the number of solutions. Predictive coding (PC)is a computational neuroscience framework that finds the most likely causes for the sensory input by minimizing the mismatch between the sensory data and the predicted input. Such a framework could be used to build sparse hierarchical internal representations of a given input.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-ruffier-perrinet-18-doctoral-day/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>PLoS Computational Biology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/khoei-masson-perrinet-17/" >
      <div class="img-hover-zoom">
        <img src="/publication/khoei-masson-perrinet-17/featured_hu23372f189af3e1bc30684e50c20cdfc7_132844_aa21158fb020295d746a655aa646f5e6.webp" height="455" width="808"
            class="article-banner" alt="The flash-lag effect as a motion-based predictive shift" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/khoei-masson-perrinet-17/" >The flash-lag effect as a motion-based predictive shift</a>
  </div>

  
  <a href="/publication/khoei-masson-perrinet-17/"  class="summary-link">
    <div class="article-style">
      <p>Due to its inherent neural delays, the visual system has an outdated access to sensory information about the current position of moving objects. In contrast, living organisms are remarkably able to track and intercept moving objects under a large range of challenging environmental conditions. Physiological, behavioral and psychophysical evidences strongly suggest that position coding is extrapolated using an explicit and reliable representation of object&rsquo;s motion but it is still unclear how these two representations interact. For instance, the so-called flash-lag effect supports the idea of a differential processing of position between moving and static objects. Although elucidating such mechanisms is crucial in our understanding of the dynamics of visual processing, a theory is still missing to explain the different facets of this visual illusion. Here, we reconsider several of the key aspects of the flash-lag effect in order to explore the role of motion upon neural coding of objects&rsquo; position. First, we formalize the problem using a Bayesian modeling framework which includes a graded representation of the degree of belief about visual motion. We introduce a motion-based prediction model as a candidate explanation for the perception of coherent motion. By including the knowledge of a fixed delay, we can model the dynamics of sensory information integration by extrapolating the information acquired at previous instants in time. Next, we simulate the optimal estimation of object position with and without delay compensation and compared it with human perception under a broad range of different psychophysical conditions. Our computational study suggests that the explicit, probabilistic representation of velocity information is crucial in explaining position coding, and therefore the flash-lag effect. We discuss these theoretical results in light of the putative corrective mechanisms that can be used to cancel out the detrimental effects of neural delays and illuminate the more general question of the dynamical representation of spatial information at the present time in the visual pathways.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01771125" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-masson-perrinet-17/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-masson-perrinet-17/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/Khoei_2017_PLoSCB" target="_blank" rel="noopener">
  Code
</a>







  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2022-11-21_flash-lag-effect/" target="_blank">
    Slides
  </a>
  




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1005068" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Doc2AMU Doctoral Day - 2017-10-13</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-franciosini-ruffier-perrinet-17-doctoral-day/" >Controlling an aerial robot with human gestures using bio-inspired algorithm</a>
  </div>

  
  <a href="/publication/boutin-franciosini-ruffier-perrinet-17-doctoral-day/"  class="summary-link">
    <div class="article-style">
      <p>Improve performances of existing recognition computer vision algorithms with biological concepts. The gain are expected in the following: Recognition latency and accuracy (faster and better), less data needed to train algorithms and of decreased power consumption.)</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-ruffier-perrinet-17-doctoral-day/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of Vision Sciences Society Annual Meeting</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/damasse-17-vss/" >Dynamic modulation of volatility by reward contingencies: effects on anticipatory smooth eye movement</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://jov.arvojournals.org/article.aspx?doi=10.1167/17.10.273" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-17-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/17.10.273" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>NeuroFrance 2017, International Conference from the Société des Neurosciences, Bordeaux, France</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-ruffier-perrinet-17-neurofrance/" >Efficient learning of sparse image representations using homeostatic regulation</a>
  </div>

  
  <a href="/publication/boutin-ruffier-perrinet-17-neurofrance/"  class="summary-link">
    <div class="article-style">
      <p>One core advantage of sparse representations is the efficient coding of complex signals using compact codes. For instance, it allows for the representation of any image as a combination of few elements drawn from a large dictionary of basis functions. In the context of the efficient processing of natural images, we propose here that such codes can be optimized by designing proper homeostatic rules between the elements of the dictionary. Indeed, a common design for unsupervised learning rules relies on a gradient descent over a cost measuring representation quality with respect to sparseness. The sparseness constraint introduces a competition which can be optimized by ensuring that each item in the dictionary is selected as often as others. We implemented this rule by introducing a gain normalization similar to what is observed in a balanced neural network. We validated this theoretical insight by challenging different sparse coding algorithms with the same learning rule but with or without homeostasis. The different sparse coding algorithms were chosen for their efficiency and generality. They include least-angle regression, orthogonal matching pursuit and basis pursuit. Simulations show that for a given homeostasis rule, gradient descent performed similarly the learning of a dataset of image patches. While the coding algorithm did not matter much, including homeostasis changed qualitatively the learned features. In particular, homeostasis results in a more homogeneous set of orientation selective filters, which is closer to what is found in the visual cortex of mammals. To further validate these results, we applied this algorithm to the optimization of a visual system embedded in an aerial robot. As a consequence, this biologically-inspired learning rule demonstrates that principles observed in neural computations can help improve real-life machine learning algorithms.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-ruffier-perrinet-17-neurofrance/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>SPARS2017, Lisbon</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/boutin-ruffier-perrinet-17-spars/" >Efficient learning of sparse image representations using homeostatic regulation</a>
  </div>

  
  <a href="/publication/boutin-ruffier-perrinet-17-spars/"  class="summary-link">
    <div class="article-style">
      <p>One core advantage of sparse representations is the efficient coding of complex signals using compact codes. For instance, it allows for the representation of any image as a combination of few elements drawn from a large dictionary of basis functions. In the context of the efficient processing of natural images, we propose here that such codes can be optimized by designing proper homeostatic rules between the elements of the dictionary. Indeed, a common design for unsupervised learning rules relies on a gradient descent over a cost measuring representation quality with respect to sparseness. The sparseness constraint introduces a competition which can be optimized by ensuring that each item in the dictionary is selected as often as others. We implemented this rule by introducing a gain normalization similar to what is observed in a balanced neural network. We validated this theoretical insight by challenging different sparse coding algorithms with the same learning rule but with or without homeostasis. The different sparse coding algorithms were chosen for their efficiency and generality. They include least-angle regression, orthogonal matching pursuit and basis pursuit. Simulations show that for a given homeostasis rule, gradient descent performed similarly the learning of a dataset of image patches. While the coding algorithm did not matter much, including homeostasis changed qualitatively the learned features. In particular, homeostasis results in a more homogeneous set of orientation selective filters, which is closer to what is found in the visual cortex of mammals. To further validate these results, we applied this algorithm to the optimization of a visual system embedded in an aerial robot. As a consequence, this biologically-inspired learning rule demonstrates that principles observed in neural computations can help improve real-life machine learning algorithms.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-ruffier-perrinet-17-spars/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/chloe-pasturel/">Chloé Pasturel</a></span>, <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GDR Vision, Lille, 2017</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/pasturel-17-gdr/" >Estimating and anticipating a dynamic probabilistic bias in visual motion direction</a>
  </div>

  
  <a href="/publication/pasturel-17-gdr/"  class="summary-link">
    <div class="article-style">
      <p> see a write-up in &ldquo;Humans adapt their anticipatory eye movements to the volatility of visual motion properties&rdquo; </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/pasturel-17-gdr" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/pasturel-17-gdr/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/etienne-rey/">Étienne Rey</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GDR Vision, Lille, 2017</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-17-gdr/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-17-gdr/featured_hu585b6125870314516d80075f49f72bbf_792291_51a4e4f0c212cd4de3b21ef0f9513dd4.webp" height="455" width="808"
            class="article-banner" alt="Expériences autour de la perception de la forme en art et science" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-17-gdr/" >Expériences autour de la perception de la forme en art et science</a>
  </div>

  
  <a href="/publication/perrinet-17-gdr/"  class="summary-link">
    <div class="article-style">
      <p>La vision utilise un faisceau d&rsquo;informations de différentes qualités pour atteindre une perception unifiée du monde environnant. Nous avons utilisé lors de plusieurs projets art-science (voir <a href="https://github.com/NaturalPatterns" target="_blank" rel="noopener">https://github.com/NaturalPatterns</a>) des installations permettant de manipuler explicitement des composantes de ce flux d&rsquo;information et de révéler des ambiguités dans notre perception. Dans l&rsquo;installation Tropique, des faisceaux de lames lumineuses sont arrangés dans l&rsquo;espace assombri de l&rsquo;installation. Les spectateurs les observent grâce à leur interaction avec une brume invisible qui est diffusée dans l&rsquo;espace. Dans Trame Élasticité, 25 parallélépipèdes de miroirs (3m de haut) sont arrangés verticalement sur une ligne horizontale. Ces lames sont rotatives et leurs mouvements est synchronisé. Suivant la dyamique qui est imposé à ces lames, la perception de l&rsquo;espace environnent fluctue conduisant à recomposer l&rsquo;espace de la concentration à l&rsquo;expansion, ou encore à générer un surface semblant transparente ou inverser la visons de ce qui est située devant et derrière l&rsquo;observateur. Enfin, dans Trame instabilité, nous explorons l&rsquo;interaction de séries périodiques de points placées sur des surfaces transparentes. À partir de premières expérimentations utilisant une technique novatrice de sérigraphie, ces trames de points sont placées afin de faire émerger  des structures selon le point de vue du spectateur. De manière générale, nous montrerons ici les différentes méthodes utilisées, comme l&rsquo;utilisation des limites perceptives, et aussi les résultats apportés par une telle collaboration.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/NaturalPatterns/2017-10-12_GDR/raw/master/2017-10-12_PerrinetRey2017poster.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-17-gdr/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/NaturalPatterns/2017-10-12_GDR" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/art-science/">
    Project
  </a>
  

  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  











  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/kiana-mansour-pour/">Kiana Mansour-Pour</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of ECVP</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mansour-17-ecvp/" >How the dynamics of human smooth pursuit is influenced by speed uncertainty</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/mansour-17-ecvp/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mansour-17-ecvp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/kiana-mansour-pour/">Kiana Mansour-Pour</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GDR Vision, Lille, 2017</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mansour-17-gdr/" >Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</a>
  </div>

  
  <a href="/publication/mansour-17-gdr/"  class="summary-link">
    <div class="article-style">
      <p>The properties of motion processing for driving smooth eye movements have bee investigated using simple, artificial stimuli such as gratings, small dots or random dot patterns. Motion processing in the context of complex, natural images is less known. We have previously investigated the human ocular following responses to a novel class of random texture stimuli of parameterized naturalistic statistics: the Motion Clouds. In Fourier space, these dynamical textures are designed with a log normal distribution of spatial frequencies power multiplied by a pink noise power spectral density that reduces the high frequency contents of the stimulus (Sanz-Leon et al. 2011). We have previously shown that the precision of reflexive tracking increases with the spatial frequency bandwidth of large (&gt; 30 deg diameter) patterns (i.e. the width of the spatial frequency distribution around a given mean spatial frequency; Simoncini et al. 2012). Now, we extend this approach to voluntary tracking and focused on the effects of spatial frequency bandwidth upon the initial phase of smooth pursuit eye movements. Participants were instructed to pursue a large patch of moving clouds (mean speeds: 5, 10 or 20 deg/s) embedded within a smoothing Gaussian window of standard deviation 5 deg. The motion stimuli were presented with four different spatial frequency bandwidths and two different mean spatial frequencies (0.3 and 1 cpd). We observed that smaller bandwidth textures exhibit a stronger spectral energy within the low spatial frequency range (below 1cpd), yielding to shorter latency of smooth pursuit eye movements. A weak and less consistent effect was found on initial eye acceleration, contrary what was previously observed with OFR. After 400ms, the steady-state tracking velocity matched the mean visual motion speed and pursuit performance was comparable with that observed with a control, small dot motion. Motion Clouds offer an efficient tool to probe the optimal window of visibility for human smooth pursuit through the manipulation of both the mean and the variability of spatial frequency.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/mansour-17-gdr" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mansour-17-gdr/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/cesar-u-ravello/">Cesar U Ravello</a></span>, <span >
      <a href="/author/maria-jose-escobar/">Maria Jose Escobar</a></span>, <span >
      <a href="/author/adrian-g-palacios/">Adrián G Palacios</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/ravello-16-droplets/" >
      <div class="img-hover-zoom">
        <img src="/publication/ravello-16-droplets/featured_hue92e3141fddc09af102f435b5c8ed6b2_392715_b2774b00686d534aa35713537279e146.webp" height="455" width="808"
            class="article-banner" alt="Differential response of the retinal neural code with respect to the sparseness of natural images" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ravello-16-droplets/" >Differential response of the retinal neural code with respect to the sparseness of natural images</a>
  </div>

  
  <a href="/publication/ravello-16-droplets/"  class="summary-link">
    <div class="article-style">
      <p>Sparse coding of images in the retina follows regular statistics at the global, not the local scale. See supplementray code.
How does the retina respond to stimuli with different sparseness?</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1611.06834" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/ravello-16-droplets/ravello-16-droplets.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ravello-16-droplets/cite.bib">
  Cite
</a>





  
    
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.5281/zenodo.5823016" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2016 6th European Workshop on Visual Information Processing (EUVIP)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-16-euvip/" >Biologically-inspired characterization of sparseness in natural images</a>
  </div>

  
  <a href="/publication/perrinet-16-euvip/"  class="summary-link">
    <div class="article-style">
      <p>Natural images follow statistics inherited by the structure of our physical (visual) environment. In particular, a prominent facet of this structure is that images can be described by a relatively sparse number of features. We designed a sparse coding algorithm biologically-inspired by the architecture of the primary visual cortex. We show here that coefficients of this representation exhibit a power-law distribution. For each image, the exponent of this distribution characterizes sparseness and varies from image to image. To investigate the role of this sparseness, we designed a new class of random textured stimuli with a controlled sparseness value inspired by measurements of natural images. Then, we provide with a method to synthesize random textures images with a given sparseness statistics that match that of some class of natural images and provide perspectives for their use in neurophysiology.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1702.02485" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://ieeexplore.ieee.org/document/7764592/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-16-euvip/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/bicv/Perrinet16EUVIP" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/EUVIP.2016.7764592" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/jeremie-jozefowiez/">Jeremie Jozefowiez</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of VSS</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/damasse-16-vss/" >Operant reinforcement versus reward expectancy: effects on anticipatory eye movements</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://jov.arvojournals.org/article.aspx?doi=10.1167/16.12.1356" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-16-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/16.12.1356" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/pascal-wallisch/">Pascal Wallisch</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Neurophysiology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/taouali-16/" >Testing the odds of inherent vs. observed overdispersion in neural spike counts</a>
  </div>

  
  <a href="/publication/taouali-16/"  class="summary-link">
    <div class="article-style">
      <p>The repeated presentation of an identical visual stimulus in the receptive field of a neuron may evoke different spiking patterns at each trial. Probabilistic methods are essential to understand the functional role of this variance within the neural activity. In that case, a Poisson process is the most common model of trial-to-trial variability. For a Poisson process, the variance of the spike count is constrained to be equal to the mean, irrespective of the duration of measurements. Numerous studies have shown that this relationship does not generally hold. Specifically, a majority of electrophysiological recordings show an &quot; over-dispersion &quot; effect: Responses that exhibit more inter-trial variability than expected from a Poisson process alone. A model that is particularly well suited to quantify over-dispersion is the Negative-Binomial distribution model. This model is well-studied and widely used but has only recently been applied to neuroscience. In this paper, we address three main issues. First, we describe how the Negative-Binomial distribution provides a model apt to account for overdispersed spike counts. Second, we quantify the significance of this model for any neurophysiological data by proposing a statistical test, which quantifies the odds that over-dispersion could be due to the limited number of repetitions (trials). We apply this test to three neurophysiological tests along the visual pathway. Finally, we compare the performance of this model to the Poisson model on a population decoding task. We show that the decoding accuracy is improved when accounting for over-dispersion, especially under the hypothesis of tuned over-dispersion.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01396311" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.ncbi.nlm.nih.gov/pubmed/26445864" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taouali-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1152/jn.00194.2015" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of VSS</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/taouali-15-vss/" >A dynamic model for decoding direction and orientation in macaque primary visual cortex</a>
  </div>

  
  <a href="/publication/taouali-15-vss/"  class="summary-link">
    <div class="article-style">
      <p>Natural scenes generally contain objects in motion. The local orientation of their contours and the direction of motion are two essential components of visual information which are processed in parallel in the early visual areas. Focusing on the primary visual cortex of the macaque monkey (V1), we challenged different models for the joint representation of orientation and direction within the neural activity. Precisely, we considered the response of V1 neurons to an oriented moving bar to investigate whether, and how, the information about the bar&rsquo;s orientation and direction could be encoded dynamically at the population activity level. For that purpose, we used a decoding approach based on a space-time receptive field model that encodes jointly orientation and direction. We based our decoding approach on the statistics of natural scenes by first determining optimal space-time receptive fields (RFs) that encode orientation and direction. For this, we first derived a set of dynamic filters from a database of natural images~[1] and following an unsupervised learning rule~[2]. More generally, this allows us to propose a dynamic generative model for the joint coding of orientation and direction. Then, using this model and a maximum likelihood paradigm, we infer the most likely representation for a given network activity~[3, 4]. We tested this model on surrogate data and on extracellular recordings in area emphV1 (67 cells) of awake macaque monkeys in response to oriented bars moving in $12$ different directions. Using a cross-validation method we could robustly decode both the orientation and the direction of the bar within the classical receptive field (cRF). Furthermore, this decoding approach shows different properties: First, information about the orientation of the bar is emerging ıt before entering the cRF if the trajectory of the bar is long enough. Second, when testing different orientations with the same direction, our approach unravels that we can decode the direction and the orientation independently. Moreover, we found that, similarly to orientation decoding, the decoding of direction is dynamic but weaker. Finally, our results demonstrate that the orientation and the direction of motion of an ambiguous moving bar can be progressively decoded in V1. This is a signature of a dynamic solution to the aperture problem in area V1, similarly to what was already found in area MT~[5]. $[1]$ J. Burge, W. Geisler. Optimal speed estimation in natural image movies predicts human performance. Nature Communications, 6, 7900. <a href="http://doi.org/10.1038/ncomms8900" target="_blank" rel="noopener">http://doi.org/10.1038/ncomms8900</a>, 2015.  $[2]$ L. Perrinet. Role of homeostasis in learning sparse representations. ıt Neural Computation, 22(7):1812&ndash;36, 2010.  $[3]$ M. Jazayeri and J.A. Movshon. Optimal representation of sensory information by neural populations. ıt Nature Neuroscience, 9(5):690&ndash;696, 2006. $[4]$ W. Taouali, G. Benvenuti, P. Wallisch, F. Chavane, L. Perrinet. Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts. ıt Journal of Neurophysiology, 2015. $[5]$ C. Pack, R. Born. Temporal dynamics of a neural solution to the aperture problem in visual area MT of macaque brain. ıt Nature, 409(6823), 1040&ndash;1042. 2001.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://jov.arvojournals.org/article.aspx?articleid=2433592" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taouali-15-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/15.12.484" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/taouali-16-areadne/" >A dynamic model for decoding direction and orientation in macaque primary visual cortex</a>
  </div>

  
  <a href="/publication/taouali-16-areadne/"  class="summary-link">
    <div class="article-style">
      <p>Natural scenes generally contain objects in motion. The local orientation of their contours and the direction of motion are two essential components of visual information which are processed in parallel in the early visual areas. Focusing on the primary visual cortex of the macaque monkey (V1), we challenged different models for the joint representation of orientation and direction within the neural activity. Precisely, we considered the response of V1 neurons to an oriented moving bar to investigate whether, and how, the information about the bar&rsquo;s orientation and direction could be encoded dynamically at the population activity level. For that purpose, we used a decoding approach based on a space-time receptive field model that encodes jointly orientation and direction. We based our decoding approach on the statistics of natural scenes by first determining optimal space-time receptive fields (RFs) that encode orientation and direction. For this, we first derived a set of dynamic filters from a database of natural images~[1] and following an unsupervised learning rule~[2]. More generally, this allows us to propose a dynamic generative model for the joint coding of orientation and direction. Then, using this model and a maximum likelihood paradigm, we infer the most likely representation for a given network activity~[3, 4]. We tested this model on surrogate data and on extracellular recordings in area emphV1 (67 cells) of awake macaque monkeys in response to oriented bars moving in $12$ different directions. Using a cross-validation method we could robustly decode both the orientation and the direction of the bar within the classical receptive field (cRF). Furthermore, this decoding approach shows different properties: First, information about the orientation of the bar is emerging ıt before entering the cRF if the trajectory of the bar is long enough. Second, when testing different orientations with the same direction, our approach unravels that we can decode the direction and the orientation independently. Moreover, we found that, similarly to orientation decoding, the decoding of direction is dynamic but weaker. Finally, our results demonstrate that the orientation and the direction of motion of an ambiguous moving bar can be progressively decoded in V1. This is a signature of a dynamic solution to the aperture problem in area V1, similarly to what was already found in area MT~[5]. $[1]$ J. Burge, W. Geisler. Optimal speed estimation in natural image movies predicts human performance. Nature Communications, 6, 7900. <a href="http://doi.org/10.1038/ncomms8900" target="_blank" rel="noopener">http://doi.org/10.1038/ncomms8900</a>, 2015.  $[2]$ L. Perrinet. Role of homeostasis in learning sparse representations. ıt Neural Computation, 22(7):1812&ndash;36, 2010.  $[3]$ M. Jazayeri and J.A. Movshon. Optimal representation of sensory information by neural populations. ıt Nature Neuroscience, 9(5):690&ndash;696, 2006. $[4]$ W. Taouali, G. Benvenuti, P. Wallisch, F. Chavane, L. Perrinet. Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts. ıt Journal of Neurophysiology, 2015. $[5]$ C. Pack, R. Born. Temporal dynamics of a neural solution to the aperture problem in visual area MT of macaque brain. ıt Nature, 409(6823), 1040&ndash;1042. 2001.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/taouali-16-areadne/taouali-16-areadne.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taouali-16-areadne/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/15.12.484" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Complex Networks: from theory to interdisciplinary applications</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-16-networks/" >Compensation of oculomotor delays in the visual system&#39;s network</a>
  </div>

  
  <a href="/publication/perrinet-16-networks/"  class="summary-link">
    <div class="article-style">
      <p>We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-16-networks" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-16-networks/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of ECVP</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/montagnini-16-ecvp/" >Effects of motion predictability on anticipatory and visually-guided eye movements: a common prior for sensory processing and motor control?</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/montagnini-16-ecvp" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-16-ecvp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of ECVP</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/damasse-16-ecvp/" >Modeling the effect of dynamic contingencies on anticipatory eye movements</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/damasse-16-ecvp" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-16-ecvp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/cyril-monier/">Cyril Monier</a></span>, <span >
      <a href="/author/jose-manuel-alonso/">Jose-Manuel Alonso</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>, <span >
      <a href="/author/yves-fregnac/">Yves Fregnac</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Frontiers in Neural Circuits</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kremkow-16/" >Push-Pull Receptive Field Organization and Synaptic Depression: Mechanisms for Reliably Encoding Naturalistic Stimuli in V1</a>
  </div>

  
  <a href="/publication/kremkow-16/"  class="summary-link">
    <div class="article-style">
      <p>Neurons in the primary visual cortex are known for responding vigorously but with high variability to classical stimuli such as drifting bars or gratings. By contrast, natural scenes are encoded more efficiently by sparse and temporal precise spiking responses. We used a conductance-based model of the visual system in higher mammals to investigate how two specific features of the thalamo-cortical pathway, namely push-pull receptive field organization and synaptic depression, can contribute to this contextual reshaping of V1 responses. By comparing cortical dynamics evoked respectively by natural vs. artificial stimuli in a comprehensive parametric space analysis, we demonstrate that the reliability and sparseness of the spiking responses during natural vision is not a mere consequence of the increased bandwidth in the sensory input spectrum. Rather, it results from the combined impacts of synaptic depression and push-pull inhibition, the later acting for natural scenes as a form of ``effective&rsquo;&rsquo; feed-forward inhibition as demonstrated in other sensory systems. Thus, the combination of feedforward-like inhibition with fast thalamo-cortical synaptic depression by simple cells receiving a direct structured input from thalamus composes a generic computational mechanism for generating a sparse and reliable encoding of natural sensory events.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/hal-02062034" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://journal.frontiersin.org/article/10.3389/fncir.2016.00037/full" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fncir.2016.00037" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/kiana-mansour-pour/">Kiana Mansour-Pour</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of ECVP</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mansour-16-ecvp/" >Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/mansour-16-ecvp" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mansour-16-ecvp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/kiana-mansour-pour/">Kiana Mansour-Pour</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>GDR Vision, Toulouse, Nov 3rd, 2016</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mansour-16-gdr/" >Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/mansour-16-gdr" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mansour-16-gdr/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/kiana-mansour-pour/">Kiana Mansour-Pour</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/mansour-16-sfn/" >Voluntary tracking the moving clouds : Effects of speed variability on human smooth pursuit</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/mansour-16-ecvp" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/mansour-16-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/matthias-s-keil/">Matthias S Keil</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Biologically Inspired Computer Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/cristobal-perrinet-keil-15-bicv-chap-1/" >Introduction</a>
  </div>

  
  <a href="/publication/cristobal-perrinet-keil-15-bicv-chap-1/"  class="summary-link">
    <div class="article-style">
      <p>This is the introductory chapter of the book, which serves as a comprehensive but rigorous reference in the area of biologically inspired computer vision modeling. Biological vision shows excellence in terms of performance and robustness. Biologically inspired vision, that is, the study of visual systems of living beings, can be considered as a two-way process. The book often follows Marr&rsquo;s classical, three-level approach to vision, but also goes beyond Marr&rsquo;s approach in the design of novel and more advanced vision sensors. It provides an overview of a few representative applications and current state of the art of the research in this area. The book also provides an overview of bioinspired computer vision, starting from fundamentals to the most recent advances and applications in the field.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://bicv.github.io/chap1/" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://bicv.github.io/chap1/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cristobal-perrinet-keil-15-bicv-chap-1/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1002/9783527680863.ch1" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Biologically Inspired Computer Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-15-bicv/" >Sparse Models for Computer Vision</a>
  </div>

  
  <a href="/publication/perrinet-15-bicv/"  class="summary-link">
    <div class="article-style">
      <p>The representation of images in the brain is known to be sparse. That is, as neural activity is recorded in a visual area, for instance the primary visual cortex of primates, only a few neurons are active at a given time with respect to the whole population. It is believed that such a property reflects the efficient match of the representation with the statistics of natural scenes. Applying such a paradigm to computer vision therefore seems a promising approach towards more biomimetic algorithms. Herein, we will describe a biologically-inspired approach to this problem. First, we will describe an unsupervised learning paradigm which is particularly adapted to the efficient coding of image patches. Then, we will outline a complete multi-scale framework (SparseLets) implementing a biologically inspired sparse representation of natural images. Finally, we will propose novel methods for integrating prior information into these algorithms and provide some preliminary experimental results. We will conclude by giving some perspective on applying such algorithms to computer vision. More specifically, we will propose that bio-inspired approaches may be applied to computer vision using predictive coding schemes, sparse models being one simple and efficient instance of such schemes.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1701.06859" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://onlinelibrary.wiley.com/doi/10.1002/9783527680863.ch14/summary" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-15-bicv/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/bicv/Perrinet2015BICV_sparse" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1002/9783527680863.ch14" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Biologically Inspired Computer Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/montagnini-15-bicv/" >Visual motion processing and human tracking behavior</a>
  </div>

  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1611.07831" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/montagnini-15-bicv/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-15-bicv/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1002/9783527680863.ch12" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/matthias-s-keil/">Matthias S Keil</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/cristobal-perrinet-keil-15-bicv/" >
      <div class="img-hover-zoom">
        <img src="/publication/cristobal-perrinet-keil-15-bicv/featured_hu5bd691c9e658dc93cf8e7980a3662f0d_45531_9b08f9c6534cc9d4890adb5f79d9b298.webp" height="455" width="808"
            class="article-banner" alt="Biologically Inspired Computer Vision" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/cristobal-perrinet-keil-15-bicv/" >Biologically Inspired Computer Vision</a>
  </div>

  
  <a href="/publication/cristobal-perrinet-keil-15-bicv/"  class="summary-link">
    <div class="article-style">
      <p>As the state-of-the-art imaging technologies became more and more advanced, yielding scientific data at unprecedented detail and volume, the need to process and interpret all the data has made image processing and computer vision also increasingly important. Sources of data that have to be routinely dealt with today applications include video transmission, wireless communication, automatic fingerprint processing, massive databases, non-weary and accurate automatic airport screening, robust night vision to name a few. Multidisciplinary inputs from other disciplines such as computational neuroscience, cognitive science, mathematics, physics and biology will have a fundamental impact in the progress of imaging and vision sciences. One of the advantages of the study of biological organisms is to devise very different type of computational paradigms beyond the usual von Neumann e.g. by implementing a neural network with a high degree of local connectivity. This is a comprehensive and rigorous reference in the area of biologically motivated vision sensors. The study of biologically visual systems can be considered as a two way avenue. On the one hand, biological organisms can provide a source of inspiration for new computational efficient and robust vision models and on the other hand machine vision approaches can provide new insights for understanding biological visual systems. Along the different chapters, this book covers a wide range of topics from fundamental to more specialized topics, including visual analysis based on a computational level, hardware implementation, and the design of new more advanced vision sensors. The last two sections of the book provide an overview of a few representative applications and current state of the art of the research in this area. This makes it a valuable book for graduate, Master, PhD students and also researchers in the field.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://bicv.github.io/" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://onlinelibrary.wiley.com/book/10.1002/9783527680863" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cristobal-perrinet-keil-15-bicv/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://bicv.github.io/toc/" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1002/9783527680863" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of VSS</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/damasse-15-vss/" >Anticipatory smooth eye movements and reinforcement</a>
  </div>

  
  <a href="/publication/damasse-15-vss/"  class="summary-link">
    <div class="article-style">
      <p>When an object is moving in the visual field, we are able to accurately track it with a combination of saccades and smooth eye movements. These movements allow us to align and stabilize the object on the fovea, thus enabling visual analysis with high acuity. Importantly, when predictive information is available about the target motion, anticipatory smooth pursuit eye movements (aSPEM) are efficiently generated before target appearance, which reduce the typical sensorimotor delay between target motion onset and foveation. By manipulating the probability for target motion direction we were able to bias the direction and mean velocity of aSPEM (baseline condition). This suggests that probabilistic information may be used to inform the internal representation of motion prediction for the initiation of anticipatory movements. To further understand the nature of this process, we investigate the effects of reinforcement on aSPEM with two distinct experiments. First, it has been previously shown that several properties of eye movements can be modulated by reinforcement paradigms based on monetary reward (Madelain et al. 2011). We adapted and extended this framework to prediction-based aSPEM, by associating a monetary reward to a criterion-matching anticipatory velocity, in the gap before the target onset. Second, it has also been reported that accurate perception per se can play the role of an efficient ecological reinforcer for visually guided saccades (Montagnini &amp; Chelazzi, 2005). With a gaze-contingent procedure, we manipulated the discriminability of a perceptual target (appearing during the pursuit trial and followed by a discrimination task) The difficulty level of this task has been matched depending on the velocity of aSPEM. This experiment taps on the very reason to produce anticipatory tracking movement, that is to grant a quicker high-acuity vision of the moving target. We compare predictive anticipatory eye movements across these conditions.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://jov.arvojournals.org/article.aspx?articleid=2434129" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-15-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/15.12.1019" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/james-a-bednar/">James A Bednar</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>European Signal Processing Conference 2015 (EUSIPCO 2015)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-15-eusipco/" >Sparse Coding Of Natural Images Using A Prior On Edge Co-Occurences</a>
  </div>

  
  <a href="/publication/perrinet-15-eusipco/"  class="summary-link">
    <div class="article-style">
      <p>Oriented edges in images of natural scenes tend to be aligned in co-linear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (the good continuation law of Gestalt psychology). The visual system appears to take advantage of this prior knowledge about natural images, with human contour detection and grouping performance well predicted by such an asociation field between edge elements. Geisler et al (2001) have estimated this prior information available to the visual system by extracting contours from a database of natural images, and showed that these statistics could predict behavioral data from humans in a line completion task. In this paper, we show that an association field of this type can be used for the sparse representation of natural images.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1109/EUSIPCO.2015.7362781" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-15-eusipco/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/EUSIPCO.2015.7362781" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jonathan-vacher/">Jonathan Vacher</a></span>, <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-peyre/">Gabriel Peyré</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>ICMS 2015 conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/vacher-15-icms/" >A Mathematical Account of Dynamic Texture Synthesis for Probing Visual Perception</a>
  </div>

  
  <a href="/publication/vacher-15-icms/"  class="summary-link">
    <div class="article-style">
      <p> See a followup in Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation. Preprint PDF Cite DOI </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vacher-15-icms/cite.bib">
  Cite
</a>





  
    
  











  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/montagnini-15-sfn/" >Anticipating a moving target: role of vision and reinforcement</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/montagnini-15-sfn" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-15-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of GDR Vision (Lyon, France)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/damasse-15-gdr/" >Anticipatory smooth eye movements as operant behavior</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/damasse-15-gdr" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-15-gdr/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jonathan-vacher/">Jonathan Vacher</a></span>, <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-peyre/">Gabriel Peyré</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Advances in Neural Information Processing Systems</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/vacher-15-nips/" >
      <div class="img-hover-zoom">
        <img src="/publication/vacher-15-nips/featured_hu9e07d39375c2c772de3ad9b884d37b1c_750952_e2d5e40bcc9b9c6b8bdf17cf94bff22d.webp" height="455" width="808"
            class="article-banner" alt="Biologically Inspired Dynamic Textures for Probing Motion Perception" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/vacher-15-nips/" >Biologically Inspired Dynamic Textures for Probing Motion Perception</a>
  </div>

  
  <a href="/publication/vacher-15-nips/"  class="summary-link">
    <div class="article-style">
      <p>Talk @ NeurIPS: https://neurips.cc/Conferences/2015/Schedule?showEvent=5418 See a followup in Jonathan Vacher, Andrew Isaac Meso, Laurent U Perrinet, Gabriel Peyré (2018). Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures. Neural Computation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1511.02705" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://papers.nips.cc/paper/5769-biologically-inspired-dynamic-textures-for-probing-motion-perception.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vacher-15-nips/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/james-a-bednar/">James A Bednar</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Scientific Reports</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-bednar-15/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-bednar-15/featured_hu5438825b9b6d1014226d20d231e650c2_24954_3ff108c4c08ba231745b146b22972224.webp" height="455" width="808"
            class="article-banner" alt="Edge co-occurrences can account for rapid categorization of natural versus animal images" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-bednar-15/" >Edge co-occurrences can account for rapid categorization of natural versus animal images</a>
  </div>

  
  <a href="/publication/perrinet-bednar-15/"  class="summary-link">
    <div class="article-style">
      <p>Making a judgment about the semantic category of a visual scene, such as whether it contains an animal, is typically assumed to involve high-level associative brain areas. Previous explanations require progressively analyzing the scene hierarchically at increasing levels of abstraction, from edge extraction to mid-level object recognition and then object categorization. Here we show that the statistics of edge co-occurrences alone are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. We first extracted the edges from images using a scale-space analysis coupled with a sparse coding algorithm. We then computed the ``association field&rsquo;&rsquo; for different categories (natural, man-made, or containing an animal) by computing the statistics of edge co-occurrences. These differed strongly, with animal images having more curved configurations. We show that this geometry alone is sufficient for categorization, and that the pattern of errors made by humans is consistent with this procedure. Because these statistics could be measured as early as the primary visual cortex, the results challenge widely held assumptions about the flow of computations in the visual system. The results also suggest new algorithms for image classification and signal processing that exploit correlations between low-level structure and the underlying semantic category.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01202447" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.nature.com/articles/srep11400" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-bednar-15/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/PerrinetBednar15" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/srep11400" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/frederic-danion/">Fréderic Danion</a></span>, <span >
      <a href="/author/caroline-landelle/">Caroline Landelle</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/danion-15-sfn/" >Eye tracking a self-moved target with complex hand-target dynamics</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/danion-15-sfn" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/danion-15-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/pascal-wallisch/">Pascal Wallisch</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>ICMNS 2015 conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/taouali-15-icmns/" >On overdispersion in neuronal evoked activity</a>
  </div>

  
  <a href="/publication/taouali-15-icmns/"  class="summary-link">
    <div class="article-style">
      <p>The repeated presentation of an identical visual stimulus in the receptive field of a neuron may evoke different spiking patterns at each trial. Probabilistic methods are essential to understand its functional role within the neural activity. In that case, a Poisson process is the most common model of trial-to-trial variability. However, the variance of the spike count is constrained to be equal to the mean, irrespective of measurement&rsquo;s duration. Numerous studies have shown that this relationship does not generally hold. Specifically, a majority of electrophysiological recordings show an ``em overdispersion&rsquo;&rsquo; effect: Responses that exhibit more inter-trial variability than expected from a Poisson process alone. A model that is particularly well suited to quantify overdispersion is the Negative-Binomial distribution model. This model is largely applied and studied but has only recently been applied to neuroscience. In this paper, we address three main issues. First, we describe how the Negative-Binomial distribution provides a model apt to account for overdispersed spike counts. Second, we quantify the significance of this model for any neurophysiological data by proposing a statistical test, which quantifies the odds that overdispersion could be due to the limited number of repetitions (trials). We apply this test to three neurophysiological tests along the visual pathway. Finally, we compare the performance of this model to the Poisson model on a population decoding task. This shows that more knowledge about the form of dispersion tuning is necessary to have a significant gain, uncovering a possible feature of the neural spiking code.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taouali-15-icmns/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/cesar-u-ravello/">Cesar U Ravello</a></span>, <span >
      <a href="/author/f.-olivares/">F. Olivares</a></span>, <span >
      <a href="/author/r.-herzog/">R. Herzog</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/maria-jose-escobar/">Maria Jose Escobar</a></span>, <span >
      <a href="/author/adrian-g-palacios/">Adrián G Palacios</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2015
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>European Retina Meeting 2015</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/ravello-15/" >Spatiotemporal tuning of retinal ganglion cells dependent on the context of signal presentation</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ravello-15/cite.bib">
  Cite
</a>





  
    
  











  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Biological Cybernetics</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-adams-friston-14/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-adams-friston-14/featured_huef098251f70535624ec23ca357d53cdd_225439_5a9b126a1851c1c2804c0f12cf51a063.webp" height="455" width="808"
            class="article-banner" alt="Active inference, eye movements and oculomotor delays" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-adams-friston-14/" >Active inference, eye movements and oculomotor delays</a>
  </div>

  
  <a href="/publication/perrinet-adams-friston-14/"  class="summary-link">
    <div class="article-style">
      <p>This paper considers the problem of sensorimotor delays in the optimal control of (smooth) eye movements under uncertainty. Specifically, we consider delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple way of compensating for both sensory and oculomotor delays. The efficacy of this scheme is illustrated using neuronal simulations of pursuit initiation responses, with and without compensation. We then consider an extension of the generative model to simulate smooth pursuit eye movements in which the visuo-oculomotor system believes both the target and its centre of gaze are attracted to a (hidden) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can recognise and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1610.05564" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://link.springer.com/article/10.1007%2Fs00422-014-0620-8" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-adams-friston-14/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00422-014-0620-8" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/meso-14-vss/" >Beyond simply faster and slower: exploring paradoxes in speed perception</a>
  </div>

  
  <a href="/publication/meso-14-vss/"  class="summary-link">
    <div class="article-style">
      <p>Estimating object speed in visual scenes is a critical part of perception. While various aspects of speed computation including discrimination thresholds, neural mechanisms and spatial integration mechanisms have been studied, there remain areas to elucidate. One is the integration of information across spatio-temporal frequency channels to compute speed. We probe this integration with a 2-AFC psychophysical task in which moving random phase noise stimuli are used with experimenter defined frequency parameters and bandwidths to target specific neural populations. They are presented for 300ms in a large square aperture with smooth eye movements recorded while speed discrimination judgements are made over two intervals. There is no instruction to observers to pursue the stimuli and no pre trial saccade to induce a classic ocular following response. After a latency, eye movements follow the stimulated direction presumably to facilitate the speed judgement. Within each of the two intervals, we randomly vary a range of spatial frequency and speed parameters respectively such that stimuli at the centre of the ranges are identical. The aim is to characterise the speed response of the eye movements recorded in a context which creates an ocular motor &lsquo;action&rsquo; during a perceptual task instead of artificially separating the two. Within the speed varied intervals, averaged eye movements are systematically modulated in strength by stimulus speed. Within the spatial frequency intervals, higher frequencies perceived as faster in discrimination responses interestingly show no corresponding strengthening of eye responses particularly at higher contrasts where they may be weaker. Thus for a pair of stimuli matched for contrast and perceived speed, this early eye response appears to be driven by a contrast dependent low level motion energy like computation. We characterise an underlying spatial frequency response which is shifted towards lower frequencies, unlike the perceptual responses and is probably separate from perception.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1167/14.10.491" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/meso-14-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/14.10.491" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/james-a-bednar/">James A Bednar</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-bednar-14-vss/" >Edge co-occurrences are sufficient to categorize natural versus animal images</a>
  </div>

  
  <a href="/publication/perrinet-bednar-14-vss/"  class="summary-link">
    <div class="article-style">
      <p>Analysis and interpretation of a visual scene to extract its category, such as whether it contains an animal, is typically assumed to involve higher-level associative brain areas. Previous proposals have been based on a series of processing steps organized in a multi-level hierarchy that would progressively analyze the scene at increasing levels of abstraction, from contour extraction to low-level object recognition and finally to object categorization (Serre, PNAS 2007). We explore here an alternative hypothesis that the statistics of edge co-occurences are sufficient to perform a rough yet robust (translation, scale, and rotation invariant) scene categorization. The method is based on a realistic model of image analysis in the primary visual cortex that extends previous work from Geisler et al. (Vis. Res. 2001). Using a scale-space analysis coupled with a sparse coding algorithm, we achieved detailed and robust extraction of edges in different sets of natural images. This edge-based representation allows for a simple characterization of the ``association field&rsquo;&rsquo; of edges by computing the statistics of co-occurrences. We show that the geometry of angles made between edges is sufficient to distinguish between different sets of natural images taken in a variety of environments (natural, man-made, or containing an animal). Specifically, a simple classifier, working solely on the basis of this geometry, gives performance similar to that of hierarchical models and of humans in rapid-categorization tasks. Such results call attention to the importance of the relative geometry of local image patches in visual computation, with implications for designing efficient image analysis systems. Most importantly, they challenge assumptions about the flow of computations in the visual system and emphasize the relative importance in this process of associative connections, and in particular of intra-areal lateral connections.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1167/14.10.1310" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-bednar-14-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/14.10.1310" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/khoei-14-vss/" >Motion-based prediction model for flash lag effect</a>
  </div>

  
  <a href="/publication/khoei-14-vss/"  class="summary-link">
    <div class="article-style">
      <p>The flash lag effect (FLE) is a well known visual illusion that reveals the perceptual difference in position coding of moving and stationary flashed objects. It has been reproduced experimentally in retina and V1 along with some relevant evidences about motion based position coding in areas MT and MT+. Numerous hypotheses for mechanisms underlying FLE such as motion extrapolation, latency difference, position persistence, temporal averaging and postdiction have been under debate for last two decades. Here, we have challenged our previous motion-based prediction model to understand FLE, consistently with the motion extrapolation account proposed by Nijhawan. Our hypothesis is based on predictability of motion trajectory and importance of motion signal in manipulation of receptive field shape for moving objects. Using a probabilistic framework, we have implemented motion-based prediction (MBP) and simulated three different demonstrations of FLE including standard, flash initiated and flash terminated cycles. This method allowed us to compare the shape of the characteristic receptive fields for moving and stationary flashed dots in the case of rightward and leftward motions. As control model, we have eliminated velocity signal from motion estimation and simulated position-based (PX) model of FLE. Results of MBP model suggest that above a minimal time for duration of flash, the development of predictive component for the moving object is sufficient to shift in the direction of motion and to produce flash lag effect. MBP model reproduces experimental data of FLE and its dependence to the contrast of flash. Against what has been argued as shortage of motion extrapolation account, in our results spatial lead of moving object is also evident in flash initiated cycle. Our model, without being restricted to one special visual area, provides a generic account for FLE by emphasize on different manipulation of stationary objects and trajectory motion by the sensory system.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1167/14.10.471" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-14-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/14.10.471" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-14-vss/" >The characteristics of microsaccadic eye movements varied with the change of strategy in a match-to-sample task</a>
  </div>

  
  <a href="/publication/simoncini-14-vss/"  class="summary-link">
    <div class="article-style">
      <p>Under natural viewing conditions, large eye movements are interspace by small eye movements (microsaccade). Recent works have shown that these two kinds of eye movements are generate by the same oculomotor mechanisms (Goffart et al., 2012) and are driven from the same visual information (Simoncini et al., VSS 2012 abstract). These results seem to demonstrate that microsaccade and saccade represent a continuum of the same ocular movement. However, if the role played in vision perception by large saccades is clearly identified, the role of the microsaccade is not clearly defined. In order to investigate the role of microsaccade, we measured pattern discrimination performance using an ABX match-to-sample task during the presentation of 1/f natural statistics texture where we varied the spatial frequency contents. We compared perceptual performance with eye movements recorded during the task. We found that the rate of microsaccadic movements changed as a function of the subjects task strategy. In particular, in the trials where the perception of the difference between the stimuli was simple (low spatial frequency) the subjects used the information provided by all the stimuli to do the task and the microsaccadic rate for all the stimuli (ABX) was the same. However, when the perception of the difference between the stimuli was harder (for instance for high spatial frequency), the subjects rather used the information provided by the last two stimuli only and the microsaccadic rate for the image BX increased respect at the image A. These results demonstrate that microsaccadic eye movements also play a role during the analysis of the visual scene and that such experiments can help decipher their participation to perception of the scene. Goffart L., Hafed Z.M., Krauzlis R.J. 2012. Visual fixation as equilibrium: evidence from superior colliculus inactivation. (31) 10627-10636.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1167/14.10.110" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-14-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/14.10.110" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/bernhard-a-kaplan/">Bernhard A Kaplan</a></span>, <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/anders-lansner/">Anders Lansner</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE International Joint Conference on Neural Networks (IJCNN) 2014 Beijing, China</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kaplan-khoei-14/" >Signature of an anticipatory response in area V1 as modeled by a probabilistic model and a spiking neural network</a>
  </div>

  
  <a href="/publication/kaplan-khoei-14/"  class="summary-link">
    <div class="article-style">
      <p>As it is confronted to inherent neural delays, how does the visual system create a coherent representation of a rapidly changing environment? In this paper, we investigate the role of motion-based prediction in estimating motion trajectories compensating for delayed information sampling. In particular, we investigate how anisotropic diffusion of information may explain the development of anticipatory response as recorded in a neural population to an approaching stimulus. We validate this using an abstract probabilistic framework and a spiking neural network (SNN) model. Inspired by a mechanism proposed by Nijhawan [1], we first use a Bayesian particle filter framework and introduce a diagonal motion-based prediction model which extrapolates the estimated response to a delayed stimulus in the direction of the trajectory. In the SNN implementation, we have used this pattern of anisotropic, recurrent connections between excitatory cells as mechanism for motion-extrapolation. Consistent with recent experimental data collected in extracellular recordings of macaque primary visual cortex [2], we have simulated different trajectory lengths and have explored how anticipatory responses may be dependent on the information accumulated along the trajectory. We show that both our probabilistic framework and the SNN model can replicate the experimental data qualitatively. Most importantly, we highlight requirements for the development of a trajectory-dependent anticipatory response, and in particular the anisotropic nature of the connectivity pattern which leads to the motion extrapolation mechanism.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/kaplan-khoei-14" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kaplan-khoei-14/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/IJCNN.2014.6889847" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/taouali-14-areadne/" >A Simple Model of Orientation Encoding Accounting For Multivariate Neural Noise</a>
  </div>

  
  <a href="/publication/taouali-14-areadne/"  class="summary-link">
    <div class="article-style">
      <p> see a follow-up in this publication </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taouali-14-areadne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>6th Workshop of the Computational Neuroscience Network in Marseille</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/taouali-14-neurocomp/" >A Simple Model of Orientation Encoding Accounting For Multivariate Neural Noise</a>
  </div>

  
  <a href="/publication/taouali-14-neurocomp/"  class="summary-link">
    <div class="article-style">
      <p> see a follow-up in this publication </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taouali-14-neurocomp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jonathan-vacher/">Jonathan Vacher</a></span>, <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-peyre/">Gabriel Peyré</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IHP workshop</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/vacher-14-ihp/" >Dynamic Textures For Probing Motion Perception</a>
  </div>

  
  <a href="/publication/vacher-14-ihp/"  class="summary-link">
    <div class="article-style">
      <p>This work extends the MotionClouds dynamic texture model testing aspects of its parametrization with an application in psychophysics.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vacher-14-ihp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of GDR Vision (Lyon, France)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/damasse-14-gdr/" >On the nature of anticipatory eye movements and the factors affecting them</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/damasse-14-gdr" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-14-gdr/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/p-philipp-rudiger/">P Philipp Rudiger</a></span>, <span >
      <a href="/author/jean-luc-stevens/">Jean-Luc Stevens</a></span>, <span >
      <a href="/author/bharath-chandra-talluri/">Bharath Chandra Talluri</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/james-a-bednar/">James A Bednar</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2014
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of COSYNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/rudiger-14-cosyne/" >Relationship between natural image statistics and lateral connectivity in the primary visual cortex</a>
  </div>

  
  <a href="/publication/rudiger-14-cosyne/"  class="summary-link">
    <div class="article-style">
      <p> see a follow-up: Laurent U Perrinet, James A Bednar (2015). Edge co-occurrences can account for rapid categorization of natural versus animal images. Scientific Reports. Preprint PDF Cite Code DOI </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://goo.gl/RJpJR4" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rudiger-14-cosyne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Physiology-Paris</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/khoei-13-jpp/" >Motion-based prediction explains the role of tracking in motion extrapolation</a>
  </div>

  
  <a href="/publication/khoei-13-jpp/"  class="summary-link">
    <div class="article-style">
      <p>During normal viewing, the continuous stream of visual input is regularly interrupted, for instance by blinks of the eye. Despite these frequents blanks (that is the transient absence of a raw sensory source), the visual system is most often able to maintain a continuous representation of motion. For instance, it maintains the movement of the eye such as to stabilize the image of an object. This ability suggests the existence of a generic neural mechanism of motion extrapolation to deal with fragmented inputs. In this paper, we have modeled how the visual system may extrapolate the trajectory of an object during a blank using motion-based prediction. This implies that using a prior on the coherency of motion, the system may integrate previous motion information even in the absence of a stimulus. In order to compare with experimental results, we simulated tracking velocity responses. We found that the response of the motion integration process to a blanked trajectory pauses at the onset of the blank, but that it quickly recovers the information on the trajectory after reappearance. This is compatible with behavioral and neural observations on motion extrapolation. To understand these mechanisms, we have recorded the response of the model to a noisy stimulus. Crucially, we found that motion-based prediction acted at the global level as a gain control mechanism and that we could switch from a smooth regime to a binary tracking behavior where the dot is tracked or lost. Our results imply that a local prior implementing motion-based prediction is sufficient to explain a large range of neural and behavioral results at a more global level. We show that the tracking behavior deteriorates for sensory noise levels higher than a certain value, where motion coherency and predictability fail to hold longer. In particular, we found that motion-based prediction leads to the emergence of a tracking behavior only when enough information from the trajectory has been accumulated. Then, during tracking, trajectory estimation is robust to blanks even in the presence of relatively high levels of noise. Moreover, we found that tracking is necessary for motion extrapolation, this calls for further experimental work exploring the role of noise in motion extrapolation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-13-jpp/khoei-13-jpp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-13-jpp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2013.08.001" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/bernhard-a-kaplan/">Bernhard A Kaplan</a></span>, <span >
      <a href="/author/anders-lansner/">Anders Lansner</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Frontiers in Computational Neuroscience</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kaplan-13/" >Anisotropic connectivity implements motion-based prediction in a spiking neural network</a>
  </div>

  
  <a href="/publication/kaplan-13/"  class="summary-link">
    <div class="article-style">
      <p>Predictive coding hypothesizes that the brain explicitly infers upcoming sensory input to establish a coherent representation of the world. Although it is becoming generally accepted, it is not clear on which level spiking neural networks may implement predictive coding and what function their connectivity may have. We present a network model of conductance-based integrate-and-fire neurons inspired by the architecture of retinotopic cortical areas that assumes predictive coding is implemented through network connectivity, namely in the connection delays and in selectiveness for the tuning properties of source and target cells. We show that the applied connection pattern leads to motion-based prediction in an experiment tracking a moving dot. In contrast to our proposed model, a network with random or isotropic connectivity fails to predict the path when the moving dot disappears. Furthermore, we show that a simple linear decoding approach is sufficient to transform neuronal spiking activity into a probabilistic estimate for reading out the target trajectory.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/kaplan-13" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kaplan-13/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fncom.2013.00112" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Annual Computational Neuroscience Meeting: CNS 2013, Paris</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-13-cns/" >Active inference, eye movements and oculomotor delays</a>
  </div>

  
  <a href="/publication/perrinet-13-cns/"  class="summary-link">
    <div class="article-style">
      <p>We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-13-cns" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-13-cns/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>The 7th Japanese-French Frontiers of Science Symposium</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-13-jffos/" >Active inference, eye movements and oculomotor delays</a>
  </div>

  
  <a href="/publication/perrinet-13-jffos/"  class="summary-link">
    <div class="article-style">
      <p>We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-13-jffos" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-13-jffos/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/rodrigo-nava/">Rodrigo Nava</a></span>, <span >
      <a href="/author/j-victor-marcos/">J Victor Marcos</a></span>, <span >
      <a href="/author/boris-escalante-ramirez/">Boris Escalante-Ramirez</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/raul-s-j-estepar/">Raúl S J Estépar</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/nava-13/" >Advances in Texture Analysis for Emphysema Classification</a>
  </div>

  
  <a href="/publication/nava-13/"  class="summary-link">
    <div class="article-style">
      <p>In recent years, with the advent of High-resolution Computed Tomography (HRCT), there has been an increased interest for diagnosing Chronic Obstructive Pulmonary Disease (COPD), which is commonly presented as emphysema. Since low-attenuation areas in HRCT images describe different emphysema patterns, the discrimination problem should focus on the characterization of both local intensities and global spatial variations. We propose a novel texture-based classification framework using complex Gabor filters and local binary patterns. We also analyzed a set of global and local texture descriptors to characterize emphysema morphology. The results have shown the effectiveness of our proposal and that the combination of descriptors provides robust features that lead to an improvement in the classification rate.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1007/978-3-642-41827-3_27" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/nava-13/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-642-41827-3_27" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>VSS Conference Abstract</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/meso-13-vss/" >How and why do image frequency properties influence perceived speed?</a>
  </div>

  
  <a href="/publication/meso-13-vss/"  class="summary-link">
    <div class="article-style">
      <p>Humans are able to interact successfully with moving objects in our dynamic world and the visual system effi ciently performs the motion computation that makes this possible. Object speed and direction are estimated following the integration of information across cortical motion sensitive channels. Speed estimation along this system is not fully understood, particularly the mapping function between the actual speed of viewed objects and that perceived by observers, a question we address in this work. It has been demonstrated that perceived speed is profoundly influenced by object contrast, spatial frequency, stimulus complexity and frequency bandwidth. In a 2 interval forced choice speed discrimination task, we present a random phase textured motion stimulus to probe small shifts in perceived speed measured using fi xed stimulus sets as reference scales while mean spatial frequency and bandwidths serve as the dependent variable in a probe. The presentations are short (200ms). Using a scale of narrowband stimuli (0.2 octaves), we measured a shift in perceived speed; higher frequencies are seen as faster moving than lower ones. On the scale of broader bandwidth (1 octave), this difference across frequency was reduced and perceived speed seems to converge on a slower representation. From these results we estimated this mapping between perceived and veridical stimulus speeds. In direct comparisons, the relative speed is faster for high frequencies and increases in bandwidth make stimuli appear slower. During this early computation, when presented with a random phase stimulus it appears that the visual systems makes assumptions about expected speeds based on the richness of the frequency content and the veridical speed is not explicitly computed. In this first 200ms, the perceptual system perhaps underestimates some speeds in an optimal response for initially stabilizing the scene. Acknowledgement: CNRS &amp; Brainscales FP7</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/meso-13-vss" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/meso-13-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/13.9.354" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>VSS Conference Abstract</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-13-vss/" >Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception</a>
  </div>

  
  <a href="/publication/simoncini-13-vss/"  class="summary-link">
    <div class="article-style">
      <p>The visual system does not process information instantaneously, but rather integrates over time. Integration occurs both for stationary objects and moving objects, with very similar time constants (Burr, 1981). We measured, as a function of exposure duration, speed discrimination and ocular following performance for rich textured motion stimuli of varying spatial frequency bandwidth. Psychometric sensitivity and Oculometric sensitivity for these patterns increased with exposure duration. However the best stimuli for ocular following (namely those with a large bandwidth for spatial frequency) was well integrated up to about 150 - 200 msec, while the best stimuli for speed discrimination (small bandwidth) was well integrated up to about 300 msec. Interestingly, discriminability of ocular tracking eye movements follow a non-monotonic time course, due to the contribution of motor noise. These results suggest that although perception and action relies work in synergy, they may be described by two different integrating mechanisms: A low level, fast one guiding the ocular movement to enable one to catch stimuli in the visual fi eld quickly; and a slower one being able to measure the speed difference between two objects translating in the visual fi eld. Burr, D.C. (1981). Temporal summation of moving images by the human visual system. Proceedings of Royal Society, B211, 321 - 339</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-13-vss/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2013
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Annual Computational Neuroscience Meeting: CNS</em>2013, Paris*
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/khoei-13-cns/" >Motion-based prediction and development of the response to an &#39;on the way&#39; stimulus</a>
  </div>

  
  <a href="/publication/khoei-13-cns/"  class="summary-link">
    <div class="article-style">
      <p>Based on Laurent U Perrinet, Guillaume S Masson (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation. Preprint PDF Cite see follow-up on motion extrapolation: Mina A Khoei, Guillaume S Masson, Laurent U Perrinet (2013).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-13-cns" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-13-cns/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1186/1471-2202-14-S1-P314" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>PLoS ONE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/adams-12/" >Smooth Pursuit and Visual Occlusion: Active Inference and Oculomotor Control in Schizophrenia</a>
  </div>

  
  <a href="/publication/adams-12/"  class="summary-link">
    <div class="article-style">
      <p>This paper introduces a model of oculomotor control during the smooth pursuit of occluded visual targets. This model is based upon active inference, in which subjects try to minimise their (proprioceptive) prediction error based upon posterior beliefs about the hidden causes of their (exteroceptive) sensory input. Our model appeals to a single principle -the minimisation of variational free energy - to provide Bayes optimal solutions to the smooth pursuit problem. However, it tries to accommodate the cardinal features of smooth pursuit of partially occluded targets that have been observed empirically in normal subjects and schizophrenia. Specifically, we account for the ability of normal subjects to anticipate periodic target trajectories and emit pre-emptive smooth pursuit eye movements -prior to the emergence of a target from behind an occluder. Furthermore, we show that a single deficit in the postsynaptic gain of prediction error units (encoding the precision of posterior beliefs) can account for several features of smooth pursuit in schizophrenia: namely, a reduction in motor gain and anticipatory eye movements during visual occlusion, a paradoxical improvement in tracking unpredicted deviations from target trajectories and a failure to recognise and exploit regularities in the periodic motion of visual targets. This model will form the basis of subsequent (dynamic causal) models of empirical eye tracking measurements, which we hope to validate, using psychopharmacology and studies of schizophrenia.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/adams-12/adams-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/adams-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pone.0047502" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>VSS Conference Abstract</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/simoncini-11-vss/" >
      <div class="img-hover-zoom">
        <img src="/publication/simoncini-11-vss/featured_hu342c8207a6600516b049009850264ef1_110610_18d984c78d01df50193a8cef54bac18b.webp" height="455" width="808"
            class="article-banner" alt="Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-11-vss/" >Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize</a>
  </div>

  
  <a href="/publication/simoncini-11-vss/"  class="summary-link">
    <div class="article-style">
      <p>In order to analyze the characteristics of a rich dynamic visual environment, the visual system must integrate information collected at different scales through different spatiotemporal frequency channels. Still, it remains unclear how reliable representations of motion direction or speed are elaborated when presented with large bandwidth motion stimuli or natural statistics. Last year, we have shown that broadening the spatiotemporal frequency content of a textured pattern moving at constant speed leads to different results on a reflexive tracking task and a speed discrimination task. Larger bandwidth stimuli increase response amplitude and sensitivity of ocular following, consistently with a maximum-likelihood (ML) model of motion decoding. In contrast, larger bandwidth stimuli impair speed discrimination performance, suggesting that the perceptual system cannot take advantage of such additional, redundant information. Instead of ML, a gain control decoding mechanism can explain the drop in performance, suggesting that action and perception rely on different decoding mechanisms. To further investigate such task-dependant pooling of motion information, we measured pattern discrimination performance using these textured stimuli. Two noise patterns were presented sequentially for 250 ms on a CRT monitor (1280 × 1024 @ 100 Hz) and covered 47$,^∘$ of visual angle with identical properties (mean SF, bandwidth SF, speed) except for a randomized phase spectrum. A test pattern was then presented and subjects were asked to match it with one or the other reference stimulus (ABX task). At small bandwidth and optimal mean spatial frequency (0.3 cpd), subjects were able to discriminate the two patterns with high accuracy. Performance dropped to chance level as spatial frequency bandwidth increased. Increasing the mean spatial frequency decreased the overall performance. Again, these results suggest that perceptual performance is deteriorated in presence of larger information.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.journalofvision.org/content/12/9/1014.abstract?sid=9c51ff88-5b9a-4d1b-aaf1-a1219bd02b0a" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-11-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/11.11.749" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neuroscience and biobehavioral reviews</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/masson-12/" >The behavioral receptive field underlying motion integration for primate tracking eye movements</a>
  </div>

  
  <a href="/publication/masson-12/"  class="summary-link">
    <div class="article-style">
      <p>Short-latency ocular following are reflexive, tracking eye movements that are observed in human and non-human primates in response to a sudden and brief translation of the image. Initial, open-loop part of the eye acceleration reflects many of the properties attributed to low-level motion processing. We review a very large set of behavioral data demonstrating several key properties of motion detection and integration stages and their dynamics. We propose that these properties can be modeled as a behavioral receptive field exhibiting linear and nonlinear mechanisms responsible for context-dependent spatial integration and gain control. Functional models similar to that used for describing neuronal properties of receptive fields can then be applied successfully.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/masson-12/masson-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/masson-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.neubiorev.2011.03.009" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/paula-sanz-leon/">Paula Sanz Leon</a></span>, <span >
      <a href="/author/ivo-vanzetta/">Ivo Vanzetta</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Neurophysiology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/sanz-12/" >
      <div class="img-hover-zoom">
        <img src="/publication/sanz-12/featured_hua6f8bdd9e0f26e3ffd6e2ca57e727cc0_714078_dbbfff533a088611b3b732bff5ba4086.webp" height="455" width="808"
            class="article-banner" alt="Motion Clouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/sanz-12/" >Motion Clouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception</a>
  </div>

  
  <a href="/publication/sanz-12/"  class="summary-link">
    <div class="article-style">
      <p>Choosing an appropriate set of stimuli is essential to characterize the response of a sensory system to a particular functional dimension, such as the eye movement following the motion of a visual scene. Here, we describe a framework to generate random texture movies with controlled information content, i.e., Motion Clouds. These stimuli are defined using a generative model that is based on controlled experimental parametrization. We show that Motion Clouds correspond to dense mixing of localized moving gratings with random positions. Their global envelope is similar to natural-like stimulation with an approximate full-field translation corresponding to a retinal slip. We describe the construction of these stimuli mathematically and propose an open-source Python-based implementation. Examples of the use of this framework are shown. We also propose extensions to other modalities such as color vision, touch, and audition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1208.6467" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/sanz-12/sanz-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sanz-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1152/jn.00737.2011" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE, Santorini, Greece, 21-24 June 2012, published by The AREADNE Foundation, Inc., Cambridge, Massachusetts, USA, <a href="http://areadne.org" target="_blank" rel="noopener">http://areadne.org</a></em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-12-areadne/" >Active inference, smooth pursuit and oculomotor delays</a>
  </div>

  
  <a href="/publication/perrinet-12-areadne/"  class="summary-link">
    <div class="article-style">
      <p>We consider the problem of sensorimotor delays in the optimal control of movement under uncertainty. Specifically, we consider axonal conduction delays in the visuo-oculomotor loop and their implications for active inference. Active inference uses a generalisation of Kalman filtering to provide Bayes optimal estimates of hidden states and action in generalised coordinates of motion. Representing hidden states in generalised coordinates provides a simple means of compensating for both sensory and oculomotor delays. This compensation is illustrated using neuronal simulations of oculomotor following responses with and without compensation. We then consider an extension of the generative model that produces ocular following to simulate smooth pursuit eye movements in which the system believes both the target and its centre of gaze are attracted by a (fictive) point moving in the visual field. Finally, the generative model is equipped with a hierarchical structure, so that it can register and remember unseen (occluded) trajectories and emit anticipatory responses. These simulations speak to a straightforward and neurobiologically plausible solution to the generic problem of integrating information from different sources with different temporal delays and the particular difficulties encountered when a system, like the oculomotor system, tries to control its environment with delayed signals. This work was supported from the European Community&rsquo;s Seventh Framework Program FP7/2007-2013 under grant agreement number 214728-2, (CODDE)</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-12-areadne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Frontiers in Computational Neuroscience</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/voges-12/" >
      <div class="img-hover-zoom">
        <img src="/publication/voges-12/featured_hue78436b010285cb015fcf208b7b11e6f_215685_be2442ad807cf6c403dd2704165fd000.webp" height="455" width="808"
            class="article-banner" alt="Complex dynamics in recurrent cortical networks based on spatially realistic connectivities" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/voges-12/" >Complex dynamics in recurrent cortical networks based on spatially realistic connectivities</a>
  </div>

  
  <a href="/publication/voges-12/"  class="summary-link">
    <div class="article-style">
      <p>Most studies on the dynamics of recurrent cortical networks are either based on purely random wiring or neighborhood couplings. Neuronal cortical connectivity, however, shows a complex spatial pattern composed of local and remote patchy connections. We ask to what extent such geometric traits influence the &rsquo;&rsquo; idle&rsquo;&rsquo; dynamics of two-dimensional (2d) cortical network models composed of conductance-based integrate-and-fire (iaf) neurons. In contrast to the typical 1 mm2 used in most studies, we employ an enlarged spatial set-up of 25 mm2 to provide for long-range connections. Our models range from purely random to distance-dependent connectivities including patchy projections, i.e., spatially clustered synapses. Analyzing the characteristic measures for synchronicity and regularity in neuronal spiking, we explore and compare the phase spaces and activity patterns of our simulation results. Depending on the input parameters, different dynamical states appear, similar to the known synchronous regular &rsquo;&rsquo; SR&rsquo;&rsquo; or asynchronous irregular &rsquo;&rsquo; AI&rsquo;&rsquo; firing in random networks. Our structured networks, however, exhibit shifted and sharper transitions, as well as more complex activity patterns. Distance-dependent connectivity structures induce a spatio-temporal spread of activity, e.g., propagating waves, that random networks cannot account for. Spatially and temporally restricted activity injections reveal that a high amount of local coupling induces rather unstable AI dynamics. We find that the amount of local versus long-range connections is an important parameter, whereas the structurally advantageous wiring cost optimization of patchy networks has little bearing on the phase space.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/voges-12/voges-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fncom.2012.00041" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>VSS Conference Abstract</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-12-vss/" >Effect of image statistics on fixational eye movements</a>
  </div>

  
  <a href="/publication/simoncini-12-vss/"  class="summary-link">
    <div class="article-style">
      <p>Under natural viewing conditions, small movements of the eyes prevent the maintenance of a steady direction of gaze. It is unclear how the spatiotemporal content of the fixated scene has an impact on the properties of miniatures, fixational eye movements. We have investigated the characteristics of fixational eye movements recorded while human subjects are instructed to fixate natural statistics random textures (Motion Clouds) in which we manipulated the spatial frequency content. We used long presentations (5 sec) of Motion Clouds stimuli (Schrater et al. 2000) of varying spatial frequency bandwidths (Bsf) around different central spatial frequency (Sf0). We found that central spatial frequency has an effect upon microsaccadic eye movements. In particular, smaller saccadic amplitudes were associated with high spatial frequencies, and larger saccades with low spatial frequencies. Broadening the spatial frequency bandwidth also changed the distribution of microsaccade amplitudes. A lower spatial frequencies, larger Bsf resulted in a large reduction of microsaccades amplitude while fixation behavior for high spatial frequencies texture was not affected. Relationship between microsaccade rate and intersaccadic timing was also dependent upon Bsf. These results suggest that the spatial frequency content of the fixated images have a strong impact upon fixation instability.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.journalofvision.org/content/12/9/1014.abstract?sid=9c51ff88-5b9a-4d1b-aaf1-a1219bd02b0a" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-12-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/12.9.1014" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Front. Neurosci. Conference Abstract: Neural Coding, Decision-Making and Integration in Time</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-12-coding/" >Measuring speed of moving textures: Different pooling of motion information for human ocular following and perception.</a>
  </div>

  
  <a href="/publication/simoncini-12-coding/"  class="summary-link">
    <div class="article-style">
      <p>To measure speed and direction of moving objects, the cortical motion system pools information across different spatiotemporal channels. One yet unsolved question is to understand how the brain pools this information and whether this pooling is generic or adaptive at the behavioral contexts. Here, we investigate in humans this integration process for two different tasks: psychophysical speed discrimination and ocular following eye movements, which are a probe of early motion detection and integration (Masson &amp; Perrinet, 2011). For both tasks, we used short presentations of ``moving textures&rsquo;&rsquo; stimuli (Schrater et al., 2000) in which the width of the spatial frequency distribution (Bsf) was varied. We found that larger Bsf elicited stronger initial eye velocity during the open-loop part of tracking responses. Moreover, richer stimuli resulted in more accurate and reliable motor responses. By contrast, larger Bsf had a detrimental effect upon speed discrimination performance: speed discrimination thresholds linearly decreased when the width of spatial frequency distribution increased. These opposite results can be explained by a different decoding strategy where speed information is under the control of different gain setting mechanisms. We tested this model by measuring contrast response functions of both ocular following and speed discrimination for each Bsf. We found that varying spatial frequency distribution had opposite effect upon contrast gain control. Increasing Bsf lowered half-saturation contrast for ocular following but increased it for perception. Our results supports the view that speed-based perception and tracking eye movements are under the control of different early decoding mechanism. References Masson, G.S. &amp; Perrinet, L.U. The behavioural receptive field underlying motion integration for primate tracking eye movements. Neurosci. BioBehav. Review 36, 1-25 (2011). Schrater, P.R., Knill, D.C. &amp; Simoncelli, E.P. Mechanism of visual motion detection. Nat. Neurosci. 3, 64-68 (2000).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.frontiersin.org/myfrontiers/abstractdetails.aspx?abs_doi=10.3389/conf.fnins.2012.86.00016" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-12-coding/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/conf.fnins.2012.86.00016" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Nature Neuroscience</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/simoncini-12/" >
      <div class="img-hover-zoom">
        <img src="/publication/simoncini-12/featured_hu342c8207a6600516b049009850264ef1_110610_18d984c78d01df50193a8cef54bac18b.webp" height="455" width="808"
            class="article-banner" alt="More is not always better: dissociation between perception and action explained by adaptive gain control" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-12/" >More is not always better: dissociation between perception and action explained by adaptive gain control</a>
  </div>

  
  <a href="/publication/simoncini-12/"  class="summary-link">
    <div class="article-style">
      <p>Moving objects generate motion information at different scales, which are processed in the visual system with a bank of spatiotemporal frequency channels. It is not known how the brain pools this information to reconstruct object speed and whether this pooling is generic or adaptive; that is, dependent on the behavioral task. We used rich textured motion stimuli of varying bandwidths to decipher how the human visual motion system computes object speed in different behavioral contexts. We found that, although a simple visuomotor behavior such as short-latency ocular following responses takes advantage of the full distribution of motion signals, perceptual speed discrimination is impaired for stimuli with large bandwidths. Such opposite dependencies can be explained by an adaptive gain control mechanism in which the divisive normalization pool is adjusted to meet the different constraints of perception and action.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/simoncini-12/simoncini-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/nn.3229" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/masson-12-areadne/" >Motion-based prediction is sufficient to solve the aperture problem</a>
  </div>

  
  <a href="/publication/masson-12-areadne/"  class="summary-link">
    <div class="article-style">
      <p>In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/masson-12-areadne" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/masson-12-areadne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neural Computation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-12-pred/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-12-pred/featured_hud71ae606cf067d7c8c576f18c76b5351_58874_9b8d1054185066d78be9364335591c17.webp" height="455" width="808"
            class="article-banner" alt="Motion-based prediction is sufficient to solve the aperture problem" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-12-pred/" >Motion-based prediction is sufficient to solve the aperture problem</a>
  </div>

  
  <a href="/publication/perrinet-12-pred/"  class="summary-link">
    <div class="article-style">
      <p>In low-level sensory systems, it is still unclear how the noisy information collected locally by neurons may give rise to a coherent global percept. This is well demonstrated for the detection of motion in the aperture problem: as luminance of an elongated line is symmetrical along its axis, tangential velocity is ambiguous when measured locally. Here, we develop the hypothesis that motion-based predictive coding is sufficient to infer global motion. Our implementation is based on a context-dependent diffusion of a probabilistic representation of motion. We observe in simulations a progressive solution to the aperture problem similar to psychophysics and behavior. We demonstrate that this solution is the result of two underlying mechanisms. First, we demonstrate the formation of a tracking behavior favoring temporally coherent features independently of their texture. Second, we observe that incoherent features are explained away while coherent information diffuses progressively to the global scale. Most previous models included ad-hoc mechanisms such as end-stopped cells or a selection layer to track specific luminance-based features. Here, we have proved that motion-based predictive coding, as it is implemented in this functional model, is sufficient to solve the aperture problem. This simpler solution may give insights in the role of prediction underlying a large class of sensory computations.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1208.6471" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-12-pred/perrinet-12-pred.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-12-pred/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/michael-breakspear/">Michael Breakspear</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Frontiers in Psychology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/friston-12/" >
      <div class="img-hover-zoom">
        <img src="/publication/friston-12/featured_hu71699162804cb677fce47f7dc608c39d_297577_ab5c6d90f18c7366c9cff29a6a01345b.webp" height="455" width="808"
            class="article-banner" alt="Perceptions as Hypotheses: Saccades as Experiments" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/friston-12/" >Perceptions as Hypotheses: Saccades as Experiments</a>
  </div>

  
  <a href="/publication/friston-12/"  class="summary-link">
    <div class="article-style">
      <p>If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organized behavior: namely, the imperative to minimize the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free-energy minimization. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or assimilated into beliefs about the world.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/friston-12/friston-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/friston-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fpsyg.2012.00151" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2012
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/khoei-12-sfn/" >Role of motion-based prediction in motion extrapolation</a>
  </div>

  
  <a href="/publication/khoei-12-sfn/"  class="summary-link">
    <div class="article-style">
      <p>Based on Laurent U Perrinet, Guillaume S Masson (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation. Preprint PDF Cite see follow-up on motion extrapolation: Mina A Khoei, Guillaume S Masson, Laurent U Perrinet (2013).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-12-sfn" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-12-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2011
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-11-pattern/" >Pattern discrimination for moving random textures: Richer stimuli are more difficult to recognize</a>
  </div>

  
  <a href="/publication/simoncini-11-pattern/"  class="summary-link">
    <div class="article-style">
      <p>In order to analyze the characteristics of a rich dynamic visual environment, the visual system must integrate information collected at different scales through different spatiotemporal frequency channels. Still, it remains unclear how reliable representations of motion direction or speed are elaborated when presented with large bandwidth motion stimuli or natural statistics. Last year, we have shown that broadening the spatiotemporal frequency content of a textured pattern moving at constant speed leads to different results on a reflexive tracking task and a speed discrimination task. Larger bandwidth stimuli increase response amplitude and sensitivity of ocular following, consistently with a maximum-likelihood (ML) model of motion decoding. In contrast, larger bandwidth stimuli impair speed discrimination performance, suggesting that the perceptual system cannot take advantage of such additional, redundant information. Instead of ML, a gain control decoding mechanism can explain the drop in performance, suggesting that action and perception rely on different decoding mechanisms. To further investigate such task-dependant pooling of motion information, we measured pattern discrimination performance using these textured stimuli. Two noise patterns were presented sequentially for 250 ms on a CRT monitor (1280 × 1024 @ 100 Hz) and covered 47$,^∘$ of visual angle with identical properties (mean SF, bandwidth SF, speed) except for a randomized phase spectrum. A test pattern was then presented and subjects were asked to match it with one or the other reference stimulus (ABX task). At small bandwidth and optimal mean spatial frequency (0.3 cpd), subjects were able to discriminate the two patterns with high accuracy. Performance dropped to chance level as spatial frequency bandwidth increased. Increasing the mean spatial frequency decreased the overall performance. Again, these results suggest that perceptual performance is deteriorated in presence of larger information.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1167/11.11.749" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-11-pattern/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/11.11.749" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2011
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>DocSciences</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-10-doc-sciences/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-10-doc-sciences/featured_hued919fe9ab410127e516b9ed6a7b6b08_201215_cafbbf24f1eaf30d8c57961ebfa39686.webp" height="455" width="808"
            class="article-banner" alt="Qui créera le premier ordinateur intelligent?" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-10-doc-sciences/" >Qui créera le premier ordinateur intelligent?</a>
  </div>

  
  <a href="/publication/perrinet-10-doc-sciences/"  class="summary-link">
    <div class="article-style">
      <p>Qui créera le premier ordinateur intelligent? Les ordinateurs classiques sont de plus en plus puissants, mais restent toujours aussi « stupides ». Impossible d’en trouver un avec lequel on puisse dialoguer de façon naturelle.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://interstices.info/qui-creera-le-premier-ordinateur-intelligent/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-10-doc-sciences/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  











  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/amarender-bogadhi/">Amarender Bogadhi</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    April 2011
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Vision research</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/bogadhi-11/" >Pursuing motion illusions: a realistic oculomotor framework for Bayesian inference</a>
  </div>

  
  <a href="/publication/bogadhi-11/"  class="summary-link">
    <div class="article-style">
      <p>Accuracy in estimating an object&rsquo;s global motion over time is not only affected by the noise in visual motion information but also by the spatial limitation of the local motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that during the initial stages of the motion information processing, 1D motion cues related to the object&rsquo;s edges have a dominating influence over the estimate of the object&rsquo;s global motion. However, during the later stages, 2D motion cues related to terminators (edge-endings) progressively take over, leading to a final correct estimate of the object&rsquo;s global motion. Here, we propose a recursive extension to the Bayesian framework for motion processing (Weiss, Simoncelli, Adelson, 2002) cascaded with a model oculomotor plant to describe the dynamic integration of 1D and 2D motion information in the context of smooth pursuit eye movements. In the recurrent Bayesian framework, the prior defined in the velocity space is combined with the two independent measurement likelihood functions, representing edge-related and terminator-related information, respectively to obtain the posterior. The prior is updated with the posterior at the end of each iteration step. The maximum-a posteriori (MAP) of the posterior distribution at every time step is fed into the oculomotor plant to produce eye velocity responses that are compared to the human smooth pursuit data. The recurrent model was tuned with the variance of pursuit responses to either p̈ure1̈D or r̈̈e ̈̈motion. The oculomotor plant was tuned with an independent set of oculomotor data, including the effects of line length (i.e. stimulus energy) and directional anisotropies in the smooth pursuit responses. The model not only provides an accurate qualitative account of dynamic motion integration but also a quantitative account that is close to the smooth pursuit response across several conditions (three contrasts and three speeds) for two human subjects.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/bogadhi-11/bogadhi-11.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/bogadhi-11/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.visres.2010.10.021" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jerome-fleuriet/">Jérome Fleuriet</a></span>, <span >
      <a href="/author/sandrine-hugues/">Sandrine Hugues</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/laurent-goffart/">Laurent Goffart</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2011
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Neurophysiology</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/fleuriet-11/" >Saccadic foveation of a moving visual target in the rhesus monkey</a>
  </div>

  
  <a href="/publication/fleuriet-11/"  class="summary-link">
    <div class="article-style">
      <p>When generating a saccade toward a moving target, the target displacement that occurs during the period spanning from its detection to the saccade end must be taken into account to accurately foveate the target and to initiate its pursuit. Previous studies have shown that these saccades are characterized by a lower peak velocity and a prolonged deceleration phase. In some cases, a second peak eye velocity appears during the deceleration phase, presumably reflecting the late influence of a mechanism that compensates for the target displacement occurring before saccade end. The goal of this work was to further determine in the head restrained monkey the dynamics of this putative compensatory mechanism. A step-ramp paradigm, where the target motion was orthogonal to a target step occurring along the primary axes, was used to estimate from the generated saccades: a component induced by the target step and another one induced by the target motion. Resulting oblique saccades were compared with saccades to a static target with matched horizontal and vertical amplitudes. This study permitted to estimate the time taken for visual motion-related signals to update the programming and execution of saccades. The amplitude of the motion-related component was slightly hypometric with an undershoot that increased with target speed. Moreover, it matched with the eccentricity that the target had 40-60 ms before saccade end. The lack of significant difference in the delay between the onsets of the horizontal and vertical components between saccades directed toward a static target and those aimed at a moving target questions the late influence of the compensatory mechanism. The results are discussed within the framework of the dual drive and mapping hypotheses.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1152/jn.00622.2010" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fleuriet-11/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1152/jn.00622.2010" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/david-fitzpatrick/">David Fitzpatrick</a></span>, <span >
      <a href="/author/james-a-bednar/">James A Bednar</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2011
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-11-sfn/" >Edge statistics in natural images versus laboratory animal environments: implications for understanding lateral connectivity in V1</a>
  </div>

  
  <a href="/publication/perrinet-11-sfn/"  class="summary-link">
    <div class="article-style">
      <p>Oriented edges in images of natural scenes tend to be aligned in collinear or co-circular arrangements, with lines and smooth curves more common than other possible arrangements of edges (Geisler et al., Vis Res 41:711-24, 2001). The visual system appears to take advantage of this prior information, and human contour detection and grouping performance is well predicted by such an association field (̈Field et al., Vis Res 33:173-93, 1993). One possible candidate substrate for implementing an association field in mammals is the set of long-range lateral connections between neurons in the primary visual cortex (V1), which could act to facilitate detection of contours matching the association field, and/or inhibit detection of other contours (Choe and Miikkulainen, Biol Cyb 90:75-88, 2004). To fill this role, the lateral connections would need to be orientation specific and aligned along contours, and indeed such an arrangement has been found in tree shrew primary visual cortex (Bosking et al., J Neurosci 17:2112-27, 1997). However, it is not yet known whether these patterns develop as a result of visual experience, or are simply hard-wired to be appropriate for the statistics of natural scenes. To investigate this issue, we examined the properties of the visual environment of laboratory animals, to determine whether the observed connection patterns are more similar to the statistics of the rearing environment or of a natural habitat. Specifically, we analyzed the cooccurence statistics of edge elements in images of natural scenes, and compared them to corresponding statistics for images taken from within the rearing environment of the animals in the Bosking et al. (1997) study. We used a modified version of the algorithm from Geisler et al. (2001), with a more general edge extraction algorithm that uses sparse coding to avoid multiple responses to a single edge. Collinearity and co-circularity results for natural images replicated qualitatively the results from Geisler et al. (2001), confirming that prior information about continuations appeared consistently in natural images. However, we find that the largely man-made environment in which these animals were reared has a significantly higher probability of collinear edge elements. We thus predict that if the lateral connection patterns are due to visual experience, the patterns in wild-raised tree shrews would be very different from those measured by Bosking et al. (1997), with shorter-range correlations and less emphasis on collinear continuations. This prediction can be tested in future experiments on matching groups of animals reared in different environments. W.H. Bosking and Y. Zhang and B. Schofield and D. Fitzpatrick (1997) Orientation selectivity and the arrangement of horizontal connections in tree shrew striate cortex Journal of Neuroscience 17:2112-27. E.M. Callaway and L.C. Katz (1990) Emergence and refinement of clustered horizontal connections in cat striate cortex. Journal of Neuroscience 10:1134&ndash;53. Y. Choe and R. Miikkulainen (2004) Contour integration and segmentation with self-organized lateral connections Biological Cybernetics 90:75-88. D.J. Field, A. Hayes, and R.F. Hess (1993) Contour integration by the human visual system: Evidence for a local s̈̈ociation field̈s̈̈ion Research 33:173&ndash;93. W.S. Geisler, J.S. Perry, B.J. Super, and D.P. Gallogly (2001) Edge co-occurrence in natural images predicts contour grouping performance. Vision Research 41:711-24.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-11-sfn" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-11-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/amarender-bogadhi/">Amarender Bogadhi</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2011
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Perception</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/khoei-11-ecvp/" >Role of motion inertia in dynamic motion integration for smooth pursuit</a>
  </div>

  
  <a href="/publication/khoei-11-ecvp/"  class="summary-link">
    <div class="article-style">
      <p>Based on Laurent U Perrinet, Guillaume S Masson (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation. Preprint PDF Cite see follow-up on motion extrapolation: Mina A Khoei, Guillaume S Masson, Laurent U Perrinet (2013).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-11-ecvp/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-11-ecvp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Physiology-Paris</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/voges-10-jpp/" >Phase space analysis of networks based on biologically realistic parameters</a>
  </div>

  
  <a href="/publication/voges-10-jpp/"  class="summary-link">
    <div class="article-style">
      <p>We study cortical network dynamics for a spatially embedded network model. It represents, in terms of spatial scale, a large piece of cortex allowing for long-range connections, resulting in a rather sparse connectivity. The spatial embedding also permits us to include distance-dependent conduction delays. We use two different types of conductance-based I&amp;F neurons as excitatory and inhibitory units, as well as specific connection probabilities. In order to remain computationally tractable, we reduce neuron density, modelling part of the missing internal input via external poissonian spike trains. Compared to previous studies, we observe significant changes in the dynamical phase space: Altered activity patterns require another regularity measures than the coefficient of variation. Hence, we compare three different regularity measure on the basis of artificial inter-spike-interval distributions. We identify two types of mixed states, where different phases coexist in certain regions of the phase space. More notably, our boundary between high and low activity states depends predominantly on the relation between excitatory and inhibitory synaptic strength instead of the input rate.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/voges-10-jpp/voges-10-jpp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-10-jpp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2009.11.004" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neural Computation</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-10-shl/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-10-shl/featured_hua3d6882b2e3b2a9f5866af9a9e5c25e3_438554_1196bde88d9736ac612e3dc85f28023a.webp" height="455" width="808"
            class="article-banner" alt="Role of homeostasis in learning sparse representations" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-10-shl/" >Role of homeostasis in learning sparse representations</a>
  </div>

  
  <a href="/publication/perrinet-10-shl/"  class="summary-link">
    <div class="article-style">
      <p>Neurons in the input layer of primary visual cortex in primates develop edge-like receptive fields. One approach to understanding the emergence of this response is to state that neural activity has to efficiently represent sensory data with respect to the statistics of natural scenes. Furthermore, it is believed that such an efficient coding is achieved using a competition across neurons so as to generate a sparse representation, that is, where a relatively small number of neurons are simultaneously active. Indeed, different models of sparse coding coupled with Hebbian learning and homeostasis have been proposed that successfully match the observed emergent response. However, the specific role of homeostasis in learning such sparse representations is still largely unknown. By quantitatively assessing the efficiency of the neural representation during learning, we derive a cooperative homeostasis mechanism which optimally tunes the competition between neurons within the sparse coding algorithm. We apply this homeostasis while learning small patches taken from natural images and compare its efficiency with state-of-the-art algorithms. Results show that while different sparse coding algorithms give similar coding results, the homeostasis provides an optimal balance for the representation of natural images within the population of neurons. Competition in sparse coding is optimized when it is fair: By contributing to optimize statistical competition across neurons, homeostasis is crucial in providing a more efficient solution to the emergence of independent components.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/0706.3177" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-10-shl/perrinet-10-shl.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-10-shl/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/bicv/SparseHebbianLearning" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1162/neco.2010.05-08-795" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Computational Neuroscience</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/kremkow-10-jcns/" >
      <div class="img-hover-zoom">
        <img src="/publication/kremkow-10-jcns/featured_hu0cbb1d54d965bdf7441c3c1097205085_160982_85923d855a7c6fe4416e0f4e56a521c6.webp" height="455" width="808"
            class="article-banner" alt="Functional consequences of correlated excitatory and inhibitory conductances in cortical networks" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kremkow-10-jcns/" >Functional consequences of correlated excitatory and inhibitory conductances in cortical networks</a>
  </div>

  
  <a href="/publication/kremkow-10-jcns/"  class="summary-link">
    <div class="article-style">
      <p>Neurons in the neocortex receive a large number of excitatory and inhibitory synaptic inputs. Excitation and inhibition dynamically balance each other, with inhibition lagging excitation by only few milliseconds. To characterize the functional consequences of such correlated excitation and inhibition, we studied models in which this correlation structure is induced by feedforward inhibition (FFI). Simple circuits show that an effective FFI changes the integrative behavior of neurons such that only synchronous inputs can elicit spikes, causing the responses to be sparse and precise. Further, effective FFI increases the selectivity for propagation of synchrony through a feedforward network, thereby increasing the stability to background activity. Last, we show that recurrent random networks with effective inhibition are more likely to exhibit dynamical network activity states as have been observed in vivo. Thus, when a feedforward signal path is embedded in such recurrent network, the stabilizing effect of effective inhibition creates an suitable substrate for signal propagation. In conclusion, correlated excitation and inhibition support the notion that synchronous spiking may be important for cortical processing.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/kremkow-10-jcns/kremkow-10-jcns.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-10-jcns/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s10827-010-0240-9" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/amarender-bogadhi/">Amarender Bogadhi</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Vision Science Society</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/bogadhi-10-vss/" >A recurrent Bayesian model of dynamic motion integration for smooth pursuit</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1167/10.7.545" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/bogadhi-10-vss/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/10.7.545" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Physiology-Paris</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/dauce-10/" >Computational Neuroscience, from Multiple Levels to Multi-level</a>
  </div>

  
  <a href="/publication/dauce-10/"  class="summary-link">
    <div class="article-style">
      <p>Despite the long and fruitful history of neuroscience, a global, multi-level description of cardinal brain functions is still far from reach. Using analytical or numerical approaches, emphComputational Neuroscience aims at the emergence of such common principles by using concepts from Dynamical Systems and Information Theory. The aim of this Special Issue of the Journal of Physiology (Paris) is to reflect the latest advances in this field which has been presented during the NeuroComp08 conference that took place in October 2008 in Marseille (France). By highlighting a selection of works presented at the conference, we wish to illustrate the intrinsic diversity of this field of research but also the need of an unification effort that is becoming more and more necessary to understand the brain in its full complexity, from multiple levels of description to a multi-level understanding.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.jphysparis.2009.11.001" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dauce-10/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2009.11.001" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Vision Science Society</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/simoncini-10-vss/" >Different pooling of motion information for perceptual speed discrimination and behavioral speed estimation</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-10-vss/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of Tauc</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/khoei-10-tauc/" >Dynamical emergence of a neural solution for motion integration</a>
  </div>

  
  <a href="/publication/khoei-10-tauc/"  class="summary-link">
    <div class="article-style">
      <p>Based on Laurent U Perrinet, Guillaume S Masson (2012). Motion-based prediction is sufficient to solve the aperture problem. Neural Computation. Preprint PDF Cite see follow-up on motion extrapolation: Mina A Khoei, Guillaume S Masson, Laurent U Perrinet (2013).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-10-tauc/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-10-tauc/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-10-areadne/" >Dynamical emergence of a neural solution for motion integration</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-10-areadne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of NeuroComp</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/voges-10-neurocomp/" >
      <div class="img-hover-zoom">
        <img src="/publication/voges-10-neurocomp/featured_hucd1239bf594689ddda770e010389b992_230736_97703bed3d2285959589286d373fe1b3.webp" height="455" width="808"
            class="article-banner" alt="Phase space analysis of networks based on biologically realistic parameters" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/voges-10-neurocomp/" >Phase space analysis of networks based on biologically realistic parameters</a>
  </div>

  
  <a href="/publication/voges-10-neurocomp/"  class="summary-link">
    <div class="article-style">
      <p>We study cortical network dynamics for a more realistic network model. It represents, in terms of spatial scale, a large piece of cortex allowing for long-range connections, resulting in a rather sparse connectivity. We use two different types of conductance-based I&amp;F neurons as excitatory and inhibitory units, as well as specific connection probabilities. In order to remain computationally tractable, we reduce neuron density, modelling part of the missing internal input via external poissonian spike trains. Compared to previous studies, we observe significant changes in the dynamical phase space: Altered activity patterns require another regularity measure than the coefficient of variation. We identify two types of mixed states, where different phases coexist in certain regions of the phase space. More notably, our boundary between high and low activity states depends predominantly on the relation between excitatory and inhibitory synaptic strength instead of the input rate. Key words:Artificial neural networks, Data analysis, Simulation, Spiking neurons. This work is supported by EC IP project FP6-015879 (FACETS).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-10-neurocomp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2010
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>LADISLAV TAUC and GDR MSPC NEUROSCIENCES CONFERENCE, From Mathematical Image Analysis to Neurogeometry of the Brain</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-10-tauc/" >Probabilistic models of the low-level visual system: the role of prediction in detecting motion</a>
  </div>

  
  <a href="/publication/perrinet-10-tauc/"  class="summary-link">
    <div class="article-style">
      <p>Sensory informations such as visual images are inherently variable. We use probabilistic models to describe how the low-level visual system could describe superposed and ambiguous information. This allows to describe the interactions of neighboring populations of neurons as inference rules that dynamically build up the overall description of the visual scene. We focus here on temporal prediction, that is by the transport of information based on an estimate of local motion in the image.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-10-tauc/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-10-tauc/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2009
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of COSYNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-09-cosyne/" >Decoding center-surround interactions in population of neurons for the ocular following response</a>
  </div>

  
  <a href="/publication/perrinet-09-cosyne/"  class="summary-link">
    <div class="article-style">
      <p>Short presentation of a large moving pattern elicits an Ocular Following Response (OFR) that exhibits many of the properties attributed to low-level motion processing such as spatial and temporal integration, contrast gain control and divisive interaction between competing motions. Similar mechanisms have been demonstrated in V1 cortical activity in response to center-surround gratings patterns measured with real-time optical imaging in awake monkeys. More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the global spatial integration of motion from an intermediate map of possible local translation velocities: (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuli [Barthelemy06,Barthelemy07]. In fact, the machinery behind the visual perception of motion and the subsequent sensorimotor transformation is confronted to uncertainties which are efficiently resolved in the primate&rsquo;s visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theory [Weiss02] and we extended in the dynamical domain the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We proved that this model is successfully adapted to model the OFR for the different experiments [Perrinet07neurocomp], that is for different levels of noise with full field gratings, with disks of various sizes and also for the effect of a flickering surround. However, another emphad hoc inhibitory mechanism has to be added in this model to account for suppressive effects of the surround. We explore here an hypothesis where this could be understood as the effect of a recurrent prediction of information in the velocity map. In fact, in previous models, the integration step assumes independence of the local information while natural scenes are very predictable: Due to the rigidity and inertia of physical objects in visual space, neighboring local spatiotemporal information is redundant and one may introduce this empha priori knowledge of the statistics of the input in the ideal observer model. We implement this in a realistic model of a layer representing velocities in a map of cortical columns, where predictions are implemented by lateral interactions within the cortical area. First, raw velocities are estimated locally from images and are propagated to this area in a feed-forward manner. Using this velocity map, we progressively learn the dependance of local velocities in a second layer of the model. This algorithm is cyclic since the prediction is using the local velocities which are themselves using both the feed-forward input and the prediction: We control the convergence of this process by measuring results for different learning rate. Results show that this simple model is sufficient to disambiguate characteristic patterns such as the Barber-Pole illusion. Due to the recursive network which is modulating the velocity map, it also explains that the representation may exhibit some memory, such as when an object suddenly disappears or when presenting a dot followed by a line (line-motion illusion). Finally, we applied this model that was tuned over a set of natural scenes to gratings of increasing sizes. We observed first that the feed-forward response as tuned to neurophysiological data gave lower responses at higher eccentricities, and that this effect was greater for higher grating frequencies. Then, we observed that depending on the size of the disk and on its spatial frequency, the recurrent network of lateral interactions Lastly, we explore how a surrounding velocity non congruous with the central excitation information shunts the ocular response and how it is topographically represented in the cortical activity.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-09-cosyne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2009
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of COSYNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/voges-09-cosyne/" >Dynamical state spaces of cortical networks representing various horizontal connectivities</a>
  </div>

  
  <a href="/publication/voges-09-cosyne/"  class="summary-link">
    <div class="article-style">
      <p>Most studies of cor tical network dynamics are either based on purely random wiring or neighborhood couplings, e.g., [Kumar, Schrader, Aer tsen, Rotter, 2008, Neural Computation 20, 1&ndash;43]. Neuronal connections in the cor tex, however, show a complex spatial pattern composed of local and long-range connections, the latter featuring a so-called patchy projection pattern, i.e., spatially clustered synapses [Binzegger, Douglas, Martin, 2007, J. Neurosci. 27(45), 12242&ndash;12254]. The idea of our project is to provide and to analyze probabilistic network models that more adequately represent horizontal connectivity in the cor tex. In particular, we investigate the effect of specific projection patterns on the dynamical state space of cor tical networks. Assuming an enlarged spatial scale we employ a distance dependent connectivity that reflects the geometr y of dendrites and axons. We simulate the network dynamics using a neuronal network simulator NEST/PyNN. Our models are composed of conductance based integrate-and-fire neurons, representing fast spiking inhibitor y and regular spiking excitator y cells. In order to compare the dynamical state spaces of previous studies with our network models we consider the following connectivity assumptions: purely random or purely local couplings, a combination of local and distant synapses, and connectivity structures with patchy projections. Similar to previous studies, we also find different dynamical states depending on the input parameters: the external input rate and the numerical relation between excitator y and inhibitor y synaptic weights. These states, e.g., synchronous regular (SR) or asynchronous irregular (AI) firing, are characterized by measures like the mean firing rate, the correlation coefficient, the coefficient of variation and so for th. On top of identified biologically realistic background states (AI), stimuli are applied in order to analyze their stability. Comparing the results of our different network models we find that the parameter space necessar y to describe all possible dynamical states of a network is much more concentrated if local couplings are involved. The transition between different states is shifted (with respect to both input parameters) and shar pened in dependence of the relative amount of local couplings. Local couplings strongly enhance the mean firing rate, and lead to smaller values of the correlation coefficient. In terms of emergence of synchronous states, however, networks with local versus non-local or patchy versus random remote connections exhibit a higher probability of synchronized spiking. Concerning stability, preliminar y results indicate that again networks with local or patchy connections show a higher probability of changing from the AI to the SR state. We conclude that the combination of local and remote projections bears important consequences on the activity of network: The apparent differences we found for distinct connectivity assumptions in the dynamical state spaces suggest that network dynamics strongly depend on the connectivity structure. This effect might be even stronger with respect to the spatio-temporal spread of signal propagation. This work is suppor ted by EC IP project FP6-015879 (FACETS).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-09-cosyne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2009
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Eighth Göttingen Meeting of the German Neuroscience Society</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/voges-09-gns/" >Dynamics of cortical networks including long-range patchy connections</a>
  </div>

  
  <a href="/publication/voges-09-gns/"  class="summary-link">
    <div class="article-style">
      <p>Most studies of cortical network dynamics are either based on purely random wiring or neighborhood couplings [1], focussing on a rather local scale. Neuronal connections in the cortex, however, show a more complex spatial pattern composed of local and long-range patchy connections [2,3] as shown in the figure: It represents a tracer injection (gray areas) in the GM of a flattened cortex (top view): Black dots indicate neuron positions, blue lines their patchy axonal ramifications, and red lines represent the local connections. Moreover, to include distant synapses, one has to enlarge the spatial scale from the typically assumed 1mm to 5mm side length. As it is our aim to analyze more realistic network models of the cortex we assume a distance dependent connectivity that reflects the geometry of dendritesand axons [3]. Here, we ask to what extent the assumption of specific geometric traits influences the resulting dynamical behavior of these networks. Analyzing various characteristic measures that describe spiking neurons (e.g., coefficient of variation, correlation coefficient), we compare the dynamical state spaces of different connectivity types: purely random or purely local couplings, a combination of local and distant synapses, and connectivity structures with patchy projections. On top of biologically realistic background states, a stimulus is applied in order to analyze their stabilities. As previous studies [1], we also find different dynamical states depending on the external input rate and the numerical relation between excitatory and inhibitory synaptic weights. Preliminary results indicate, however, that transitions between these states are much sharper in case of local or patchy couplings. This work is supported by EU Grant 15879 (FACETS). Thanks to Stefan Rotter who supervised the PhD project [3] this work is based on. Network dynamics are simulated with NEST/PyNN [4]. [1] A. Kumar, S. Schrader, A. Aertsen and S. Rotter, Neural Computation 20, 2008, 1-43. [2] T. Binzegger, R.J. Douglas and K.A.C. Martin, J. of Neurosci., 27(45), 2007, 12242-12254. [3] Voges N, Fakultaet fuer Biologie, Albert-Ludwigs-Universitaet Freiburg, 2007. [4] NEST. M.O. Gewaltig and M. Diesmann, Scholarpedia 2(4):1430.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-09-gns/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2009
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Eighth Göttingen Meeting of the German Neuroscience Society</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kremkow-09-gns/" >Functional consequences of correlated excitation and inhibition on single neuron integration and signal propagation through synfire chains</a>
  </div>

  
  <a href="/publication/kremkow-09-gns/"  class="summary-link">
    <div class="article-style">
      <p>Neurons receive a large number of excitatory and inhibitory synaptic inputs whose temporal interplay determines their spiking behavior. On average, excitation (Gexc) and inhibition (Ginh) balance each other, such that spikes are elicited by fluctuations [1]. In addition, it has been shown in vivo that Gexc and Ginh are correlated, with Ginh lagging Gexc only by few milliseconds (6ms), creating a small temporal integration window [2,3]. This correlation structure could be induced by feed-forward inhibition (FFI), which has been shown to be present at many sites in the central nervous system. To characterize the functional consequences of the FFI, we first modeled a simple circuit using spiking neurons with conductance based synapses and studied the effect on the single neuron integration. We then coupled many of such circuits to construct a feed-forward network (synfire chain [4,5]) and investigated the effect of FFI on signal propagation along such feed-forward network. We found that the small temporal integration window, induced by the FFI, changes the integrative properties of the neuron. Only transient stimuli could produce a response when the FFI was active whereas without FFI the neuron responded to both steady and transient stimuli. Due to the increase in selectivity to transient inputs, the conditions of signal propagation through the feed-forward network changed as well. Whereas synchronous inputs could reliable propagate, high asynchronous input rates, which are known to induce synfire activity [6], failed to do so. In summary, the FFI increased the stability of the synfire chain. Supported by DFG SFB 780, EU-15879-FACETS, BMBF 01GQ0420 to BCCN Freiburg [1] Kumar A., Schrader S., Aertsen A. and Rotter S. (2008). The high-conductance state of cortical networks. Neural Computation, 20(1):1&ndash;43. [2] Okun M. and Lampl I. (2008). Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities. Nat Neurosci, 11(5):535&ndash;7. [3] Baudot P., Levy M., Marre O., Monier C. and Frégnac (2008). [4] Abeles M. (1991). Corticonics: Neural circuits of the cerebral cortex. Cambridge, UK [5] Diesmann M., Gewaltig M-O and Aertsen A. (1999). Stable propagation of synchronous spiking in cortical neural networks. Nature, 402(6761):529&ndash;33. [6] Kumar A., Rotter S. and Aertsen A. (2008), Conditions for propagating synchronous spiking and asynchronous firing rates in a cortical network model. J Neurosci 28 (20), 5268&ndash;80. Preliminary Program</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-09-gns/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/alexandre-reynaud/">Alexandre Reynaud</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2009
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Vision Science Society</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-09-vss/" >Inferring monkey ocular following responses from V1 population dynamics using a probabilistic model of motion integration</a>
  </div>

  
  <a href="/publication/perrinet-09-vss/"  class="summary-link">
    <div class="article-style">
      <p>Short presentation of a large moving pattern elicits an ocular following response that exhibits many of the properties attributed to low-level motion processing such as spatial and temporal integration, contrast gain control and divisive interaction between competing motions. Similar mechanisms have been demonstrated in V1 cortical activity in response to center-surround gratings patterns measured with real-time optical imaging in awake monkeys (see poster of Reynaud et al., VSS09). Based on a previously developed Bayesian framework, we have developed an optimal statistical decoder of such an observed cortical population activity as recorded by optical imaging. This model aims at characterizing the statistical dependence between early neuronal activity and ocular responses and its performance was analyzed by comparing this neuronal read-out and the actual motor responses on a trial-by-trial basis. First, we show that relative performance of the behavioral contrast response function is similar to the best estimate obtained from the neural activity. In particular, we show that the latency of ocular response increases with low contrast conditions as well as with noisier instances of the behavioral task as decoded by the model. Then, we investigate the temporal dynamics of both neuronal and motor responses and show how motion information as represented by the model is integrated in space to improve population decoding over time. Lastly, we explore how a surrounding velocity non congruous with the central excitation information shunts the ocular response and how it is topographically represented in the cortical activity. Acknowledgment: European integrated project FACETS IST-15879.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-09-vss/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/pierre-yger/">Pierre Yger</a></span>, <span >
      <a href="/author/daniel-bruderle/">Daniel Bruderle</a></span>, <span >
      <a href="/author/jochen-eppler/">Jochen Eppler</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/dejan-pecevski/">Dejan Pecevski</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/michael-schmuker/">Michael Schmuker</a></span>, <span >
      <a href="/author/eilif-muller/">Eilif Muller</a></span>, <span >
      <a href="/author/andrew-p-davison/">Andrew P Davison</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2009
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Eighth Göttingen Meeting of the German Neuroscience Society</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/yger-09-gns/" >NeuralEnsemble: Towards a meta-environment for network modeling and data analysis</a>
  </div>

  
  <a href="/publication/yger-09-gns/"  class="summary-link">
    <div class="article-style">
      <p>NeuralEnsemble (<a href="http://neuralensemble.org" target="_blank" rel="noopener">http://neuralensemble.org</a>) is a multilateral effort to coordinate and organise neuroscience software development efforts based around the Python programming language into a larger, meta-simulator software system. To this end, NeuralEnsemble hosts services for source code management and bug tracking (Subversion/Trac) for a number of open-source neuroscience tools, organizes an annual workshop devoted to collaborative software development in neuroscience, and manages a google-group discussion forum. Here, we present two NeuralEnsemble hosted projects: PyNN (<a href="http://neuralensemble.org/PyNN" target="_blank" rel="noopener">http://neuralensemble.org/PyNN</a>) is a package for simulator-independent specification of neuronal network models. You can write the code for a model once, using the PyNN API, and then run it without modification on any simulator that PyNN supports. Currently NEURON, NEST, PCSIM and a VLSI hardware implementation are fully supported. NeuroTools (<a href="http://neuralensemble.org/NeuroTools" target="_blank" rel="noopener">http://neuralensemble.org/NeuroTools</a>) is a set of tools to manage, store and analyse computational neuroscience simulations. It has been designed around PyNN, but can also be used for data from other simulation environments or even electrophysiological measurements. We will illustrate how the use of PyNN and NeuroTools ease the developmental process of models in computational neuroscience, enhancing collaboration between different groups and increasing the confidence in correctness of results. NeuralEnsemble efforts are supported by the European FACETS project (EU-IST-2005-15879)</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/yger-09-gns/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/open-science/">
    Project
  </a>
  











  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of NeuroComp08, Marseille</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/voges-08-neurocomp/" >Analyzing cortical network dynamics with respect to different connectivity assumptions</a>
  </div>

  
  <a href="/publication/voges-08-neurocomp/"  class="summary-link">
    <div class="article-style">
      <p>Based on Nicole Voges, Laurent U Perrinet (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris. PDF Cite DOI see follow-up : Nicole Voges, Laurent U Perrinet (2012).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-08-neurocomp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of NeuroComp08, Marseille</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kremkow-08-neurocomp/" >Functional properties of feed-forward inhibition</a>
  </div>

  
  <a href="/publication/kremkow-08-neurocomp/"  class="summary-link">
    <div class="article-style">
      <p> see this subsequent paper in the Journal of Computational Neuroscience </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-08-neurocomp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of NeuroComp08, Marseille</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-08-neurocomp/" >Proceedings of the second french conference on Computational Neuroscience, Marseille</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/NEUROCOMP08" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-08-neurocomp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/frederic-v-barthelemy/">Frédéric V Barthélemy</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/eric-castet/">Eric Castet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Vision research</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/barthelemy-08/" >
      <div class="img-hover-zoom">
        <img src="/publication/barthelemy-08/featured_huc03a835ac4dbf5c7b3d19eb2cd8490c7_259869_511a7d644961e19a5c2b9237bb7d6b92.webp" height="455" width="808"
            class="article-banner" alt="Dynamics of distributed 1D and 2D motion representations for short-latency ocular following" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/barthelemy-08/" >Dynamics of distributed 1D and 2D motion representations for short-latency ocular following</a>
  </div>

  
  <a href="/publication/barthelemy-08/"  class="summary-link">
    <div class="article-style">
      <p>Integrating information is essential to measure the physical 2D motion of a surface from both ambiguous local 1D motion of its elongated edges and non-ambiguous 2D motion of its features such as corners or texture elements. The dynamics of this motion integration shows a complex time course as read from tracking eye movements: first, local 1D motion signals are extracted and pooled to initiate ocular responses, then 2D motion signals are integrated to adjust the tracking direction until it matches the surface motion direction. The nature of these 1D and 2D motion computations are still unclear. One hypothesis is that their different dynamics may be explained from different contrast sensitivities. To test this, we measured contrast-response functions of early, 1D-driven and late, 2D-driven components of ocular following responses to different motion stimuli: gratings, plaids and barberpoles. We found that contrast dynamics of 1D-driven responses are nearly identical across the different stimuli. On the contrary, late 2D-driven components with either plaids or barberpoles have similar latencies but different contrast dynamics. Temporal dynamics of both 1D- and 2D-driven responses demonstrates that the different contrast gains are set very early during the response time course. Running a Bayesian model of motion integration, we show that a large family of contrast-response functions can be predicted from the probability distributions of 1D and 2D motion signals for each stimulus and by the shape of the prior distribution. However, the pure delay (i.e. largely independent upon contrast) observed between 1D- and 2D-motion supports the fact that 1D and 2D probability distributions are computed independently. This two-pathway Bayesian model supports the idea that 1D and 2D mechanisms represent edges and features motion in parallel.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/barthelemy-08/barthelemy-08.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/barthelemy-08/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.visres.2007.10.020" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Optical and Digital Image Processing Conference 7000 - Proceedings of SPIE Volume 7000, 7 - 11 April 2008</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-08-spie/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-08-spie/featured_hue65338586c7bced1ea9de155b0ba827b_655301_10685ad40daebf22f36420f8bbf86793.webp" height="455" width="808"
            class="article-banner" alt="Adaptive Sparse Spike Coding : applications of Neuroscience to the compression of natural images" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-08-spie/" >Adaptive Sparse Spike Coding : applications of Neuroscience to the compression of natural images</a>
  </div>

  
  <a href="/publication/perrinet-08-spie/"  class="summary-link">
    <div class="article-style">
      <p>If modern computers are sometimes superior to cognition in some specialized tasks such as playing chess or browsing a large database, they can&rsquo;t beat the efficiency of biological vision for such simple tasks as recognizing a relative or following an object in a complex background. We present in this paper our attempt at outlining the dynamical, parallel and event-based representation for vision in the architecture of the central nervous system. We will illustrate this by showing that in a signal matching framework, a L/LN (linear/non-linear) cascade may efficiently transform a sensory signal into a neural spiking signal and we apply this framework to a model retina. However, this code gets redundant when using an over-complete basis as is necessary for modeling the primary visual cortex: we therefore optimize the efficiency cost by increasing the sparseness of the code. This is implemented by propagating and canceling redundant information using lateral interactions. We compare the efficiency of this representation in terms of compression as the reconstruction quality as a function of the coding length. This will correspond to a modification of the Matching Pursuit algorithm where the ArgMax function is optimized for competition, or Competition Optimized Matching Pursuit (COMP). We will particularly focus on bridging neuroscience and image processing and on the advantages of such an interdisciplinary approach.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/0804.4830" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/0804.4830" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-08-spie/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/pierre-baudot/">Pierre Baudot</a></span>, <span >
      <a href="/author/manu-levy/">Manu Levy</a></span>, <span >
      <a href="/author/olivier-marre/">Olivier Marre</a></span>, <span >
      <a href="/author/cyril-monier/">Cyril Monier</a></span>, <span >
      <a href="/author/yves-fregnac/">Yves Fregnac</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of the Society for Neuroscience conference</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kremkow-08-sfn/" >Control of the temporal interplay between excitation and inhibition by the statistics of visual input: a V1 network modelling study</a>
  </div>

  
  <a href="/publication/kremkow-08-sfn/"  class="summary-link">
    <div class="article-style">
      <p>In the primary visual cortex (V1), single cell responses to simple visual stimuli (gratings) are usually dense but with a high trial-by-trial variability. In contrast, when exposed to full field natural scenes, the firing patterns of these neurons are sparse but highly reproducible over trials (Marre et al., 2005; Frégnac et al., 2006). It is still not understood how these two classes of stimuli can elicit these two distinct firing behaviours. A common model for simple-cell computation in layer 4 is the ``push-pull&rsquo;&rsquo; circuitry (Troyer et al. 1998). It accounts for the observed anti-phase behaviour between excitatory and inhibitory conductances in response to a drifting grating (Anderson et al., 2000; Monier et al., 2008), creating a wide temporal integration window during which excitation is integrated without the shunting or opponent effect of inhibition and allowed to elicit multiple spikes. This is in contrast to recent results from intracellular recordings in vivo during presentation of natural scenes (Baudot et al., 2013). Here the excitatory and inhibitory conductances were highly correlated, with inhibition lagging excitation only by few milliseconds (̃6 ms). This small lag creates a narrow temporal integration window such that only synchronized excitatory inputs can elicit a spike, similar to parallel observations in other cortical sensory areas (Wehr and Zador, 2003; Okun and Lampl, 2008). To investigate the cellular and network mechanisms underlying these two different correlation structures, we constructed a realistic model of the V1 network using spiking neurons with conductance based synapses. We calibrated our model to fit the irregular ongoing activity pattern as well as in vivo conductance measurements during drifting grating stimulation and then extracted predicted responses to natural scenes seen through eye-movements. Our simulations reproduced the above described experimental observation, together with anti-phase behaviour between excitation and inhibition during gratings and phase lagged activation during natural scenes. In conclusion, the same cortical network that shows dense and variable responses to gratings exhibits sparse and precise spiking to natural scenes. Work is under way to show to which extent this feature is specific for the feedforward vs recurrent nature of the modelled circuit.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-08-sfn/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of AREADNE, 2008</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-08-areadne/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-08-areadne/featured_hu8e0a45f7608af6cb175970157cefaad2_146444_ca79b4b2e08da9642d60dd34f71d0dae.webp" height="455" width="808"
            class="article-banner" alt="Decoding the population dynamics underlying ocular following response using a probabilistic framework" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-08-areadne/" >Decoding the population dynamics underlying ocular following response using a probabilistic framework</a>
  </div>

  
  <a href="/publication/perrinet-08-areadne/"  class="summary-link">
    <div class="article-style">
      <p>The machinery behind the visual perception of motion and the subsequent sensorimotor transformation, such as in Ocular Following Response (OFR), is confronted to uncertainties which are efficiently resolved in the primate&rsquo;s visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theorỹWeiss02 which we previously proved to be successfully adapted to model the OFR for different levels of noise with full field gratings or with disk of various sizes and the effect of a flickering surround̃Perrinet07neurocomp. More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the global spatial integration of motion from an intermediate map of possible local translation velocities: (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuliB̃arthelemy06,Barthelemy07. Herein, we extended in the dynamical domain the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We present analytical results which show that the hypothesis of independence of local measures can describe the initial segment of spatial integration of motion signal. Within this framework, we successfully accounted for the dynamical contrast gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another inhibitory mechanism had to be added to account for suppressive effects of the surround. We explore here an hypothesis where this could be understood as the effect of a recurrent integration of information in the velocity map. F. Barthelemy, L. U. Perrinet, E. Castet, and G. S. Masson. Dynamics of distributed 1D and 2D motion representations for short-latency ocular following. Vision Research, 48(4):501&ndash;22, feb 2007. doi: 10.1016/j.visres.2007.10.020. F. V. Barthelemy, I. Vanzetta, and G. S. Masson. Behavioral receptive field for ocular following in humans: Dynamics of spatial summation and center-surround interactions. Journal of Neurophysiology, (95):3712&ndash;26, Mar 2006. doi: 10.1152/jn.00112.2006. L. U. Perrinet and G. S. Masson. Modeling spatial integration in the ocular following response using a probabilistic framework. Journal of Physiology (Paris), 2007. doi: 10.1016/j.jphysparis.2007.10.011. Y. Weiss, E. P. Simoncelli, and E. H. Adelson. Motion illusions as optimal percepts. Nature Neuroscience, 5(6):598&ndash;604, Jun 2002. doi: 10.1038/nn858. This work was supported by EC IP project FP6-015879, &lsquo;&lsquo;FACETS&rsquo;&rsquo;.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-08-areadne/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>FENS Abstract</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/voges-08/" >Dynamics of cortical networks based on patchy connectivity patterns</a>
  </div>

  
  <a href="/publication/voges-08/"  class="summary-link">
    <div class="article-style">
      <p>Based on Nicole Voges, Laurent U Perrinet (2010). Phase space analysis of networks based on biologically realistic parameters. Journal of Physiology-Paris. PDF Cite DOI see follow-up : Nicole Voges, Laurent U Perrinet (2012).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-08/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of COSYNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-08-a/" >Modeling spatial integration in the ocular following response to center-surround stimulation using a probabilistic framework</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-08-a/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/andrew-p-davison/">Andrew P Davison</a></span>, <span >
      <a href="/author/daniel-bruderle/">Daniel Bruderle</a></span>, <span >
      <a href="/author/jochen-eppler/">Jochen Eppler</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/eilif-muller/">Eilif Muller</a></span>, <span >
      <a href="/author/dejan-pecevski/">Dejan Pecevski</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/pierre-yger/">Pierre Yger</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Frontiers in Neuroinformatics</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/davison-08/" >PyNN: A Common Interface for Neuronal Network Simulators</a>
  </div>

  
  <a href="/publication/davison-08/"  class="summary-link">
    <div class="article-style">
      <p>Computational neuroscience has produced a diversity of software for simulations of networks of spiking neurons, with both negative and positive consequences. On the one hand, each simulator uses its own programming or configuration language, leading to considerable difficulty in porting models from one simulator to another. This impedes communication between investigators and makes it harder to reproduce and build on the work of others. On the other hand, simulation results can be cross-checked between different simulators, giving greater confidence in their correctness, and each simulator has different optimizations, so the most appropriate simulator can be chosen for a given modelling task. A common programming interface to multiple simulators would reduce or eliminate the problems of simulator diversity while retaining the benefits. PyNN is such an interface, making it possible to write a simulation script once, using the Python programming language, and run it without modification on any supported simulator (currently NEURON, NEST, PCSIM, Brian and the Heidelberg VLSI neuromorphic hardware). PyNN increases the productivity of neuronal network modelling by providing high-level abstraction, by promoting code sharing and reuse, and by providing a foundation for simulator-agnostic analysis, visualization and data-management tools. PyNN increases the reliability of modelling studies by making it much easier to check results on multiple simulators. PyNN is open-source software and is available from <a href="http://neuralensemble.org/PyNN" target="_blank" rel="noopener">http://neuralensemble.org/PyNN</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-00586786" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/davison-08/davison-08.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/davison-08/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/open-science/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/neuro.11.011.2008" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2008
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of COSYNE</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-08/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-08/featured_huf90af6d15ba533b5d6ef408cf2965659_106730_58c97423c861319d4139eb2b8d99667e.webp" height="455" width="808"
            class="article-banner" alt="What adaptive code for efficient spiking representations? A model for the formation of receptive fields of simple cells" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-08/" >What adaptive code for efficient spiking representations? A model for the formation of receptive fields of simple cells</a>
  </div>

  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-08/perrinet-08.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-08/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/arvind-kumar/">Arvind Kumar</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Annual Computational Neuroscience Meeting: BMC Neuroscience</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kremkow-07-cns/" >Synchrony in thalamic inputs enhances propagation of activity through cortical layers</a>
  </div>

  
  <a href="/publication/kremkow-07-cns/"  class="summary-link">
    <div class="article-style">
      <p> see this subsequent paper in the Journal of Computational Neuroscience </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1186/1471-2202-8-S2-P180" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-07-cns/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1186/1471-2202-8-S2-P180" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-07/" >Dynamical Neural Networks: modeling low-level vision at short latencies</a>
  </div>

  
  <a href="/publication/perrinet-07/"  class="summary-link">
    <div class="article-style">
      <p>The machinery behind the visual perception of motion and the subsequent sensori-motor transformation, such as in ocular following response (OFR), is confronted to uncertainties which are efficiently resolved in the primate&rsquo;s visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theory [Weiss, Y., Simoncelli, E.P., Adelson, E.H., 2002. Motion illusions as optimal percepts. Nature Neuroscience, 5(6), 598-604, doi:10.1038/nn858] which we previously proved to be successfully adapted to model the OFR for different levels of noise with full field gratings. More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the spatial integration of motion: (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuli [Barthélemy, F.V., Vanzetta, I., Masson, G.S., 2006. Behavioral receptive field for ocular following in humans: dynamics of spatial summation and center-surround interactions. Journal of Neurophysiology, (95), 3712-3726, doi:10.1152/jn.00112.2006]. Herein, we extended the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We present analytical results which show that the hypothesis of independence of local measures can describe the spatial integration of the motion signal. Within this framework, we successfully accounted for the contrast gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another inhibitory mechanism had to be added to account for suppressive effects of the surround.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-07/perrinet-07.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-07/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1140/epjst/e2007-00061-7" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/bruno-cessac/">Bruno Cessac</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/cessac-07-a/" >Introduction to Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision</a>
  </div>

  
  <a href="/publication/cessac-07-a/"  class="summary-link">
    <div class="article-style">
      <p>Dynamical Neural Networks (DyNNs) are a class of models for networks of neurons where particular focus is put on the role of time in the emergence of functional computational properties. The definition and study of these models involves the cooperation of a large range of scientific fields from statistical physics, probabilistic modelling, neuroscience and psychology to control theory.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.springerlink.com/index/10.1140/epjst/e2007-00057-3" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cessac-07-a/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1140/epjst/e2007-00057-3" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/bruno-cessac/">Bruno Cessac</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/cessac-07/" >Topics in Dynamical Neural Networks: From Large Scale Neural Networks to Motor Control and Vision</a>
  </div>

  
  <a href="/publication/cessac-07/"  class="summary-link">
    <div class="article-style">
      <p>Dynamical Neural Networks (DyNNs) are a class of models for networks of neurons where particular focus is put on the role of time in the emergence of functional computational properties. The definition and study of these models involves the cooperation of a large range of scientific fields from statistical physics, probabilistic modelling, neuroscience and psychology to control theory.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/cessac-07/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/sylvain-fischer/">Sylvain Fischer</a></span>, <span >
      <a href="/author/filip-sroubek/">Filip Šroubek</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rafael-redondo/">Rafael Redondo</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Journal of Computer Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/fischer-07-cv/" >
      <div class="img-hover-zoom">
        <img src="/publication/fischer-07-cv/featured_hu5206ff3169ce7959a7c79d783ee48db0_121003_18a22346e3ada03fad40cc36470b7036.webp" height="455" width="808"
            class="article-banner" alt="Self-Invertible 2D Log-Gabor Wavelets" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/fischer-07-cv/" >Self-Invertible 2D Log-Gabor Wavelets</a>
  </div>

  
  <a href="/publication/fischer-07-cv/"  class="summary-link">
    <div class="article-style">
      <p>Meanwhile biorthogonal wavelets got a very popular image processing tool, alternative multiresolution transforms have been proposed for solving some of their drawbacks, namely the poor selectivity in orientation and the lack of translation invariance due to the aliasing between subbands. These transforms are generally overcomplete and consequently offer huge degrees of freedom in their design. At the same time their optimization get a challenging task. We proposed here a log-Gabor wavelet transform gathering the excellent mathematical properties of the Gabor functions with a carefully construction to maintain the properties of the filters and to permit exact reconstruction. Two major improvements are proposed: first the highest frequency bands are covered by narrowly localized oriented filters. And second, all the frequency bands including the highest and lowest frequencies are uniformly covered so as exact reconstruction is achieved using the same filters in both the direct and the inverse transforms (which means that the transform is self-invertible). The transform is optimized not only mathematically but it also follows as much as possible the knowledge on the receptive field of the simple cells of the Primary Visual Cortex (V1) of primates and on the statistics of natural images. Compared to the state of the art, the log-Gabor wavelets show excellent behavior in their ability to segregate the image information (e.g. the contrast edges) from incoherent Gaussian noise by hard thresholding and to code the image features through a reduced set of coefficients with large magnitude. Such characteristics make the transform a promising tool for general image processing tasks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/fischer-07-cv/fischer-07-cv.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fischer-07-cv/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/bicv/LogGabor" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s11263-006-0026-8" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/eric-castet/">Eric Castet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neuro-Computation: From Sensorimotor Integration to Computational Frameworks</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/montagnini-07/" >Bayesian modeling of dynamic motion integration</a>
  </div>

  
  <a href="/publication/montagnini-07/"  class="summary-link">
    <div class="article-style">
      <p>The quality of the representation of an object&rsquo;s motion is limited by the noise in the sensory input as well as by an intrinsic ambiguity due to the spatial limitation of the visual motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that motion processing of extended ob jects is initially dominated by the local 1D motion cues orthogonal to the ob ject&rsquo;s edges, whereas 2D information takes progressively over and leads to the final correct representation of global motion. A Bayesian framework accounting for the sensory noise and general expectancies for ob ject velocities has proven successful in explaining several experimental findings concerning early motion processing [1, 2, 3]. However, a complete functional model, encompassing the dynamical evolution of object motion perception is still lacking. Here we outline several experimental observations concerning human smooth pursuit of moving ob jects and more particularly the time course of its initiation phase. In addition, we propose a recursive extension of the Bayesian model, motivated and constrained by our oculomotor data, to describe the dynamical integration of 1D and 2D motion information.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/montagnini-07/montagnini-07.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-07/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2007.10.013" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/eric-castet/">Eric Castet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Perception 36 ECVP Abstract Supplement</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/montagnini-07-a/" >Dynamic inference for motion tracking</a>
  </div>

  
  <a href="/publication/montagnini-07-a/"  class="summary-link">
    <div class="article-style">
      <p>When the visual information about an object&rsquo;s motion differs at the local level, the visuomotor system needs to integrate information across time to solve this ambiguity and converge to the final motion solution. For an oblique line moving horizontally, edge-related motion cues differ from terminator-related information, the latter being coherent with the line&rsquo;s global motion. We have previously shown that ocular tracking of this kind of stimuli is transiently biased toward the edge-orthogonal direction, before converging to the global motion direction. Here, we model the dynamic convergence to the global-motion solution as a recursive update of inferential knowledge in the velocity space. We assume that motion estimation is based on a prior distribution and two independent likelihood functions representing edge-related and terminator-related information. Importantly, the shape of the Bayesian functions is constrained by smooth-pursuit eye-movement data. Model predictions about the dynamic convergence to the correct motion solution are compared to human smooth-pursuit recordings when varying different stimulus parameters (speed, contrast).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-07-a/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Physiology-Paris</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-07-neurocomp/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-07-neurocomp/featured_huc7452f7dccaf06930e122d5ade6d70cf_148774_e6c95a0dc34e45b45fe7082e3a1d266e.webp" height="455" width="808"
            class="article-banner" alt="Modeling spatial integration in the ocular following response using a probabilistic framework" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-07-neurocomp/" >Modeling spatial integration in the ocular following response using a probabilistic framework</a>
  </div>

  
  <a href="/publication/perrinet-07-neurocomp/"  class="summary-link">
    <div class="article-style">
      <p>The machinery behind the visual perception of motion and the subsequent sensori-motor transformation, such as in Ocular Following Response (OFR), is confronted to uncertainties which are efficiently resolved in the primate&rsquo;s visual system. We may understand this response as an ideal observer in a probabilistic framework by using Bayesian theory (Weiss et al., 2002) which we previously proved to be successfully adapted to model the OFR for different levels of noise with full field gratings (Perrinet et al., 2005). More recent experiments of OFR have used disk gratings and bipartite stimuli which are optimized to study the dynamics of center-surround integration. We quantified two main characteristics of the spatial integration of motion : (i) a finite optimal stimulus size for driving OFR, surrounded by an antagonistic modulation and (ii) a direction selective suppressive effect of the surround on the contrast gain control of the central stimuli (Barthélemy et al., 2006). Herein, we extended the ideal observer model to simulate the spatial integration of the different local motion cues within a probabilistic representation. We present analytical results which show that the hypothesis of independence of local measures can describe the integration of the spatial motion signal. Within this framework, we successfully accounted for the contrast gain control mechanisms observed in the behavioral data for center-surround stimuli. However, another inhibitory mechanism had to be added to account for suppressive effects of the surround.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-07-neurocomp/perrinet-07-neurocomp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-07-neurocomp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2007.10.011" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Mathematical image processing meeting (Marseille, France) September 5, 2007</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-07-mipm/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-07-mipm/featured_hu8ce7646902ebc51cb0e29e8aedfd34f9_31258_40ca18c9dade1c82ef7678b9b67e545a.webp" height="455" width="808"
            class="article-banner" alt="Neural Codes for Adaptive Sparse Representations of Natural Images" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-07-mipm/" >Neural Codes for Adaptive Sparse Representations of Natural Images</a>
  </div>

  
  <a href="/publication/perrinet-07-mipm/"  class="summary-link">
    <div class="article-style">
      <p>I will illustrate in this talk how computational neuroscience may inspire and be inspired by mathematical image processing. Focusing on efficiently representing natural images in the primary visual cortex, we derive an event-based adaptive algorithm inspired by statistical inference, Matching Pursuit and Hebbian learning. This algorithm allows to learn efficient ëdge-likë̊eceptive fields similarly to Independent Components Analysis. The correlation-based inhibition has been shown to be a necessary condition for the fomation of this type of receptive fields and shows the putative functional role of lateral propagation of information in cortical layers. I&rsquo;ll first present state-of-the-art neural algorithms for this task, the results of a detailed analysis of this Sparse Hebbian Learning algorithm and finally draw a comparison with similar strategies.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-07-mipm/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Sixteenth Annual Computational Neuroscience Meeting: CNS</em>2007, Toronto, Canada. 7&ndash;12 July 2007*
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-07-cns/" >On efficient sparse spike coding schemes for learning natural scenes in the primary visual cortex</a>
  </div>

  
  <a href="/publication/perrinet-07-cns/"  class="summary-link">
    <div class="article-style">
      <p>We describe the theoretical formulation of a learning algorithm in a model of the primary visual cortex (V1) and present results of the efficiency of this algorithm by comparing it to the SparseNet algorithm [1]. As the SparseNet algorithm, it is based on a model of signal synthesis as a Linear Generative Model but differs in the efficiency criteria for the representation. This learning algorithm is in fact based on an efficiency criteria based on the Occam razor: for a similar quality, the shortest representation should be privileged. This inverse problem is NP-complete and we propose here a greedy solution which is based on the architecture and nature of neural computations [2]). It proposes that the supra-threshold neural activity progressively removes redundancies in the representation based on a correlation-based inhibition and provides a dynamical implementation close to the concept of neural assemblies from Hebb [3]). We present here results of simulation of this network with small natural images and compare it to the Sparsenet solution. Extending it to realistic images and to the NEST simulator <a href="http://www.nest-initiative.org/" target="_blank" rel="noopener">http://www.nest-initiative.org/</a>, we show that this learning algorithm based on the properties of neural computations produces adaptive and efficient representations in V1. 1. Olshausen B, Field DJ: Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Res 1997, 37:3311-3325. 2. Perrinet L: Feature detection using spikes: the greedy approach. J Physiol Paris 2004, 98(4&ndash;6):530-539. 3. Hebb DO: The organization of behavior. Wiley, New York; 1949.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1186/1471-2202-8-S2-P206" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-07-cns/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1186/1471-2202-8-S2-P206" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/andrew-p-davison/">Andrew P Davison</a></span>, <span >
      <a href="/author/pierre-yger/">Pierre Yger</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/eilif-muller/">Eilif Muller</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Sixteenth Annual Computational Neuroscience Meeting: CNS</em>2007, Toronto, Canada. 7&ndash;12 July 2007*
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/davison-07-cns/" >PyNN: towards a universal neural simulator API in Python</a>
  </div>

  
  <a href="/publication/davison-07-cns/"  class="summary-link">
    <div class="article-style">
      <p>Trends in programming language development and adoption point to Python as the high-level systems integration language of choice. Python leverages a vast developer-base external to the neuroscience community, and promises leaps in simulation complexity and maintainability to any neural simulator that adopts it. PyNN <a href="http://neuralensemble.org/PyNN" target="_blank" rel="noopener">http://neuralensemble.org/PyNN</a> strives to provide a uniform application programming interface (API) across neural simulators. Presently NEURON and NEST are supported, and support for other simulators and neuromorphic VLSI hardware is under development. With PyNN it is possible to write a simulation script once and run it without modification on any supported simulator. It is also possible to write a script that uses capabilities specific to a single simulator. While this sacrifices simulator-independence, it adds flexibility, and can be a useful step in porting models between simulators. The design goals of PyNN include allowing access to low-level details of a simulation where necessary, while providing the capability to model at a high level of abstraction, with concomitant gains in development speed and simulation maintainability. Another of our aims with PyNN is to increase the productivity of neuroscience modeling, by making it faster to develop models de novo, by promoting code sharing and reuse across simulator communities, and by making it much easier to debug, test and validate simulations by running them on more than one simulator. Modelers would then become free to devote more software development effort to innovation, building on the simulator core with new tools such as network topology databases, stimulus programming, analysis and visualization tools, and simulation accounting. The resulting, community-developed &lsquo;meta-simulator&rsquo; system would then represent a powerful tool for overcoming the so-called complexity bottleneck that is presently a major roadblock for neural modeling.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1186/1471-2202-8-S2-P2" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/davison-07-cns/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/open-science/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1186/1471-2202-8-S2-P2" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/sylvain-fischer/">Sylvain Fischer</a></span>, <span >
      <a href="/author/rafael-redondo/">Rafael Redondo</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>EURASIP Journal on Advances in Signal Processing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/fischer-07/" >
      <div class="img-hover-zoom">
        <img src="/publication/fischer-07/featured_hu26618a75f04f1ffbe2c94c35f6894345_342795_1451b80093c784843a400f8e8dc14e20.webp" height="455" width="808"
            class="article-banner" alt="Sparse Approximation of Images Inspired from the Functional Architecture of the Primary Visual Areas" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/fischer-07/" >Sparse Approximation of Images Inspired from the Functional Architecture of the Primary Visual Areas</a>
  </div>

  
  <a href="/publication/fischer-07/"  class="summary-link">
    <div class="article-style">
      <p>relies on log-Gabor filters: Sylvain Fischer, Filip Šroubek, Laurent U Perrinet, Rafael Redondo, Gabriel Cristóbal (2007). Self-Invertible 2D Log-Gabor Wavelets. International Journal of Computer Vision. PDF Cite Code DOI Schematic structure of the primary visual cortex implemented in the present study.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/fischer-07/fischer-07.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fischer-07/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1155/2007/90727" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2007
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Vision</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/montagnini-07-b/" >Visual tracking of ambiguous moving objects: A recursive Bayesian model</a>
  </div>

  
  <a href="/publication/montagnini-07-b/"  class="summary-link">
    <div class="article-style">
      <p>Perceptual and oculomotor data demonstrate that, when the visual information about an object&rsquo;s motion differs on the local (edge-related) and global levels, the local 1D motion cues dominate initially, whereas 2D information takes progressively over and leads to the final correct representation of global motion. Previous models have explained the initial errors (deviations from the global motion) in terms of best perceptual guess in the Bayesian sense. These models accounted for the intrinsic sensory noise of the image and general expectancies for object velocities. Here we propose a recursive extension of the Bayesian model, with the purpose of encompassing the whole dynamical evolution of motion processing, from the 1D cues to the correct global motion. Our model is motivated and constrained by smooth pursuit oculomotor data. Eye movements were recorded in 3 participants using the scleral search coil technique. Participants were asked to track either a single line (vertical or oblique) or a Gaussian blob moving horizontally. In our model, oculomotor data obtained with non ambiguous stimuli (e.g. with coherent local and global information, such as a Gaussian blob or a vertical line moving horizontally) are combined to constrain the initial likelihood and prior functions for the general, ambiguous case (e.g. a tilted line moving horizontally). The prior knowledge is then recursively updated by using the previous posterior probability as the current prior. The idea is that the recursive injection of posterior distribution boosts the spread of information about the object&rsquo;s shape, favoring the integration of 1D and 2D cues. In addition, a simple model of the sensory-oculomotor loop is taken into account, including transmission delays and the evolution of the retinal motion during pursuit. Preliminary results show substantial agreement between the model prediction and the oculomotor data.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-07-b/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2006
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Fifteenth Annual Computational Neuroscience Meeting: CNS</em>2006*
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-06-cns/" >An efficiency razor for model selection and adaptation in the primary visual cortex</a>
  </div>

  
  <a href="/publication/perrinet-06-cns/"  class="summary-link">
    <div class="article-style">
      <p>We describe the theoretical formulation of a learning algorithm in a model of the primary visual cortex (V1) and present results of the efficiency of this algorithm by comparing it to the SparseNet algorithm (Olshausen, 1996). As the SparseNet algorithm, it is based on a model of signal synthesis as a Linear Generative Model but differs in the efficiency criteria for the representation. This learning algorithm is in fact based on an efficiency criteria based on the Occam razor: for a similar quality, the shortest representation should be privilegied. This inverse problem is NP-complete and we propose here a greedy solution which is based on the architecture and nature of neural computations (Perrinet, 2006). We present here results of a simulation of this network of small natural images (available at <a href="https://github.com/bicv/SparseHebbianLearning" target="_blank" rel="noopener">https://github.com/bicv/SparseHebbianLearning</a>) and compare it to the SparseNet solution. We show that this solution based on neural computations produces an adaptive algorithm for efficient representations in V1.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ocns.memberclicks.net/assets/docs/CNS_Program_books/2006booklet.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-06-cns/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/eric-castet/">Eric Castet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2006
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>1ère conférence francophone NEUROsciences COMPutationnelles (NeuroComp)</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/montagnini-06-neurocomp/" >Bayesian modeling of dynamic motion integration</a>
  </div>

  
  <a href="/publication/montagnini-06-neurocomp/"  class="summary-link">
    <div class="article-style">
      <p>The quality of the representation of an object&rsquo;s motion is limited by the noise in the sensory input as well as by an intrinsic ambiguity due to the spatial limitation of the visual motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that motion processing of extended ob jects is initially dominated by the local 1D motion cues orthogonal to the ob ject&rsquo;s edges, whereas 2D information takes progressively over and leads to the final correct represen- tation of global motion. A Bayesian framework accounting for the sensory noise and general expectancies for ob ject velocities has proven successful in explaining several experimental findings concerning early motion processing [1, 2, 3]. However, a complete functional model, encompassing the dynamical evolution of object motion perception is still lacking. Here we outline several experimental observations concerning human smooth pursuit of moving ob jects and more particu- larly the time course of its initiation phase. In addition, we propose a recursive extension of the Bayesian model, motivated and constrained by our oculomotor data, to describe the dynamical integration of 1D and 2D motion information.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.jphysparis.2007.10.013" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-06-neurocomp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2007.10.013" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/adrien-wohrer/">Adrien Wohrer</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/pierre-kornprobst/">Pierre Kornprobst</a></span>, <span >
      <a href="/author/thierry-vieville/">Thierry Vieville</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2006
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Perception</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/wohrer-06/" >Contrast sensitivity adaptation in a virtual spiking retina and its adequation with mammalians retinas</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/wohrer-06/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2006
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Physiogenic and pathogenic oscillations: the beauty and the beast, 5th INMED/TINS CONFERENCE SEPTEMBER 9 - 12, 2006, La Ciotat, France</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-06-ciotat/" >Dynamical contrast gain control mechanisms in a layer 2/3 model of the primary visual cortex</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-06-ciotat/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2006
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>The Functional Architecture of the Brain : from Dendrites to Networks. Symposium in honour of Dr Suzanne Tyc-Dumont. 4- 5 May 2006. GLM, Marseille, France</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-06-fab/" >Dynamical contrast gain control mechanisms in a layer 2/3 model of the primary visual cortex</a>
  </div>

  
  <a href="/publication/perrinet-06-fab/"  class="summary-link">
    <div class="article-style">
      <p>Computations in a cortical column are characterized by the dynamical, event-based nature of neuronal signals and are structured by the layered and parallel structure of cortical areas. But they are also characterized by their efficiency in terms of rapidity and robustness. We propose and study here a model of information integration in the primary visual cortex (V1) thanks to the parallel and interconnected network of similar cortical columns. In particular, we focus on the dynamics of contrast gain control mechanisms as a function of the distribution of information relevance in a small population of cortical columns. This cortical area is modeled as a collection of similar cortical columns which receive input and are linked according to a specific connectivity pattern which is relevant to this area. These columns are simulated using the sc Nest simulator Morrison04 using conductance-based Integrate-and-Fire neurons and consist vertically in 3 different layers. The architecture was inspired by neuro-physiological observations on the influence of neighboring activities on pyramidal cells activity and correlates with the lateral flow of information observed in the primary visual cortex, notably in optical imaging experiments Jancke04, and is similar in its final implementation to local micro-circuitry of the cortical column presented by Grossberg05.  They show prototypical spontaneous dynamical behavior to different levels of noise which are relevant to the generic modeling of biological cortical columns Kremkow05. In the future, the connectivity will be derived from an algorithm that was used for modeling the transient spiking response of a layer of neurons to a flashed image and which was based on the Matching Pursuit algorithm Perrinet04. The visual input is first transmitted from the Lateral Geniculate Nucleus (LGN) using the model of Gazeres98. It transforms the image flow into a stream of spikes with contrast gain control mechanisms specific to the retina and the LGN. This spiking activity converges to the pyramidal cells of layer 2/3 thanks to the specification of receptive fields in layer 4 providing a preference for oriented local contrasts in the spatio-temporal visual flow. In particular, we use in these experiments visual input organized in a center-surround spatial pattern which was optimized in size to maximize the response of a column in the center and to the modulation of this response by the surround (bipartite stimulus). This class of stimuli provide different levels of input activation and of visual ambiguity in the visual space which were present in the spatio-temporal correlations in the input spike flow optimized to the resolution of cortical columns in the visual space. It thus provides a method to reveal the dynamics of information integration and particularly of contrast gain control which are characteristic to the function of V1.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-06-fab/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/frederic-v-barthelemy/">Frédéric V Barthélemy</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2006
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>FENS</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-06-fens/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-06-fens/featured_hu6d0915f822ecda6788243f95be67f23d_60694_e6053fedb04e6e84c4a215a77daf12ba.webp" height="455" width="808"
            class="article-banner" alt="Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-06-fens/" >Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-06-fens/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/frederic-v-barthelemy/">Frédéric V Barthélemy</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2006
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>1ère conférence francophone NEUROsciences COMPutationnelles - NeuroComp</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-06-neurocomp/" >Input-output transformation in the visuo-oculomotor loop: modeling the ocular following response to center-surround stimulation in a probabilistic framework</a>
  </div>

  
  <a href="/publication/perrinet-06-neurocomp/"  class="summary-link">
    <div class="article-style">
      <p>The quality of the representation of an object&rsquo;s motion is limited by the noise in the sensory input as well as by an intrinsic ambiguity due to the spatial limitation of the visual motion analyzers (aperture problem). Perceptual and oculomotor data demonstrate that motion processing of extended ob jects is initially dominated by the local 1D motion cues orthogonal to the ob ject&rsquo;s edges, whereas 2D information takes progressively over and leads to the final correct representation of global motion. A Bayesian framework accounting for the sensory noise and general expectancies for ob ject velocities has proven successful in explaining several experimental findings concerning early motion processing [1, 2, 3]. However, a complete functional model, encompassing the dynamical evolution of object motion perception is still lacking. Here we outline several experimental observations concerning human smooth pursuit of moving ob jects and more particularly the time course of its initiation phase. In addition, we propose a recursive extension of the Bayesian model, motivated and constrained by our oculomotor data, to describe the dynamical integration of 1D and 2D motion information.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-06-neurocomp/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/rafael-redondo/">Rafael Redondo</a></span>, <span >
      <a href="/author/sylvain-fischer/">Sylvain Fischer</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August 2005
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Perception</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/redondo-05/" >Modeling of simple cells through a sparse overcomplete gabor wavelet representation based on local inhibition and facilitation</a>
  </div>

  
  <a href="/publication/redondo-05/"  class="summary-link">
    <div class="article-style">
      <p>We present a biologically plausible model of simple cortical cells as 1) a linear transform representing edges and 2) a non-linear iterative stage of inhibition and facilitation between neighboring coefficients. The linear transform is a complex log-Gabor wavelet transform which is overcomplete (i.e. there are more coefficients than pixels in the image) and has exact reconstruction. The inhibition consists in diminishing down the coefficients which are not at a local-maxima along the direction normal to the edge filter orientation, whereas the facilitation enhances the collinear and co-aligned local-maximum coefficients. At each iteration and after the inhibition and facilitation stages, the reconstructed error is subtracted in the transform domain for keeping an exact reconstruction. Such process concentrates the signal energy on a few coefficients situated along the edges of the objects, yielding a sparse representation. The rationale for such procedure is: (1) th e overcompleteness offers flexibility for activity reassignment; (2) images can be coded by sparse Gabor coefficients located on object edges; (3) image contours produce aligned and collinear local-maxima in the transform domain; (4) the inhibition/facilitation processes are able to extract the contours. The sparse Gabor coefficients are mostly connected each other and located along object contours. Such layout makes chain coding suitable for compression purposes. Specially adapted to Gabor wavelets features, our chain coding represents every chain by its end-points (head and tail) and the elementary movements necessary to walk along the chain from head to tail. Moreover it predicts the module and phase of each Gabor coefficient according to the previous chain coefficient. As a result, redundancy of the transform domain is further reduced. Used for compression, the scheme limits particularly the high-frequency artifacts. The model performs also efficiently in tasks the Human Visual System is supposed to deal with, as for instance edge extraction and image denoising.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/redondo-05/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/sylvain-fischer/">Sylvain Fischer</a></span>, <span >
      <a href="/author/rafael-redondo/">Rafael Redondo</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2005
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Microtechnologies for the New Millennium 2005</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/fischer-05-a/" >Sparse Gabor wavelets by local operations</a>
  </div>

  
  <a href="/publication/fischer-05-a/"  class="summary-link">
    <div class="article-style">
      <p>Efficient sparse coding of overcomplete transforms remains still anopen problem. Different methods have been proposed in theliterature, but most of them are limited by a heavy computationalcost and by difficulties to find the optimal solutions. We proposehere an algorithm suitable for Gabor wavelets and based onbiological models. It is composed by local operations betweenneighboring transform coefficients and achieves a sparserepresentation with a relatively low computational cost. Used with achain coder, this sparse Gabor wavelet transform is suitable forimage compression but is also of interest also for otherapplications, in particular for edge and contour extraction andimage denoising.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1117/12.608403" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fischer-05-a/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1117/12.608403" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/frederic-v-barthelemy/">Frédéric V Barthélemy</a></span>, <span >
      <a href="/author/eric-castet/">Eric Castet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2005
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Perception</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-05-a/" >Dynamics of motion representation in short-latency ocular following: A two-pathways Bayesian model</a>
  </div>

  
  <a href="/publication/perrinet-05-a/"  class="summary-link">
    <div class="article-style">
      <p>The integration of information is essential to measure the exact 2D motion of a surface from both local ambiguous 1D motion produced by elongated edges and local non-ambiguous 2D motion from features such as corners, end-points or texture elements. The dynamics of this motion integration shows a complex time course which can be read from tracking eye movements: local 1D motion signals are extracted first and then pooled to initiate the ocular responses before that 2D motion signals are taken into account to refine the tracking direction until it matches the surface motion direction. The nature of these 1D and 2D motion computations is still unclear. Previously, we have shown that the late, 2D-driven response components to either plaids or barber-poles have very similar latencies over a large range of contrast, suggesting a shared mechanism. However, they showed different contrast response functions with these different motion stimuli, suggesting different motion processing. We designed a two-pathways Bayesian model of motion integration and showed that this family of contrast response functions can be predicted from the probability distributions of 1D and 2D motion signals for each type of stimulus. Indeed, this formulation may explain contrast response functions that could not be explained by a simple bayesian model (Weiss et al., 2002 em Nature Neuroscience bf 5 , 598&ndash;604) and gives a quantitative argument to study how local information with different relative ambiguities values may be pooled to provide an integrated response of the system. Finally, we formulate how different spatial information may be pooled and we draw the analogy of this method with methods using the partial derivative equations. This simple model correctly explains some non-linear interactions between neighboring neurons selective to motion direction which are observed in short-latency ocular following and neuro-physiological data.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-05-a/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/sylvain-fischer/">Sylvain Fischer</a></span>, <span >
      <a href="/author/rafael-redondo/">Rafael Redondo</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2005
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Perception</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/fischer-05/" >Efficient representation of natural images using local cooperation</a>
  </div>

  
  <a href="/publication/fischer-05/"  class="summary-link">
    <div class="article-style">
      <p>Low-level perceptual computations may be understood in terms of efficient codes (Simoncelli and Olshausen, 2001, Annual Review of Neuroscience 24 1193-216). Following this argument, we explore models of representation for natural static images as a way to understand the processing of information in the primary visual cortex. This representation is here based on a generative linear model of the synthesis of images using an over-complete multi-resolution dictionary of edges. This transform is implemented using log-Gabor filters and permits an exact reconstruction of any image. However, this linear representation is redundant and since to any image may correspond different representations, we explore more efficient representations of the image. The problem is stated as an ill-posed inverse problem and we compare first different known strategies by computing the efficiency of the solutions given by Matching Pursuit (Perrinet, 2004, IEEE Trans. Neural Networks 15 1164-75) and sparse edge coding (Fischer, in press, Trans. Image Processing) with classical representation methods such as JPEG. This comparison allows us to provide a synthesized approach using a probabilistic representation which would progressively construct the neural representation by using lateral cooperations. We propose an algorithm which dynamically diffuses information to correlated filters so as to yield a progressively disambiguated representation. This approach takes advantage of the computational properties of spiking neurons such as Integrate-and-Fire neurons and provides an efficient yet simple model for the representation of natural images. This representation is directly linked with the edge content of natural images and we show applications of this method to edge extraction, denoising and compression. We also show that this dynamical approach fits with neuro-physiological observations and may explain the non-linear interactions between neighboring neurons which may be observed in the cortex.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fischer-05/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2005
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Conference on Artificial Neural Networks</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-05/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-05/featured_huae60da5db10374fa5838b4ff33428f2e_33903_d5b9a3b7b7afdbe5067f078e5dec813b.webp" height="455" width="808"
            class="article-banner" alt="Efficient Source Detection Using Integrate-and-Fire Neurons" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-05/" >Efficient Source Detection Using Integrate-and-Fire Neurons</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1007/11550822_27" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-05/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/11550822_27" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2004
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Neural Networks</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-03-ieee/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-03-ieee/featured_hu012e2aaf68a5a61f3f58b08de648099c_24956_ee03513178bcbbdcc243d373ca3f9057.webp" height="455" width="808"
            class="article-banner" alt="Coding static natural images using spiking event times: do neurons cooperate?" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-03-ieee/" >Coding static natural images using spiking event times: do neurons cooperate?</a>
  </div>

  
  <a href="/publication/perrinet-03-ieee/"  class="summary-link">
    <div class="article-style">
      <p>To understand possible strategies of temporal spike coding in the central nervous system, we study functional neuromimetic models of visual processing for static images. We will first present the retinal model which was introduced by Van Rullen and Thorpe [1] and which represents the multiscale contrast values of the image using an orthonormal wavelet transform. These analog values activate a set of spiking neurons which each fire once to produce an asynchronous wave of spikes. According to this model, the image may be progressively reconstructed from this spike wave thanks to regularities in the statistics of the coefficients determined with natural images. Here, we study mathematically how the quality of information transmission carried by this temporal representation varies over time. In particular, we study how these regularities can be used to optimize information transmission by using a form of temporal cooperation of neurons to code analog values. The original model used wavelet transforms that are close to orthogonal. However, the selectivity of realistic neurons overlap, and we propose an extension of the previous model by adding a spatial cooperation between filters. This model extends the previous scheme for arbitrary -and possibly non-orthogonal representations of features in the images. In particular, we compared the performance of increasingly over-complete representations in the retina. Results show that this algorithm provides an efficient spike coding strategy for low-level visual processing which may adapt to the complexity of the visual input.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/q-bio/0611002" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-03-ieee/perrinet-03-ieee.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-03-ieee/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TNN.2004.833303" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2004
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Journal of Physiology-Paris</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-04-tauc/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-04-tauc/featured_hua3bc927deaf292746d5bee0125cf99c5_87834_b0c08a81561d861b93b35530930b5b21.webp" height="455" width="808"
            class="article-banner" alt="Feature detection using spikes : the greedy approach" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-04-tauc/" >Feature detection using spikes : the greedy approach</a>
  </div>

  
  <a href="/publication/perrinet-04-tauc/"  class="summary-link">
    <div class="article-style">
      <p>A goal of low-level neural processes is to build an efficient code extracting the relevant information from the sensory input. It is believed that this is implemented in cortical areas by elementary inferential computations dynamically extracting the most likely parameters corresponding to the sensory signal. We explore here a neuro-mimetic feed-forward model of the primary visual area (V1) solving this problem in the case where the signal may be described by a robust linear generative model. This model uses an over-complete dictionary of primitives which provides a distributed probabilistic representation of input features. Relying on an efficiency criterion, we derive an algorithm as an approximate solution which uses incremental greedy inference processes. This algorithm is similar to &lsquo;Matching Pursuit&rsquo; and mimics the parallel architecture of neural computations. We propose here a simple implementation using a network of spiking integrate-and-fire neurons which communicate using lateral interactions. Numerical simulations show that this Sparse Spike Coding strategy provides an efficient model for representing visual data from a set of natural images. Even though it is simplistic, this transformation of spatial data into a spatio-temporal pattern of binary events provides an accurate description of some complex neural patterns observed in the spiking activity of biological neural networks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/q-bio/0611003" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.jphysparis.2005.09.012" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-04-tauc/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2005.09.012" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2004
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neurocomputing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-02-sparse/" >Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit</a>
  </div>

  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-00276638" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.neucom.2004.01.010" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-02-sparse/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.neucom.2004.01.010" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2004
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Natural Computing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-04/" >Finding Independent Components using spikes : a natural result of Hebbian learning in a sparse spike coding scheme</a>
  </div>

  
  <a href="/publication/perrinet-04/"  class="summary-link">
    <div class="article-style">
      <p>To understand possible strategies of temporal spike coding in the central nervous system, we study functional neuromimetic models of visual processing for static images. We will first present the retinal model which was introduced by Van Rullen and Thorpe [1] and which represents the multiscale contrast values of the image using an orthonormal wavelet transform. These analog values activate a set of spiking neurons which each fire once to produce an asynchronous wave of spikes. According to this model, the image may be progressively reconstructed from this spike wave thanks to regularities in the statistics of the coefficients determined with natural images. Here, we study mathematically how the quality of information transmission carried by this temporal representation varies over time. In particular, we study how these regularities can be used to optimize information transmission by using a form of temporal cooperation of neurons to code analog values. The original model used wavelet transforms that are close to orthogonal. However, the selectivity of realistic neurons overlap, and we propose an extension of the previous model by adding a spatial cooperation between filters. This model extends the previous scheme for arbitrary -and possibly non-orthogonal representations of features in the images. In particular, we compared the performance of increasingly over-complete representations in the retina. Results show that this algorithm provides an efficient spike coding strategy for low-level visual processing which may adapt to the complexity of the visual input.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1023/B:NACO.0000027753.27593.a7" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-04/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1023/B:NACO.0000027753.27593.a7" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2003
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-03-these/" >Comment déchiffrer le code impulsionnel de la vision ? Étude du flux parallèle, asynchrone et épars dans le traitement visuel ultra-rapide</a>
  </div>

  
  <a href="/publication/perrinet-03-these/"  class="summary-link">
    <div class="article-style">
      <p>Le jury était consistué (de gauche à droite) de Jeanny Hérault (Rapporteur), Michel Imbert (Président), Yves Burnod (Rapporteur, absent de la photo), Manuel Samuelides (Directeur de thèse) et Simon Thorpe (Co-directeur de thèse).</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://tel.archives-ouvertes.fr/tel-00002693/file/tel-000026931.pdf" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-03-these" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-03-these/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2003
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neurocomputing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-03/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-03/featured_hu5f66bff7003a61ee81bff37def98049c_37334_71a80aef65e1c09f2055f59896ab5468.webp" height="455" width="808"
            class="article-banner" alt="Emergence of filters from natural scenes in a sparse spike coding scheme" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-03/" >Emergence of filters from natural scenes in a sparse spike coding scheme</a>
  </div>

  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.neucom.2004.01.133" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-03/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.neucom.2004.01.133" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    June 2002
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neurocomputing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-02-stdp/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-02-stdp/featured_huc22323a6b39584b1a3894d825187698f_140268_ca6b76ceb149350f9bfdaddb5addda2d.webp" height="455" width="808"
            class="article-banner" alt="Coherence detection in a spiking neuron via Hebbian learning" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-02-stdp/" >Coherence detection in a spiking neuron via Hebbian learning</a>
  </div>

  
  <a href="/publication/perrinet-02-stdp/"  class="summary-link">
    <div class="article-style">
      <p>It is generally assumed that neurons in the central nervous system communicate through temporal firing patterns. As a first step, we will study the learning of a layer of realistic neurons in the particular case where the relevant messages are formed by temporally correlated patterns, or synfire patterns. The model is a layer of Integrate-and-Fire (IF) neurons with synaptic current dynamics that adapts by minimizing a cost according to a gradient descent scheme. This leads to a rule similar to Spike-Time Dependent Hebbian Plasticity (STDHP). Our results show that the rule that we derive is biologically plausible and leads to the detection of the coherence in the input in an unsupervised way. An application to shape recognition is shown as an illustration.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-02-stdp/perrinet-02-stdp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-02-stdp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/S0925-2312%2802%2900374-0" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2002
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of ESANN</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-02-esann/" >Sparse Image Coding Using an Asynchronous Spiking Neural Network</a>
  </div>

  
  <a href="/publication/perrinet-02-esann/"  class="summary-link">
    <div class="article-style">
      <p> Progressive reconstruction of a static image using spikes in a Laplacian pyramid. </p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-02-esann/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2002
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Actes de Neurosciences et Sciences de l&rsquo;Ingenieur, L&rsquo;Agelonde,</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/publication/perrinet-02-nsi/" >
      <div class="img-hover-zoom">
        <img src="/publication/perrinet-02-nsi/featured_hu8d40457096bac30a7b45725fab0b8317_150379_274ae1e8d120eaeee31ec3536de722c0.webp" height="455" width="808"
            class="article-banner" alt="Visual Strategies for Sparse Spike Coding" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-02-nsi/" >Visual Strategies for Sparse Spike Coding</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-02-nsi/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/arnaud-delorme/">Arnaud Delorme</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2001
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neurocomputing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-01/" >Network of integrate-and-fire neurons using Rank Order Coding A: how to implement spike timing dependant plasticity</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-01/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/S0925-2312%2801%2900460-X" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/arnaud-delorme/">Arnaud Delorme</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2001
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Neurocomputing</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/delorme-01/" >Network of integrate-and-fire neurons using Rank Order Coding B: spike timing dependant plasticity and emergence of orientation selectivity</a>
  </div>

  
  <a href="/publication/delorme-01/"  class="summary-link">
    <div class="article-style">
      <p>Rank Order Coding is an alternative to conventional rate coding schemes that uses the order in which a neuron&rsquo;s inputs fire to encode information. In a visual system framework, we simulated the asynchronous waves of retinal spikes produced in response to natural scenes and used them to stimulate integrate-and-fire V1 neurons that implemented a standard learning rule based on spike timing. After propagating thousands of images, orientation like receptive fields arise in these neurons despite the &hellip;</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.4990" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/delorme-01/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1.1.18.4990" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2000
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Proceedings of DYNN</em>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-00/" >A generative model for Spike Time Dependent Hebbian Plasticity</a>
  </div>

  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-00/perrinet-00.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-00/cite.bib">
  Cite
</a>















  </div>
  

</div>

  
    












<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 1999
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/perrinet-99/" >Apprentissage hebbien d&#39;un reseau de neurones asynchrone a codage par rang</a>
  </div>

  

  
  <div class="btn-links">
    








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-99/cite.bib">
  Cite
</a>















  </div>
  

</div>

  

  
  
  

</div>


  
    </div>
  

  </div>
</section>


  






























































<section id="posts" class="home-section wg-collection  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Recent Events</h1>
          
        </div>
      
    
  

    










  









  
  
  
  
  




















  




<div class="col-12 col-lg-8">

  

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/post/2023-12-16-risc/" >Participation au jury du RISC</a>
    </div>

    
    <a href="/post/2023-12-16-risc/"  class="summary-link">
      <div class="article-style">
        Rencontres Internationales Sciences Et Cinémas
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Dec 16, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    1 min read
  </span>
  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/art-science/">
    Project
  </a>
  

  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="http://festivalrisc.org/14e-edition/" target="_blank" rel="noopener">
    URL</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/post/2023-07-28-sciencesetavenir/" >Comment notre cerveau fait-il face à l’incertitude ?</a>
    </div>

    
    <a href="/post/2023-07-28-sciencesetavenir/"  class="summary-link">
      <div class="article-style">
        Participation à un article de dissémination pour le magazine en ligne Sciences &amp; Avenir, écrit par Alice Carliez: Comment notre cerveau fait-il face à l’incertitude ?
Une équipe du CNRS et d&rsquo;Aix-Marseille Université a élucidé des mécanismes neuronaux qui représentent la perception de stimuli visuels plus ou moins précis.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jul 28, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    1 min read
  </span>
  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.sciencesetavenir.fr/sante/cerveau-et-psy/l-incertitude-est-dans-notre-tete-litteralement_172883" target="_blank" rel="noopener">
    URL</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/post/2023-07-26-epsiloon/" >Ce que le paranormal dit de notre cerveau</a>
    </div>

    
    <a href="/post/2023-07-26-epsiloon/"  class="summary-link">
      <div class="article-style">
        Article de dissémination sur ce que l&rsquo;étrange et le paranormal peut révéler sur notre cerveau.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jul 26, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    1 min read
  </span>
  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.epsiloon.com/tous-les-numeros/n26/ce_que_le_paranormal_dit_de_notre_cerveau/" target="_blank" rel="noopener">
    URL</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/post/2023-05-01_postdoc-position_polychronies/" >Postdoc position &#34;Accurate detection of precise spiking motifs in neurobiological data&#34;</a>
    </div>

    
    <a href="/post/2023-05-01_postdoc-position_polychronies/"  class="summary-link">
      <div class="article-style">
        THE POSITION HAS BEEN FILLED. Dear colleagues,
Applications are welcome for a fully funded 18-month postdoctoral position for the development of an algorithm for the accurate detection of precise spiking motifs in neurobiological data.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 1, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  
  
  

  
  

</div>

      
    </div>

    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/post/2023-05-01_postdoc-position_polychronies/" >
        <img src="/post/2023-05-01_postdoc-position_polychronies/featured_hu4b94a7d822f46d57b9e6c817a3126011_520198_92c534657957803bf85b3b6f4d8c2536.webp" height="58" width="150"
            alt="Postdoc position &#34;Accurate detection of precise spiking motifs in neurobiological data&#34;" loading="lazy">
      </a>
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/post/2022-09-30_cristal-no2/" >Cristal N°2</a>
    </div>

    
    <a href="/post/2022-09-30_cristal-no2/"  class="summary-link">
      <div class="article-style">
        Cristal N°2 - Arbre théorique / 2014
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/etienne-rey/">Étienne Rey</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Sep 30, 2022
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    1 min read
  </span>
  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/art-science/">
    Project
  </a>
  











    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/post/2022-09-30_cristal-no2/" >
        <img src="/post/2022-09-30_cristal-no2/featured_huf16f41a7865936c1a400f6904d455748_375137_2f3b24c505c8be343f88135e8d024239.webp" height="84" width="150"
            alt="Cristal N°2" loading="lazy">
      </a>
    
  </div>
</div>

  

  
  
  

    
    
      
    

    
    
    
    
      
    

    

    <div class="see-all">
      <a href="/post/">
        See all posts
        <i class="fas fa-angle-right"></i>
      </a>
    </div>
  

</div>


  
    </div>
  

  </div>
</section>


  






























































<section id="projects" class="home-section wg-portfolio  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Projects</h1>
          
        </div>
      
    
  

    












<div class="col-12 col-lg-8">

  

  

  <div class=" row js-layout-row ">

    
    

    
    
      
      
    
    
    
    
    
      
    

    
    
    
    
    

    
      
        
          <div class="col-12 isotope-item js-id-research-interests">
        
        







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/project/courses/" >Cours et tutoriels</a>
    </div>

    
    <a href="/project/courses/"  class="summary-link">
      <div class="article-style">
        Liste de cours et tutoriels.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
    </div>

    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/project/courses/" >
        <img src="/project/courses/featured_hub470942410a0dc91cb113d02f5417e5b_70418_fa8b7d1a51a2b0ac317667c804a8d0a0.webp" height="93" width="150"
            alt="Cours et tutoriels" loading="lazy">
      </a>
    
  </div>
</div>

      </div>
    
      
        
          <div class="col-12 isotope-item js-id-research-interests">
        
        







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/project/art-science/" >Art &lt;&gt; Sciences</a>
    </div>

    
    <a href="/project/art-science/"  class="summary-link">
      <div class="article-style">
        Liste d&rsquo;actions entre art et sciences.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
    </div>

    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/project/art-science/" >
        <img src="/project/art-science/featured_hue3822df57c1b9a9eb20e3c7e65067536_920785_27ebc72f4f50378a5579d4ab13b722c2.webp" height="80" width="150"
            alt="Art &lt;&gt; Sciences" loading="lazy">
      </a>
    
  </div>
</div>

      </div>
    
      
        
          <div class="col-12 isotope-item js-id-log-gabor js-id-psychophysics js-id-motion-clouds">
        
        







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/project/open-science/" >Open Science</a>
    </div>

    
    <a href="/project/open-science/"  class="summary-link">
      <div class="article-style">
        To enable the dissemination of the knowledge that is produced in our lab, we share all source code with open source licences.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
    </div>

    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/project/open-science/" >
        <img src="/project/open-science/featured_hub98f048a5335e0d9fa9d564b8f2ceb5d_283815_d74347a19735b3a23358a2b03cb83171.webp" height="93" width="150"
            alt="Open Science" loading="lazy">
      </a>
    
  </div>
</div>

      </div>
    
      
        
          <div class="col-12 isotope-item js-id-research-interests js-id-etiennerey js-id-pollymaggoo">
        
        







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/project/tout-public/" >Tout public!</a>
    </div>

    
    <a href="/project/tout-public/"  class="summary-link">
      <div class="article-style">
        Liste d&rsquo;actions destinées à la culture scientifique et au public en général.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
    </div>

    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/project/tout-public/" >
        <img src="/project/tout-public/featured_hu3d03a01dcc18bc5be0e67db3d8d209a6_329522_72daf7a1b1c85efe917ebdea3815b7c8.webp" height="84" width="150"
            alt="Tout public!" loading="lazy">
      </a>
    
  </div>
</div>

      </div>
    

  </div>
</div>


  
    </div>
  

  </div>
</section>


  






























































<section id="people" class="home-section wg-people  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  

    











<div class="row justify-content-center people-widget">
  
  <div class="col-md-12 section-heading">
    <h1>This would not be possible without&hellip;</h1>
    
  </div>
  

  

  
  

  
  <div class="col-md-12">
    <h2 class="mb-4">Mentors</h2>
  </div>
  

  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/yves-fregnac/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/yves-fregnac/avatar_hu7f243e40992babb9913c4d41003b14d5_58451_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/yves-fregnac/">Yves Fregnac</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=S1vqJ6YAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://neuropsi.cnrs.fr/annuaire/yves-fregnac/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/karl-friston/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/karl-friston/avatar_hu92039d33632f44c312192b53453beb0d_7327_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/karl-friston/">Karl Friston</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=q_4u0aoAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.fil.ion.ucl.ac.uk/~karl/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/manuel-samuelides/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/manuel-samuelides/avatar_hub9b161703a895f4d30df100f4615038d_7557_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/manuel-samuelides/">Manuel Samuelides</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Manuel_Samuelides" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/simon-thorpe/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/simon-thorpe/avatar_hud391edb759e5f0db43dc841911ffaee9_20018_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/simon-thorpe/">Simon Thorpe</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/scientific-contributions/2162934512_Simon_J_Thorpe" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=uR-7ex4AAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://cerco.cnrs.fr/page-perso-simon-thorpe/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  

  
  <div class="col-md-12">
    <h2 class="mb-4">Collaborators</h2>
  </div>
  

  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/emmanuel-dauce/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/emmanuel-dauce/avatar_hu1fde9367436786238e788425b403acc0_7959_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/gdbmanu" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Emmanuel_Dauce2" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://emmanuel.dauce.free.fr/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/ryad-benosman/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/ryad-benosman/avatar_hu2d5c77d2b84468d5dedad740d610465b_7790_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/ryad-benosman/">Ryad Benosman</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=_ZTFUooAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.institut-vision.org/fr/vision-and-natural-computation.html" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/jean-martinet/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/jean-martinet/avatar_hu1fb3148343823105600c1093497f8340_7839_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/jean-martinet/">Jean Martinet</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Jean_Martinet" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=dyxxR_gAAAAJ&amp;hl" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/etienne-rey/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/etienne-rey/avatar_hu9475e40123017607cf3e014e405fbc8c_82944_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/etienne-rey/">Étienne Rey</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/etienne-rey-a07b5718/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://ondesparalleles.org/" target="_blank" rel="noopener">
        <i class="fas fa-code"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://www.documentsdartistes.org/artistes/rey/repro.html" target="_blank" rel="noopener">
        <i class="fas fa-house-user"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.instagram.com/etienne_rey_/" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/EtienneRey" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/rosa-cossart/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/rosa-cossart/avatar_huaf5feebe594af9a561667d5fa95b4a29_502072_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/rosa-cossart/">Rosa Cossart</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.inmed.fr/developpement-des-microcircuits-gabaergiques-corticaux-fr" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/thomas-schatz/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/thomas-schatz/avatar_hu39780d764ac4a46a3f00748b9d881828_112007_270x270_fill_lanczos_center_3.png" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/thomas-schatz/">Thomas Schatz</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=dQ0I23kAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://thomas.schatz.cogserver.net/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/frederic-chavane/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/frederic-chavane/avatar_hu939e22bfda132e5dfe756f4281477277_68002_270x270_fill_lanczos_center_3.png" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/frederic-chavane/">Frédéric Chavane</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Frederic_Chavane" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=LT0P6OwAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/anna-montagnini/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/anna-montagnini/avatar_hub896de861e914f410b9e146cfca223b5_17258_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/anna-montagnini/">Anna Montagnini</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/annamontagnini" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Anna_Montagnini" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/anna-montagnini-a292606" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/laurent-madelain/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/laurent-madelain/avatar_hu421c3ace755d2d1853ec11ea5cf6a0c3_58272_270x270_fill_lanczos_center_3.png" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/laurent-madelain/">Laurent Madelain</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Laurent_Madelain" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=kdP3TKQAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://pro.univ-lille.fr/laurent-madelain/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/stephane-viollet/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/stephane-viollet/avatar_hubd61016f0516c23f97ee4a475538d118_14706_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/stephane-viollet/">Stéphane Viollet</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=iIGoymcAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/chloe-pasturel/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/chloe-pasturel/avatar_hucfafec1f19aab4f9e024b0deff0af481_1015588_270x270_fill_lanczos_center_3.png" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/chloe-pasturel/">Chloé Pasturel</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/chloepasturel" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/maria-jose-escobar/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/maria-jose-escobar/avatar_hue4e97eb5dd83356e0e66932cde85a779_6082_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/maria-jose-escobar/">Maria Jose Escobar</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/mjescobar" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=EQfyAUAAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/andrew-isaac-meso/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/andrew-isaac-meso/avatar_hue5138c45e540402f69689ed0a618949e_12015_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></h2>
      
      <h3>Lecturer, King&rsquo;s College London (IOPPN).</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=yPCBdoUAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/jonathan-vacher/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/jonathan-vacher/avatar_hu37f7ac16ff525828e4a365d2eb0f3d24_25580_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/jonathan-vacher/">Jonathan Vacher</a></h2>
      
      <h3>Maître de Conférence (Associate Professor) @ MAP5, Université Paris-Cité.</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/JonathanVacher" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/JonathanVacher" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://jonathanvacher.github.io/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://stackoverflow.com/users/8684783/Jonathan%20Vacher" target="_blank" rel="noopener">
        <i class="ai ai-stackoverflow"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/rick-a-adams/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/rick-a-adams/avatar_hu6ef1e30419e2c3a3e92e6ad40e4620b0_40299_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/rick-a-adams/">Rick A Adams</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Rick_Adams3" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=dAi_S_MAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/james-a-bednar/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/james-a-bednar/avatar_huc34f96cf930cec3e58ab3359b634bce6_7490_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/james-a-bednar/">James A Bednar</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/jbednar" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/7217561_James_A_Bednar" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/james-bednar-7602911b" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=h0e2kMQAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/paula-sanz-leon/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/paula-sanz-leon/avatar_hu53ac77df19e2ab2358b560c93fd43eab_21656_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/paula-sanz-leon/">Paula Sanz Leon</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/pausz" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Paula_Sanz-leon" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/paulasanzleon" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=ZbJ97FAAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/ede-rancz/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/ede-rancz/avatar_hu14a206cf0775cc579ea4c2f4412d06e4_104057_270x270_fill_q75_lanczos_center.jpeg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/ede-rancz/">Ede Rancz</a></h2>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Ede-Rancz" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=_kpvz3gAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.inmed.fr/en/en-circuits-corticaux" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/matthieu-gilson/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/matthieu-gilson/avatar_hu649687aaa067e9fe98cb8aa085fbad98_57526_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/matthieu-gilson/">Matthieu Gilson</a></h2>
      
      <h3>Researcher in Computational Neuroscience</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/matthieu.gilson%20[at]%20univ-amu.fr" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0002-6726-7207" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://matthieugilson.eu" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      <p class="people-interests">Computational Neuroscience, Spiking Neural Networks, Machine Learning, Vision</p>
    </div>
  </div>
  
  
  

  
  <div class="col-md-12">
    <h2 class="mb-4">Current Students</h2>
  </div>
  

  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/hugo-ladret/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/hugo-ladret/avatar_hud84a483f73decf0a7d85ed2f62cffba3_42123_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/hugo-ladret/">Hugo Ladret</a></h2>
      
      <h3>Phd candidate in Computational Neuroscience</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/hugoladret" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/hugoladret/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/hugoladret" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://hugoladret.github.io" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/adrien-fois/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/adrien-fois/avatar_hue62199eaca6c09141a552f0fe3bf66d9_5103_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/adrien-fois/">Adrien Fois</a></h2>
      
      <h3>Post-doctoral Researcher</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Adrien-Fois-3" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/jean-nicolas-jeremie/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/jean-nicolas-jeremie/avatar_hua40b5f042c40f2b2bbad89397dde279b_69684_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></h2>
      
      <h3>Phd candidate in Computational Neuroscience</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/JNJER/" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/JnJerem" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/jeremie-jean-nicolas-91306a1a1/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/antoine-grimaldi/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/antoine-grimaldi/avatar_hu85406bb2d5f7db2dce1cab01b4e48063_27520_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></h2>
      
      <h3>Phd candidate in Computational Neuroscience</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/AntoineGrimaldi" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0002-3107-4788" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://antoinegrimaldi.fr/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Antoine-Grimaldi-2" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/A_Grismaldi" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/hilde-langengen-teigen/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/hilde-langengen-teigen/avatar_hu5e03a6cb149ecb2adadf4ea7a34b485f_16172_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/hilde-langengen-teigen/">Hilde Langengen-Teigen</a></h2>
      
      <h3>Phd candidate in Neuroscience</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  

  
  <div class="col-md-12">
    <h2 class="mb-4">Former Students</h2>
  </div>
  

  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/victor-boutin/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/victor-boutin/avatar_hubb760f67d067bc916243ef34dd4e5e56_607577_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/victor-boutin/">Victor Boutin</a></h2>
      
      <h3>Post-doc @ Serre Lab, BRown University.</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/VictorBoutin" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://serre-lab.clps.brown.edu/person/victor-boutin/" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/amelie-gruel/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/amelie-gruel/avatar_hu87feaa9b2d24ed5615584d1918cd82dd_6010_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/amelie-gruel/">Amélie Gruel</a></h2>
      
      <h3>PhD student in Computer Sciences at i3S/CNRS</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/ameliegruel" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Amelie-Gruel" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/angelo-franciosini/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/angelo-franciosini/avatar_huf20cb217f6bf92656c2488dfbefe1a2d_4587_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/angelo-franciosini/">Angelo Franciosini</a></h2>
      
      <h3>Biomedical Engineer @ Avicenna.AI.</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/AngeloFranciosini" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Angelo_Franciosini" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/Angelo_RDN" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.instagram.com/__r__d__n__/" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/angelo-franciosini/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/alberto-arturo-vergani/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/alberto-arturo-vergani/avatar_hu87e2f7c3276e79b9f6b4b7da9ab11035_8487_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/alberto-arturo-vergani/">Alberto Arturo Vergani</a></h2>
      
      <h3>Post-doctoral Researcher @ Sant&rsquo;Anna School of Advanced Studies, Pisa, Italy.</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/albertoarturovergani" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Alberto_Vergani" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/alberto-arturo-vergani-75288330" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.youtube.com/channel/UCzxesD7KEVc55s5aQaZug4Q" target="_blank" rel="noopener">
        <i class="fab fa-youtube"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://albertovergani.eu" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/wahiba-taouali/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/wahiba-taouali/avatar_hu029f67fb37dda8ae792ac4a8ef7262e0_7582_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/wahiba-taouali/">Wahiba Taouali</a></h2>
      
      <h3>Consulting manager @ Enthought, Cambridge, United Kingdom.</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?hl=en&amp;user=t8cu3zUAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/wahiba-taouali/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/taoualiw" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/jean-bernard-damasse/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/jean-bernard-damasse/avatar_hu1f18696b14f1af976569d240ad3d87a3_5289_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></h2>
      
      <h3>Medical Doctor</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Jean-Bernard-Damasse-2" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/kiana-mansour-pour/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/kiana-mansour-pour/avatar_huec98d101139f2e730cde89f29cadc5e2_6788_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/kiana-mansour-pour/">Kiana Mansour-Pour</a></h2>
      
      <h3>Executive DirectorExecutive Director, Shotise</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/kiana-mansour-pour/?originalSubdomain=fr" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/mina-a-khoei/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/mina-a-khoei/avatar_hu5529952055b822b4cca806b537b8ea77_67608_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/mina-a-khoei/">Mina A Khoei</a></h2>
      
      <h3>Senior AI/ML scientist @ SynSense, Zurich, Switzerland.</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=oHLjQTEAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/mina-a-khoei-58673526/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Mina-A-Khoei" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/jens-kremkow/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/jens-kremkow/avatar_hu5ae35407bb8f613867a17170c74e89e2_27242_270x270_fill_q75_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/jens-kremkow/">Jens Kremkow</a></h2>
      
      <h3>PI @ Neuroscience Research Center, Charité, Berlin, Germany.</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=YEdekcAAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Jens-Kremkow" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://jens.kremkow.de" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://twitter.com/KremkowL" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/nicole-voges/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/nicole-voges/avatar_huf9122dbe1c5ae24d662b7d93c13da43f_5851_270x270_fill_lanczos_center_1.gif" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/nicole-voges/">Nicole Voges</a></h2>
      
      <h3>PostDoc in Computational Neuroscience</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=oHLjQTEAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/wahiba-taouali/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/author/jenna-fradin/"><img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/author/jenna-fradin/avatar_hub36d471c8f118079e4c65745b9d21b38_372499_270x270_fill_lanczos_center_3.png" alt="Avatar"></a>
    

    <div class="portrait-title">
      <h2><a href="/author/jenna-fradin/">Jenna Fradin</a></h2>
      
      <h3>Research Engineer at ISIR (Institut des Systèmes Intelligents et de Robotique)</h3>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://jennafradin.github.io" target="_blank" rel="noopener">
        <i class="fas fa-external-link-alt"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/jennafradin" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/jenna-fradin-686845189/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

      
    </div>
  </div>
  
  
</div>


  

  </div>
</section>


  






























































<section id="publications" class="home-section wg-collection  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Publications</h1>
          
        </div>
      
    
  

    










  









  
  
  
  
  




  
  

















  




<div class="col-12 col-lg-8">

  <div class="alert alert-note">
  <div>
    Quickly discover relevant content by <a href="./publication/">filtering publications</a>.
  </div>
</div>


  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/ilias-rentzeperis/">Ilias Rentzeperis</a></span>, <span >
      <a href="/author/luca-calatroni/">Luca Calatroni</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/dario-prandi/">Dario Prandi</a></span>
  </span>
  (2023).
  <a href="/publication/rentzeperis-23/">Beyond $\ell_1$ sparse coding in V1</a>.
  <em>PLOS Computational Biology</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2301.10002" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/rentzeperis-23" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/rentzeperis-23/cite.bib">
  Cite
</a>





  
    
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1011459" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2023).
  <a href="/publication/grimaldi-23-bc/">Learning heterogeneous delays in a layer of spiking neurons for fast motion detection</a>.
  <em>Biological Cybernetics</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-23-bc/grimaldi-23-bc.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-23-bc/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00422-023-00975-8" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/sio-hoi-ieng/">Sio-Hoi Ieng</a></span>, <span >
      <a href="/author/ryad-benosman/">Ryad Benosman</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2023).
  <a href="/publication/grimaldi-23/">A Robust Event-Driven Approach to Always-on Object Recognition</a>.
  <em>In revision</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-23/grimaldi-23.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-23/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.36227/techrxiv.18003077" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laurentperrinet.github.io/publication/grimaldi-23/" target="_blank" rel="noopener">
    URL</a>

</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/hugo-ladret/">Hugo Ladret</a></span>, <span >
      <a href="/author/nelson-cortes/">Nelson Cortes</a></span>, <span >
      <a href="/author/lamyae-ikan/">Lamyae Ikan</a></span>, <span >
      <a href="/author/frederic-chavane/">Frederic Chavane</a></span>, <span >
      <a href="/author/christian-casanova/">Christian Casanova</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2023).
  <a href="/publication/ladret-23/">Cortical recurrence supports resilience to sensory variance in the primary visual cortex</a>.
  <em>Nature Communications Biology</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/ladret-23/ladret-23.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ladret-23/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/s42003-023-05042-3" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2023).
  <a href="/publication/jeremie-23-ultra-fast-cat/">Ultra-Fast Image Categorization in biology and in neural models</a>.
  <em>Vision</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://arxiv.org/abs/2205.03635" target="_blank" rel="noopener">
  Preprint
</a>




  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/jeremie-23-ultra-fast-cat/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3390/vision7020029" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/amelie-gruel/">Amélie Gruel</a></span>, <span >
      <a href="/author/dalia-hareb/">Dalia Hareb</a></span>, <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/jean-martinet/">Jean Martinet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/bernabe-linares-barranco/">Bernabé Linares-Barranco</a></span>, <span >
      <a href="/author/teresa-serrano-gotarredona/">Teresa Serrano-Gotarredona</a></span>
  </span>
  (2023).
  <a href="/publication/gruel-23-bc/">Stakes of Neuromorphic Foveation: a promising future for embedded event cameras</a>.
  <em>Biological Cybernetics</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.researchsquare.com/article/rs-2120721" target="_blank" rel="noopener">
  Preprint
</a>




  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/gruel-23-bc/cite.bib">
  Cite
</a>














</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/antoine-grimaldi/">Antoine Grimaldi</a></span>, <span >
      <a href="/author/amelie-gruel/">Amélie Gruel</a></span>, <span >
      <a href="/author/camille-besnainou/">Camille Besnainou</a></span>, <span >
      <a href="/author/jean-nicolas-jeremie/">Jean-Nicolas Jérémie</a></span>, <span >
      <a href="/author/jean-martinet/">Jean Martinet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2022).
  <a href="/publication/grimaldi-22-polychronies/">Precise spiking motifs in neurobiological and neuromorphic data</a>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/grimaldi-22-polychronies/grimaldi-22-polychronies.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/grimaldi-22-polychronies/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3390/brainsci13010068" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2022).
  <a href="/publication/franciosini-21/">Pooling in a predictive model of V1 explains functional and structural diversity across species</a>.
  <em>PLoS Computational Biology</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/2021.04.19.440444" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/franciosini-21" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/franciosini-21/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1010270" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/frederic-chavane/">Frederic Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/james-rankin/">James Rankin</a></span>
  </span>
  (2022).
  <a href="/publication/chavane-22/">Revisiting Horizontal Connectivity Rules in V1: From like-to-like towards like-to-All</a>.
  <em>Brain Structure and Function</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00429-022-02455-4" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/chavane-22/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00429-022-02455-4" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/frederic-v-barthelemy/">Frédéric V Barthélemy</a></span>, <span >
      <a href="/author/jerome-fleuriet/">Jérome Fleuriet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2022).
  <a href="/publication/barthelemy-22/">A Behavioral Receptive Field for Ocular Following in Monkeys: Spatial Summation and Its Spatial Frequency Tuning</a>.
  <em>eNeuro</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/hal-03741144" target="_blank" rel="noopener">
  Preprint
</a>




  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/barthelemy-22/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1523/ENEURO.0374-21.2022" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2021).
  <a href="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/">Sparse Deep Predictive Coding captures contour integration capabilities of the early visual system</a>.
  <em>PLoS Computational Biology</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1902.07651" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/boutin-franciosini-chavane-ruffier-perrinet-20.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-chavane-ruffier-perrinet-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/VictorBoutin/InteractionMap" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1008629" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2021).
  <a href="/publication/perrinet-21-hasard/">Le jeu du cerveau et du hasard</a>.
  <em>The Conversation</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://theconversation.com/le-jeu-du-cerveau-et-du-hasard-159388" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-21-hasard/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-21-hasard/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  










</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/pierre-albiges/">Pierre Albigès</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2020).
  <a href="/publication/dauce-20/">A dual foveal-peripheral visual processing model implements efficient saccade selection</a>.
  <em>Journal of Vision</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/725879" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/dauce-20/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dauce-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/WhereIsMyMNIST" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/jov.20.8.22" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/franck-ruffier/">Franck Ruffier</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2020).
  <a href="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/">Effect of top-down connections in Hierarchical Sparse Coding</a>.
  <em>Neural Computation</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2002.00892" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/boutin-franciosini-ruffier-perrinet-20-feedback.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/boutin-franciosini-ruffier-perrinet-20-feedback/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1162/neco_a_01325" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/chloe-pasturel/">Chloé Pasturel</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2020).
  <a href="/publication/pasturel-montagnini-perrinet-20/">Humans adapt their anticipatory eye movements to the volatility of visual motion properties</a>.
  <em>PLoS Computational Biology</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/784116" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/hal-02394142" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/pasturel-montagnini-perrinet-20/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/PasturelMontagniniPerrinet2020" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1007438" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/sandrine-chemla/">Sandrine Chemla</a></span>, <span >
      <a href="/author/arjan-boonman/">Arjan Boonman</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/frederic-chavane/">Frederic Chavane</a></span>
  </span>
  (2020).
  <a href="/publication/benvenuti-22/">Anticipatory Responses along Motion Trajectories in Awake Monkey Area V1</a>.
  <em>bioRxiv</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.biorxiv.org/content/10.1101/2020.03.26.010017" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/benvenuti-22/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10/ggqj77" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/sandrine-chemla/">Sandrine Chemla</a></span>, <span >
      <a href="/author/alexandre-reynaud/">Alexandre Reynaud</a></span>, <span >
      <a href="/author/matteo-divolo/">Matteo diVolo</a></span>, <span >
      <a href="/author/yann-zerlaut/">Yann Zerlaut</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/alain-destexhe/">Alain Destexhe</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>
  </span>
  (2019).
  <a href="/publication/chemla-19/">Suppressive waves disambiguate the representation of long-range apparent motion in awake monkey V1</a>.
  <em>Journal of Neuroscience</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.science/hal-02190752" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.jneurosci.org/content/39/22/4282" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/chemla-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1523/JNEUROSCI.2792-18.2019" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/cesar-u-ravello/">Cesar U Ravello</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/maria-jose-escobar/">Maria Jose Escobar</a></span>, <span >
      <a href="/author/adrian-g-palacios/">Adrián G Palacios</a></span>
  </span>
  (2019).
  <a href="/publication/ravello-19/">Speed-Selectivity in Retinal Ganglion Cells is Sharpened by Broad Spatial Frequency, Naturalistic Stimuli</a>.
  <em>Scientific Reports</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-02007905" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038%2Fs41598-018-36861-8" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/ravello-19/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/s41598-018-36861-8" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2019).
  <a href="/publication/perrinet-19-hulk/">An adaptive homeostatic algorithm for the unsupervised learning of visual features</a>.
  <em>Vision</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-hulk/" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-hulk/perrinet-19-hulk.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-19-hulk/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/SpikeAI/HULK" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3390/vision3030047" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2019).
  <a href="/publication/perrinet-19-illusions/">Illusions et hallucinations visuelles : une porte sur la perception</a>.
  <em>The Conversation</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://theconversation.com/illusions-et-hallucinations-visuelles-une-porte-sur-la-perception-117389" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-illusions/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-19-illusions/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  







  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=jJKTdlChefc" target="_blank" rel="noopener">
  Video
</a>




</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2019).
  <a href="/publication/perrinet-19-temps/">Temps et cerveau : comment notre perception nous fait voyager dans le temps</a>.
  <em>The Conversation</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://theconversation.com/temps-et-cerveau-comment-notre-perception-nous-fait-voyager-dans-le-temps-127567" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-19-temps/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-19-temps/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  










</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/jonathan-vacher/">Jonathan Vacher</a></span>, <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-peyre/">Gabriel Peyré</a></span>
  </span>
  (2018).
  <a href="/publication/vacher-16/">Bayesian Modeling of Motion Perception using Dynamical Stochastic Textures</a>.
  <em>Neural Computation</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1611.01390" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/vacher-16/vacher-16.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vacher-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1162/neco_a_01142" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/jean-bernard-damasse/">Jean-Bernard Damasse</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>
  </span>
  (2018).
  <a href="/publication/damasse-18/">Reinforcement effects in anticipatory smooth eye movements</a>.
  <em>Journal of Vision</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01901640v1" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://jov.arvojournals.org/article.aspx?articleid=2707670" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/damasse-18/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1167/18.11.14" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2017).
  <a href="/publication/khoei-masson-perrinet-17/">The flash-lag effect as a motion-based predictive shift</a>.
  <em>PLoS Computational Biology</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01771125" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-masson-perrinet-17/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-masson-perrinet-17/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/Khoei_2017_PLoSCB" target="_blank" rel="noopener">
  Code
</a>







  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2022-11-21_flash-lag-effect/" target="_blank">
    Slides
  </a>
  




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pcbi.1005068" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/wahiba-taouali/">Wahiba Taouali</a></span>, <span >
      <a href="/author/giacomo-benvenuti/">Giacomo Benvenuti</a></span>, <span >
      <a href="/author/pascal-wallisch/">Pascal Wallisch</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2016).
  <a href="/publication/taouali-16/">Testing the odds of inherent vs. observed overdispersion in neural spike counts</a>.
  <em>Journal of Neurophysiology</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01396311" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.ncbi.nlm.nih.gov/pubmed/26445864" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/taouali-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1152/jn.00194.2015" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/cyril-monier/">Cyril Monier</a></span>, <span >
      <a href="/author/jose-manuel-alonso/">Jose-Manuel Alonso</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>, <span >
      <a href="/author/yves-fregnac/">Yves Fregnac</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2016).
  <a href="/publication/kremkow-16/">Push-Pull Receptive Field Organization and Synaptic Depression: Mechanisms for Reliably Encoding Naturalistic Stimuli in V1</a>.
  <em>Frontiers in Neural Circuits</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal.archives-ouvertes.fr/hal-02062034" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://journal.frontiersin.org/article/10.3389/fncir.2016.00037/full" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-16/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fncir.2016.00037" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/jonathan-vacher/">Jonathan Vacher</a></span>, <span >
      <a href="/author/andrew-isaac-meso/">Andrew Isaac Meso</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-peyre/">Gabriel Peyré</a></span>
  </span>
  (2015).
  <a href="/publication/vacher-15-nips/">Biologically Inspired Dynamic Textures for Probing Motion Perception</a>.
  <em>Advances in Neural Information Processing Systems</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1511.02705" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://papers.nips.cc/paper/5769-biologically-inspired-dynamic-textures-for-probing-motion-perception.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/vacher-15-nips/cite.bib">
  Cite
</a>














</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/james-a-bednar/">James A Bednar</a></span>
  </span>
  (2015).
  <a href="/publication/perrinet-bednar-15/">Edge co-occurrences can account for rapid categorization of natural versus animal images</a>.
  <em>Scientific Reports</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-01202447" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://www.nature.com/articles/srep11400" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-bednar-15/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/laurentperrinet/PerrinetBednar15" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/srep11400" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </span>
  (2014).
  <a href="/publication/perrinet-adams-friston-14/">Active inference, eye movements and oculomotor delays</a>.
  <em>Biological Cybernetics</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1610.05564" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://link.springer.com/article/10.1007%2Fs00422-014-0620-8" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-adams-friston-14/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s00422-014-0620-8" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/mina-a-khoei/">Mina A Khoei</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2013).
  <a href="/publication/khoei-13-jpp/">Motion-based prediction explains the role of tracking in motion extrapolation</a>.
  <em>Journal of Physiology-Paris</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/khoei-13-jpp/khoei-13-jpp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/khoei-13-jpp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2013.08.001" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/bernhard-a-kaplan/">Bernhard A Kaplan</a></span>, <span >
      <a href="/author/anders-lansner/">Anders Lansner</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2013).
  <a href="/publication/kaplan-13/">Anisotropic connectivity implements motion-based prediction in a spiking neural network</a>.
  <em>Frontiers in Computational Neuroscience</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/kaplan-13" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kaplan-13/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fncom.2013.00112" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/rodrigo-nava/">Rodrigo Nava</a></span>, <span >
      <a href="/author/j-victor-marcos/">J Victor Marcos</a></span>, <span >
      <a href="/author/boris-escalante-ramirez/">Boris Escalante-Ramirez</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/raul-s-j-estepar/">Raúl S J Estépar</a></span>
  </span>
  (2013).
  <a href="/publication/nava-13/">Advances in Texture Analysis for Emphysema Classification</a>.
  <em>Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1007/978-3-642-41827-3_27" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/nava-13/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-642-41827-3_27" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>
  </span>
  (2012).
  <a href="/publication/adams-12/">Smooth Pursuit and Visual Occlusion: Active Inference and Oculomotor Control in Schizophrenia</a>.
  <em>PLoS ONE</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/adams-12/adams-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/adams-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1371/journal.pone.0047502" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2012).
  <a href="/publication/masson-12/">The behavioral receptive field underlying motion integration for primate tracking eye movements</a>.
  <em>Neuroscience and biobehavioral reviews</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/masson-12/masson-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/masson-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.neubiorev.2011.03.009" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/paula-sanz-leon/">Paula Sanz Leon</a></span>, <span >
      <a href="/author/ivo-vanzetta/">Ivo Vanzetta</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2012).
  <a href="/publication/sanz-12/">Motion Clouds: Model-based stimulus synthesis of natural-like random textures for the study of motion perception</a>.
  <em>Journal of Neurophysiology</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1208.6467" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/sanz-12/sanz-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/sanz-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1152/jn.00737.2011" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2012).
  <a href="/publication/voges-12/">Complex dynamics in recurrent cortical networks based on spatially realistic connectivities</a>.
  <em>Frontiers in Computational Neuroscience</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/voges-12/voges-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fncom.2012.00041" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/claudio-simoncini/">Claudio Simoncini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2012).
  <a href="/publication/simoncini-12/">More is not always better: dissociation between perception and action explained by adaptive gain control</a>.
  <em>Nature Neuroscience</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/simoncini-12/simoncini-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/simoncini-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1038/nn.3229" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2012).
  <a href="/publication/perrinet-12-pred/">Motion-based prediction is sufficient to solve the aperture problem</a>.
  <em>Neural Computation</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1208.6471" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-12-pred/perrinet-12-pred.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-12-pred/cite.bib">
  Cite
</a>














</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/karl-friston/">Karl Friston</a></span>, <span >
      <a href="/author/rick-a-adams/">Rick A Adams</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/michael-breakspear/">Michael Breakspear</a></span>
  </span>
  (2012).
  <a href="/publication/friston-12/">Perceptions as Hypotheses: Saccades as Experiments</a>.
  <em>Frontiers in Psychology</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/friston-12/friston-12.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/friston-12/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/fpsyg.2012.00151" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2011).
  <a href="/publication/perrinet-10-doc-sciences/">Qui créera le premier ordinateur intelligent?</a>.
  <em>DocSciences</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://interstices.info/qui-creera-le-premier-ordinateur-intelligent/" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-10-doc-sciences/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/tout-public/">
    Project
  </a>
  










</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/amarender-bogadhi/">Amarender Bogadhi</a></span>, <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2011).
  <a href="/publication/bogadhi-11/">Pursuing motion illusions: a realistic oculomotor framework for Bayesian inference</a>.
  <em>Vision research</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/bogadhi-11/bogadhi-11.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/bogadhi-11/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.visres.2010.10.021" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/jerome-fleuriet/">Jérome Fleuriet</a></span>, <span >
      <a href="/author/sandrine-hugues/">Sandrine Hugues</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/laurent-goffart/">Laurent Goffart</a></span>
  </span>
  (2011).
  <a href="/publication/fleuriet-11/">Saccadic foveation of a moving visual target in the rhesus monkey</a>.
  <em>Journal of Neurophysiology</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1152/jn.00622.2010" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fleuriet-11/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1152/jn.00622.2010" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/nicole-voges/">Nicole Voges</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2010).
  <a href="/publication/voges-10-jpp/">Phase space analysis of networks based on biologically realistic parameters</a>.
  <em>Journal of Physiology-Paris</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/voges-10-jpp/voges-10-jpp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/voges-10-jpp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2009.11.004" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2010).
  <a href="/publication/perrinet-10-shl/">Role of homeostasis in learning sparse representations</a>.
  <em>Neural Computation</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/0706.3177" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-10-shl/perrinet-10-shl.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-10-shl/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/bicv/SparseHebbianLearning" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1162/neco.2010.05-08-795" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>, <span >
      <a href="/author/ad-m-aertsen/">Ad M Aertsen</a></span>
  </span>
  (2010).
  <a href="/publication/kremkow-10-jcns/">Functional consequences of correlated excitatory and inhibitory conductances in cortical networks</a>.
  <em>Journal of Computational Neuroscience</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/kremkow-10-jcns/kremkow-10-jcns.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/kremkow-10-jcns/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s10827-010-0240-9" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2010).
  <a href="/publication/dauce-10/">Computational Neuroscience, from Multiple Levels to Multi-level</a>.
  <em>Journal of Physiology-Paris</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.jphysparis.2009.11.001" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/dauce-10/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2009.11.001" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/frederic-v-barthelemy/">Frédéric V Barthélemy</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/eric-castet/">Eric Castet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2008).
  <a href="/publication/barthelemy-08/">Dynamics of distributed 1D and 2D motion representations for short-latency ocular following</a>.
  <em>Vision research</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/barthelemy-08/barthelemy-08.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/barthelemy-08/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.visres.2007.10.020" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/andrew-p-davison/">Andrew P Davison</a></span>, <span >
      <a href="/author/daniel-bruderle/">Daniel Bruderle</a></span>, <span >
      <a href="/author/jochen-eppler/">Jochen Eppler</a></span>, <span >
      <a href="/author/jens-kremkow/">Jens Kremkow</a></span>, <span >
      <a href="/author/eilif-muller/">Eilif Muller</a></span>, <span >
      <a href="/author/dejan-pecevski/">Dejan Pecevski</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/pierre-yger/">Pierre Yger</a></span>
  </span>
  (2008).
  <a href="/publication/davison-08/">PyNN: A Common Interface for Neuronal Network Simulators</a>.
  <em>Frontiers in Neuroinformatics</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-00586786" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/davison-08/davison-08.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/davison-08/cite.bib">
  Cite
</a>





  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/project/open-science/">
    Project
  </a>
  









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.3389/neuro.11.011.2008" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/sylvain-fischer/">Sylvain Fischer</a></span>, <span >
      <a href="/author/filip-sroubek/">Filip Šroubek</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/rafael-redondo/">Rafael Redondo</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>
  </span>
  (2007).
  <a href="/publication/fischer-07-cv/">Self-Invertible 2D Log-Gabor Wavelets</a>.
  <em>International Journal of Computer Vision</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/fischer-07-cv/fischer-07-cv.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fischer-07-cv/cite.bib">
  Cite
</a>


  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/bicv/LogGabor" target="_blank" rel="noopener">
  Code
</a>












<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/s11263-006-0026-8" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/pascal-mamassian/">Pascal Mamassian</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/eric-castet/">Eric Castet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2007).
  <a href="/publication/montagnini-07/">Bayesian modeling of dynamic motion integration</a>.
  <em>Neuro-Computation: From Sensorimotor Integration to Computational Frameworks</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/montagnini-07/montagnini-07.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/montagnini-07/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2007.10.013" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/guillaume-s-masson/">Guillaume S Masson</a></span>
  </span>
  (2007).
  <a href="/publication/perrinet-07-neurocomp/">Modeling spatial integration in the ocular following response using a probabilistic framework</a>.
  <em>Journal of Physiology-Paris</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-07-neurocomp/perrinet-07-neurocomp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-07-neurocomp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2007.10.011" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/sylvain-fischer/">Sylvain Fischer</a></span>, <span >
      <a href="/author/rafael-redondo/">Rafael Redondo</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/gabriel-cristobal/">Gabriel Cristóbal</a></span>
  </span>
  (2007).
  <a href="/publication/fischer-07/">Sparse Approximation of Images Inspired from the Functional Architecture of the Primary Visual Areas</a>.
  <em>EURASIP Journal on Advances in Signal Processing</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/fischer-07/fischer-07.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/fischer-07/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1155/2007/90727" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>
  </span>
  (2004).
  <a href="/publication/perrinet-03-ieee/">Coding static natural images using spiking event times: do neurons cooperate?</a>.
  <em>IEEE Transactions on Neural Networks</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/q-bio/0611002" target="_blank" rel="noopener">
  Preprint
</a>




  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-03-ieee/perrinet-03-ieee.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-03-ieee/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1109/TNN.2004.833303" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2004).
  <a href="/publication/perrinet-04-tauc/">Feature detection using spikes : the greedy approach</a>.
  <em>Journal of Physiology-Paris</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/q-bio/0611003" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.jphysparis.2005.09.012" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-04-tauc/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.jphysparis.2005.09.012" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>
  </span>
  (2004).
  <a href="/publication/perrinet-02-sparse/">Sparse spike coding in an asynchronous feed-forward multi-layer neural network using matching pursuit</a>.
  <em>Neurocomputing</em>.
  
  <p>





  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://hal-amu.archives-ouvertes.fr/hal-00276638" target="_blank" rel="noopener">
  Preprint
</a>




  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.neucom.2004.01.010" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-02-sparse/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.neucom.2004.01.010" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </span>
  (2004).
  <a href="/publication/perrinet-04/">Finding Independent Components using spikes : a natural result of Hebbian learning in a sparse spike coding scheme</a>.
  <em>Natural Computing</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1023/B:NACO.0000027753.27593.a7" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-04/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1023/B:NACO.0000027753.27593.a7" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>
  </span>
  (2003).
  <a href="/publication/perrinet-03/">Emergence of filters from natural scenes in a sparse spike coding scheme</a>.
  <em>Neurocomputing</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://dx.doi.org/10.1016/j.neucom.2004.01.133" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-03/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/j.neucom.2004.01.133" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </span>
  (2002).
  <a href="/publication/perrinet-02-stdp/">Coherence detection in a spiking neuron via Hebbian learning</a>.
  <em>Neurocomputing</em>.
  
  <p>








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/perrinet-02-stdp/perrinet-02-stdp.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-02-stdp/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/S0925-2312%2802%2900374-0" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/arnaud-delorme/">Arnaud Delorme</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </span>
  (2001).
  <a href="/publication/perrinet-01/">Network of integrate-and-fire neurons using Rank Order Coding A: how to implement spike timing dependant plasticity</a>.
  <em>Neurocomputing</em>.
  
  <p>








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/perrinet-01/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1016/S0925-2312%2801%2900460-X" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  
    


<div class="pub-list-item view-citation" style="margin-bottom: 1rem">
  <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>

  
  

  <span class="article-metadata li-cite-author">
    

  <span >
      <a href="/author/arnaud-delorme/">Arnaud Delorme</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/simon-thorpe/">Simon Thorpe</a></span>, <span >
      <a href="/author/manuel-samuelides/">Manuel Samuelides</a></span>
  </span>
  (2001).
  <a href="/publication/delorme-01/">Network of integrate-and-fire neurons using Rank Order Coding B: spike timing dependant plasticity and emergence of orientation selectivity</a>.
  <em>Neurocomputing</em>.
  
  <p>








  
    
  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.4990" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/delorme-01/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1.1.18.4990" target="_blank" rel="noopener">
  DOI
</a>


</p>

  
  
</div>

  

  
  
  

</div>


  
    </div>
  

  </div>
</section>


  






























































<section id="talks" class="home-section wg-collection  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Recent &amp; Upcoming Talks</h1>
          
        </div>
      
    
  

    










  








  
  





















  




<div class="col-12 col-lg-8">

  <div class="alert alert-note">
  <div>
    Access the list of <a href="./talk/">all previous talks</a>.
  </div>
</div>


  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/talk/2023-12-14-jraf/" >Event-based vision</a>
    </div>

    
    <a href="/talk/2023-12-14-jraf/"  class="summary-link">
      <div class="article-style">
        Event-based cameras mimic the way biological retinas process visual information: each pixel independently reports brightness changes as asynchronous temporal events. This departs from conventional cameras that capture static frames at fixed intervals. I will first discuss how the biological retina detects light intensity changes and communicates this to the brain. Compared to traditional cameras, the event-based paradigm enables new vision applications with high-speed, low latency and energy-efficiency. I will highlight recent works applying event cameras to tasks such as visual odometry, motion detection or gesture recognition. The goal is to demonstrate the advantages for computer vision that emulate biological principles inspired by neurosciences.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/adrien-fois/">Adrien Fois</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Dec 14, 2023
  </span>
  

  

  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/talk/2023-12-14-jraf/cite.bib">
  Cite
</a>








  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2023-12-14-jraf/" target="_blank">
    Slides
  </a>
  






    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/talk/2023-12-01-biocomp/" >Event-based vision</a>
    </div>

    
    <a href="/talk/2023-12-01-biocomp/"  class="summary-link">
      <div class="article-style">
        Event-based cameras mimic the way biological retinas process visual information: each pixel independently reports brightness changes as asynchronous temporal events. This departs from conventional cameras that capture static frames at fixed intervals. I will first discuss how the biological retina detects light intensity changes and communicates this to the brain. Compared to traditional cameras, the event-based paradigm enables new vision applications with high-speed, low latency and energy-efficiency. I will highlight recent works applying event cameras to tasks such as visual odometry, motion detection or gesture recognition. The goal is to demonstrate the advantages for computer vision that emulate biological principles inspired by neurosciences.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Nov 29, 2023
  </span>
  

  

  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/talk/2023-12-01-biocomp/cite.bib">
  Cite
</a>








  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2023-12-01-biocomp/" target="_blank">
    Slides
  </a>
  






    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/talk/2023-11-07-snufa/" >Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network</a>
    </div>

    
    <a href="/talk/2023-11-07-snufa/"  class="summary-link">
      <div class="article-style">
        Recently, there has been growing interest in exploring the hypothesis that neural activity conveys information through precise spiking motifs. To investigate this hypothesis, various algorithms have been proposed to detect such motifs in spiking activity recorded from populations of neurons. In this study, we present a detection model that takes the form of logistic regression combined with temporal convolution. A key advantage of this model is its differentiability, which allows us to formulate a gradient descent on any appropriate loss. We prove its efficiency on synthetic data for which the ground truth is available to use supervised learning. However, this ground truth information is not available for neurobiological data, where a self-learning procedure is required. We show that a contrastive learning method can recover the synthetically generated spiking motifs without knowing the ground truth. In the future, we aim to extend this method to real neurobiological data to explore and detect spiking motifs in a more natural and biologically relevant context.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Sep 10, 2023
  </span>
  

  

  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/talk/2023-11-07-snufa/cite.bib">
  Cite
</a>








  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2023-11-07-snufa/" target="_blank">
    Slides
  </a>
  





  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laurentperrinet.github.io/publication/perrinet-23-icann/perrinet-23-icann.pdf" target="_blank" rel="noopener">
    PDF</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/talk/2023-09-27-icann/" >Accurate Detection of Spiking Motifs by Learning Heterogeneous Delays of a Spiking Neural Network</a>
    </div>

    
    <a href="/talk/2023-09-27-icann/"  class="summary-link">
      <div class="article-style">
        Hybrid Session, Room 2
Chair: Sander Bohté, Sebastian Otte
https://link.springer.com/chapter/10.1007/978-3-031-44207-0_31 - https://doi.org/10.1007/978-3-031-44207-0_31
https://arxiv.org/abs/2307.11555
code: https://github.com/laurentperrinet/2023-09-27_HDSNN-ICANN
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Sep 10, 2023
  </span>
  

  

  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/talk/2023-09-27-icann/2023-09-27-icann.pdf" target="_blank" rel="noopener">
  PDF
</a>



<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/talk/2023-09-27-icann/cite.bib">
  Cite
</a>








  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2023-09-27_icann/" target="_blank">
    Slides
  </a>
  




<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.1007/978-3-031-44207-0_31" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://laurentperrinet.github.io/publication/perrinet-23-icann/" target="_blank" rel="noopener">
    URL</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  
    













  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/talk/2023-09-08-fresnel/" >Event-based vision</a>
    </div>

    
    <a href="/talk/2023-09-08-fresnel/"  class="summary-link">
      <div class="article-style">
        Event-based cameras mimic the way biological retinas process visual information: each pixel independently reports brightness changes as asynchronous temporal events. This departs from conventional cameras that capture static frames at fixed intervals. I will first discuss how the retina detects light intensity changes and communicates this to the brain. Compared to traditional cameras, the event-based paradigm enables new vision applications with high-speed, low latency and energy-efficiency. I will highlight recent works applying event cameras to tasks such as visual odometry, motion detection or gesture recognition. The goal is to demonstrate the advantages for computer vision that emulate biological principles.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
        


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Sep 5, 2023
  </span>
  

  

  

  
  
  
  

  
  

</div>

      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/talk/2023-09-08-fresnel/cite.bib">
  Cite
</a>








  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2023-09-08_fresnel/" target="_blank">
    Slides
  </a>
  






    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

  

  
  
  

    
    
      
    

    
    
    
    
      
    

    

    <div class="see-all">
      <a href="/talk/">
        See all
        <i class="fas fa-angle-right"></i>
      </a>
    </div>
  

</div>


  
    </div>
  

  </div>
</section>


  























  








































<section id="grants" class="home-section wg-portfolio  "  >
 <div class="home-section-bg " style="background-color: SkyBlue;">
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Grants</h1>
          
        </div>
      
    
  

    












<div class="col-12 col-lg-8">

  

  

    

    
    
    
    
      
      
        
      
    

    <span class="d-none default-project-filter">.js-id-current-grant</span>

    
    
    <div class="project-toolbar">
      <div class="project-filters">
        <div class="btn-toolbar ">
          <div class="btn-group flex-wrap">
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-current-grant" class="btn btn-primary btn-lg active">Current Grants</a>
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-past-grant" class="btn btn-primary btn-lg">Past Grants</a>
            
              
              
              
                
                  
                
              
              <a href="#" data-filter=".js-id-grant" class="btn btn-primary btn-lg">All Grants</a>
            
          </div>
        </div>
      </div>
    </div>
    
  

  <div class="isotope projects-container js-layout-masonry ">

    
    

    
    
      
      
    
    
    
    
    
      
    

    
    
    
    
    

    
      
        
          <div class="project-card project-item isotope-item js-id-polychronies js-id-grant js-id-current-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Oct 5, 2023
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/emergences/" >
      <div class="img-hover-zoom">
        <img src="/grant/emergences/featured_hue9b582315461dd5abc542b7f862bd9ba_150954_32c177bb6c03c5c0aa41e54053e87e44.webp" height="455" width="808"
            class="article-banner" alt="Emergences (2023 / 2027)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/emergences/" >Emergences (2023 / 2027)</a>
  </div>

  
  <a href="/grant/emergences/"  class="summary-link">
    <div class="article-style">
      <p>Near-physics emerging models for embedded AI (PhD position, 2023 / 2027).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-polychronies js-id-grant js-id-current-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/rosa-cossart/">Rosa Cossart</a></span>, <span >
      <a href="/author/thomas-schatz/">Thomas Schatz</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jul 18, 2022
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/polychronies/" >
      <div class="img-hover-zoom">
        <img src="/grant/polychronies/featured_hu4b94a7d822f46d57b9e6c817a3126011_520198_290428ebb5821fb66229bae458bf1298.webp" height="455" width="808"
            class="article-banner" alt="Polychronies (2022 / 2025)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/polychronies/" >Polychronies (2022 / 2025)</a>
  </div>

  
  <a href="/grant/polychronies/"  class="summary-link">
    <div class="article-style">
      <p>A grant from the Ph.D. program in Integrative and Clinical Neuroscience (Post-doctoral position, 2022 / 2025).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-current-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>, <span >
      <a href="/author/laurent-madelain/">Laurent Madelain</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Jul 13, 2021
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-aces/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-aces/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR ACES (2022/2026)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-aces/" >ANR ACES (2022/2026)</a>
  </div>

  
  <a href="/grant/anr-aces/"  class="summary-link">
    <div class="article-style">
      <p>Assignment of credit and constraints on eye movement learning (2022/2026).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-current-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 1, 2021
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-rubinvase/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-rubinvase/featured_hud9eae7fe11fdb1eddbcc3bf025db974b_16211_9d3d1b7fdbcd3b9629a3d50acd27f53d.webp" height="455" width="808"
            class="article-banner" alt="ANR RubinVase (2021/2024)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-rubinvase/" >ANR RubinVase (2021/2024)</a>
  </div>

  
  <a href="/grant/anr-rubinvase/"  class="summary-link">
    <div class="article-style">
      <p>RedUndancy-free neuro-BIological desigN of Visual and Auditory SEnsing</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-current-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/emmanuel-dauce/">Emmanuel Daucé</a></span>, <span >
      <a href="/author/stephane-viollet/">Stéphane Viollet</a></span>, <span >
      <a href="/author/ryad-benosman/">Ryad Benosman</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Dec 7, 2020
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-anr/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-anr/featured_hud9eae7fe11fdb1eddbcc3bf025db974b_16211_9d3d1b7fdbcd3b9629a3d50acd27f53d.webp" height="455" width="808"
            class="article-banner" alt="ANR AgileNeuRobot (2021/2024)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-anr/" >ANR AgileNeuRobot (2021/2024)</a>
  </div>

  
  <a href="/grant/anr-anr/"  class="summary-link">
    <div class="article-style">
      <p>Robots aériens agiles bio-mimetiques pour le vol en conditions réelles</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  












  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/slides/2020-12-10_agileneurobot_anr/" target="_blank">
    Slides
  </a>
  






  </div>
  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-current-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2020
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-priosens/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-priosens/featured_hud9eae7fe11fdb1eddbcc3bf025db974b_16211_9d3d1b7fdbcd3b9629a3d50acd27f53d.webp" height="455" width="808"
            class="article-banner" alt="ANR PRIOSENS (2021/2024)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-priosens/" >ANR PRIOSENS (2021/2024)</a>
  </div>

  
  <a href="/grant/anr-priosens/"  class="summary-link">
    <div class="article-style">
      <p>Integration sensory and prior information to control behavior (CRCNS US-French Research Proposal)</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2020
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-shootingstar/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-shootingstar/featured_hud9eae7fe11fdb1eddbcc3bf025db974b_16211_9d3d1b7fdbcd3b9629a3d50acd27f53d.webp" height="455" width="808"
            class="article-banner" alt="ANR ShootingStar (2021/2024)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-shootingstar/" >ANR ShootingStar (2021/2024)</a>
  </div>

  
  <a href="/grant/anr-shootingstar/"  class="summary-link">
    <div class="article-style">
      <p>Processing of naturalistic motion in early vision</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Sep 19, 2019
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/mesocentre/" >
      <div class="img-hover-zoom">
        <img src="/grant/mesocentre/featured_hu5109c081ecb095422d53bc05ad2d00c5_53708_85372e509a1344717d386a933c63f5db.webp" height="455" width="808"
            class="article-banner" alt="MesoCentre (2018/2022)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/mesocentre/" >MesoCentre (2018/2022)</a>
  </div>

  
  <a href="/grant/mesocentre/"  class="summary-link">
    <div class="article-style">
      <p>MesoCentre (2018/2022) : access to the HPC resources of Aix-Marseille Université.</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-current-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/jean-martinet/">Jean Martinet</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Sep 10, 2019
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/aprovis-3-d/" >
      <div class="img-hover-zoom">
        <img src="/grant/aprovis-3-d/featured_hua598ee660eefd784bf49fa76668f2622_2693771_f53c1fe89c6906f763f43835693de42f.webp" height="455" width="808"
            class="article-banner" alt="APROVIS3D (2019/2023)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/aprovis-3-d/" >APROVIS3D (2019/2023)</a>
  </div>

  
  <a href="/grant/aprovis-3-d/"  class="summary-link">
    <div class="article-style">
      <p>Analog PROcessing of bioinspired VIsion Sensors for 3D reconstruction (APROVIS3D) is <a href="http://www.chistera.eu/projects/aprovis3d" target="_blank" rel="noopener">2018 <em>CHIST-ERA</em> laureate</a>.</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 15, 2019
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/spikeai/" >
      <div class="img-hover-zoom">
        <img src="/grant/spikeai/featured_huf1b05ff6317052c34d59704b12bfa116_214441_693bf9e5b843738d601e6ece279f8909.webp" height="455" width="808"
            class="article-banner" alt="SpikeAI: laureat du Défi Biomimétisme (2019)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/spikeai/" >SpikeAI: laureat du Défi Biomimétisme (2019)</a>
  </div>

  
  <a href="/grant/spikeai/"  class="summary-link">
    <div class="article-style">
      <p>Algorithmes événementiels d’Intelligence Artificielle / Event-Based Artificial Inteligence (2019).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-bala-v1/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-bala-v1/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR BalaV1 (2013/2016)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-bala-v1/" >ANR BalaV1 (2013/2016)</a>
  </div>

  
  <a href="/grant/anr-bala-v1/"  class="summary-link">
    <div class="article-style">
      <p>ANR BalaV1: Balanced states in area V1 (2013&ndash;2016)</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-causal/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-causal/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR CausaL (2018/2020)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-causal/" >ANR CausaL (2018/2020)</a>
  </div>

  
  <a href="/grant/anr-causal/"  class="summary-link">
    <div class="article-style">
      <p>ANR CausaL (2018/2020) : Cognitive​ ​architectures​ ​of​ Causal​ ​Learning.</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/yves-fregnac/">Yves Fregnac</a></span>, <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-horizontal-v1/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-horizontal-v1/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR Horizontal-V1 (2017/2021)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-horizontal-v1/" >ANR Horizontal-V1 (2017/2021)</a>
  </div>

  
  <a href="/grant/anr-horizontal-v1/"  class="summary-link">
    <div class="article-style">
      <p>Connectivité Horizontale et Prédiction de Cohérences dans l&rsquo;Intégration de Contour et Mouvement dans le Cortex Visuel Primaire</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-predicteye/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-predicteye/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR PredictEye (2018/2020)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-predicteye/" >ANR PredictEye (2018/2020)</a>
  </div>

  
  <a href="/grant/anr-predicteye/"  class="summary-link">
    <div class="article-style">
      <p>ANR PredictEye (2018/2020) : Mapping and predicting trajectories for eye  movements</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-rem/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-rem/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR REM (2013/2016)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-rem/" >ANR REM (2013/2016)</a>
  </div>

  
  <a href="/grant/anr-rem/"  class="summary-link">
    <div class="article-style">
      <p>ANR REM : Renforcement et mouvements oculaires (2013/2016).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-speed/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-speed/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR SPEED (2013/2016)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-speed/" >ANR SPEED (2013/2016)</a>
  </div>

  
  <a href="/grant/anr-speed/"  class="summary-link">
    <div class="article-style">
      <p>ANR SPEED: Traitement de la vitesse dans les scènes visuelles naturelles (2013/2016).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/frederic-chavane/">Frédéric Chavane</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/anr-trajectory/" >
      <div class="img-hover-zoom">
        <img src="/grant/anr-trajectory/featured_hu3bc8a76be4783dc6edddc464c7e9ac00_16940_13fd21224ee6b92e4099538206d8d5f8.webp" height="455" width="808"
            class="article-banner" alt="ANR TRAJECTORY (2016/2019)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/anr-trajectory/" >ANR TRAJECTORY (2016/2019)</a>
  </div>

  
  <a href="/grant/anr-trajectory/"  class="summary-link">
    <div class="article-style">
      <p>ANR TRAJECTORY (2016/2019).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/victor-boutin/">Victor Boutin</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/doc-2-amu/" >
      <div class="img-hover-zoom">
        <img src="/grant/doc-2-amu/featured_hu1df07709740a4be8d24d183bc6d741fc_41294_e63f529812b938d5e4f580cde9e1c16c.webp" height="455" width="808"
            class="article-banner" alt="DOC2AMU (2016/2019)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/doc-2-amu/" >DOC2AMU (2016/2019)</a>
  </div>

  
  <a href="/grant/doc-2-amu/"  class="summary-link">
    <div class="article-style">
      <p>DOC2AMU: An Excellence Fellowship (2016/2019).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant js-id-phd-icn">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/angelo-franciosini/">Angelo Franciosini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2016
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/phd-icn/" >
      <div class="img-hover-zoom">
        <img src="/grant/phd-icn/featured_hua33861e06569025e088b1f99485ff4dd_8934_dbd84270995c073af82d513fb7b7d2d0.webp" height="455" width="808"
            class="article-banner" alt="PhD ICN (2017 / 2021)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/phd-icn/" >PhD ICN (2017 / 2021)</a>
  </div>

  
  <a href="/grant/phd-icn/"  class="summary-link">
    <div class="article-style">
      <p>A grant from the Ph.D. program in Integrative and Clinical Neuroscience (PhD position, 2017 / 2021).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/yves-fregnac/">Yves Fregnac</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2015
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/brain-scales/" >
      <div class="img-hover-zoom">
        <img src="/grant/brain-scales/featured_hu8422f08c18548ec680da4510754d33b3_8013_fd7e0322be177d46556f1a3ead5dcfed.webp" height="455" width="808"
            class="article-banner" alt="BrainScaleS (2011/2014) " loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/brain-scales/" >BrainScaleS (2011/2014) </a>
  </div>

  
  <a href="/grant/brain-scales/"  class="summary-link">
    <div class="article-style">
      <p>BrainScaleS: Brain-inspired multiscale computation in neuromorphic hybrid systems (2011/2014).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2015
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/codde/" >
      <div class="img-hover-zoom">
        <img src="/grant/codde/featured_hu402026df924b38a8dd19337be110c1ef_56531_82743b5498f0856ba509f2ef88853563.webp" height="455" width="808"
            class="article-banner" alt="CODDE (2008/2012)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/codde/" >CODDE (2008/2012)</a>
  </div>

  
  <a href="/grant/codde/"  class="summary-link">
    <div class="article-style">
      <p>CODDE: understanding brain and behaviour (2008/2012).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/yves-fregnac/">Yves Fregnac</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2015
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/facets/" >
      <div class="img-hover-zoom">
        <img src="/grant/facets/featured_hu784b691e67e8019b0acf0b4d2c0545e6_8638_586efffb79541720f753c48ca3bfd944.webp" height="455" width="808"
            class="article-banner" alt="FACETS (2006/2010)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/facets/" >FACETS (2006/2010)</a>
  </div>

  
  <a href="/grant/facets/"  class="summary-link">
    <div class="article-style">
      <p>FACETS: Fast Analog Computing with Emergent Transient States (2006/2010).</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/yves-fregnac/">Yves Fregnac</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2015
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/facets-itn/" >
      <div class="img-hover-zoom">
        <img src="/grant/facets-itn/featured_hub3db26eef63b285175a6b4d4a2c8890f_668722_a6fca19645e0df5a21ffd743a1556308.webp" height="455" width="808"
            class="article-banner" alt="FACETS-ITN (2010/2013)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/facets-itn/" >FACETS-ITN (2010/2013)</a>
  </div>

  
  <a href="/grant/facets-itn/"  class="summary-link">
    <div class="article-style">
      <p>FACETS-ITN: From Neuroscience to neuro-inspired computing (2010/2013)</p>
    </div>
  </a>
  

  

</div>

      </div>
    
      
        
          <div class="project-card project-item isotope-item js-id-grant js-id-past-grant">
        
        











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/author/anna-montagnini/">Anna Montagnini</a></span>, <span >
      <a href="/author/laurent-u-perrinet/">Laurent U Perrinet</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Apr 27, 2015
  </span>
  

  

  

  
  
  
  

  
  

</div>

  

  
  
  
    
    
    <a href="/grant/pace-itn/" >
      <div class="img-hover-zoom">
        <img src="/grant/pace-itn/featured_hu730eb526759b8301a1d62b3c0eebefe0_7996_4eced796e55208275c43f30a6b0d251f.webp" height="455" width="808"
            class="article-banner" alt="PACE-ITN (2015/2019)" loading="lazy">
      </div>
    </a>
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/grant/pace-itn/" >PACE-ITN (2015/2019)</a>
  </div>

  
  <a href="/grant/pace-itn/"  class="summary-link">
    <div class="article-style">
      <p>PACE-ITN: ITN Marie Curie network (2015/2019).</p>
    </div>
  </a>
  

  

</div>

      </div>
    

  </div>
</div>


  
    </div>
  

  </div>
</section>


  






























































<section id="tags" class="home-section wg-tag-cloud  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Tag Cloud</h1>
          
        </div>
      
    
  

    












  








<div class="col-12 col-lg-8">
  

  

    
    
    
    
    
    

    <div class="tag-cloud">
      
        
        
        
        <a href="/tag/active-inference/" style="font-size:0.7rem">active inference</a>
      
        
        
        
        <a href="/tag/area-v1/" style="font-size:1.031662115695553rem">area-v1</a>
      
        
        
        
        <a href="/tag/bayesian-model/" style="font-size:1.937193974434608rem">Bayesian model</a>
      
        
        
        
        <a href="/tag/biologically-inspired-computer-vision/" style="font-size:1.398894891968296rem">Biologically Inspired Computer Vision</a>
      
        
        
        
        <a href="/tag/center-surround-interactions/" style="font-size:0.8467842606781668rem">center-surround interactions</a>
      
        
        
        
        <a href="/tag/coding-decoding/" style="font-size:0.973934570788253rem">coding decoding</a>
      
        
        
        
        <a href="/tag/deep-learning/" style="font-size:1.398894891968296rem">deep-learning</a>
      
        
        
        
        <a href="/tag/efficient-coding/" style="font-size:0.9124801605900215rem">efficient coding</a>
      
        
        
        
        <a href="/tag/event-based-vision/" style="font-size:0.7rem">event-based vision</a>
      
        
        
        
        <a href="/tag/events/" style="font-size:1.031662115695553rem">events</a>
      
        
        
        
        <a href="/tag/eye-movements/" style="font-size:1.8928970170072623rem">eye movements</a>
      
        
        
        
        <a href="/tag/grant/" style="font-size:1.3600236635586747rem">grant</a>
      
        
        
        
        <a href="/tag/lateral-connections/" style="font-size:0.8467842606781668rem">lateral connections</a>
      
        
        
        
        <a href="/tag/motion-detection/" style="font-size:1.9795215146836231rem">motion detection</a>
      
        
        
        
        <a href="/tag/motion-prediction/" style="font-size:1.031662115695553rem">motion prediction</a>
      
        
        
        
        <a href="/tag/motion-clouds/" style="font-size:1.506807924236841rem">motion-clouds</a>
      
        
        
        
        <a href="/tag/neuromorphic-hardware/" style="font-size:0.7762176960990677rem">neuromorphic hardware</a>
      
        
        
        
        <a href="/tag/past-grant/" style="font-size:1.031662115695553rem">past-grant</a>
      
        
        
        
        <a href="/tag/psychophysics/" style="font-size:1.1864147313782745rem">psychophysics</a>
      
        
        
        
        <a href="/tag/sparse-coding/" style="font-size:1.8464383949369492rem">sparse coding</a>
      
    </div>
  

</div>


  
    </div>
  

  </div>
</section>


  






























































<section id="contact" class="home-section wg-contact  "  >
 <div class="home-section-bg " >
   
 </div>
  <div class="container">

  
    <div class="row  ">
    
      
        <div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start">
          <h1 class="mb-0">Contact</h1>
          <p class="mt-1">How to reach me</p>
        </div>
      
    
  

    























<div class="col-12 col-lg-8">
  

  

    
    
      
      
      
    

    <div class="mb-3">
      <form name="contact" method="POST" action="https://formspree.io/f/test"   >
        <div class="form-group form-inline">
          <label class="sr-only" for="inputName">Name</label>
          <input type="text" name="name" class="form-control w-100" id="inputName" placeholder="Name" required>
        </div>
        <div class="form-group form-inline">
          <label class="sr-only" for="inputEmail">Email</label>
          <input type="email" name="email" class="form-control w-100" id="inputEmail" placeholder="Email" required>
        </div>
        <div class="form-group">
          <label class="sr-only" for="inputMessage">Message</label>
          <textarea name="message" class="form-control" id="inputMessage" rows="5" placeholder="Message" required></textarea>
        </div>
        
        <div class="d-none">
          <label>Do not fill this field unless you are a bot: <input name="_gotcha"></label>
        </div>
        
        <button type="submit" class="btn btn-primary px-3 py-2 w-100">Send</button>
      </form>
    </div>
  

  <ul class="fa-ul">

    
    <li>
      <i class="fa-li fas fa-envelope fa-2x" aria-hidden="true"></i>
      <span id="person-email"><a href="mailto:laurent.perrinet@univ-amu.fr">laurent.perrinet@univ-amu.fr</a></span>
    </li>
    

    
    <li>
      <i class="fa-li fas fa-phone fa-2x" aria-hidden="true"></i>
      <span id="person-telephone"><a href="tel:&#43;33%20619%20478%20120">&#43;33 619 478 120</a></span>
    </li>
    

    
    
      
      <li>
        <i class="fa-li fas fa-map-marker fa-2x" aria-hidden="true"></i>
        <span id="person-address">NeOpTo Team &lt;BR&gt;  [Institut de Neurosciences de la Timone](https://ror.org/043hw6336) ([UMR 7289](https://www.wikidata.org/wiki/Q30261469)) &lt;br&gt; Aix Marseille Université, CNRS &lt;br&gt; Faculté de Médecine - Bâtiment Neurosciences,&lt;br&gt; 27, Bd Jean Moulin, Marseille, PACA 13385 Marseille Cedex 05</span>
      </li>
    

    
    <li>
      <i class="fa-li fas fa-compass fa-2x" aria-hidden="true"></i>
      <span>Enter INT Building 1 and take the stairs to Floor 2</span>
    </li>
    

    

    

    
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-orcid fa-2x" aria-hidden="true"></i>
      <a href="http://orcid.org/0000-0002-9536-010X" target="_blank" rel="noopener">OrcID</a>
    </li>
    
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <i class="fa-li fab fa-linkedin fa-2x" aria-hidden="true"></i>
      <a href="https://www.linkedin.com/in/laurent-perrinet-1857b9/" target="_blank" rel="noopener">LinkedIn</a>
    </li>
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-researcherid fa-2x" aria-hidden="true"></i>
      <a href="https://www.researcherid.com/rid/C-4900-2009" target="_blank" rel="noopener">ResearcherID</a>
    </li>
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-pubpeer fa-2x" aria-hidden="true"></i>
      <a href="https://neurotree.org/neurotree/peopleinfo.php?pid=18540" target="_blank" rel="noopener">NeuroTree</a>
    </li>
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-google-scholar fa-2x" aria-hidden="true"></i>
      <a href="https://scholar.google.com/citations?user=TVyUV38AAAAJ" target="_blank" rel="noopener">Google Scholar</a>
    </li>
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-zotero fa-2x" aria-hidden="true"></i>
      <a href="https://www.zotero.org/groups/2485979/laurent_perrinet/library" target="_blank" rel="noopener">Zotero</a>
    </li>
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-publons fa-2x" aria-hidden="true"></i>
      <a href="https://publons.com/a/1206845/" target="_blank" rel="noopener">Publons</a>
    </li>
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-arxiv fa-2x" aria-hidden="true"></i>
      <a href="https://arxiv.org/search/advanced?advanced=&amp;terms-0-operator=AND&amp;terms-0-term=Perrinet%2C&#43;L&amp;terms-0-field=author&amp;classification-physics_archives=all&amp;classification-include_cross_list=include&amp;date-filter_by=all_dates&amp;date-year=&amp;date-from_date=&amp;date-to_date=&amp;date-date_type=submitted_date&amp;abstracts=show&amp;size=50&amp;order=-announced_date_first" target="_blank" rel="noopener">arXiv</a>
    </li>
    
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <i class="fa-li fab fa-github fa-2x" aria-hidden="true"></i>
      <a href="https://github.com/laurentperrinet" target="_blank" rel="noopener">GitHub</a>
    </li>
    
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <i class="fa-li fab fa-twitter fa-2x" aria-hidden="true"></i>
      <a href="https://twitter.com/laurentperrinet" target="_blank" rel="noopener">DM Me</a>
    </li>
    
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <i class="fa-li fab fa-instagram fa-2x" aria-hidden="true"></i>
      <a href="https://www.instagram.com/laurentperrinet/" target="_blank" rel="noopener">instagram</a>
    </li>
    
    
    
    
    
    
    
    
      
    
    <li>
      <i class="fa-li ai ai-stackoverflow fa-2x" aria-hidden="true"></i>
      <a href="https://stackoverflow.com/users/234547/meduz" target="_blank" rel="noopener">stackoverflow</a>
    </li>
    
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <i class="fa-li fab fa-mastodon fa-2x" aria-hidden="true"></i>
      <a href="https://neuromatch.social/@laurentperrinet" target="_blank" rel="noopener">mastodon</a>
    </li>
    

  </ul>

  
  <div class="d-none">
    <input id="map-provider" value="mapnik">
    <input id="map-lat" value="43.2869">
    <input id="map-lng" value="5.4035">
    <input id="map-dir" value="NeOpTo Team &lt;BR&gt;  [Institut de Neurosciences de la Timone](https://ror.org/043hw6336) ([UMR 7289](https://www.wikidata.org/wiki/Q30261469)) &lt;br&gt; Aix Marseille Université, CNRS &lt;br&gt; Faculté de Médecine - Bâtiment Neurosciences,&lt;br&gt; 27, Bd Jean Moulin, Marseille, PACA 13385 Marseille Cedex 05">
    <input id="map-zoom" value="15">
    <input id="map-api-key" value="">
  </div>
  <div id="map"></div>
  

</div>


  
    </div>
  

  </div>
</section>




  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2024 <a rel="me" href="https://neuromatch.social/@laurentperrinet">Laurent U Perrinet</a>. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  
    <script src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>








  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":false}</script>











  
  


<script src="/en/js/wowchemy.min.62586ca65ca61821fe707eb9fa6268b7.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js" type="module"></script>




  
  <script async defer src="https://buttons.github.io/buttons.js"></script>

















</body>
</html>
